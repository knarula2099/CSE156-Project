Title,Published,Authors,Abstract,Link
Financial climate risk: a review of recent advances and key challenges,2024-04-10T20:17:43Z,Victor Cardenas,"The document provides an overview of financial climate risks. It delves into
how climate change impacts the global financial system, distinguishing between
physical risks (such as extreme weather events) and transition risks (stemming
from policy changes and economic transitions towards low carbon technologies).
The paper underlines the complexity of accurately defining financial climate
risk, citing the integration of climate science with financial risk analysis as
a significant challenge. The paper highlights the pivotal role of microfinance
institutions (MFIs) in addressing financial climate risk, especially for
populations vulnerable to climate change. The document emphasizes the
importance of updating risk management practices within MFIs to explicitly
include climate risk assessments and suggests leveraging technology to improve
these practices.",http://arxiv.org/abs/2404.07331v1
"Indexing and Visualization of Climate Change Narratives Using BERT and
  Causal Extraction",2024-08-03T11:05:41Z,"Hiroki Sakaji, Noriyasu Kaneda","In this study, we propose a methodology to extract, index, and visualize
``climate change narratives'' (stories about the connection between causal and
consequential events related to climate change). We use two natural language
processing methods, BERT (Bidirectional Encoder Representations from
Transformers) and causal extraction, to textually analyze newspaper articles on
climate change to extract ``climate change narratives.'' The novelty of the
methodology could extract and quantify the causal relationships assumed by the
newspaper's writers. Looking at the extracted climate change narratives over
time, we find that since 2018, an increasing number of narratives suggest the
impact of the development of climate change policy discussion and the
implementation of climate change-related policies on corporate behaviors,
macroeconomics, and price dynamics. We also observed the recent emergence of
narratives focusing on the linkages between climate change-related policies and
monetary policy. Furthermore, there is a growing awareness of the negative
impacts of natural disasters (e.g., abnormal weather and severe floods) related
to climate change on economic activities, and this issue might be perceived as
a new challenge for companies and governments. The methodology of this study is
expected to be applied to a wide range of fields, as it can analyze causal
relationships among various economic topics, including analysis of inflation
expectation or monetary policy communication strategy.",http://arxiv.org/abs/2408.01745v1
"The Cost of Climate Action: Experimental Evidence on the Impact of
  Climate Information on Charitable Donations to Climate Activism",2024-09-25T21:45:13Z,"Samantha Gonsalves Wetherell, Anna Josephson","We examine the propensity of individuals to donate to climate activism,
evaluating the impact of different informational treatments on an incentive
compatible charitable donation and stated climate change-related concerns.
Participants were evaluated on climate literacy and general climate attitudes
before being randomly assigned to a treatment which provided either education
or neutral language about climate change, either with or without images of
protest. After the treatment, participants engaged in an incentive compatible
dictator game. We find that participants gave more to climate activism than
seen in previous dictator game and charitable giving experiments, in both
average amount given and proportion of participants who gave their entire
endowment. However, we determine that climate activism information negatively
influenced the amount of money donated. We also found that protest imagery
moderated this negative effect and had a positive significant effect of
increasing participants' climate concern. Finally, we found that the climate
concern was significantly positively correlated with donations, while being a
male was significantly negatively associated with donation amounts.",http://arxiv.org/abs/2409.17378v1
"Climate Impact Assessment Requires Weighting: Introducing the Weighted
  Climate Dataset",2024-12-20T09:18:22Z,"Marco Gortan, Lorenzo Testa, Giorgio Fagiolo, Francesco Lamperti","High-resolution gridded climate data are readily available from multiple
sources, yet climate research and decision-making increasingly require country
and region-specific climate information weighted by socio-economic factors.
Moreover, the current landscape of disparate data sources and inconsistent
weighting methodologies exacerbates the reproducibility crisis and undermines
scientific integrity. To address these issues, we have developed a globally
comprehensive dataset at both country (GADM0) and region (GADM1) levels,
encompassing various climate indicators (precipitation, temperature, SPEI, wind
gust). Our methodology involves weighting gridded climate data by population
density, night-time light intensity, cropland area, and concurrent population
count -- all proxies for socio-economic activity -- before aggregation. We
process data from multiple sources, offering daily, monthly, and annual climate
variables spanning from 1900 to 2023. A unified framework streamlines our
preprocessing steps, and rigorous validation against leading climate impact
studies ensures data reliability. The resulting Weighted Climate Dataset is
publicly accessible through an online dashboard at
https://weightedclimatedata.streamlit.app/.",http://arxiv.org/abs/2412.15699v1
"The Reality of Climate Change: Evidence, Impacts and Engineering
  Solutions",2024-10-16T09:55:31Z,Sihua Lu,"Climate change is one of the most significant global challenges, yet
misconceptions persist regarding its causes and impact. This report addresses
common myths surrounding climate change and presents scientific evidence to
clarify its reality. Utilising data from NASA, NOAA, and the NSW government,
this study provides evidence of rising global temperatures, melting ice sheets,
rising sea levels, and extreme weather patterns in regions like New South
Wales. The analysis demonstrates the human-driven nature of climate change,
primarily caused by increased carbon emissions. Engineering solutions,
including renewable energy technologies, green buildings, and carbon capture
methods, are essential to mitigating the effects of climate change. Future
research should focus on improving the scalability of these technologies and
addressing the broader impact on ecosystems and human societies.",http://arxiv.org/abs/2410.12412v1
"Augmented CARDS: A machine learning approach to identifying triggers of
  climate change misinformation on Twitter",2024-04-24T06:03:07Z,"Cristian Rojas, Frank Algra-Maschio, Mark Andrejevic, Travis Coan, John Cook, Yuan-Fang Li","Misinformation about climate change poses a significant threat to societal
well-being, prompting the urgent need for effective mitigation strategies.
However, the rapid proliferation of online misinformation on social media
platforms outpaces the ability of fact-checkers to debunk false claims.
Automated detection of climate change misinformation offers a promising
solution. In this study, we address this gap by developing a two-step
hierarchical model, the Augmented CARDS model, specifically designed for
detecting contrarian climate claims on Twitter. Furthermore, we apply the
Augmented CARDS model to five million climate-themed tweets over a six-month
period in 2022. We find that over half of contrarian climate claims on Twitter
involve attacks on climate actors or conspiracy theories. Spikes in climate
contrarianism coincide with one of four stimuli: political events, natural
events, contrarian influencers, or convinced influencers. Implications for
automated responses to climate misinformation are discussed.",http://arxiv.org/abs/2404.15673v1
"From Correlation to Causation: Understanding Climate Change through
  Causal Analysis and LLM Interpretations",2024-12-21T16:33:07Z,Shan Shan,"This research presents a three-step causal inference framework that
integrates correlation analysis, machine learning-based causality discovery,
and LLM-driven interpretations to identify socioeconomic factors influencing
carbon emissions and contributing to climate change. The approach begins with
identifying correlations, progresses to causal analysis, and enhances decision
making through LLM-generated inquiries about the context of climate change. The
proposed framework offers adaptable solutions that support data-driven
policy-making and strategic decision-making in climate-related contexts,
uncovering causal relationships within the climate change domain.",http://arxiv.org/abs/2412.16691v1
"RAIN: Reinforcement Algorithms for Improving Numerical Weather and
  Climate Models",2024-08-28T20:10:46Z,"Pritthijit Nath, Henry Moss, Emily Shuckburgh, Mark Webb","This study explores integrating reinforcement learning (RL) with idealised
climate models to address key parameterisation challenges in climate science.
Current climate models rely on complex mathematical parameterisations to
represent sub-grid scale processes, which can introduce substantial
uncertainties. RL offers capabilities to enhance these parameterisation
schemes, including direct interaction, handling sparse or delayed feedback,
continuous online learning, and long-term optimisation. We evaluate the
performance of eight RL algorithms on two idealised environments: one for
temperature bias correction, another for radiative-convective equilibrium (RCE)
imitating real-world computational constraints. Results show different RL
approaches excel in different climate scenarios with exploration algorithms
performing better in bias correction, while exploitation algorithms proving
more effective for RCE. These findings support the potential of RL-based
parameterisation schemes to be integrated into global climate models, improving
accuracy and efficiency in capturing complex climate dynamics. Overall, this
work represents an important first step towards leveraging RL to enhance
climate model accuracy, critical for improving climate understanding and
predictions. Code accessible at https://github.com/p3jitnath/climate-rl.",http://arxiv.org/abs/2408.16118v2
"Towards Specialized Supercomputers for Climate Sciences: Computational
  Requirements of the Icosahedral Nonhydrostatic Weather and Climate Model",2024-05-18T08:03:31Z,"Torsten Hoefler, Alexandru Calotoiu, Anurag Dipankar, Thomas Schulthess, Xavier Lapillonne, Oliver Fuhrer","We discuss the computational challenges and requirements for high-resolution
climate simulations using the Icosahedral Nonhydrostatic Weather and Climate
Model (ICON). We define a detailed requirements model for ICON which emphasizes
the need for specialized supercomputers to accurately predict climate change
impacts and extreme weather events. Based on the requirements model, we outline
computational demands for km-scale simulations, and suggests machine learning
techniques to enhance model accuracy and efficiency. Our findings aim to guide
the design of future supercomputers for advanced climate science.",http://arxiv.org/abs/2405.13043v1
"Managing Financial Climate Risk in Banking Services: A Review of Current
  Practices and the Challenges Ahead",2024-05-27T22:19:40Z,Victor Cardenas,"The document discusses the financial climate risk in the context of the
banking industry, emphasizing the need for a comprehensive understanding of
climate change across different spatial and temporal scales. It highlights the
challenges in estimating physical and transition risks, specifically extreme
events and limitations of current climate models. The document also reviews
current gaps in assessing physical and transition risks, including the
development, improvement of modeling frameworks, highlighting the need for
detailed databases of exposed physical assets and climatic hazard modeling. It
also emphasizes the importance of integrating financial climate risks into
financial risk management practices, particularly in smaller banks and lending
organizations.",http://arxiv.org/abs/2405.17682v1
A Run on Fossil Fuel? Climate Change and Transition Risk,2024-10-01T17:45:48Z,Michael Barnett,"I study the dynamic, general equilibrium implications of
climate-change-linked transition risk on macroeconomic outcomes and asset
prices. Climate-change-linked expectations of fossil fuel restrictions can
produce a ``run on fossil fuels'' with accelerated production and decreasing
spot prices, or a ``reverse run'' with restrained production and increased spot
prices. The response depends on the expected economic consequences of the
anticipated transition shock, and existing climate policies. Fossil fuel firm
prices decrease in each case. I use a novel empirical measure of innovations in
climate-related transition risk likelihood to show that dynamic empirical
responses are consistent with a ``run on fossil fuel.''",http://arxiv.org/abs/2410.00902v1
"A Survey on Exploratory Spatiotemporal Visual Analytics Approaches for
  Climate Science",2024-07-30T21:23:14Z,"Abdullah-Al-Raihan Nayeem, Dongyun Han, Huikyo Lee, Donghoon Kim, Daniel Feldman, William J. Tolone, Daniel Crichton, Isaac Cho","Climate science produces a wealth of complex, high-dimensional, multivariate
data from observations and numerical models. These data are critical for
understanding climate changes and their socioeconomic impacts. Climate
scientists are continuously evaluating output from numerical models against
observations. This model evaluation process provides useful guidance to improve
the numerical models and subsequent climate projections. Exploratory visual
analytics systems possess the potential to significantly reduce the burden on
scientists for traditional spatiotemporal analyses. In addition, technology and
infrastructure advancements are further facilitating broader access to climate
data. Climate scientists today can access climate data in distributed analytic
environments and render exploratory visualizations for analyses. Efforts are
ongoing to optimize the computational efficiency of spatiotemporal analyses to
enable efficient exploration of massive data. These advances present further
opportunities for the visualization community to innovate over the full
landscape of challenges and requirements raised by scientists. In this report,
we provide a comprehensive review of the challenges, requirements, and current
approaches for exploratory spatiotemporal visual analytics solutions for
climate data. We categorize the visual analytic techniques, systems, and tools
presented in the relevant literature based on task requirements, data sources,
statistical techniques, interaction methods, visualization techniques,
performance evaluation methods, and application domains. Moreover, our analytic
review identifies trends, limitations, and key challenges in visual analysis.
This report will advance future research activities in climate visualizations
and enables the end-users of climate data to identify effective climate change
mitigation strategies.",http://arxiv.org/abs/2407.21199v1
"Dhoroni: Exploring Bengali Climate Change and Environmental Views with a
  Multi-Perspective News Dataset and Natural Language Processing",2024-10-22T17:47:05Z,"Azmine Toushik Wasi, Wahid Faisal, Taj Ahmad, Abdur Rahman, Mst Rafia Islam","Climate change poses critical challenges globally, disproportionately
affecting low-income countries that often lack resources and linguistic
representation on the international stage. Despite Bangladesh's status as one
of the most vulnerable nations to climate impacts, research gaps persist in
Bengali-language studies related to climate change and NLP. To address this
disparity, we introduce Dhoroni, a novel Bengali (Bangla) climate change and
environmental news dataset, comprising a 2300 annotated Bangla news articles,
offering multiple perspectives such as political influence,
scientific/statistical data, authenticity, stance detection, and stakeholder
involvement. Furthermore, we present an in-depth exploratory analysis of
Dhoroni and introduce BanglaBERT-Dhoroni family, a novel baseline model family
for climate and environmental opinion detection in Bangla, fine-tuned on our
dataset. This research contributes significantly to enhancing accessibility and
analysis of climate discourse in Bengali (Bangla), addressing crucial
communication and research gaps in climate-impacted regions like Bangladesh
with 180 million people.",http://arxiv.org/abs/2410.17225v2
"Incorporating climate change effects into the European power system
  adequacy assessment using a post-processing method",2024-02-26T21:44:36Z,"In√®s Harang, Fabian Heymann, Laurens P. Stoop","The demand-supply balance of electricity systems is fundamentally linked to
climate conditions. In light of this, the present study aims to model the
effect of climate change on the European electricity system, specifically on
its long-term reliability. A resource adequate power system -- a system where
electricity supply covers demand -- is sensitive to generation capacity, demand
patterns, and the network structure and capacity. Climate change is foreseen to
affect each of these components.
  In this analysis, we focused on two drivers of power system adequacy: the
impact of temperature variations on electricity demand, and of water inflows
changes on hydro generation. Using a post-processing approach, based on results
found in the literature, the inputs of a large-scale electricity market model
covering the European region were modified. The results show that climate
change may decrease total LOLE (Loss of Load Expectation) hours in Europe by
more than 50%, as demand will largely decrease because of a higher temperatures
during winter. We found that the climate change impact on demand tends to
decrease LOLE values, while the climate change effects on hydrological
conditions tend to increase LOLE values.
  The study is built on a limited amount of open-source data and can flexibly
incorporate various sets of assumptions. Outcomes also show the current
difficulties to reliably model the effects of climate change on power system
adequacy. Overall, our presented method displays the relevance of climate
change effects in electricity network studies.",http://arxiv.org/abs/2402.17039v2
"Large role of anthropogenic climate change in driving smoke exposure
  across the western United States from 1992 to 2020",2024-12-04T21:55:10Z,"Xu Feng, Loretta J. Mickley, Jed O. Kaplan, Makoto Kelp, Yang Li, Tianjia Liu","Wildfire activity has increased dramatically in the western United States
(US) over the last three decades, having a significant impact on air quality
and human health. However, quantifying the drivers of trends in wildfires and
subsequent smoke exposure is challenging, as both natural variability and
anthropogenic climate change play important roles. Here we devise an approach
involving observed meteorology and vegetation and a range of models to
determine the relative roles of anthropogenic climate change and natural
variability in driving burned area across the western US. We also examine the
influence of anthropogenic climate change on smoke exposure. We estimate that
anthropogenic climate change accounts for 33-82% of observed total burned area,
depending on the ecoregion, yielding 65% of total fire emissions on average
across the western US from 1992 to 2020. In all ecoregions except Mediterranean
California, anthropogenic climate change contributes to a greater percentage of
burned area in lightning-caused wildfires than in human-caused wildfires. On
average, anthropogenic climate change contributes 49% to smoke PM2.5
concentrations in the western US from 1997 to 2020, and explains 58% of the
increasing trend in smoke PM2.5 from 2010 to 2020. We further find that
populations in northern California, western Oregon, Washington, and parts of
Idaho have experienced the greatest smoke exposure attributable to
anthropogenic climate change in recent years. Our work highlights the
significant role of anthropogenic climate change in degrading air quality in
the western US and identifies those regions most vulnerable to wildfire smoke
and thus adverse health impacts.",http://arxiv.org/abs/2412.03733v1
"ClimDetect: A Benchmark Dataset for Climate Change Detection and
  Attribution",2024-08-28T17:58:53Z,"Sungduk Yu, Brian L. White, Anahita Bhiwandiwalla, Musashi Hinck, Matthew Lyle Olson, Tung Nguyen, Vasudev Lal","Detecting and attributing temperature increases due to climate change is
crucial for understanding global warming and guiding adaptation strategies. The
complexity of distinguishing human-induced climate signals from natural
variability has challenged traditional detection and attribution (D&A)
approaches, which seek to identify specific ""fingerprints"" in climate response
variables. Deep learning offers potential for discerning these complex patterns
in expansive spatial datasets. However, lack of standard protocols has hindered
consistent comparisons across studies. We introduce ClimDetect, a standardized
dataset of over 816k daily climate snapshots, designed to enhance model
accuracy in identifying climate change signals. ClimDetect integrates various
input and target variables used in past research, ensuring comparability and
consistency. We also explore the application of vision transformers (ViT) to
climate data, a novel and modernizing approach in this context. Our open-access
data and code serve as a benchmark for advancing climate science through
improved model evaluations. ClimDetect is publicly accessible via Huggingface
dataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.",http://arxiv.org/abs/2408.15993v1
PACER: Physics Informed Uncertainty Aware Climate Emulator,2024-10-29T01:53:40Z,"Hira Saleem, Flora Salim, Cormac Purcell","Climate models serve as critical tools for evaluating the effects of climate
change and projecting future climate scenarios. However, the reliance on
numerical simulations of physical equations renders them computationally
intensive and inefficient. While deep learning methodologies have made
significant progress in weather forecasting, they are still unstable for
climate emulation tasks. Here, we propose PACER, a lightweight 684K parameter
Physics Informed Uncertainty Aware Climate Emulator. PACER emulates temperature
and precipitation stably for 86 years while only being trained on greenhouse
gas emissions data. We incorporate a fundamental physical law of
advection-diffusion in PACER accounting for boundary conditions and empirically
estimating the diffusion co-efficient and flow velocities from emissions data.
PACER has been trained on 15 climate models provided by ClimateSet
outperforming baselines across most of the climate models and advancing a new
state of the art in a climate diagnostic task.",http://arxiv.org/abs/2410.21657v2
Exploring Large Language Models for Climate Forecasting,2024-11-20T21:58:19Z,"Yang Wang, Hassan A. Karimi","With the increasing impacts of climate change, there is a growing demand for
accessible tools that can provide reliable future climate information to
support planning, finance, and other decision-making applications. Large
language models (LLMs), such as GPT-4, present a promising approach to bridging
the gap between complex climate data and the general public, offering a way for
non-specialist users to obtain essential climate insights through natural
language interaction. However, an essential challenge remains under-explored:
evaluating the ability of LLMs to provide accurate and reliable future climate
predictions, which is crucial for applications that rely on anticipating
climate trends. In this study, we investigate the capability of GPT-4 in
predicting rainfall at short-term (15-day) and long-term (12-month) scales. We
designed a series of experiments to assess GPT's performance under different
conditions, including scenarios with and without expert data inputs. Our
results indicate that GPT, when operating independently, tends to generate
conservative forecasts, often reverting to historical averages in the absence
of clear trend signals. This study highlights both the potential and challenges
of applying LLMs for future climate predictions, providing insights into their
integration with climate-related applications and suggesting directions for
enhancing their predictive capabilities in the field.",http://arxiv.org/abs/2411.13724v1
"Stress-testing the coupled behavior of hybrid physics-machine learning
  climate simulations on an unseen, warmer climate",2024-01-04T07:08:35Z,"Jerry Lin, Mohamed Aziz Bhouri, Tom Beucler, Sungduk Yu, Michael Pritchard","Accurate and computationally-viable representations of clouds and turbulence
are a long-standing challenge for climate model development. Traditional
parameterizations that crudely but efficiently approximate these processes are
a leading source of uncertainty in long-term projected warming and
precipitation patterns. Machine Learning (ML)-based parameterizations have long
been hailed as a promising alternative with the potential to yield higher
accuracy at a fraction of the cost of more explicit simulations. However, these
ML variants are often unpredictably unstable and inaccurate in \textit{coupled}
testing (i.e. in a downstream hybrid simulation task where they are dynamically
interacting with the large-scale climate model). These issues are exacerbated
in out-of-distribution climates. Certain design decisions such as
``climate-invariant"" feature transformation for moisture inputs, input vector
expansion, and temporal history incorporation have been shown to improve
coupled performance, but they may be insufficient for coupled
out-of-distribution generalization. If feature selection and transformations
can inoculate hybrid physics-ML climate models from non-physical,
out-of-distribution extrapolation in a changing climate, there is far greater
potential in extrapolating from observational data. Otherwise, training on
multiple simulated climates becomes an inevitable necessity. While our results
show generalization benefits from these design decisions, the obtained
improvment does not sufficiently preclude the necessity of using multi-climate
simulated training data.",http://arxiv.org/abs/2401.02098v1
Regional impacts poorly constrained by climate sensitivity,2024-04-18T06:38:15Z,"Ranjini Swaminathan, Jacob Schewe, Jeremy Walton, Klaus Zimmermann, Colin Jones, Richard A. Betts, Chantelle Burton, Chris D. Jones, Matthias Mengel, Christopher P. O. Reyer, Andrew G. Turner, Katja Weigel","Climate risk assessments must account for a wide range of possible futures,
so scientists often use simulations made by numerous global climate models to
explore potential changes in regional climates and their impacts. Some of the
latest-generation models have high effective climate sensitivities or EffCS. It
has been argued these so-called hot models are unrealistic and should therefore
be excluded from analyses of climate change impacts. Whether this would improve
regional impact assessments, or make them worse, is unclear. Here we show there
is no universal relationship between EffCS and projected changes in a number of
important climatic drivers of regional impacts. Analysing heavy rainfall
events, meteorological drought, and fire weather in different regions, we find
little or no significant correlation with EffCS for most regions and climatic
drivers. Even when a correlation is found, internal variability and processes
unrelated to EffCS have similar effects on projected changes in the climatic
drivers as EffCS. Model selection based solely on EffCS appears to be
unjustified and may neglect realistic impacts, leading to an underestimation of
climate risks.",http://arxiv.org/abs/2404.11939v1
"Machine learning emulation of precipitation from km-scale regional
  climate simulations using a diffusion model",2024-07-19T09:42:20Z,"Henry Addison, Elizabeth Kendon, Suman Ravuri, Laurence Aitchison, Peter AG Watson","High-resolution climate simulations are very valuable for understanding
climate change impacts and planning adaptation measures. This has motivated use
of regional climate models at sufficiently fine resolution to capture important
small-scale atmospheric processes, such as convective storms. However, these
regional models have very high computational costs, limiting their
applicability. We present CPMGEM, a novel application of a generative machine
learning model, a diffusion model, to skilfully emulate precipitation
simulations from such a high-resolution model over England and Wales at much
lower cost. This emulator enables stochastic generation of high-resolution
(8.8km), daily-mean precipitation samples conditioned on coarse-resolution
(60km) weather states from a global climate model. The output is fine enough
for use in applications such as flood inundation modelling. The emulator
produces precipitation predictions with realistic intensities and spatial
structures and captures most of the 21st century climate change signal. We show
evidence that the emulator has skill for extreme events up to and including
1-in-100 year intensities. Potential applications include producing
high-resolution precipitation predictions for large-ensemble climate
simulations and downscaling different climate models and climate change
scenarios to better sample uncertainty in climate changes at local-scale.",http://arxiv.org/abs/2407.14158v1
"The impact of climate and wealth on energy consumption in small tropical
  islands",2024-08-22T14:36:29Z,Julien Gargani,"Anthropic activities have a significant causal effect on climatic change but
climate has also major impact on human societies. Population vulnerability to
natural hazards and limited natural resources are deemed problematic,
particularly on small tropical islands. Lifestyles and activities are heavily
reliant on energy consumption. The relationship between climatic variations and
energy consumption must be clearly understood. We demonstrate that it is
possible to determine the impact of climate change on energy consumption. In
small tropical islands, the relationship between climate and energy consumption
is primarily driven by air conditioner electricity consumption during hotter
months. Temperatures above 26{\deg}C correlate with increased electricity
consumption. Energy consumption is sensitive to: (1) climatic seasonal
fluctuations, (2) cyclonic activity, (3) temperature warming over the last 20
years. On small tropical islands, demographic and wealth variations also have a
significant impact on energy consumption. The relationship between climate and
energy consumption suggests reconsidering the production and consumption of
carbon-based energy.",http://arxiv.org/abs/2408.12438v1
"Quantization of Climate Change Impacts on Renewable Energy Generation
  Capacity: A Super-Resolution Recurrent Diffusion Model",2024-12-16T02:54:21Z,"Xiaochong Dong, Jun Dan, Yingyun Sun, Yang Liu, Xuemin Zhang, Shengwei Mei","Driven by global climate change and the ongoing energy transition, the
coupling between power supply capabilities and meteorological factors has
become increasingly significant. Over the long term, accurately quantifying the
power generation capacity of renewable energy under the influence of climate
change is essential for the development of sustainable power systems. However,
due to interdisciplinary differences in data requirements, climate data often
lacks the necessary hourly resolution to capture the short-term variability and
uncertainties of renewable energy resources. To address this limitation, a
super-resolution recurrent diffusion model (SRDM) has been developed to enhance
the temporal resolution of climate data and model the short-term uncertainty.
The SRDM incorporates a pre-trained decoder and a denoising network, that
generates long-term, high-resolution climate data through a recurrent coupling
mechanism. The high-resolution climate data is then converted into power value
using the mechanism model, enabling the simulation of wind and photovoltaic
(PV) power generation capacity on future long-term scales. Case studies were
conducted in the Ejina region of Inner Mongolia, China, using fifth-generation
reanalysis (ERA5) and coupled model intercomparison project (CMIP6) data under
two climate pathways: SSP126 and SSP585. The results demonstrate that the SRDM
outperforms existing generative models in generating super-resolution climate
data. For the Ejina region, under a high-emission pathway, the annual
utilization hours of wind power are projected to decrease by 2.82 hours/year,
while those for PV power are projected to decrease by 0.26 hours/year.
Furthermore, the research highlights the estimation biases introduced when
low-resolution climate data is used for power conversion.",http://arxiv.org/abs/2412.11399v1
Unlearning Climate Misinformation in Large Language Models,2024-05-29T23:11:53Z,"Michael Fore, Simranjit Singh, Chaehong Lee, Amritanshu Pandey, Antonios Anastasopoulos, Dimitrios Stamoulis","Misinformation regarding climate change is a key roadblock in addressing one
of the most serious threats to humanity. This paper investigates factual
accuracy in large language models (LLMs) regarding climate information. Using
true/false labeled Q&A data for fine-tuning and evaluating LLMs on
climate-related claims, we compare open-source models, assessing their ability
to generate truthful responses to climate change questions. We investigate the
detectability of models intentionally poisoned with false climate information,
finding that such poisoning may not affect the accuracy of a model's responses
in other domains. Furthermore, we compare the effectiveness of unlearning
algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually
grounding LLMs on climate change topics. Our evaluation reveals that unlearning
algorithms can be effective for nuanced conceptual claims, despite previous
findings suggesting their inefficacy in privacy contexts. These insights aim to
guide the development of more factually reliable LLMs and highlight the need
for additional work to secure LLMs against misinformation attacks.",http://arxiv.org/abs/2405.19563v1
"Behaviors of Martian CO2-driven dry climate system and conditions for
  atmospheric collapses",2024-11-11T11:12:02Z,"Yasuto Watanabe, Eiichi Tajika, Arihiro Kamada","The present Martian climate is characterized by a cold and dry environment
with a thin atmosphere of carbon dioxides (CO2). In such conditions, the
planetary climate and habitability are determined by the distribution of CO2
between exchangeable reservoirs, that is the atmosphere, ice caps, and
regolith. This produces unique responses of the Martian CO2-driven climate
system to variations of astronomical forcings. Specifically, it has been shown
that the phenomenon called an atmospheric collapse occurs when the axial
obliquity is low, affecting the Martian climatic evolution. However, the
behavior of the Martian climate system and the accompanying changes in climate
and habitability of such planets remain ambiguous. Here we employed a
latitudinally-resolved Martian energy balance model and assessed the possible
climate on Mars for wider ranges of orbital parameters, solar irradiance, and
total exchangeable CO2 mass. We show that the atmospheric collapse occurs when
the obliquity is below ~10 degrees when other parameters are kept at the
present Mars condition. We also show that the climate solutions on Mars depend
on orbital parameters, solar luminosity, and the total exchangeable CO2 mass.
We found that the atmospheric collapse would have occurred repeatedly in the
history of Mars following the variation of the axial obliquity, while the
long-term evolution of atmospheric pCO2 is also affected by the changes in the
total exchangeable CO2 mass in Martian history. Even considering the broad
ranges of these parameters, the habitable conditions in the Martian CO2-driven
dry climate system would be limited to high-latitude summers.",http://arxiv.org/abs/2411.06871v1
Spatial quantile clustering of climate data,2024-02-16T10:16:39Z,"Carlo Gaetan, Paolo Girardi, Victor Muthama Musau","In the era of climate change, the distribution of climate variables evolves
with changes not limited to the mean value. Consequently, clustering algorithms
based on central tendency could produce misleading results when used to
summarize spatial and/or temporal patterns. We present a novel approach to
spatial clustering of time series based on quantiles using a Bayesian framework
that incorporates a spatial dependence layer based on a Markov random field. A
series of simulations tested the proposal, then applied to the sea surface
temperature of the Mediterranean Sea, one of the first seas to be affected by
the effects of climate change.",http://arxiv.org/abs/2402.10545v1
"Save the Farms: Nonlinear Impact of Climate Change on Banks'
  Agricultural Lending",2024-09-28T21:53:01Z,Teng Liu,"The agricultural sector is particularly susceptible to the impact of climate
change. In this paper, I investigate how vulnerability to climate change
affects U.S. farms' credit access, and demonstrates that such impact is
unequally distributed across farms. I first construct a theoretical framework
of bank lending to farms faced with climate risks, and the model helps
discipline ensuing empirical analyses that use novel panel datasets at county
and at bank levels. I find that higher exposure to climate change, measured by
temperature anomaly, reduces bank lending to farms. Such impact is persistent,
nonlinear, and heterogeneous. Small and medium farms almost always experience
loss of loan access. In comparison, large farms see less severe credit
contraction, and in some cases may even see improvement in funding. While small
banks carry the burden of continuing to lend to small farms, their limited
market share cannot compensate for the reduction of lending from medium and
large banks. These results suggest that factors such as farm size and bank type
can amplify the financial impact of climate change.",http://arxiv.org/abs/2409.19463v1
CLIMATELI: Evaluating Entity Linking on Climate Change Data,2024-06-24T15:36:00Z,"Shijia Zhou, Siyao Peng, Barbara Plank","Climate Change (CC) is a pressing topic of global importance, attracting
increasing attention across research fields, from social sciences to Natural
Language Processing (NLP). CC is also discussed in various settings and
communication platforms, from academic publications to social media forums.
Understanding who and what is mentioned in such data is a first critical step
to gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),
the first manually annotated CC dataset that links 3,087 entity spans to
Wikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing
entity linking (EL) systems on the CC topic across various genres and propose
automated filtering methods for CC entities. We find that the performance of EL
models notably lags behind humans at both token and entity levels. Testing
within the scope of retaining or excluding non-nominal and/or non-CC entities
particularly impacts the models' performances.",http://arxiv.org/abs/2406.16732v2
"Bayesian Inference for Stochastic Predictions of Non-Gaussian Systems
  with Applications in Climate Change",2024-06-20T22:31:58Z,Yunjin Tong,"Climate change poses significant challenges for accurate climate modeling due
to the complexity and variability of non-Gaussian climate systems. To address
the complexities of non-Gaussian systems in climate modeling, this thesis
proposes a Bayesian framework utilizing the Unscented Kalman Filter (UKF),
Ensemble Kalman Filter (EnKF), and Unscented Particle Filter (UPF) for
one-dimensional and two-dimensional stochastic climate models, evaluated with
real-world temperature and sea level data. We study these methods under varying
conditions, including measurement noise, sample sizes, and observed and hidden
variables, to highlight their respective advantages and limitations. Our
findings reveal that merely increasing data is insufficient for accurate
predictions; instead, selecting appropriate methods is crucial. This research
provides insights into issues related to information barrier, curse of
dimensionality, prediction variability, and measurement noise quantification,
thereby enhancing the application of these techniques in real-world climate
scenarios.",http://arxiv.org/abs/2406.18606v1
"Assessing the climate benefits of afforestation: processes, methods, and
  frameworks",2024-07-19T18:20:13Z,"Kevin Bradley Dsouza, Enoch Ofosu, Jack Salkeld, Richard Boudreault, Juan Moreno-Cruz, Yuri Leonenko","Afforestation greatly influences several earth system processes, making it
essential to understand these effects to accurately assess its potential for
climate change mitigation. Although our understanding of forest-climate system
interactions has improved, significant knowledge gaps remain, preventing
definitive assessments of afforestation's net climate benefits. In this review,
focusing on the Canadian northern boreal and southern arctic, we identify these
gaps and synthesize existing knowledge. The review highlights regional
realities, Earth's climatic history, uncertainties in biogeochemical (BGC) and
biogeophysical (BGP) changes following afforestation, and limitations in
current assessment methodologies, emphasizing the need to reconcile these
uncertainties before drawing firm conclusions about the climate benefits of
afforestation. Finally, we propose an assessment framework which considers
multiple forcing components, temporal analysis, future climatic contexts, and
implementation details. We hope that the research gaps and assessment framework
discussed in this review inform afforestation policy in Canada and other
circumpolar nations.",http://arxiv.org/abs/2407.14617v3
"Amplified Summer Wind Stilling and Land Warming Compound Energy Risks in
  Northern Midlatitudes",2024-05-30T17:48:24Z,Gan Zhang,"Wind energy plays a critical role in mitigating climate change and meeting
growing energy demands. However, the long-term impacts of anthropogenic warming
on wind resources, particularly their seasonal variations and potential
compounding risks, remain understudied. Here we analyze large-ensemble climate
simulations in high-emission scenarios to assess the projected changes in
near-surface wind speed and their broader implications. Our analyses show
robust wind changes including a decrease of wind speed (i.e., stilling) up to
~15% during the summer months in Northern Midlatitudes. This stilling is linked
to amplified warming of the midlatitude land and the overlying troposphere.
Despite regional and model uncertainties, robust signals of warming-induced
wind stilling will likely emerge from natural climate variations in the late
21st century of the high-emission scenarios. Importantly, the summertime wind
stilling coincides with a projected surge in cooling demand, and their
compounding may disrupt the energy supply-demand balance earlier. These
findings highlight the importance of considering the seasonal responses of wind
resources and the associated climate-energy risks in a warming climate. By
integrating these insights into future energy planning decisions, we can better
adapt to a changing climate and ensure a reliable and resilient energy future.",http://arxiv.org/abs/2405.20302v3
"Reviewing climate change attribution in UK natural hazards and their
  impacts",2024-06-18T12:55:14Z,"Regan Mudhar, Dann M. Mitchell, Peter A. Stott, Richard A. Betts","The field of Detection and Attribution is rapidly moving beyond weather and
climate, and towards incorporating hazards and their impacts on natural and
human systems. Here, we review the comprehensive literature base relevant for
the UK ahead of the next Climate Change Risk Assessment. The current literature
highlights a detectable and non-trivial influence of climate change in many UK
impact sectors already - notably health, agriculture, and infrastructure. We
found that heatwaves were the most studied hazard overall, with a unanimous
consensus on a strong attributable signal of human-induced climate change in
their increased frequency and intensity over the last century. The most notable
gap identified overall was in attributing climate-related impacts to human
influence, with a few impact studies for only a handful of the hazards
assessed. Furthermore, just under half of the 29 hazards were not found to have
any UK-relevant attribution studies, with most of the remainder having three or
fewer. This review highlights requirements for and opportunities to develop
attribution scicnce to meet the needs of the UK. Diversifying hazards and
impacts studied, in conjunction with the techniques and approaches used, will
undoubtedly benefit the community.",http://arxiv.org/abs/2406.12951v1
Robustness of AI-based weather forecasts in a changing climate,2024-09-27T08:11:49Z,"Thomas Rackow, Nikolay Koldunov, Christian Lessig, Irina Sandu, Mihai Alexe, Matthew Chantry, Mariana Clare, Jesper Dramsch, Florian Pappenberger, Xabier Pedruzo-Bagazgoitia, Steffen Tietsche, Thomas Jung","Data-driven machine learning models for weather forecasting have made
transformational progress in the last 1-2 years, with state-of-the-art ones now
outperforming the best physics-based models for a wide range of skill scores.
Given the strong links between weather and climate modelling, this raises the
question whether machine learning models could also revolutionize climate
science, for example by informing mitigation and adaptation to climate change
or to generate larger ensembles for more robust uncertainty estimates. Here, we
show that current state-of-the-art machine learning models trained for weather
forecasting in present-day climate produce skillful forecasts across different
climate states corresponding to pre-industrial, present-day, and future 2.9K
warmer climates. This indicates that the dynamics shaping the weather on short
timescales may not differ fundamentally in a changing climate. It also
demonstrates out-of-distribution generalization capabilities of the machine
learning models that are a critical prerequisite for climate applications.
Nonetheless, two of the models show a global-mean cold bias in the forecasts
for the future warmer climate state, i.e. they drift towards the colder
present-day climate they have been trained for. A similar result is obtained
for the pre-industrial case where two out of three models show a warming. We
discuss possible remedies for these biases and analyze their spatial
distribution, revealing complex warming and cooling patterns that are partly
related to missing ocean-sea ice and land surface information in the training
data. Despite these current limitations, our results suggest that data-driven
machine learning models will provide powerful tools for climate science and
transform established approaches by complementing conventional physics-based
models.",http://arxiv.org/abs/2409.18529v1
"Analyzing Regional Impacts of Climate Change using Natural Language
  Processing Techniques",2024-01-11T16:44:59Z,"Tanwi Mallick, John Murphy, Joshua David Bergerson, Duane R. Verner, John K Hutchison, Leslie-Anne Levy","Understanding the multifaceted effects of climate change across diverse
geographic locations is crucial for timely adaptation and the development of
effective mitigation strategies. As the volume of scientific literature on this
topic continues to grow exponentially, manually reviewing these documents has
become an immensely challenging task. Utilizing Natural Language Processing
(NLP) techniques to analyze this wealth of information presents an efficient
and scalable solution. By gathering extensive amounts of peer-reviewed articles
and studies, we can extract and process critical information about the effects
of climate change in specific regions. We employ BERT (Bidirectional Encoder
Representations from Transformers) for Named Entity Recognition (NER), which
enables us to efficiently identify specific geographies within the climate
literature. This, in turn, facilitates location-specific analyses. We conduct
region-specific climate trend analyses to pinpoint the predominant themes or
concerns related to climate change within a particular area, trace the temporal
progression of these identified issues, and evaluate their frequency, severity,
and potential development over time. These in-depth examinations of
location-specific climate data enable the creation of more customized
policy-making, adaptation, and mitigation strategies, addressing each region's
unique challenges and providing more effective solutions rooted in data-driven
insights. This approach, founded on a thorough exploration of scientific texts,
offers actionable insights to a wide range of stakeholders, from policymakers
to engineers to environmentalists. By proactively understanding these impacts,
societies are better positioned to prepare, allocate resources wisely, and
design tailored strategies to cope with future climate conditions, ensuring a
more resilient future for all.",http://arxiv.org/abs/2401.06817v1
"Tackling extreme urban heat: a machine learning approach to assess the
  impacts of climate change and the efficacy of climate adaptation strategies
  in urban microclimates",2024-11-08T20:29:11Z,"Grant Buster, Jordan Cox, Brandon N. Benton, Ryan N. King","As urbanization and climate change progress, urban heat becomes a priority
for climate adaptation efforts. High temperatures concentrated in urban heat
can drive increased risk of heat-related death and illness as well as increased
energy demand for cooling. However, estimating the effects of urban heat is an
ongoing field of research typically burdened by an imprecise description of the
built environment, significant computational cost, and a lack of
high-resolution estimates of the impacts of climate change. Here, we present
open-source, computationally efficient machine learning methods that can
improve the accuracy of urban temperature estimates when compared to historical
reanalysis data. These models are applied to residential buildings in Los
Angeles, and we compare the energy benefits of heat mitigation strategies to
the impacts of climate change. We find that cooling demand is likely to
increase substantially through midcentury, but engineered high-albedo surfaces
could lessen this increase by more than 50%. The corresponding increase in
heating demand complicates this narrative, but total annual energy use from
combined heating and cooling with electric heat pumps in the Los Angeles urban
climate is shown to benefit from the engineered cooling strategies under both
current and future climates.",http://arxiv.org/abs/2411.05952v1
"Resilience Dynamics in Coupled Natural-Industrial Systems: A Surrogate
  Modeling Approach for Assessing Climate Change Impacts on Industrial
  Ecosystems",2024-12-22T13:00:14Z,"William Farlessyost, Shweta Singh","Industrial ecosystems are coupled with natural systems through utilization of
feedstocks and waste disposal. To ensure resilience in production of industrial
systems under the threat of climate change scenarios, it is necessary to
evaluate the impact of this coupling on productivity and waste generation. In
this work, we present a novel methodology for modeling and assessing the
resilience of coupled natural-industrial ecosystems under climate change
scenarios. We develop a computationally efficient framework that integrates
liquid time-constant (LTC) neural networks as surrogate models to capture
complex, nonlinear dynamics of coupled agricultural and industrial systems. The
approach is demonstrated through a case study of a soybean-based biodiesel
production network in Champaign County, Illinois. LTC models are trained to
capture dynamics of nodes and are then coupled and driven by statistically
downscaled climate projections for RCP 4.5 and 8.5 scenarios from 2006-2096.
The framework enables rapid simulation of system-wide material flow dynamics
and exploration of cascading effects from climate-induced disruptions. Results
reveal non-linear behaviors and potential tipping points in system resilience
under different climate scenarios and farm sizes. The RCP 8.5 scenario led to
earlier and more frequent production failures, increased reliance on imports
for smaller farms, and complex patterns of waste accumulation and stock levels.
The methodology provides valuable insights into system vulnerabilities and
adaptive capacities, offering decision support for enhancing the resilience and
sustainability of coupled natural-industrial ecosystems in the face of climate
change. The framework's adaptability suggests potential applications across
various industrial ecosystems and climate-sensitive sectors",http://arxiv.org/abs/2412.17006v1
"The Need for Climate Data Stewardship: 10 Tensions and Reflections
  regarding Climate Data Governance",2024-03-26T21:16:03Z,Stefaan Verhulst,"Datafication -- the increase in data generation and advancements in data
analysis -- offers new possibilities for governing and tackling worldwide
challenges such as climate change. However, employing new data sources in
policymaking carries various risks, such as exacerbating inequalities,
introducing biases, and creating gaps in access. This paper articulates ten
core tensions related to climate data and its implications for climate data
governance, ranging from the diversity of data sources and stakeholders to
issues of quality, access, and the balancing act between local needs and global
imperatives. Through examining these tensions, the article advocates for a
paradigm shift towards multi-stakeholder governance, data stewardship, and
equitable data practices to harness the potential of climate data for public
good. It underscores the critical role of data stewards in navigating these
challenges, fostering a responsible data ecology, and ultimately contributing
to a more sustainable and just approach to climate action and broader social
issues.",http://arxiv.org/abs/2403.18107v1
"Detecting Fallacies in Climate Misinformation: A Technocognitive
  Approach to Identifying Misleading Argumentation",2024-05-14T01:01:44Z,"Francisco Zanartu, John Cook, Markus Wagner, Julian Garcia","Misinformation about climate change is a complex societal issue requiring
holistic, interdisciplinary solutions at the intersection between technology
and psychology. One proposed solution is a ""technocognitive"" approach,
involving the synthesis of psychological and computer science research.
Psychological research has identified that interventions in response to
misinformation require both fact-based (e.g., factual explanations) and
technique-based (e.g., explanations of misleading techniques) content. However,
little progress has been made on documenting and detecting fallacies in climate
misinformation. In this study, we apply a previously developed critical
thinking methodology for deconstructing climate misinformation, in order to
develop a dataset mapping different types of climate misinformation to
reasoning fallacies. This dataset is used to train a model to detect fallacies
in climate misinformation. Our study shows F1 scores that are 2.5 to 3.5 better
than previous works. The fallacies that are easiest to detect include fake
experts and anecdotal arguments, while fallacies that require background
knowledge, such as oversimplification, misrepresentation, and slothful
induction, are relatively more difficult to detect. This research lays the
groundwork for development of solutions where automatically detected climate
misinformation can be countered with generative technique-based corrections.",http://arxiv.org/abs/2405.08254v1
"Cross-Country Comparative Analysis of Climate Resilience and Localized
  Mapping in Data-Sparse Regions",2024-09-13T12:12:26Z,Ronald Katende,"Climate resilience across sectors varies significantly in low-income
countries (LICs), with agriculture being the most vulnerable to climate change.
Existing studies typically focus on individual countries, offering limited
insights into broader cross-country patterns of adaptation and vulnerability.
This paper addresses these gaps by introducing a framework for cross-country
comparative analysis of sectoral climate resilience using meta-analysis and
cross-country panel data techniques. The study identifies shared
vulnerabilities and adaptation strategies across LICs, enabling more effective
policy design. Additionally, a novel localized climate-agriculture mapping
technique is developed, integrating sparse agricultural data with
high-resolution satellite imagery to generate fine-grained maps of agricultural
productivity under climate stress. Spatial interpolation methods, such as
kriging, are used to address data gaps, providing detailed insights into
regional agricultural productivity and resilience. The findings offer
policymakers tools to prioritize climate adaptation efforts and optimize
resource allocation both regionally and nationally.",http://arxiv.org/abs/2409.08765v1
"Rapid Climate Model Downscaling to Assess Risk of Extreme Rainfall in
  Bangladesh in a Warming Climate",2024-12-21T00:06:47Z,"Anamitra Saha, Sai Ravela","As climate change drives an increase in global extremes, it is critical for
Bangladesh, a nation highly vulnerable to these impacts, to assess future risks
for effective adaptation and mitigation planning. Downscaling coarse-resolution
climate models to finer scales is essential for accurately evaluating the risk
of extremes. In this study, we apply our downscaling method, which integrates
data, physics, and machine learning, to quantify the risks of extreme
precipitation in Bangladesh. The proposed approach successfully captures the
observed spatial patterns and risks of extreme rainfall in the current climate
while generating risk and uncertainty estimates by efficiently downscaling
multiple models under future climate scenarios. Our findings project that the
risk of extreme rainfall rises across the country, with the most significant
increases in the northeastern hilly and southeastern coastal areas. Projections
show that the daily maximum rainfall for a 100-year return period could
increase by approximately 50 mm/day by mid-century and around 100 mm/day by the
end of the century. However, substantial uncertainties remain due to variations
across multiple climate models and scenarios.",http://arxiv.org/abs/2412.16407v1
Harnessing Visualization for Climate Action and Sustainable Future,2024-10-22T20:36:27Z,Narges Mahyar,"The urgency of climate change is now recognized globally. As humanity
confronts the critical need to mitigate climate change and foster
sustainability, data visualization emerges as a powerful tool with a unique
capacity to communicate insights crucial for understanding environmental
complexities. This paper explores the critical need for designing and
investigating responsible data visualization that can act as a catalyst for
engaging communities within global climate action and sustainability efforts.
Grounded in prior work and reflecting on a decade of community engagement
research, I propose five critical considerations: (1) inclusive and accessible
visualizations for enhancing climate education and communication, (2)
interactive visualizations for fostering agency and deepening engagement,
  (3) in-situ visualizations for reducing spatial indirection,
  (4) shared immersive experiences for catalyzing collective action, and (5)
accurate, transparent, and credible visualizations for ensuring trust and
integrity. These considerations offer strategies and new directions for
visualization research, aiming to enhance community engagement, deepen
involvement, and foster collective action on critical socio-technical including
and beyond climate change.",http://arxiv.org/abs/2410.17411v1
Untangling Climate's Complexity: Methodological Insights,2024-05-28T17:31:15Z,"Alka Yadav, Sourish Das, Anirban Chakraborti","In this article, we review the interdisciplinary techniques (borrowed from
physics, mathematics, statistics, machine-learning, etc.) and methodological
framework that we have used to understand climate systems, which serve as
examples of ""complex systems"". We believe that this would offer valuable
insights to comprehend the complexity of climate variability and pave the way
for drafting policies for action against climate change, etc. Our basic aim is
to analyse time-series data structures across diverse climate parameters,
extract Fourier-transformed features to recognize and model the
trends/seasonalities in the climate variables using standard methods like
detrended residual series analyses, correlation structures among climate
parameters, Granger causal models, and other statistical machine-learning
techniques. We cite and briefly explain two case studies: (i) the relationship
between the Standardised Precipitation Index (SPI) and specific climate
variables including Sea Surface Temperature (SST), El Ni\~no Southern
Oscillation (ENSO), and Indian Ocean Dipole (IOD), uncovering temporal shifts
in correlations between SPI and these variables, and reveal complex patterns
that drive drought and wet climate conditions in South-West Australia; (ii) the
complex interactions of North Atlantic Oscillation (NAO) index, with SST and
sea ice extent (SIE), potentially arising from positive feedback loops.",http://arxiv.org/abs/2405.18391v1
"Stakeholder-driven research in the European Climate and Energy Modelling
  Forum",2024-06-03T05:29:51Z,"Emir Fejzic, Will Usher","A fast-paced policy context is characteristic of energy and climate research,
which strives to develop solutions to wicked problems such as climate change.
Funding agencies in the European Union recognize the importance of linking
research and policy in climate and energy research. This calls for an increased
understanding of how stakeholder engagement can effectively be used to
co-design research questions that include stakeholders' concerns. This paper
reviews the current literature on stakeholder engagement, from which we create
a set of criteria. These are used to critically assess recent and relevant
papers on stakeholder engagement in climate and energy projects. We obtained
the papers from a scoping review of stakeholder engagement through workshops in
EU climate and energy research. With insights from the literature and current
EU climate and energy projects, we developed a workshop programme for
stakeholder engagement. This programme was applied to the European Climate and
Energy Modelling Forum project, aiming to co-design the most pressing and
urgent research questions according to European stakeholders. The outcomes
include 82 co-designed and ranked research questions for nine specific climate
and energy research themes. Findings from the scoping review indicate that
papers rarely define the term 'stakeholder'. Additionally, the concepts of
co-creation, co-design, and co-production are used interchangeably and often
without definition. We propose that workshop planners use stakeholder
identification and selection methods from the broader stakeholder engagement
literature.",http://arxiv.org/abs/2406.01640v1
Conditional multi-step attribution for climate forcings,2024-09-02T17:55:52Z,"Christopher R. Wentland, Michael Weylandt, Laura P. Swiler, Thomas S. Ehrmann, Diana Bull","Attribution of climate impacts to a source forcing is critical to
understanding, communicating, and addressing the effects of human influence on
the climate. While standard attribution methods, such as optimal
fingerprinting, have been successfully applied to long-term, widespread effects
such as global surface temperature warming, they often struggle in low
signal-to-noise regimes, typical of short-term climate forcings or climate
variables which are loosely related to the forcing. Single-step approaches,
which directly relate a source forcing and final impact, are unable to utilize
additional climate information to improve attribution certainty. To address
this shortcoming, this paper presents a novel multi-step attribution approach
which is capable of analyzing multiple variables conditionally. A connected
series of climate effects are treated as dependent, and relationships found in
intermediary steps of a causal pathway are leveraged to better characterize the
forcing impact. This enables attribution of the forcing level responsible for
the observed impacts, while equivalent single-step approaches fail. Utilizing a
scalar feature describing the forcing impact, simple forcing response models,
and a conditional Bayesian formulation, this method can incorporate several
causal pathways to identify the correct forcing magnitude. As an exemplar of a
short-term, high-variance forcing, we demonstrate this method for the 1991
eruption of Mt. Pinatubo. Results indicate that including stratospheric and
surface temperature and radiative flux measurements increases attribution
certainty compared to analyses derived solely from temperature measurements.
This framework has potential to improve climate attribution assessments for
both geoengineering projects and long-term climate change, for which standard
attribution methods may fail.",http://arxiv.org/abs/2409.01396v1
"We must re-evaluate assumptions about carbon trading for effective
  climate change mitigation",2024-11-06T18:03:32Z,"Alyssa R. Pfadt-Trilling, Marie-Odile P. Fortier","Effective climate action depends on dismantling the assumptions and
oversimplifications that have become the basis of climate policy. The
assumption that greenhouse gases (GHG) are fungible and the use of single-point
values in normalizing GHG species to CO2-equivalents can propagate inaccuracies
in carbon accounting and have already led to failures of carbon offset systems.
Separate emission reduction targets and tracking by GHG species are recommended
to achieve long-term climate stabilization.",http://arxiv.org/abs/2411.08053v1
"Eternagram: Probing Player Attitudes in Alternate Climate Scenarios
  Through a ChatGPT-Driven Text Adventure",2024-03-26T23:54:49Z,"Suifang Zhou, Latisha Besariani Hendra, Qinshi Zhang, Jussi Holopainen, RAY LC","Conventional methods of assessing attitudes towards climate change are
limited in capturing authentic opinions, primarily stemming from a lack of
context-specific assessment strategies and an overreliance on simplistic
surveys. Game-based Assessments (GBA) have demonstrated the ability to overcome
these issues by immersing participants in engaging gameplay within carefully
crafted, scenario-based environments. Concurrently, advancements in AI and
Natural Language Processing (NLP) show promise in enhancing the gamified
testing environment, achieving this by generating context-aware, human-like
dialogues that contribute to a more natural and effective assessment. Our study
introduces a new technique for probing climate change attitudes by actualizing
a GPT-driven chatbot system in harmony with a game design depicting a
futuristic climate scenario. The correlation analysis reveals an assimilation
effect, where players' post-game climate awareness tends to align with their
in-game perceptions. Key predictors of pro-climate attitudes are identified as
traits like 'Openness' and 'Agreeableness', and a preference for democratic
values.",http://arxiv.org/abs/2403.18160v1
"Predictability of climate anomalies in the regions of Northern Eurasia
  in the spring-summer months in 2024 in connection with El Nino",2024-03-30T19:01:05Z,I. I. Mokhov,"The predictability of climate anomalies in the regions of Northern Eurasia in
connection with El Nino phenomena is analyzed. Particular attention is paid to
the most likely transition in 2024 from an El Nino phase at the beginning of
the year to a La Nina phase at the end of the year, with the greatest
probability of high temperatures and dry conditions in European Russia during
the spring and summer months, as in 2010. The predictability levels of regional
climate anomalies using different El Nino indices are compared. The
relationship of the noted seasonal anomalies with atmospheric blockings is
considered, taking into account the different phases of the key modes of
climate variability like El Nino phenomena and the Pacific Decadal Oscillation.
Changes in the predictability of regional climate anomalies under global
climate change are discussed.",http://arxiv.org/abs/2404.00453v1
Quantum Computing for Climate Resilience and Sustainability Challenges,2024-07-23T08:54:12Z,"Kin Tung Michael Ho, Kuan-Cheng Chen, Lily Lee, Felix Burt, Shang Yu, Po-Heng, Lee","The escalating impacts of climate change and the increasing demand for
sustainable development and natural resource management necessitate innovative
technological solutions. Quantum computing (QC) has emerged as a promising tool
with the potential to revolutionize these critical areas. This review explores
the application of quantum machine learning and optimization techniques for
climate change prediction and enhancing sustainable development. Traditional
computational methods often fall short in handling the scale and complexity of
climate models and natural resource management. Quantum advancements, however,
offer significant improvements in computational efficiency and problem-solving
capabilities. By synthesizing the latest research and developments, this paper
highlights how QC and quantum machine learning can optimize
multi-infrastructure systems towards climate neutrality. The paper also
evaluates the performance of current quantum algorithms and hardware in
practical applications and presents realistic cases, i.e., waste-to-energy in
anaerobic digestion, disaster prevention in flooding prediction, and new
material development for carbon capture. The integration of these quantum
technologies promises to drive significant advancements in achieving climate
resilience and sustainable development.",http://arxiv.org/abs/2407.16296v1
"Turbine location-aware multi-decadal wind power predictions for Germany
  using CMIP6",2024-08-27T09:04:08Z,"Nina Effenberger, Nicole Ludwig","Climate change will impact wind and therefore wind power generation with
largely unknown effect and magnitude. Climate models can provide insights and
should be used for long-term power planning. In this work we use Gaussian
processes to predict power output given wind speeds from a global climate model
and compare the aggregated predictions to actual power generation. Analyzing
past climate model data supports the use of CMIP6 climate model data for
multi-decadal wind power predictions and highlights the importance of being
location-aware. Our predictions up to 2050 reveal only minor changes in yearly
wind power generation. We find that wind power projections of the two
in-between climate scenarios SSP2-4.5 and SSP3-7.0 closely align with actual
wind power generation between 2015 and 2023. Our analysis also reveals larger
uncertainty associated with Germany's coastal areas in the North as compared to
Germany's South, motivating wind power expansion in regions where future wind
is likely more reliable. Overall, our results indicate that wind energy will
likely remain a reliable energy source in the future.",http://arxiv.org/abs/2408.14889v2
"Towards unearthing neglected climate innovations from scientific
  literature using Large Language Models",2024-11-15T09:17:40Z,"C√©sar Quilodr√°n-Casas, Christopher Waite, Nicole Alhadeff, Diyona Dsouza, Cathal Hughes, Larissa Kunstel-Tabet, Alyssa Gilbert","Climate change poses an urgent global threat, needing the rapid
identification and deployment of innovative solutions. We hypothesise that many
of these solutions already exist within scientific literature but remain
underutilised. To address this gap, this study employs a curated dataset
sourced from OpenAlex, a comprehensive repository of scientific papers.
Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate
title-abstract pairs from scientific papers on seven dimensions, covering
climate change mitigation potential, stage of technological development, and
readiness for deployment. The outputs of the language models are then compared
with human evaluations to assess their effectiveness in identifying promising
yet overlooked climate innovations. Our findings suggest that these LLM-based
models can effectively augment human expertise, uncovering climate solutions
that are potentially impactful but with far greater speed, throughput and
consistency. Here, we focused on UK-based solutions, but the workflow is
region-agnostic. This work contributes to the discovery of neglected
innovations in scientific literature and demonstrates the potential of AI in
enhancing climate action strategies.",http://arxiv.org/abs/2411.10055v1
"Predicting unobserved climate time series data at distant areas via
  spatial correlation using reservoir computing",2024-06-05T08:39:10Z,"Shihori Koyama, Daisuke Inoue, Hiroaki Yoshida, Kazuyuki Aihara, Gouhei Tanaka","Collecting time series data spatially distributed in many locations is often
important for analyzing climate change and its impacts on ecosystems. However,
comprehensive spatial data collection is not always feasible, requiring us to
predict climate variables at some locations. This study focuses on a prediction
of climatic elements, specifically near-surface temperature and pressure, at a
target location apart from a data observation point. Our approach uses two
prediction methods: reservoir computing (RC), known as a machine learning
framework with low computational requirements, and vector autoregression models
(VAR), recognized as a statistical method for analyzing time series data. Our
results show that the accuracy of the predictions degrades with the distance
between the observation and target locations. We quantitatively estimate the
distance in which effective predictions are possible. We also find that in the
context of climate data, a geographical distance is associated with data
correlation, and a strong data correlation significantly improves the
prediction accuracy with RC. In particular, RC outperforms VAR in predicting
highly correlated data within the predictive range. These findings suggest that
machine learning-based methods can be used more effectively to predict climatic
elements in remote locations by assessing the distance to them from the data
observation point in advance. Our study on low-cost and accurate prediction of
climate variables has significant value for climate change strategies.",http://arxiv.org/abs/2406.03061v1
"Impacts and risks of ""realistic"" global warming projections for the 21st
  century",2024-01-10T11:02:17Z,Nicola Scafetta,"The IPCC AR6 assessment of the impacts and risks associated with projected
climate changes for the 21st century is both alarming and ambiguous. According
to computer projections, the global surface may warm from 1.3 to 8.0 {\deg}C by
2100, depending on the global climate model (GCM) and the shared socioeconomic
pathway (SSP) scenario used for the simulations. However a substantial number
of CMIP6 GCMs run ""too hot"" because they appear to be too sensitive to
radiative forcing, and that the high/extreme emission scenarios SSP3-7.0 and
SSP5-8.5 must be rejected because judged to be ""unlikely"" and ""highly
unlikely"", respectively. This paper examines the impacts and risks of
""realistic"" climate change projections for the 21st century generated by
assessing the theoretical models and integrating them with the existing
empirical knowledge on global warming and the various natural cycles of climate
change that have been recorded by a variety of scientists and historians. This
is achieved by combining the ""realistic"" SSP2-4.5 scenario and empirically
optimized climate modeling. The obtained climate projections show that the
expected global surface warming for the 21st century will likely be mild, that
is, no more than 2.5-3.0 {\deg}C and, on average, likely below the 2.0 {\deg}C
threshold. This should allow for the mitigation and management of the most
dangerous climate-change-related hazards through appropriate low-cost
adaptation policies. In conclusion, enforcing expensive decarbonization and
net-zero emission scenarios, such as SSP1-2.6, is not required because the
Paris Agreement temperature target of keeping global warming below 2 {\deg}C
throughout the 21st century should be compatible also with moderate and
pragmatic shared socioeconomic pathways such as the SSP2-4.5.",http://arxiv.org/abs/2401.08674v1
Granger causal inference for climate change attribution,2024-08-13T18:55:07Z,"Mark D. Risser, Mohammed Ombadi, Michael F. Wehner","Climate change detection and attribution (D&A) is concerned with determining
the extent to which anthropogenic activities have influenced specific aspects
of the global climate system. D&A fits within the broader field of causal
inference, the collection of statistical methods that identify cause and effect
relationships. There are a wide variety of methods for making attribution
statements, each of which require different types of input data and focus on
different types of weather and climate events and each of which are conditional
to varying extents. Some methods are based on Pearl causality while others
leverage Granger causality, and the causal framing provides important context
for how the resulting attribution conclusion should be interpreted. However,
while Granger-causal attribution analyses have become more common, there is no
clear statement of their strengths and weaknesses and no clear consensus on
where and when Granger-causal perspectives are appropriate. In this prospective
paper, we provide a formal definition for Granger-based approaches to trend and
event attribution and a clear comparison with more traditional methods for
assessing the human influence on extreme weather and climate events. Broadly
speaking, Granger-causal attribution statements can be constructed quickly from
observations and do not require computationally-intesive dynamical experiments.
These analyses also enable rapid attribution, which is useful in the aftermath
of a severe weather event, and provide multiple lines of evidence for
anthropogenic climate change when paired with Pearl-causal attribution.
Confidence in attribution statements is increased when different methodologies
arrive at similar conclusions. Moving forward, we encourage the D&A community
to embrace hybrid approaches to climate change attribution that leverage the
strengths of both Granger and Pearl causality.",http://arxiv.org/abs/2408.16004v1
The Economics of Climate Adaptation: An Assessment,2024-11-25T19:57:14Z,"Anna Josephson, Rodrigo Guerra Su, Greg Collins, Katharine Jacobs","The cost of the impacts of climate change have already proven to be larger
than previously believed. Understanding the costs and benefits of adapting to
the changing climate is necessary to make targeted and appropriate investment
decisions. In this paper, we use a narrative review to synthesize the current
literature on the economic case for climate adaptation, with the objective of
assessing the value (economic and otherwise) of climate change adaptation, as
well as the strength of the methods and evidence that have been used to date.
We find that skepticism is warranted about many of the estimates about costs
and benefits of climate adaptation and their underlying assumptions, due to a
range of complexities associated with (1) uncertainty in distinguishing the
economic impacts of climate change from seasonal variability; (2) difficulties
in non-market valuation; (3) lack of consistent data collection over time at
multiple scales; and (4) distributional inequities in access to proactive
adaptation and recovery funding. While useful for broad stroke advocacy
purposes, these estimates fall short of the refinement and rigor needed to
inform investment decision-making, particularly at micro and local scales. Most
estimates rely on cost benefit analysis and do not effectively address these
issues. An emergent and promising literature tackles alternative estimation
strategies and attempts to address some of them, including the complexities of
uncertainty and non-market valuation.",http://arxiv.org/abs/2411.16893v1
"Climate Adaptation with Reinforcement Learning: Experiments with
  Flooding and Transportation in Copenhagen",2024-09-27T09:18:57Z,"Miguel Costa, Morten W. Petersen, Arthur Vandervoort, Martin Drews, Karyn Morrissey, Francisco C. Pereira","Due to climate change the frequency and intensity of extreme rainfall events,
which contribute to urban flooding, are expected to increase in many places.
These floods can damage transport infrastructure and disrupt mobility,
highlighting the need for cities to adapt to escalating risks. Reinforcement
learning (RL) serves as a powerful tool for uncovering optimal adaptation
strategies, determining how and where to deploy adaptation measures
effectively, even under significant uncertainty. In this study, we leverage RL
to identify the most effective timing and locations for implementing measures,
aiming to reduce both direct and indirect impacts of flooding. Our framework
integrates climate change projections of future rainfall events and floods,
models city-wide motorized trips, and quantifies direct and indirect impacts on
infrastructure and mobility. Preliminary results suggest that our RL-based
approach can significantly enhance decision-making by prioritizing
interventions in specific urban areas and identifying the optimal periods for
their implementation. Our framework is publicly available:
\url{https://github.com/MLSM-at-DTU/floods_transport_rl}.",http://arxiv.org/abs/2409.18574v2
"A cross-platform analysis of polarization and echo chambers in climate
  change discussions",2024-10-28T16:27:37Z,"Aleix Bassolas, Joan Massachs, Emanuele Cozzo, Julian Vicens","With the intensification of climate change discussion, social media has
become prominent in disseminating reliable and unreliable content. In this
study, we present a cross-platform analysis on Youtube and Twitter, and examine
the polarization and echo chambers in social media discussions in four datasets
related to climate change: COP27, IPCC, Climate Refugees, and Do\~{n}ana. We
have identified communities of users spreading misinformation on Twitter,
although they remain relatively isolated from the rest of the network. The
analysis by interaction type reveals that climate change sceptics use mentions
to draw the attention of other communities. The YouTube posts referenced on
Twitter reveal a strong correlation in the community organisation of social
media, suggesting a platform alignment. Moreover, we report the presence of
echo chambers in YouTube post-sharing related to mainstream and sceptical
content.",http://arxiv.org/abs/2410.21187v2
"ACE2-SOM: Coupling an ML atmospheric emulator to a slab ocean and
  learning the sensitivity of climate to changed CO$_2$",2024-12-05T18:44:33Z,"Spencer K. Clark, Oliver Watt-Meyer, Anna Kwa, Jeremy McGibbon, Brian Henn, W. Andre Perkins, Elynn Wu, Lucas M. Harris, Christopher S. Bretherton","While autoregressive machine-learning-based emulators have been trained to
produce stable and accurate rollouts in the climate of the present-day and
recent past, none so far have been trained to emulate the sensitivity of
climate to substantial changes in CO$_2$ or other greenhouse gases. As an
initial step we couple the Ai2 Climate Emulator version 2 to a slab ocean model
(hereafter ACE2-SOM) and train it on output from a collection of
equilibrium-climate physics-based reference simulations with varying levels of
CO$_2$. We test it in equilibrium and non-equilibrium climate scenarios with
CO$_2$ concentrations seen and unseen in training.
  ACE2-SOM performs well in equilibrium-climate inference with both in-sample
and out-of-sample CO$_2$ concentrations, accurately reproducing the emergent
time-mean spatial patterns of surface temperature and precipitation change with
CO$_2$ doubling, tripling, or quadrupling. In addition, the vertical profile of
atmospheric warming and change in extreme precipitation rates up to the
99.9999th percentile closely agree with the reference model.
Non-equilibrium-climate inference is more challenging. With CO$_2$ increasing
gradually at a rate of 2% year$^{-1}$, ACE2-SOM can accurately emulate the
global annual mean trends of surface and lower-to-middle atmosphere fields but
produces unphysical jumps in stratospheric fields. With an abrupt quadrupling
of CO$_2$, ML-controlled fields transition unrealistically quickly to the
4xCO$_2$ regime. In doing so they violate global energy conservation and
exhibit unphysical sensitivities of and surface and top of atmosphere radiative
fluxes to instantaneous changes in CO$_2$. Future emulator development needed
to address these issues should improve its generalizability to diverse climate
change scenarios.",http://arxiv.org/abs/2412.04418v2
"A non-autonomous framework for climate change and extreme weather events
  increase in a stochastic energy balance model",2024-06-12T09:43:36Z,"Gianmarco Del Sarto, Franco Flandoli","We develop a three-timescale framework for modelling climate change and
introduce a space-heterogeneous one-dimensional energy balance model. This
model, addressing temperature fluctuations from rising carbon dioxide levels
and the super-greenhouse effect in tropical regions, fits within the setting of
stochastic reaction-diffusion equations. Our results show how both mean and
variance of temperature increase, without the system going through a
bifurcation point. This study aims to advance the conceptual understanding of
the extreme weather events frequency increase due to climate change.",http://arxiv.org/abs/2406.11881v2
Tipping detection using climate networks,2024-07-26T13:23:23Z,"Laure Moinat, J√©r√¥me Kasparian, Maura Brunetti","The development of robust Early Warning Signals (EWS) is necessary to
quantify the risk of crossing tipping points in the present-day climate change.
Classically, EWS are statistical measures based on time series of climate state
variables, without exploiting their spatial distribution. However, spatial
information is crucial to identify the starting location of a transition
process, and can be directly inferred by satellite observations. By using
complex networks constructed from several climate variables on the numerical
grid of climate simulations, we seek for network properties that can serve as
EWS when approaching a state transition. We show that network indicators such
as the normalized degree, the average length distance and the betweenness
centrality are capable of detecting tipping points at the global scale, as
obtained by the MIT general circulation model in a coupled-aquaplanet
configuration for CO$_2$ concentration-driven simulations. The applicability of
such indicators as EWS is assessed and compared to traditional methods. We also
analyse the ability of climate networks to identify nonlinear dynamical
patterns.",http://arxiv.org/abs/2407.18727v1
"Rapid Statistical-Physical Adversarial Downscaling Reveals Bangladesh's
  Rising Rainfall Risk in a Warming Climate",2024-08-21T17:22:29Z,"Anamitra Saha, Sai Ravela","In Bangladesh, a nation vulnerable to climate change, accurately quantifying
the risk of extreme weather events is crucial for planning effective adaptation
and mitigation strategies. Downscaling coarse climate model projections to
finer resolutions is key in improving risk and uncertainty assessments. This
work develops a new approach to rainfall downscaling by integrating statistics,
physics, and machine learning and applies it to assess Bangladesh's extreme
rainfall risk. Our method successfully captures the observed spatial pattern
and risks associated with extreme rainfall in the present climate. It also
produces uncertainty estimates by rapidly downscaling multiple models in a
future climate scenario(s). Our analysis reveals that the risk of extreme
rainfall is projected to increase throughout Bangladesh mid-century, with the
highest risk in the northeast. The daily maximum rainfall at a 100-year return
period is expected to rise by approximately 50 mm per day. However, using
multiple climate models also indicates considerable uncertainty in the
projected risk.",http://arxiv.org/abs/2408.11790v1
"On the Extrapolation of Generative Adversarial Networks for downscaling
  precipitation extremes in warmer climates",2024-09-20T22:42:38Z,"Neelesh Rampal, Peter B. Gibson, Steven Sherwood, Gab Abramowitz","While deep-learning downscaling algorithms can generate fine-scale climate
projections cost-effectively, it is still unclear how well they will
extrapolate to unobserved climates. We assess the extrapolation capabilities of
a deterministic Convolutional Neural Network baseline and a Generative
Adversarial Network (GAN) built with this baseline, trained to predict daily
precipitation simulated by a Regional Climate Model (RCM). Both approaches
emulate future changes in annual mean precipitation well, even when trained on
historical data, though training on a future climate improves performance. For
extreme precipitation (99.5th percentile), RCM simulations predict a robust
end-of-century increase with future warming (~5.8%/{\deg}C on average from five
simulations). When trained on a future climate, GANs capture 97% of the
warming-driven increase in extreme precipitation compared to 65% in a
deterministic baseline. Even GANs trained historically capture 77% of this
increase. Overall, GANs offer better generalization for downscaling extremes,
which is important in applications relying on historical data.",http://arxiv.org/abs/2409.13934v1
Climate Policy Elites' Twitter Interactions across Nine Countries,2024-12-20T04:08:20Z,"Ted Hsuan Yun Chen, Arttu Malkam√§ki, Ali Faqeeh, Esa Palosaari, Anniina Kotkaniemi, Laura Funke, C√°it Gleeson, James Goodman, Antti Gronow, Marlene Kammerer, Myanna Lahsen, Alexandre Marques, Petr Ocelik, Shivangi Seth, Mark Stoddart, Martin Svozil, Pradip Swarnakar, Matthew Trull, Paul Wagner, Yixi Yang, Mikko Kivel√§, Tuomas Yl√§-Anttila","We identified the Twitter accounts of 941 climate change policy actors across
nine countries, and collected their activities from 2017--2022, totalling 48
million activities from 17,700 accounts at different organizational levels.
There is considerable temporal and cross-national variation in how prominent
climate-related activities were, but all national policy systems generally
responded to climate-related events, such as climate protests, in a similar
manner. Examining patterns of interaction within and across countries, we find
that these national policy systems rarely directly interact with one another,
but are connected through consistently engaging with the same content produced
by accounts of international organizations, climate activists, and researchers.",http://arxiv.org/abs/2412.15545v1
"Intelligent Agricultural Management Considering N$_2$O Emission and
  Climate Variability with Uncertainties",2024-02-13T22:29:40Z,"Zhaoan Wang, Shaoping Xiao, Jun Wang, Ashwin Parab, Shivam Patel","This study examines how artificial intelligence (AI), especially
Reinforcement Learning (RL), can be used in farming to boost crop yields,
fine-tune nitrogen use and watering, and reduce nitrate runoff and greenhouse
gases, focusing on Nitrous Oxide (N$_2$O) emissions from soil. Facing climate
change and limited agricultural knowledge, we use Partially Observable Markov
Decision Processes (POMDPs) with a crop simulator to model AI agents'
interactions with farming environments. We apply deep Q-learning with Recurrent
Neural Network (RNN)-based Q networks for training agents on optimal actions.
Also, we develop Machine Learning (ML) models to predict N$_2$O emissions,
integrating these predictions into the simulator. Our research tackles
uncertainties in N$_2$O emission estimates with a probabilistic ML approach and
climate variability through a stochastic weather model, offering a range of
emission outcomes to improve forecast reliability and decision-making. By
incorporating climate change effects, we enhance agents' climate adaptability,
aiming for resilient agricultural practices. Results show these agents can
align crop productivity with environmental concerns by penalizing N$_2$O
emissions, adapting effectively to climate shifts like warmer temperatures and
less rain. This strategy improves farm management under climate change,
highlighting AI's role in sustainable agriculture.",http://arxiv.org/abs/2402.08832v1
Causal Modeling of Climate Activism on Reddit,2024-10-14T14:41:09Z,"Jacopo Lenti, Luca Maria Aiello, Corrado Monti, Gianmarco De Francisci Morales","Climate activism is crucial in stimulating collective societal and behavioral
change towards sustainable practices through political pressure. Although
multiple factors contribute to the participation in activism, their complex
relationships and the scarcity of data on their interactions have restricted
most prior research to studying them in isolation, thus preventing the
development of a quantitative, causal understanding of why people approach
activism. In this work, we develop a comprehensive causal model of how and why
Reddit users engage with activist communities driving mass climate protests
(mainly the 2019 Earth Strike, Fridays for Future, and Extinction Rebellion).
Our framework, based on Stochastic Variational Inference applied to Bayesian
Networks, learns the causal pathways over multiple time periods. Distinct from
previous studies, our approach uses large-scale and fine-grained longitudinal
data (2016 to 2022) to jointly model the roles of sociodemographic makeup,
experience of extreme weather events, exposure to climate-related news, and
social influence through online interactions. We find that among users
interested in climate change, participation in online activist communities is
indeed influenced by direct interactions with activists and largely by recent
exposure to media coverage of climate protests. Among people aware of climate
change, left-leaning people from lower socioeconomic backgrounds are
particularly represented in online activist groups. Our findings offer
empirical validation for theories of media influence and critical mass, and lay
the foundations to inform interventions and future studies to foster public
participation in collective action.",http://arxiv.org/abs/2410.10562v1
"Advancing Data-driven Weather Forecasting: Time-Sliding Data
  Augmentation of ERA5",2024-02-13T03:01:22Z,"Minjong Cheon, Daehyun Kang, Yo-Hwan Choi, Seon-Yu Kang","Modern deep learning techniques, which mimic traditional numerical weather
prediction (NWP) models and are derived from global atmospheric reanalysis
data, have caused a significant revolution within a few years. In this new
paradigm, our research introduces a novel strategy that deviates from the
common dependence on high-resolution data, which is often constrained by
computational resources, and instead utilizes low-resolution data (2.5 degrees)
for global weather prediction and climate data analysis. Our main focus is
evaluating data-driven weather prediction (DDWP) frameworks, specifically
addressing sample size adequacy, structural improvements to the model, and the
ability of climate data to represent current climatic trends. By using the
Adaptive Fourier Neural Operator (AFNO) model via FourCastNet and a proposed
time-sliding method to inflate the dataset of the ECMWF Reanalysis v5 (ERA5),
this paper improves on conventional approaches by adding more variables and a
novel approach to data augmentation and processing. Our findings reveal that
despite the lower resolution, the proposed approach demonstrates considerable
accuracy in predicting atmospheric conditions, effectively rivaling
higher-resolution models. Furthermore, the study confirms the model's
proficiency in reflecting current climate trends and its potential in
predicting future climatic events, underscoring its utility in climate change
strategies. This research marks a pivotal step in the realm of meteorological
forecasting, showcasing the feasibility of lower-resolution data in producing
reliable predictions and opening avenues for more accessible and inclusive
climate modeling. The insights gleaned from this study not only contribute to
the advancement of climate science but also lay the groundwork for future
innovations in the field.",http://arxiv.org/abs/2402.08185v1
"InvestESG: A multi-agent reinforcement learning benchmark for studying
  climate investment as a social dilemma",2024-11-15T00:31:45Z,"Xiaoxuan Hou, Jiayi Yuan, Joel Z. Leibo, Natasha Jaques","InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark
designed to study the impact of Environmental, Social, and Governance (ESG)
disclosure mandates on corporate climate investments. The benchmark models an
intertemporal social dilemma where companies balance short-term profit losses
from climate mitigation efforts and long-term benefits from reducing climate
risk, while ESG-conscious investors attempt to influence corporate behavior
through their investment decisions. Companies allocate capital across
mitigation, greenwashing, and resilience, with varying strategies influencing
climate outcomes and investor preferences. We are releasing open-source
versions of InvestESG in both PyTorch and JAX, which enable scalable and
hardware-accelerated simulations for investigating competing incentives in
mitigate climate change. Our experiments show that without ESG-conscious
investors with sufficient capital, corporate mitigation efforts remain limited
under the disclosure mandate. However, when a critical mass of investors
prioritizes ESG, corporate cooperation increases, which in turn reduces climate
risks and enhances long-term financial stability. Additionally, providing more
information about global climate risks encourages companies to invest more in
mitigation, even without investor involvement. Our findings align with
empirical research using real-world data, highlighting MARL's potential to
inform policy by providing insights into large-scale socio-economic challenges
through efficient testing of alternative policy and market designs.",http://arxiv.org/abs/2411.09856v3
"Saving for sunny days: The impact of climate (change) on consumer prices
  in the euro area",2024-01-08T09:00:56Z,"Paulo M. M. Rodrigues, Mirjam Salish, Nazarii Salish","Climate (change) affects the prices of goods and services in different
countries or regions differently. Simply relying on aggregate measures or
summary statistics, such as the impact of average country temperature changes
on HICP headline inflation, conceals a large heterogeneity across (sub-)sectors
of the economy. Additionally, the impact of a weather anomaly on consumer
prices depends not only on its sign and magnitude, but also on its location and
the size of the area affected by the shock. This is especially true for larger
countries or regions with diverse climate zones, since the geographical
distribution of climatic effects plays a role in shaping economic outcomes.
Using time series data of geolocations, we demonstrate that relying solely on
country averages fails to adequately capture and explain the influence of
weather on consumer prices in the euro area. We conclude that the information
content hidden in rich and complex surface data can provide valuable insights
into the role of weather and climate variables for price stability, and more
generally may help to inform economic policy.",http://arxiv.org/abs/2401.03740v1
"Experimental verification of the optimal fingerprint method for
  detecting climate change",2024-06-11T04:23:04Z,"Jinbo Hu, Hong Yuan, Letian Chen, Nan Zhao, C. P. Sun","The optimal fingerprint method serves as a potent approach for detecting and
attributing climate change. However, its experimental validation encounters
challenges due to the intricate nature of climate systems. Here, we
experimentally examine the optimal fingerprint method simulated by a precisely
controlled magnetic resonance system of spins. The spin dynamic under an
applied deterministic driving field and a noise field is utilized to emulate
the complex climate system with external forcing and internal variability. Our
experimental results affirm the theoretical prediction regarding the existence
of an optimal detection direction which maximizes the signal-to-noise ratio,
thereby validating the optimal fingerprint method. This work offers direct
empirical verification of the optimal fingerprint method, crucial for
comprehending climate change and its societal impacts.",http://arxiv.org/abs/2406.11879v1
"Leveraging AI for Climate Resilience in Africa: Challenges,
  Opportunities, and the Need for Collaboration",2024-04-24T14:05:22Z,"Rendani Mbuvha, Yassine Yaakoubi, John Bagiliko, Santiago Hincapie Potes, Amal Nammouchi, Sabrina Amrouche","As climate change issues become more pressing, their impact in Africa calls
for urgent, innovative solutions tailored to the continent's unique challenges.
While Artificial Intelligence (AI) emerges as a critical and valuable tool for
climate change adaptation and mitigation, its effectiveness and potential are
contingent upon overcoming significant challenges such as data scarcity,
infrastructure gaps, and limited local AI development. This position paper
explores the role of AI in climate change adaptation and mitigation in Africa.
It advocates for a collaborative approach to build capacity, develop
open-source data repositories, and create context-aware, robust AI-driven
climate solutions that are culturally and contextually relevant.",http://arxiv.org/abs/2407.05210v1
Resolution-Agnostic Transformer-based Climate Downscaling,2024-11-22T07:32:11Z,"Declan Curran, Hira Saleem, Sanaa Hobeichi, Flora Salim","Understanding future weather changes at regional and local scales is crucial
for planning and decision-making, particularly in the context of extreme
weather events, as well as for broader applications in agriculture, insurance,
and infrastructure development. However, the computational cost of downscaling
Global Climate Models (GCMs) to the fine resolutions needed for such
applications presents a significant barrier. Drawing on advancements in weather
forecasting models, this study introduces a cost-efficient downscaling method
using a pretrained Earth Vision Transformer (Earth ViT) model. Initially
trained on ERA5 data to downscale from 50 km to 25 km resolution, the model is
then tested on the higher resolution BARRA-SY dataset at a 3 km resolution.
Remarkably, it performs well without additional training, demonstrating its
ability to generalize across different resolutions. This approach holds promise
for generating large ensembles of regional climate simulations by downscaling
GCMs with varying input resolutions without incurring additional training
costs. Ultimately, this method could provide more comprehensive estimates of
potential future changes in key climate variables, aiding in effective planning
for extreme weather events and climate change adaptation strategies.",http://arxiv.org/abs/2411.14774v2
"The role of the surface evapotranspiration in regional climate
  modelling: Evaluation and near-term future changes",2024-01-14T09:23:25Z,"Matilde Garc√≠a-Valdecasas Ojeda, Juan Jos√© Rosa-C√°novas, Emilio Romero-Jim√©nez, P. Yeste, Sonia R. G√°miz-Fortis, Yolanda Castro-D√≠ez, Mar√≠a Jes√∫s Esteban-Parra","The surface evapotranspiration (SFCEVP) plays an essential role in climate,
being the link between the hydrological and energy cycles. Therefore, how it is
approximated and its implication in the regional climate are important aspects
to understand the effects of climate change, especially over transitional zones
such as the Iberian Peninsula (IP). This study aims to investigate the
spatiotemporal patterns of the SFCEVP using a regional climate model (RCM), the
Weather Research and Forecasting (WRF) model. With this purpose, a set of WRF
simulations were completed using different driving data. On the first hand, a
recent present (1980-2017) simulation driven by the ERA-Interim reanalysis was
carried out to evaluate the suitability of the RCM performance. On the other
hand, two global climate models (GCMs) from the CMIP5 initiative, the CCSM4 and
the MPI-ESM-LR, were used as driving data to evaluate the GCM-RCM couplings,
which is essential to climate change applications. Finally, projected changes
were also investigated for a near-term future (2021-2050) paradigm. In general,
the results pointed out the WRF model as a valuable tool to study the
spatiotemporal patterns of the SFCEVP in the IP, showing an overall and
acceptable ability at different spatial and temporal scales. Concerning
projections, the results indicate that the IP is likely to undergo significant
changes in the SFCEVP in the near future. These changes will be more apparent
over the southernmost, and particularly during spring and summer, being in the
latter season the SFCEVP fundamentally reduced. These results agree with
projected changes in soil moisture, which is probably associated with changes
in precipitation patterns. Additionally, the results reveal the major role of
SFCEVP in modulating the climate over this region, which is involved in the
complex land-atmosphere processes.",http://arxiv.org/abs/2401.08697v1
The Climate Cost of Climate Investment: A Two-Period Perspective,2024-08-26T15:33:43Z,"Shaunak Kulkarni, Rohan Ajay Dubey","A one-size-fits-all paradigm that only adapts the scale and immediate outcome
of climate investment to economic circumstances will provide a short-lived,
economically inadequate response to climate issues; given the limited resources
allocated to green finance, it stands to reason that the shortcomings of this
will be exacerbated by the fact that it comes at the cost of long-term,
self-perpetuating, systemic solutions. Financial commitments that do not
consider the capital structure of green finance in an economy will cumulatively
dis-aggregate the economic cost of climate investment, to erode the competitive
advantage of the most innovative economies, while simultaneously imposing the
greatest financial burden on economies that are most vulnerable to the impact
of climate change; such disaggregation will also leave 'middle' economies in a
state of flux - honouring similar financial commitments to vulnerable or highly
developed peers, but unable to generate comparable return, yet sufficiently
insulated from the impact of extreme climate phenomena to not organically
develop solutions.
  In the face of these changing realities, green innovation needs to expand
beyond technology and address systemic inefficiencies - lack of clear
responsibility, ambiguously defined commitments, and inadequate checks &
balances to name a few.
  Clever application of financial engineering demonstrates promise, and simple
measures like carbon-credit exchanges have been effective in mitigating
imperfections at the grassroots level. We believe that information- and
incentive-centric systemic advancements can usher a fresh wave of green
innovation that stands on the shoulders of giants to ensure effective
implementation of technological breakthroughs; economic development that will
create an international community equipped with a robust framework to deal with
long-term crises in a strategic manner.",http://arxiv.org/abs/2408.14359v1
"Nonlocal, Pattern-aware Response and Feedback Framework for Regional
  Climate Change",2024-10-29T18:33:56Z,"Parvathi Kooloth, Jian Lu, Yi Huang, Derek DeSantis, Yiling Huo, Fukai Liu, Hailong Wang","We devise a pattern-aware feedback framework for representing the forced
climate response using a suite of Green's function experiments with solar
radiation perturbations. By considering the column energy balance, a
comprehensive linear response function (CLRF) forimportant climate variables
and feedback quantities such as moist static energy, sea surface temperature,
albedo, cloud optical depth, and lapse rate is learned from Green's function
data. The learned CLRF delineates the effects of the energy diffusion in both
the ocean and atmosphere and the pattern-aware feedbacks from the
aforementioned radiatively active processes. The CLRF can then be decomposed
into forcing-response mode pairs which are in turn used to construct a
reduced-order model (ROM) describing the dominant dynamics of climate
responses. These mode pairs capture nonlocal effects and teleconnections in the
climate and thus, make the ROM apt for capturing regional features of climate
change response. A key observation is that the CLRF captures the polar
amplified response as the most excitable mode of the climate system and this
mode is explainable in the data-learned pattern-aware feedback framework. The
ROM can be used for predicting the response for a given forcing and for
reconstructing the forcing from a given response; we demonstrate these
capabilities for independent forcing pattern.",http://arxiv.org/abs/2410.22450v1
"Impacts of Climate Change on Mortality: An extrapolation of temperature
  effects based on time series data in France",2024-06-04T07:37:31Z,"Quentin Guibert, Ga√´lle Pincemin, Fr√©d√©ric Planchet","Most contemporary mortality models rely on extrapolating trends or past
events. However, population dynamics will be significantly impacted by climate
change, notably the influence of temperatures on mortality. In this paper, we
introduce a novel approach to incorporate temperature effects on projected
mortality using a multi-population mortality model. This method combines a
stochastic mortality model with a climate epidemiology model, predicting
mortality variations due to daily temperature fluctuations, be it excesses or
insufficiencies. The significance of this approach lies in its ability to
disrupt mortality projections by utilizing temperature forecasts from climate
models and to assess the impact of this unaccounted risk factor in conventional
mortality models. We illustrate this proposed mortality model using French data
stratified by gender, focusing on past temperatures and mortality. Utilizing
climate model predictions across various IPCC scenarios, we investigate gains
and loss in life expectancy linked to temperatures and the additional mortality
induced by extreme heatwaves, and quantify them by assessing this new risk
factor in prediction intervals. Furthermore, we analyze the geographical
differences across the Metropolitan France.",http://arxiv.org/abs/2406.02054v3
"Substantial Risk of 21st Century AMOC Tipping even under Moderate
  Climate Change",2024-07-29T11:36:37Z,"Ren√© M. van Westen, Elian Y. P. Vanderborght, Michael Kliphuis, Henk A. Dijkstra","The Atlantic Meridional Overturning Circulation (AMOC) is a key component of
the climate system and considered to be a tipping element. There is still a
large uncertainty on the critical global warming level at which the AMOC will
start to collapse. Here we analyse targeted climate model simulations, together
with observations, reanalysis products and a suite of state-of-the-art climate
model results to reassess this critical global warming level. We find a
critical threshold of +3C global mean surface temperature increase compared to
pre-industrial with a lower bound of +2.2C (10%-Cl). Such global mean surface
temperature anomalies are expected to be reached after 2050. This means that
the AMOC is more likely than not (> 50%) to tip within the 21st century under a
middle-of-the-road climate change scenario and very likely (> 90%) to tip under
a high emissions scenario. The AMOC collapse induced cooling is shown to be
offset by the regional warming over Northwestern Europe during the 21st
century, but will still induce severe impacts on society.",http://arxiv.org/abs/2407.19909v1
"Spatio-temporal Multivariate Cluster Evolution Analysis for Detecting
  and Tracking Climate Impacts",2024-10-21T22:13:09Z,"Warren L. Davis IV, Max Carlson, Irina Tezaur, Diana Bull, Kara Peterson, Laura Swiler","Recent years have seen a growing concern about climate change and its
impacts. While Earth System Models (ESMs) can be invaluable tools for studying
the impacts of climate change, the complex coupling processes encoded in ESMs
and the large amounts of data produced by these models, together with the high
internal variability of the Earth system, can obscure important
source-to-impact relationships. This paper presents a novel and efficient
unsupervised data-driven approach for detecting statistically-significant
impacts and tracing spatio-temporal source-impact pathways in the climate
through a unique combination of ideas from anomaly detection, clustering and
Natural Language Processing (NLP). Using as an exemplar the 1991 eruption of
Mount Pinatubo in the Philippines, we demonstrate that the proposed approach is
capable of detecting known post-eruption impacts/events. We additionally
describe a methodology for extracting meaningful sequences of post-eruption
impacts/events by using NLP to efficiently mine frequent multivariate cluster
evolutions, which can be used to confirm or discover the chain of physical
processes between a climate source and its impact(s).",http://arxiv.org/abs/2410.16544v1
Building Interpretable Climate Emulators for Economics,2024-11-16T10:22:23Z,"Aryan Eftekhari, Doris Folini, Aleksandra Friedl, Felix K√ºbler, Simon Scheidegger, Olaf Schenk","This paper presents a framework for developing efficient and interpretable
carbon-cycle emulators (CCEs) as part of climate emulators in Integrated
Assessment Models, enabling economists to custom-build CCEs accurately
calibrated to advanced climate science. We propose a generalized
multi-reservoir linear box-model CCE that preserves key physical quantities and
can be use-case tailored for specific use cases. Three CCEs are presented for
illustration: the 3SR model (replicating DICE-2016), the 4PR model (including
the land biosphere), and the 4PR-X model (accounting for dynamic land-use
changes like deforestation that impact the reservoir's storage capacity).
Evaluation of these models within the DICE framework shows that land-use
changes in the 4PR-X model significantly impact atmospheric carbon and
temperatures -- emphasizing the importance of using tailored climate emulators.
By providing a transparent and flexible tool for policy analysis, our framework
allows economists to assess the economic impacts of climate policies more
accurately.",http://arxiv.org/abs/2411.10768v1
The future of offshore wind power production: wake and climate impacts,2024-08-27T11:10:37Z,"Simon C Warder, Matthew D Piggott","Rapid deployment of offshore wind is expected within the coming decades to
help meet climate goals. With offshore wind turbine lifetimes of 25-30 years,
and new offshore leases spanning 60 years, it is vital to consider long-term
changes in potential wind power resource at the farm planning stage. Such
changes may arise from multiple sources, including climate change, and
increasing wake-induced power losses. In this work, we investigate and compare
these two sources of long-term change in wind power, for a case study
consisting of 21 wind farms within the German Bight. Consistent with previous
studies, we find a small but significant reduction in wind resource due to
climate change by the end of the 21st century under the high-emission RCP8.5
scenario, compared with a historical period, with a mean power reduction (over
an ensemble of seven climate models) of 2.1%. To assess the impact of
wake-induced losses due to increasingly dense farm build-out, we model wakes
within the German Bight region using an engineering wake model, under various
stages of (planned) build-out corresponding to the years 2010-2027. By
identifying clusters of wind farms, we decompose wake effects into long-range
(inter-cluster), medium-range (intra-cluster) and short-range (intra-farm)
effects. Inter-cluster wake-induced losses increase from 0 for the 2010
scenario to 2.5% for the 2027 scenario, with intra-cluster losses also
increasing from 0 to 4.3%. Intra-farm losses are relatively constant, at around
13%. While the evolution of wake effects therefore outweighs the climate
effect, and impacts over a shorter timescale, both factors are significant. We
also find evidence of an interaction between the climate and wake effects. Both
climate change and evolving wake effects must therefore be considered within
resource assessment and wind farm planning.",http://arxiv.org/abs/2408.14963v2
"ClimateQ&A: Bridging the gap between climate scientists and the general
  public",2024-03-18T08:16:02Z,"Natalia De La Calzada, Th√©o Alves Da Costa, Annabelle Blangero, Nicolas Chesneau","This research paper investigates public views on climate change and
biodiversity loss by analyzing questions asked to the ClimateQ&A platform.
ClimateQ&A is a conversational agent that uses LLMs to respond to queries based
on over 14,000 pages of scientific literature from the IPCC and IPBES reports.
Launched online in March 2023, the tool has gathered over 30,000 questions,
mainly from a French audience. Its chatbot interface allows for the free
formulation of questions related to nature*. While its main goal is to make
nature science more accessible, it also allows for the collection and analysis
of questions and their themes. Unlike traditional surveys involving closed
questions, this novel method offers a fresh perspective on individual
interrogations about nature. Running NLP clustering algorithms on a sample of
3,425 questions, we find that a significant 25.8% inquire about how climate
change and biodiversity loss will affect them personally (e.g., where they live
or vacation, their consumption habits) and the specific impacts of their
actions on nature (e.g., transportation or food choices). This suggests that
traditional methods of surveying may not identify all existing knowledge gaps,
and that relying solely on IPCC and IPBES reports may not address all
individual inquiries about climate and biodiversity, potentially affecting
public understanding and action on these issues. *we use 'nature' as an
umbrella term for 'climate change' and 'biodiversity loss'",http://arxiv.org/abs/2403.14709v1
"Deep learning meets tree phenology modeling: PhenoFormer vs.
  process-based models",2024-10-30T15:40:55Z,"Vivien Sainte Fare Garnot, Lynsay Spafford, Jelle Lever, Christian Sigg, Barbara Pietragalla, Yann Vitasse, Arthur Gessler, Jan Dirk Wegner","Phenology, the timing of cyclical plant life events such as leaf emergence
and coloration, is crucial in the bio-climatic system. Climate change drives
shifts in these phenological events, impacting ecosystems and the climate
itself. Accurate phenology models are essential to predict the occurrence of
these phases under changing climatic conditions. Existing methods include
hypothesis-driven process models and data-driven statistical approaches.
Process models account for dormancy stages and various phenology drivers, while
statistical models typically rely on linear or traditional machine learning
techniques. Research shows that process models often outperform statistical
methods when predicting under climate conditions outside historical ranges,
especially with climate change scenarios. However, deep learning approaches
remain underexplored in climate phenology modeling. We introduce PhenoFormer, a
neural architecture better suited than traditional statistical methods at
predicting phenology under shift in climate data distribution, while also
bringing significant improvements or performing on par to the best performing
process-based models. Our numerical experiments on a 70-year dataset of 70,000
phenological observations from 9 woody species in Switzerland show that
PhenoFormer outperforms traditional machine learning methods by an average of
13% R2 and 1.1 days RMSE for spring phenology, and 11% R2 and 0.7 days RMSE for
autumn phenology, while matching or exceeding the best process-based models.
Our results demonstrate that deep learning has the potential to be a valuable
methodological tool for accurate climate-phenology prediction, and our
PhenoFormer is a first promising step in improving phenological predictions
before a complete understanding of the underlying physiological mechanisms is
available.",http://arxiv.org/abs/2410.23327v1
Projections of standardised energy indices in future climate scenarios,2024-10-21T22:30:29Z,"Edgar Dolores-Tesillos, Noelia Otero, Sam Allen","Renewable energy is becoming an increasingly important component of energy
systems. However, renewable energy production is heavily dependent on the
prevailing weather conditions, which are changing as a result of climate
change. It is therefore necessary to build energy systems that are robust to
energy shortages caused by weather-dependent changes to energy demand and
renewable energy production. To design such systems, we must monitor how
changes in the climate are expected to influence future energy production and
demand; this is important for policymakers to decide when, where, and by how
much renewable energy installed capacities should be increased, for example. In
this paper, we study the behaviour of standardised energy indices in future
European climate projections, and use this to monitor how characteristics of
energy production droughts in Europe are expected to change in the future. We
use these results to make suggestions regarding how the energy mix should be
adapted in the future to decrease the risk of energy production droughts.",http://arxiv.org/abs/2410.16556v2
"Exploring Climate Change Discourse: Measurements and Analysis of Reddit
  Data",2024-12-02T04:35:27Z,"Smriti Janaswamy, Jeremy Blackburn","Social media is very popular for facilitating conversations about important
topics and bringing forth insights and issues related to these topics. Reddit
serves as a platform that fosters social interactions and hosts engaging
discussions on a wide array of topics, thus forming narratives around these
topics. One such topic is climate change. There are extensive discussions on
Reddit about climate change, indicating high interest in its various aspects.
In this paper, we explore 11 subreddits that discuss climate change for the
duration of 2014 to 2022 and conduct a data-driven analysis of the posts on
these subreddits. We present a basic characterization of the data and show the
distribution of the posts and authors across our dataset for all years.
Additionally, we analyze user engagement metrics like scores for the posts and
how they change over time. We also offer insights into the topics of discussion
across the subreddits, followed by entities referenced throughout the dataset.",http://arxiv.org/abs/2412.01111v1
Emerging AI-based weather prediction models as downscaling tools,2024-06-25T23:19:59Z,"Nikolay Koldunov, Thomas Rackow, Christian Lessig, Sergey Danilov, Suvarchal K. Cheedela, Dmitry Sidorenko, Irina Sandu, Thomas Jung","The demand for high-resolution information on climate change is critical for
accurate projections and decision-making. Presently, this need is addressed
through high-resolution climate models or downscaling. High-resolution models
are computationally demanding and creating ensemble simulations with them is
typically prohibitively expensive. Downscaling methods are more affordable but
are typically limited to small regions. This study proposes the use of existing
AI-based numerical weather prediction systems (AI-NWP) to perform global
downscaling of climate information from low-resolution climate models. Our
results demonstrate that AI-NWP initalized from low-resolution initial
conditions can develop detailed forecasts closely resembling the resolution of
the training data using a one day lead time. We constructed year-long
atmospheric fields using AI-NWP forecasts initialized from smoothed ERA5 and
low-resolution CMIP6 models. Our analysis for 2-metre temperature indicates
that AI-NWP can generate high-quality, long-term datasets and potentially
perform bias correction, bringing climate model outputs closer to observed
data. The study highlights the potential for off-the-shelf AI-NWP to enhance
climate data downscaling, offering a simple and computationally efficient
alternative to traditional downscaling techniques. The downscaled data can be
used either directly for localized climate information or as boundary
conditions for further dynamical downscaling.",http://arxiv.org/abs/2406.17977v1
Machine Learning for the Physics of Climate,2024-08-19T01:19:16Z,"Annalisa Bracco, Julien Brajard, Henk A. Dijkstra, Pedram Hassanzadeh, Christian Lessig, Claire Monteleoni","An exponential growth in computing power, which has brought more
sophisticated and higher resolution simulations of the climate system, and an
exponential increase in observations since the first weather satellite was put
in orbit, are revolutionizing climate science. Big data and associated
algorithms, coalesced under the field of Machine Learning (ML), offer the
opportunity to study the physics of the climate system in ways, and with an
amount of detail, infeasible few years ago. The inference provided by ML has
allowed to ask causal questions and improve prediction skills beyond classical
barriers. Furthermore, when paired with modeling experiments or robust research
in model parameterizations, ML is accelerating computations, increasing
accuracy and allowing for generating very large ensembles at a fraction of the
cost. In light of the urgency imposed by climate change and the rapidly growing
role of ML, we review its broader accomplishments in climate physics. Decades
long standing problems in observational data reconstruction, representation of
sub-grid scale phenomena and climate (and weather) prediction are being tackled
with new and justified optimism. Ultimately, this review aims at providing a
perspective on the benefits and major challenges of exploiting ML in studying
complex systems.",http://arxiv.org/abs/2408.09627v1
Self Supervised Vision for Climate Downscaling,2024-01-09T10:20:49Z,"Karandeep Singh, Chaeyoon Jeong, Naufal Shidqi, Sungwon Park, Arjun Nellikkattil, Elke Zeller, Meeyoung Cha","Climate change is one of the most critical challenges that our planet is
facing today. Rising global temperatures are already bringing noticeable
changes to Earth's weather and climate patterns with an increased frequency of
unpredictable and extreme weather events. Future projections for climate change
research are based on Earth System Models (ESMs), the computer models that
simulate the Earth's climate system. ESMs provide a framework to integrate
various physical systems, but their output is bound by the enormous
computational resources required for running and archiving higher-resolution
simulations. For a given resource budget, the ESMs are generally run on a
coarser grid, followed by a computationally lighter $downscaling$ process to
obtain a finer-resolution output. In this work, we present a deep-learning
model for downscaling ESM simulation data that does not require high-resolution
ground truth data for model optimization. This is realized by leveraging
salient data distribution patterns and the hidden dependencies between weather
variables for an $\textit{individual}$ data point at $\textit{runtime}$.
Extensive evaluation with $2$x, $3$x, and $4$x scaling factors demonstrates
that the proposed model consistently obtains superior performance over that of
various baselines. The improved downscaling performance and no dependence on
high-resolution ground truth data make the proposed method a valuable tool for
climate research and mark it as a promising direction for future research.",http://arxiv.org/abs/2401.09466v1
"Crafting desirable climate trajectories with RL explored
  socio-environmental simulations",2024-10-09T13:21:50Z,"James Rudd-Jones, Fiona Thendean, Mar√≠a P√©rez-Ortiz","Climate change poses an existential threat, necessitating effective climate
policies to enact impactful change. Decisions in this domain are incredibly
complex, involving conflicting entities and evidence. In the last decades,
policymakers increasingly use simulations and computational methods to guide
some of their decisions. Integrated Assessment Models (IAMs) are one of such
methods, which combine social, economic, and environmental simulations to
forecast potential policy effects. For example, the UN uses outputs of IAMs for
their recent Intergovernmental Panel on Climate Change (IPCC) reports.
Traditionally these have been solved using recursive equation solvers, but have
several shortcomings, e.g. struggling at decision making under uncertainty.
Recent preliminary work using Reinforcement Learning (RL) to replace the
traditional solvers shows promising results in decision making in uncertain and
noisy scenarios. We extend on this work by introducing multiple interacting RL
agents as a preliminary analysis on modelling the complex interplay of
socio-interactions between various stakeholders or nations that drives much of
the current climate crisis. Our findings show that cooperative agents in this
framework can consistently chart pathways towards more desirable futures in
terms of reduced carbon emissions and improved economy. However, upon
introducing competition between agents, for instance by using opposing reward
functions, desirable climate futures are rarely reached. Modelling competition
is key to increased realism in these simulations, as such we employ policy
interpretation by visualising what states lead to more uncertain behaviour, to
understand algorithm failure. Finally, we highlight the current limitations and
avenues for further work to ensure future technology uptake for policy
derivation.",http://arxiv.org/abs/2410.07287v1
Motivated Reasoning and the Political Economy of Climate Change Inaction,2024-10-28T13:00:42Z,Philipp Denter,"Two office-driven politicians compete in an election by proposing policies.
There are two possible states of the world: climate change is either mild, with
no lasting effect on welfare if addressed properly, or severe, leading to
reduced welfare even with appropriate measures. Voters receive signals about
the state but may interpret them in a non-Bayesian way, holding motivated
beliefs. An equilibrium always exists where voters ignore signals suggesting
severe consequences, causing politicians to propose policies for mild climate
change -- even when they know otherwise. If severe climate change leads to only
moderate welfare losses, another efficient equilibrium exists. In this
equilibrium, voters trust politicians to choose the optimal policies, implying
voters choose to trust their signals, which in turn encourages optimal policy
choices by politicians. The model highlights the role of political rhetoric and
trust in government, and a first glance at the data reveals patterns consistent
with the models predictions.",http://arxiv.org/abs/2410.20982v1
"Promoting Reliable Knowledge about Climate Change: A Systematic Review
  of Effective Measures to Resist Manipulation on Social Media",2024-10-31T10:58:38Z,"Aliaksandr Herasimenka, Xianlingchen Wang, Ralph Schroeder","We present a systematic review of peer-reviewed research into ways to
mitigate manipulative information about climate change on social media. Such
information may include disinformation, harmful influence campaigns, or the
unintentional spread of misleading information. We find that commonly
recommended approaches to addressing manipulation about climate change include
corrective information sharing and education campaigns targeting media
literacy. However, most relevant research fails to test the approaches and
interventions it proposes. We locate research gaps that include the lack of
attention to large commercial and political entities involved in generating and
disseminating manipulation, video- and image-focused platforms, and
computational methods to collect and analyze data. Evidence drawn from many
studies demonstrates an emerging consensus about policies required to promote
reliable knowledge about climate change and resist manipulation.",http://arxiv.org/abs/2410.23814v1
"A Techno-Economic Analysis of the Interconnectedness between Energy
  Resources, Climate Change, and Sustainable Development",2024-12-16T17:34:23Z,"MohammadReza Askari, Navid Parsa","Abstract: The rising global temperatures caused by climate change
significantly impact energy consumption and electricity generation. Fluctuating
temperatures and frequent extreme weather events disrupt energy production and
consumption patterns. Addressing these challenges has become a priority,
prompting governments, industries, and societies to pursue sustainable
development and embrace eco-friendly economies. This strategy aims to decouple
economic growth from environmental harm, ensuring a sustainable future for
generations. Understanding the link between climate change, energy resources,
and sustainable development is crucial. Techno-economic analysis provides a
framework for evaluating energy-related projects and policies, guiding
decision-makers toward sustainable solutions. A case study highlights the
interaction between hydroponic unit energy needs, electricity pricing from wind
farms, and product sales prices. Findings suggest that smaller 2-megawatt
investments are more efficient and adaptable than larger 18-megawatt projects,
proving economically viable and technologically flexible. However, such
investments must also consider their social and environmental impacts on local
communities. Sustainable development seeks to ensure that progress benefits all
stakeholders while protecting the environment. Achieving this requires
collaboration among governments, businesses, researchers, and individuals. By
fostering innovation, adopting eco-friendly practices, and creating supportive
policies, society can transition to a green economy, mitigating climate change
and promoting a sustainable, resilient future.",http://arxiv.org/abs/2412.12235v1
"Comparing Data-Driven and Mechanistic Models for Predicting Phenology in
  Deciduous Broadleaf Forests",2024-01-08T15:29:23Z,"Christian Reimers, David Hafezi Rachti, Guahua Liu, Alexander J. Winkler","Understanding the future climate is crucial for informed policy decisions on
climate change prevention and mitigation. Earth system models play an important
role in predicting future climate, requiring accurate representation of complex
sub-processes that span multiple time scales and spatial scales. One such
process that links seasonal and interannual climate variability to cyclical
biological events is tree phenology in deciduous broadleaf forests.
Phenological dates, such as the start and end of the growing season, are
critical for understanding the exchange of carbon and water between the
biosphere and the atmosphere. Mechanistic prediction of these dates is
challenging. Hybrid modelling, which integrates data-driven approaches into
complex models, offers a solution. In this work, as a first step towards this
goal, train a deep neural network to predict a phenological index from
meteorological time series. We find that this approach outperforms traditional
process-based models. This highlights the potential of data-driven methods to
improve climate predictions. We also analyze which variables and aspects of the
time series influence the predicted onset of the season, in order to gain a
better understanding of the advantages and limitations of our model.",http://arxiv.org/abs/2401.03960v1
"Evaluating the transferability potential of deep learning models for
  climate downscaling",2024-07-17T12:10:24Z,"Ayush Prasad, Paula Harder, Qidong Yang, Prasanna Sattegeri, Daniela Szwarcman, Campbell Watson, David Rolnick","Climate downscaling, the process of generating high-resolution climate data
from low-resolution simulations, is essential for understanding and adapting to
climate change at regional and local scales. Deep learning approaches have
proven useful in tackling this problem. However, existing studies usually focus
on training models for one specific task, location and variable, which are
therefore limited in their generalizability and transferability. In this paper,
we evaluate the efficacy of training deep learning downscaling models on
multiple diverse climate datasets to learn more robust and transferable
representations. We evaluate the effectiveness of architectures zero-shot
transferability using CNNs, Fourier Neural Operators (FNOs), and vision
Transformers (ViTs). We assess the spatial, variable, and product
transferability of downscaling models experimentally, to understand the
generalizability of these different architecture types.",http://arxiv.org/abs/2407.12517v1
"Climate-Driven Doubling of U.S. Maize Loss Probability: Interactive
  Simulation with Neural Network Monte Carlo",2024-08-05T03:38:38Z,"A Samuel Pottinger, Lawson Connor, Brookie Guzder-Williams, Maya Weltman-Fahs, Nick Gondek, Timothy Bowles","Climate change not only threatens agricultural producers but also strains
related public agencies and financial institutions. These important food system
actors include government entities tasked with insuring grower livelihoods and
supporting response to continued global warming. We examine future risk within
the U.S. Corn Belt geographic region for one such crucial institution: the U.S.
Federal Crop Insurance Program. Specifically, we predict the impacts of
climate-driven crop loss at a policy-salient ""risk unit"" scale. Built through
our presented neural network Monte Carlo method, simulations anticipate both
more frequent and more severe losses that would result in a costly doubling of
the annual probability of maize Yield Protection insurance claims at
mid-century. We also provide an open source pipeline and interactive
visualization tools to explore these results with configurable statistical
treatments. Altogether, we fill an important gap in current understanding for
climate adaptation by bridging existing historic yield estimation and climate
projection to predict crop loss metrics at policy-relevant granularity.",http://arxiv.org/abs/2408.02217v2
A Deconfounding Approach to Climate Model Bias Correction,2024-08-22T01:53:35Z,"Wentao Gao, Jiuyong Li, Debo Cheng, Lin Liu, Jixue Liu, Thuc Duy Le, Xiaojing Du, Xiongren Chen, Yanchang Zhao, Yun Chen","Global Climate Models (GCMs) are crucial for predicting future climate
changes by simulating the Earth systems. However, GCM outputs exhibit
systematic biases due to model uncertainties, parameterization simplifications,
and inadequate representation of complex climate phenomena. Traditional bias
correction methods, which rely on historical observation data and statistical
techniques, often neglect unobserved confounders, leading to biased results.
This paper proposes a novel bias correction approach to utilize both GCM and
observational data to learn a factor model that captures multi-cause latent
confounders. Inspired by recent advances in causality based time series
deconfounding, our method first constructs a factor model to learn latent
confounders from historical data and then applies them to enhance the bias
correction process using advanced time series forecasting models. The
experimental results demonstrate significant improvements in the accuracy of
precipitation outputs. By addressing unobserved confounders, our approach
offers a robust and theoretically grounded solution for climate model bias
correction.",http://arxiv.org/abs/2408.12063v1
"Tracing the impacts of Mount Pinatubo eruption on global climate using
  spatially-varying changepoint detection",2024-09-13T15:23:05Z,"Samantha Shi-Jun, Lyndsay Shand, Bo Li","Significant events such as volcanic eruptions can have global and long
lasting impacts on climate. These global impacts, however, are not uniform
across space and time. Understanding how the Mt. Pinatubo eruption affects
global and regional climate is of great interest for predicting impact on
climate due to similar events. We propose a Bayesian framework to
simultaneously detect and estimate spatially-varying temporal changepoints for
regional climate impacts. Our approach takes into account the diffusing nature
of the changes caused by the volcanic eruption and leverages spatial
correlation. We illustrate our method on simulated datasets and compare it with
an existing changepoint detection method. Finally, we apply our method on
monthly stratospheric aerosol optical depth and surface temperature data from
1985 to 1995 to detect and estimate changepoints following the 1991 Mt.
Pinatubo eruption.",http://arxiv.org/abs/2409.08908v1
"Urban Computing for Climate and Environmental Justice: Early
  Perspectives From Two Research Initiatives",2024-10-06T00:32:03Z,"Carolina Veiga, Ashish Sharma, Daniel de Oliveira, Marcos Lage, Fabio Miranda","The impacts of climate change are intensifying existing vulnerabilities and
disparities within urban communities around the globe, as extreme weather
events, including floods and heatwaves, are becoming more frequent and severe,
disproportionately affecting low-income and underrepresented groups. Tackling
these increasing challenges requires novel approaches that integrate expertise
across multiple domains, including computer science, engineering, climate
science, and public health. Urban computing can play a pivotal role in these
efforts by integrating data from multiple sources to support decision-making
and provide actionable insights into weather patterns, infrastructure
weaknesses, and population vulnerabilities. However, the capacity to leverage
technological advancements varies significantly between the Global South and
Global North. In this paper, we present two multiyear, multidisciplinary
projects situated in Chicago, USA and Niter\'oi, Brazil, highlighting the
opportunities and limitations of urban computing in these diverse contexts.
Reflecting on our experiences, we then discuss the essential requirements, as
well as existing gaps, for visual analytics tools that facilitate the
understanding and mitigation of climate-related risks in urban environments.",http://arxiv.org/abs/2410.04318v1
"Towards Kriging-informed Conditional Diffusion for Regional Sea-Level
  Data Downscaling",2024-10-21T04:24:10Z,"Subhankar Ghosh, Arun Sharma, Jayant Gupta, Aneesh Subramanian, Shashi Shekhar","Given coarser-resolution projections from global climate models or satellite
data, the downscaling problem aims to estimate finer-resolution regional
climate data, capturing fine-scale spatial patterns and variability.
Downscaling is any method to derive high-resolution data from low-resolution
variables, often to provide more detailed and local predictions and analyses.
This problem is societally crucial for effective adaptation, mitigation, and
resilience against significant risks from climate change. The challenge arises
from spatial heterogeneity and the need to recover finer-scale features while
ensuring model generalization. Most downscaling methods \cite{Li2020} fail to
capture the spatial dependencies at finer scales and underperform on real-world
climate datasets, such as sea-level rise. We propose a novel Kriging-informed
Conditional Diffusion Probabilistic Model (Ki-CDPM) to capture spatial
variability while preserving fine-scale features. Experimental results on
climate data show that our proposed method is more accurate than
state-of-the-art downscaling techniques.",http://arxiv.org/abs/2410.15628v3
Zero-shot Microclimate Prediction with Deep Learning,2024-01-05T06:46:56Z,"Iman Deznabi, Peeyush Kumar, Madalina Fiterau","Weather station data is a valuable resource for climate prediction, however,
its reliability can be limited in remote locations. To compound the issue,
making local predictions often relies on sensor data that may not be accessible
for a new, previously unmonitored location. In response to these challenges, we
propose a novel zero-shot learning approach designed to forecast various
climate measurements at new and unmonitored locations. Our method surpasses
conventional weather forecasting techniques in predicting microclimate
variables by leveraging knowledge extracted from other geographic locations.",http://arxiv.org/abs/2401.02665v1
"The Language of Weather: Social Media Reactions to Weather Accounting
  for Climatic and Linguistic Baselines",2024-07-10T14:08:24Z,"James C. Young, Rudy Arthur, Hywel T. P. Williams","This study explores how different weather conditions influence public
sentiment on social media, focusing on Twitter data from the UK. By considering
climate and linguistic baselines, we improve the accuracy of weather-related
sentiment analysis. Our findings show that emotional responses to weather are
complex, influenced by combinations of weather variables and regional language
differences. The results highlight the importance of context-sensitive methods
for better understanding public mood in response to weather, which can enhance
impact-based forecasting and risk communication in the context of climate
change.",http://arxiv.org/abs/2407.07683v1
"Capturing Climatic Variability: Using Deep Learning for Stochastic
  Downscaling",2024-05-31T03:04:10Z,"Kiri Daust, Adam Monahan","Adapting to the changing climate requires accurate local climate information,
a computationally challenging problem. Recent studies have used Generative
Adversarial Networks (GANs), a type of deep learning, to learn complex
distributions and downscale climate variables efficiently. Capturing
variability while downscaling is crucial for estimating uncertainty and
characterising extreme events - critical information for climate adaptation.
Since downscaling is an undetermined problem, many fine-scale states are
physically consistent with the coarse-resolution state. To quantify this
ill-posed problem, downscaling techniques should be stochastic, able to sample
realisations from a high-resolution distribution conditioned on low-resolution
input. Previous stochastic downscaling attempts have found substantial
underdispersion, with models failing to represent the full distribution. We
propose approaches to improve the stochastic calibration of GANs in three ways:
a) injecting noise inside the network, b) adjusting the training process to
explicitly account for the stochasticity, and c) using a probabilistic loss
metric. We tested our models first on a synthetic dataset with known
distributional properties, and then on a realistic downscaling scenario,
predicting high-resolution wind components from low-resolution climate
covariates. Injecting noise, on its own, substantially improved the quality of
conditional and full distributions in tests with synthetic data, but performed
less well for wind field downscaling, where models remained underdispersed. For
wind downscaling, we found that adjusting the training method and including the
probabilistic loss improved calibration. The best model, with all three
changes, showed much improved skill at capturing the full variability of the
high-resolution distribution and thus at characterising extremes.",http://arxiv.org/abs/2406.02587v1
"A Few Hypocrites: Few-Shot Learning and Subtype Definitions for
  Detecting Hypocrisy Accusations in Online Climate Change Debates",2024-09-25T10:56:28Z,"Paulina Garcia Corral, Avishai Green, Hendrik Meyer, Anke Stoll, Xiaoyue Yan, Myrthe Reuver","The climate crisis is a salient issue in online discussions, and hypocrisy
accusations are a central rhetorical element in these debates. However, for
large-scale text analysis, hypocrisy accusation detection is an understudied
tool, most often defined as a smaller subtask of fallacious argument detection.
In this paper, we define hypocrisy accusation detection as an independent task
in NLP, and identify different relevant subtypes of hypocrisy accusations. Our
Climate Hypocrisy Accusation Corpus (CHAC) consists of 420 Reddit climate
debate comments, expert-annotated into two different types of hypocrisy
accusations: personal versus political hypocrisy. We evaluate few-shot
in-context learning with 6 shots and 3 instruction-tuned Large Language Models
(LLMs) for detecting hypocrisy accusations in this dataset. Results indicate
that the GPT-4o and Llama-3 models in particular show promise in detecting
hypocrisy accusations (F1 reaching 0.68, while previous work shows F1 of 0.44).
However, context matters for a complex semantic concept such as hypocrisy
accusations, and we find models struggle especially at identifying political
hypocrisy accusations compared to personal moral hypocrisy. Our study
contributes new insights in hypocrisy detection and climate change discourse,
and is a stepping stone for large-scale analysis of hypocrisy accusation in
online climate debates.",http://arxiv.org/abs/2409.16807v1
"Spatiotemporal Density Correction of Multivariate Global Climate Model
  Projections using Deep Learning",2024-11-27T22:55:48Z,"Reetam Majumder, Shiqi Fang, A. Sankarasubramanian, Emily C. Hector, Brian J. Reich","Global Climate Models (GCMs) are numerical models that simulate complex
physical processes within the Earth's climate system and are essential for
understanding and predicting climate change. However, GCMs suffer from systemic
biases due to simplifications made to the underlying physical processes. GCM
output therefore needs to be bias corrected before it can be used for future
climate projections. Most common bias correction methods, however, cannot
preserve spatial, temporal, or inter-variable dependencies. We propose a new
semi-parametric conditional density estimation (SPCDE) for density correction
of the joint distribution of daily precipitation and maximum temperature data
obtained from gridded GCM spatial fields. The Vecchia approximation is employed
to preserve dependencies in the observed field during the density correction
process, which is carried out using semi-parametric quantile regression. The
ability to calibrate joint distributions of GCM projections has potential
advantages not only in estimating extremes, but also in better estimating
compound hazards, like heat waves and drought, under potential climate change.
Illustration on historical data from 1951-2014 over two 5x5 spatial grids in
the US indicate that SPCDE can preserve key marginal and joint distribution
properties of precipitation and maximum temperature, and predictions obtained
using SPCDE are better calibrated compared to predictions using asynchronous
quantile mapping and canonical correlation analysis, two commonly used bias
correction approaches.",http://arxiv.org/abs/2411.18799v2
"Global Lightning-Ignited Wildfires Prediction and Climate Change
  Projections based on Explainable Machine Learning Models",2024-09-16T07:19:08Z,"Assaf Shmuel, Teddy Lazebnik, Oren Glickman, Eyal Heifetz, Colin Price","Wildfires pose a significant natural disaster risk to populations and
contribute to accelerated climate change. As wildfires are also affected by
climate change, extreme wildfires are becoming increasingly frequent. Although
they occur less frequently globally than those sparked by human activities,
lightning-ignited wildfires play a substantial role in carbon emissions and
account for the majority of burned areas in certain regions. While existing
computational models, especially those based on machine learning, aim to
predict lightning-ignited wildfires, they are typically tailored to specific
regions with unique characteristics, limiting their global applicability. In
this study, we present machine learning models designed to characterize and
predict lightning-ignited wildfires on a global scale. Our approach involves
classifying lightning-ignited versus anthropogenic wildfires, and estimating
with high accuracy the probability of lightning to ignite a fire based on a
wide spectrum of factors such as meteorological conditions and vegetation.
Utilizing these models, we analyze seasonal and spatial trends in
lightning-ignited wildfires shedding light on the impact of climate change on
this phenomenon. We analyze the influence of various features on the models
using eXplainable Artificial Intelligence (XAI) frameworks. Our findings
highlight significant global differences between anthropogenic and
lightning-ignited wildfires. Moreover, we demonstrate that, even over a short
time span of less than a decade, climate changes have steadily increased the
global risk of lightning-ignited wildfires. This distinction underscores the
imperative need for dedicated predictive models and fire weather indices
tailored specifically to each type of wildfire.",http://arxiv.org/abs/2409.10046v1
"Climate change denial and anti-science communities on brazilian
  Telegram: climate disinformation as a gateway to broader conspiracy networks",2024-08-27T17:14:27Z,Ergon Cugler de Moraes Silva,"Conspiracy theories related to climate change denial and anti-science have
found fertile ground on Telegram, particularly among Brazilian communities that
distrust scientific institutions and oppose global environmental policies. This
study seeks to answer the research question: how are Brazilian conspiracy
theory communities on climate change and anti-science themes characterized and
articulated on Telegram? It is worth noting that this study is part of a series
of seven studies aimed at understanding and characterizing Brazilian conspiracy
theory communities on Telegram. This series of studies is openly and originally
available on arXiv from Cornell University, applying a mirrored method across
all seven studies, changing only the thematic focus of analysis, and providing
replicable investigation methods, including custom-developed and proprietary
codes, contributing to the culture of open-source software. Regarding the main
findings of this study, the following observations were made: Climate change
denial and anti-science communities interact synergistically, creating a
complex network that mutually reinforces disinformation narratives; Apocalyptic
themes, such as Apocalypse and Survivalism, act as gateways to climate denial,
with 5,057 links directed to these communities; Anti-science communities
function as gatekeepers, distributing links evenly to theories such as the New
World Order and Globalism, among others; During the COVID-19 pandemic,
anti-science discussions experienced a significant peak, driven by vaccine
disinformation; The intersection between anti-science narratives and esoteric
beliefs reinforces the idea of a supposed alternative truth that challenges
science; Since 2022, discussions on climate change have evolved to align with
global domination theories; Additionally, the UN's 2030 Agenda is portrayed as
part of a global conspiracy.",http://arxiv.org/abs/2408.15311v2
International environmental treaties: An honest or a misguided effort,2024-04-11T09:06:19Z,"Reza Hafezi, David A. Wood, Firouzeh Rosa Taghikhah","Climate change and environmental concerns represent a global crisis
accompanied by significant economic challenges. Regular international
conferences held to address these issues, such as in the UK (2021) and Egypt
(2022), spark debate about the effectiveness and practicality of international
commitments. This study examines international treaties from a different
perspective, emphasizing the need to understand the power dynamics and
stakeholder interests that delay logical actions to mitigate anthropogenic
contributions to climate change and their impacts. Environmental and social
concerns tend to increase within nations as their economies develop, where they
fight to keep acceptable standards of living while reducing emissions volume.
So, nations play disproportionate roles in global decision-making based on the
size of their economies. Addressing climate change requires a paradigm shift to
emphasize acknowledging and adhering to global commitments through civil
pressure, rather than relying on traditional yet biased systems of
international political diplomacy. Here, climate-friendly actions are evaluated
and ideas to promote such activities are proposed. We introduce a ""transition
regime"" as a solution to this metastasis challenge which gradually infects all
nations.",http://arxiv.org/abs/2404.07574v1
"Contributions of greenhouse gases and solar activity to global climate
  change from CMIP6 models simulations",2024-06-08T13:12:34Z,"Igor I. Mokhov, Dmitry A. Smirnov","Quantitative estimates of the contributions of the anthropogenic forcing,
characterized by changes in the radiative forcing of atmospheric greenhouse
gases (CO2, in particular), and solar activity variations to the trends of the
global surface temperature on secular temporal horizons are obtained with the
aid of autoregressive models from simulations with climate models of the CMIP6
ensemble and from long-term observational data since the 19th century. The
results for the simulations with climate models characterized by low, medium
and high temperature sensitivity to changes in the CO2 content are compared. It
is found, in particular, that the estimates from observation data revealing the
determinative contribution of the CO2 content to the global surface temperature
trends on half-century and century-long time intervals are most consistent with
the estimates from simulations with the climate model with the lowest
sensitivity of the global surface temperature to doubling the CO2 atmospheric
content.",http://arxiv.org/abs/2406.05468v1
"Mapping global offshore wind resource: wake losses, optimisation
  potential and climate effects",2024-08-27T12:57:17Z,"Simon C Warder, Matthew D Piggott","In this work, we assess global offshore wind energy resources, wake-induced
losses, array layout optimisation potential and climate change impacts. We
first map global offshore ambient wind resource from reanalysis data. We
estimate wake-induced losses using an engineering wake model, revealing that
locations with low (high) resource typically experience larger (smaller)
percentage losses. However, we further find that the specific wind speed
distribution is important, with narrower distributions generally leading to
greater losses. This is due to the overlap between the wind speed distribution
and the high-sensitivity region of the turbine thrust and power curves.
Broadly, this leads to much stronger wake-induced losses in the tropics (which
experience the trade winds) than mid-latitudes. However, the tropics also
experience a narrower wind direction distribution; our results demonstrate that
this leads to greater potential for mitigation of wake effects via layout
optimisation. Finally, we assess projected changes in wind resource and wake
losses due to climate change under a high-emission scenario. Many regions are
projected to decrease in ambient wind resources, and furthermore these regions
will typically experience greater wake-induced losses, exacerbating the climate
impact. These results highlight the different challenges and opportunities
associated with exploiting offshore wind resources across the globe.",http://arxiv.org/abs/2408.15028v1
"AI-driven weather forecasts enable anticipated attribution of extreme
  events to human-made climate change",2024-08-29T10:48:46Z,"Bernat Jim√©nez-Esteve, David Barriopedro, Juan Emmanuel Johnson, Ricardo Garcia-Herrera","Anthropogenic climate change (ACC) is altering the frequency and intensity of
extreme weather events. Attributing individual extreme events (EEs) to ACC is
becoming crucial to assess the risks of climate change. Traditional attribution
methods often suffer from a selection bias, are computationally demanding, and
provide answers after the EE occurs. This study presents a ground-breaking
hybrid attribution method by combining physics-based ACC estimates from global
climate models with deep-learning weather forecasts. This hybrid approach
circumvents the framing choices and accelerates the attribution process, paving
the way for operational anticipated global forecast-based attribution. We apply
this methodology to three distinct high-impact weather EEs. Despite some
limitations in predictability, the method uncovers ACC fingerprints in the
forecasted fields of EEs. Specifically, forecasts successfully anticipate that
ACC exacerbated the 2018 Iberian heatwave, deepened hurricane Florence, and
intensified the wind and precipitable water of the explosive cyclone Ciar\'an.",http://arxiv.org/abs/2408.16433v1
Mapping climate change awareness through spatial hierarchical clustering,2024-09-16T12:03:25Z,"Gianpaolo Zammarchi, Paolo Maranzano","Climate change is a critical issue that will be in the political agenda for
the next decades. While it is important for this topic to be discussed at
higher levels, it is also of paramount importance that the populations became
aware of the problem. As different countries may face more or less severe
repercussions, it is also useful to understand the degree of awareness of
specific populations. In this paper, we present a geographically-informed
hierarchical clustering analysis aimed at identify groups of countries with a
similar level of climate change awareness. We employ a Ward-like clustering
algorithm that combines information pertaining climate change awareness,
socio-economic factors, climate-related characteristics of different countries,
and the physical distances between countries. To choose suitable values for the
clustering hyperparameters, we propose a customized algorithm that takes into
account the within-clusters homogeneity, the between-clusters separation and
that explicitly compares the geographically-informed and non-geographical
partitioning. The results show that the geographically-informed clustering
provides more stability of the partitions and leads to interpretable and
geographically-compact aggregations compared to a clustering in which the
geographical component is absent. In particular, we identify a clear contrast
among Western countries, characterized by high and compact awareness, and
Asian, African, and Middle Eastern countries having greater variability but
still lower awareness.",http://arxiv.org/abs/2409.13760v1
"Deciphering public attention to geoengineering and climate issues using
  machine learning and dynamic analysis",2024-05-11T13:10:57Z,"Ramit Debnath, Pengyu Zhang, Tianzhu Qin, R. Michael Alvarez, Shaun D. Fitzgerald","As the conversation around using geoengineering to combat climate change
intensifies, it is imperative to engage the public and deeply understand their
perspectives on geoengineering research, development, and potential deployment.
Through a comprehensive data-driven investigation, this paper explores the
types of news that captivate public interest in geoengineering. We delved into
30,773 English-language news articles from the BBC and the New York Times,
combined with Google Trends data spanning 2018 to 2022, to explore how public
interest in geoengineering fluctuates in response to news coverage of broader
climate issues. Using BERT-based topic modeling, sentiment analysis, and
time-series regression models, we found that positive sentiment in
energy-related news serves as a good predictor of heightened public interest in
geoengineering, a trend that persists over time. Our findings suggest that
public engagement with geoengineering and climate action is not uniform, with
some topics being more potent in shaping interest over time, such as climate
news related to energy, disasters, and politics. Understanding these patterns
is crucial for scientists, policymakers, and educators aiming to craft
effective strategies for engaging with the public and fostering dialogue around
emerging climate technologies.",http://arxiv.org/abs/2405.07010v1
"Towards Physically Consistent Deep Learning For Climate Model
  Parameterizations",2024-06-06T10:02:49Z,"Birgit K√ºhbacher, Fernando Iglesias-Suarez, Niki Kilbertus, Veronika Eyring","Climate models play a critical role in understanding and projecting climate
change. Due to their complexity, their horizontal resolution of about 40-100 km
remains too coarse to resolve processes such as clouds and convection, which
need to be approximated via parameterizations. These parameterizations are a
major source of systematic errors and large uncertainties in climate
projections. Deep learning (DL)-based parameterizations, trained on data from
computationally expensive short, high-resolution simulations, have shown great
promise for improving climate models in that regard. However, their lack of
interpretability and tendency to learn spurious non-physical correlations
result in reduced trust in the climate simulation. We propose an efficient
supervised learning framework for DL-based parameterizations that leads to
physically consistent models with improved interpretability and negligible
computational overhead compared to standard supervised training. First, key
features determining the target physical processes are uncovered. Subsequently,
the neural network is fine-tuned using only those relevant features. We show
empirically that our method robustly identifies a small subset of the inputs as
actual physical drivers, therefore removing spurious non-physical
relationships. This results in by design physically consistent and
interpretable neural networks while maintaining the predictive performance of
unconstrained black-box DL-based parameterizations.",http://arxiv.org/abs/2406.03920v4
Vegetation-climate feedbacks across scales,2024-09-07T17:19:36Z,"Diego G. Miralles, Jordi Vila-Guerau de Arellano, Tim R. McVicar, Miguel D. Mahecha","Vegetation often understood merely as the result of long-term climate
conditions. However, vegetation itself plays a fundamental role in shaping
Earth's climate by regulating the energy, water, and biogeochemical cycles
across terrestrial landscapes. It exerts influence by altering surface
roughness, consuming significant water resources through transpiration and
interception, lowering atmospheric CO2 concentration, and controlling net
radiation and its partitioning into sensible and latent heat fluxes. This
influence propagates through the atmosphere, from microclimate scales to the
entire atmospheric boundary layer, subsequently impacting large-scale
circulation and the global transport of heat and moisture. Understanding the
feedbacks between vegetation and atmosphere across multiple scales is crucial
for predicting the influence of land use and cover changes and for accurately
representing these processes in climate models. This short review aims to
discuss the mechanisms through which vegetation modulates climate across
spatial and temporal scales. Particularly, we evaluate the influence of
vegetation on circulation patterns, precipitation and temperature, both in
terms of trends and extreme events, such as droughts and heatwaves. The main
goal is to highlight the state of science and review recent studies that may
help advance our collective understanding of vegetation feedbacks and the role
they play in climate.",http://arxiv.org/abs/2409.04872v1
"Automated Fact-Checking of Climate Change Claims with Large Language
  Models",2024-01-23T08:49:23Z,"Markus Leippold, Saeid Ashraf Vaghefi, Dominik Stammbach, Veruska Muccione, Julia Bingler, Jingwei Ni, Chiara Colesanti-Senni, Tobias Wekhof, Tobias Schimanski, Glen Gostlow, Tingyu Yu, Juerg Luterbacher, Christian Huggel","This paper presents Climinator, a novel AI-based tool designed to automate
the fact-checking of climate change claims. Utilizing an array of Large
Language Models (LLMs) informed by authoritative sources like the IPCC reports
and peer-reviewed scientific literature, Climinator employs an innovative
Mediator-Advocate framework. This design allows Climinator to effectively
synthesize varying scientific perspectives, leading to robust, evidence-based
evaluations. Our model demonstrates remarkable accuracy when testing claims
collected from Climate Feedback and Skeptical Science. Notably, when
integrating an advocate with a climate science denial perspective in our
framework, Climinator's iterative debate process reliably converges towards
scientific consensus, underscoring its adeptness at reconciling diverse
viewpoints into science-based, factual conclusions. While our research is
subject to certain limitations and necessitates careful interpretation, our
approach holds significant potential. We hope to stimulate further research and
encourage exploring its applicability in other contexts, including political
fact-checking and legal domains.",http://arxiv.org/abs/2401.12566v1
Temporal and spatial downscaling for solar radiation,2024-05-17T18:41:51Z,"Maggie Bailey, Doug Nychka, Manajit Sengupta, Jaemo Yang, Soutir Bandyopadhyay","Global and regional climate model projections are useful for gauging future
patterns of climate variables, including solar radiation, but data from these
models is often too coarse to assess local impacts. Within the context of solar
radiation, the changing climate may have an effect on photovoltaic (PV)
production, especially as the PV industry moves to extend plant lifetimes to 50
years. Predicting PV production while taking into account a changing climate
requires data at a resolution that is useful for building PV plants. Although
temporal and spatial downscaling of solar radiation data is widely studied, we
present a novel method to downscale solar radiation data from daily averages to
hourly profiles, while maintaining spatial correlation of parameters
characterizing the diurnal profile of solar radiation. The method focuses on
the use of a diurnal template which can be shifted and scaled according to the
time or year and location and the use of thin plate splines for spatial
downscaling. This analysis is applied to data from the National Solar Radiation
Database housed at the National Renewable Energy Lab and a case study of the
mentioned methods over several sub-regions of continental United States is
presented.",http://arxiv.org/abs/2405.11046v1
"Bridging Climate Awareness and Sustainable Entrepreneurship: A
  Conceptual Framework Based on the Theory of Planned Behavior",2024-07-23T21:02:42Z,"Muhammad Rofiqul Islam, Abdullah Al Mehdi","Many studies have examined the connection between the intention to start a
business and environmental values. However, there still needs to be more
knowledge in the extant literature about how climate change campaigns influence
sustainable entrepreneurial intention. This study uses the Theory of Planned
Behavior (TPB) to develop a theoretical framework to explain how climate change
campaigns affect the intention to start a sustainable business. This
interdisciplinary conceptual research model bridges the gap between climate
awareness, sustainable values, and entrepreneurial intentions, offering a
robust framework for understanding and fostering sustainable entrepreneurial
behaviors. Our study lays the groundwork for future empirical studies and
real-world interventions to advance sustainability through entrepreneurship.",http://arxiv.org/abs/2407.16838v1
"Are Deep Learning Methods Suitable for Downscaling Global Climate
  Projections? Review and Intercomparison of Existing Models",2024-11-06T18:05:45Z,"Jose Gonz√°lez-Abad, Jos√© Manuel Guti√©rrez","Deep Learning (DL) has shown promise for downscaling global climate change
projections under different approaches, including Perfect Prognosis (PP) and
Regional Climate Model (RCM) emulation. Unlike emulators, PP downscaling models
are trained on observational data, so it remains an open question whether they
can plausibly extrapolate unseen conditions and changes in future emissions
scenarios. Here we focus on this problem as the main drawback for the
operationalization of these methods and present the results of 1) a literature
review to identify state-of-the-art DL models for PP downscaling and 2) an
intercomparison experiment to evaluate the performance of these models and to
assess their extrapolation capability using a common experimental framework,
taking into account the sensitivity of results to different training replicas.
We focus on minimum and maximum temperatures and precipitation over Spain, a
region with a range of climatic conditions with different influential regional
processes. We conclude with a discussion of the findings, limitations of
existing methods, and prospects for future development.",http://arxiv.org/abs/2411.05850v1
"Unravelling compound risks of hydrological extremes in a changing
  climate: Typology, methods and futures",2024-09-20T16:14:14Z,"Kwok P Chun, Thanti Octavianti, Georgia Papacharalampous, Hristos Tyralis, Samuel J. Sutanto, Pavel Terskii, Paola Mazzoglio, Dario Treppiedi, Juan Rivera, Nilay Dogulu, Adeyemi Olusola, Bastien Dieppois, Moctar Demb√©l√©, Simon Moulds, Cheng Li, Luis Alejandro Morales-Marin, Neil Macdonald, Toundji Olivier Amoussou, Roland Yonaba, Salomon Obahoundje, Nicolas Massei, David M. Hannah, Sivarama Krishna Reddy Chidepudi, Byman Hamududu","We have witnessed and experienced increasing compound extreme events
resulting from simultaneous or sequential occurrence of multiple events in a
changing climate. In addition to a growing demand for a clearer explanation of
compound risks from a hydrological perspective, there has been a lack of
attention paid to socioeconomic factors driving and impacted by these risks.
Through a critical review and co-production approaches, we identified four
types of compound hydrological events based on autocorrelated, multivariate,
and spatiotemporal patterns. A framework to quantify compound risks based on
conditional probability is offered, including an argument on the potential use
of generative Artificial Intelligence (AI) algorithms for identifying emerging
trends and patterns for climate change. Insights for practices are discussed,
highlighting the implications for disaster risk reduction and knowledge
co-production. Our argument centres on the importance of meaningfully
considering the socioeconomic contexts in which compound risks may have
impacts, and the need for interdisciplinary collaboration to effectively
translate climate science to climate actions.",http://arxiv.org/abs/2409.19003v1
"Droughts in Germany -- Why global climate change amplifies hydrological
  extremes",2024-03-25T08:54:46Z,Axel Kleidon,"The warmer temperatures of global climate change strengthen the water cycle,
evaporation and precipitation increase. But the extremes of heavy rain, floods,
dry periods and droughts will also increase. How does this fit together? Simple
physical considerations show which factors mainly regulate the strength of the
water cycle in the Earth system, and how this determines water availability on
land. This can be used to interpret the observed changes in the water balance
in Germany and explain the increasing dryness in Germany.",http://arxiv.org/abs/2403.16551v1
"MERCURY: A fast and versatile multi-resolution based global emulator of
  compound climate hazards",2024-12-24T04:56:21Z,"Shruti Nath, Julie Carreau, Kai Kornhuber, Peter Pfleiderer, Carl-Friedrich Schleussner, Philippe Naveau","High-impact climate damages are often driven by compounding climate
conditions. For example, elevated heat stress conditions can arise from a
combination of high humidity and temperature. To explore future changes in
compounding hazards under a range of climate scenarios and with large
ensembles, climate emulators can provide light-weight, data-driven complements
to Earth System Models. Yet, only a few existing emulators can jointly emulate
multiple climate variables. In this study, we present the Multi-resolution
EmulatoR for CompoUnd climate Risk analYsis: MERCURY. MERCURY extends
multi-resolution analysis to a spatio-temporal framework for versatile
emulation of multiple variables. MERCURY leverages data-driven, image
compression techniques to generate emulations in a memory-efficient manner.
MERCURY consists of a regional component that represents the monthly, regional
response of a given variable to yearly Global Mean Temperature (GMT) using a
probabilistic regression based additive model, resolving regional
cross-correlations. It then adapts a reverse lifting-scheme operator to jointly
spatially disaggregate regional, monthly values to grid-cell level. We
demonstrate MERCURY's capabilities on representing the humid-heat metric, Wet
Bulb Globe Temperature, as derived from temperature and relative humidity
emulations. The emulated WBGT spatial correlations correspond well to those of
ESMs and the 95% and 97.5% quantiles of WBGT distributions are well captured,
with an average of 5% deviation. MERCURY's setup allows for region-specific
emulations from which one can efficiently ""zoom"" into the grid-cell level
across multiple variables by means of the reverse lifting-scheme operator. This
circumvents the traditional problem of having to emulate complete,
global-fields of climate data and resulting storage requirements.",http://arxiv.org/abs/2501.04018v1
The Role of Electric Grid Research in Addressing Climate Change,2024-06-25T23:18:38Z,"Le Xie, Subir Majumder, Tong Huang, Qian Zhang, Ping Chang, David J. Hill, Mohammad Shahidehpour","Addressing the urgency of climate change necessitates a coordinated and
inclusive effort from all relevant stakeholders. Critical to this effort is the
modeling, analysis, control, and integration of technological innovations
within the electric energy system, which plays a crucial role in scaling up
climate change solutions. This perspective article presents a set of research
challenges and opportunities in the area of electric power systems that would
be crucial in accelerating Gigaton-level decarbonization. Furthermore, it
highlights institutional challenges associated with developing market
mechanisms and regulatory architectures, ensuring that incentives are aligned
for stakeholders to effectively implement the technological solutions on a
large scale.",http://arxiv.org/abs/2406.17976v2
"Impacts of Climate Change-Induced Salinity Intrusion on Physiological
  Parameters of Aquatic Hydrophytes from Coastal Rivers of Bangladesh",2024-12-07T01:02:32Z,"Ulfat Jahan Farha, Zarin Subah, Md Helal Uddin, Harunur Rashid","Changing temperature, precipitation regimes, and sea level rise, often
associated with climate change, cause salinity intrusion into groundwater and
surface water, affecting aquatic ecosystems. This study investigates the
impacts of salinity on the physiological traits of freshwater hydrophytes,
including Water Hyacinth (Eichhornia crassipes), Buffalo Spinach (Enhydra
fluctuans), and Taro (Colocasia esculenta). The plants were exposed to salinity
concentrations of 0, 10, 20, and 30 ppt for 48 hours. Parameters such as
biomass, stomata density, transpiration rate, chlorophyll content, relative
water content, and histo-architectural changes were analyzed. The results
showed a decline in biomass, stomatal density, and relative water content with
increasing salinity. Taro demonstrated higher salt tolerance compared to other
species. Histological observations revealed deformities in root and tuber
tissues under saline stress. These findings highlight the critical impacts of
climate change-induced salinity on aquatic plant ecosystems.",http://arxiv.org/abs/2412.05483v1
"Changes in mesoscale convective system precipitation structures in
  response to a warming climate",2024-06-14T20:50:35Z,"Wenjun Cui, Thomas Galarneau, Kimberly Hoogewind","Mesoscale convective systems (MCSs) are crucial components of the
hydrological cycle and often produce flash floods. Given their impact, it is
crucial to understand how they will change under a warming climate. This study
uses a satellite- and radar-based MCS tracking algorithm on
convection-permitting climate model simulations and examines changes in MCS
properties and precipitation structures between historical and future
simulations. An underestimation in MCS total precipitation is evident in
historical simulation compared to observations, due to model's depiction of MCS
precipitation area and summertime occurrence frequency. Under pseudo-global
warming, increases in MCS frequency and total warm season precipitation are
observed, most notably in the southern U.S. The precipitation intensity and
precipitating area generated by future MCSs also rises and results in an
increase in precipitation volume. MCS precipitation structures are further
classified into convective core and stratiform regions to understand how change
in these structures contributes to future rainfall changes. In a warmer
climate, the stratiform region demonstrates minimal change in size, but
increases in mean precipitation rate and mean maximum precipitation rate by 15%
and 29% are noted, respectively. A more robust future response is observed in
the convective core region, with its size, mean precipitation rate and mean
maximum precipitation rate increasing significantly by 24%, 37% and 42%,
respectively. Finally, by examining the environmental properties of MCS initial
condition, future intensification of convective rain may be attributed to a
combined effect of substantial increases in atmospheric instability and
moisture availability.",http://arxiv.org/abs/2406.10410v1
"Assessing Climate Transition Risks in the Colombian Processed Food
  Sector: A Fuzzy Logic and Multicriteria Decision-Making Approach",2024-04-13T21:49:49Z,"Juan F. P√©rez-P√©rez, Pablo Isaza G√≥mez, Isis Bonet, Mar√≠a Solange S√°nchez-Pinz√≥n, Fabio Caraffini, Christian Lochmuller","Climate risk assessment is becoming increasingly important. For
organisations, identifying and assessing climate-related risks is challenging,
as they can come from multiple sources. This study identifies and assesses the
main climate transition risks in the colombian processed food sector. As
transition risks are vague, our approach uses Fuzzy Logic and compares it to
various multi-criteria decision-making methods to classify the different
climate transition risks an organisation may be exposed to. This approach
allows us to use linguistic expressions for risk analysis and to better
describe risks and their consequences. The results show that the risks ranked
as the most critical for this organisation in their order were price volatility
and raw materials availability, the change to less carbon-intensive production
or consumption patterns, the increase in carbon taxes and technological change,
and the associated development or implementation costs. These risks show a
critical risk level, which implies that they are the most significant risks for
the organisation in the case study. These results highlight the importance of
investments needed to meet regulatory requirements, which are the main drivers
for organisations at the financial level.",http://arxiv.org/abs/2404.16055v1
"Feedback Processes causing an AMOC Collapse in the Community Earth
  System Model",2024-10-04T09:00:22Z,"Elian Vanderborght, Ren√© M. van Westen, Henk A. Dijkstra","The Atlantic Meridional Overturning Circulation (AMOC) is recognized as a
tipping element within the global climate system. Central to its tipping
behavior is the salt-advection feedback mechanism, which has been extensively
studied in box models and models of intermediate complexity. However, in
contemporary, highly complex climate models, the importance and functioning of
this feedback mechanism is less clear due to the intricate interplay of
numerous ocean-atmosphere-sea ice feedbacks. In this study, we conduct a
detailed mechanistic analysis of an AMOC collapse under quasi-equilibrium
forcing conditions using the Community Earth System Model (CESM). By
reconstructing the AMOC strength from the meridional density contrast across
the Atlantic Ocean, we demonstrate that AMOC stability can be related to the
Atlantic freshwater budget, revealing several important feedbacks. The dominant
contribution is the destabilising salt-advection feedback, which is quantified
through a negative sign of the overturning freshwater transport at
34$^{\circ}$S, indicated by $F_{\mathrm{ovS}}$. Other feedbacks are related to
changes in North Atlantic sea-ice melt (destabilising), ocean-atmosphere
freshwater fluxes (destabilising) and gyre transports (stabilising). Our study
clarifies the role of $F_{\mathrm{ovS}}$ as an indicator of the background
state stability of the AMOC. As many modern climate models have a positive
$F_{\mathrm{ovS}}$ bias this implies that their AMOC is too stable which leads
to an underestimation of the risk of an AMOC collapse under climate change.",http://arxiv.org/abs/2410.03236v1
"Is climate variability the result of frequency modulation by the solar
  cycle? Evidence from the El Nino Southern Oscillation, Australian climate,
  Central England Temperature, and reconstructed solar activity and climate
  records",2024-04-21T05:53:27Z,Ian R. Edmonds,"Oceanic atmospheric oscillations and climate variability are tightly linked
and both exhibit broad band spectral content that ranges, with roughly equal
strength, from annual to centennial periodicity. The explanation for
variability based on the integration of weather noise leads to a spectral
content heavily weighted to low frequencies; explaining the variability as
resulting from solar forcing leads to a narrow band, approximately eleven year
period, spectral content. In both cases the spectral content is incompatible
with the observed spectrum. It is known that the Southern Oscillation is
frequency modulated, i.e. the time interval between successive events varies on
an approximately centenary scale. In this paper we develop a model of the
Southern Oscillation responding to the slowly changing frequency of the solar
cycle. This results in a frequency modulated oscillation, the spectrum of which
is intrinsically broad and flat and therefore compatible with the observed
spectrum. Fortunately, the change in frequency of the solar cycle with time has
been reconstructed from tree ring data for the last millennium. It is possible
to identify time intervals when the frequency was dominated by a single
frequency in which case the model oscillation is relatively simple. The 11 year
period component of the model time variation was shown to correlate closely
with the 11 year period components of observed Southern Oscillation and climate
variability. A characteristic of a frequency modulated variable, the equal
spacing of spectral peaks, was utilized via a double Fourier transform method
to recover solar cycle periodicity from instrumental and reconstructed climate
records, with the recovered periodicity and the known periodicity of the solar
cycle in good agreement. The concept outlined provides a new way of viewing and
assessing the Sun climate connection.",http://arxiv.org/abs/2404.13542v1
Fermi Resonance and the Quantum Mechanical Basis of Global Warming,2024-01-26T19:54:14Z,"Robin Wordsworth, Jacob Seeley, Keith Shine","Although the scientific principles of anthropogenic climate change are
well-established, existing calculations of the warming effect of carbon dioxide
rely on spectral absorption databases, which obscures the physical foundations
of the climate problem. Here we show how CO2 radiative forcing can be expressed
via a first-principles description of the molecule's key vibrational-rotational
transitions. Our analysis elucidates the dependence of carbon dioxide's
effectiveness as a greenhouse gas on the Fermi resonance between the symmetric
stretch mode $\nu_1$ and bending mode $\nu_2$. It is remarkable that an
apparently accidental quantum resonance in an otherwise ordinary three-atom
molecule has had such a large impact on our planet's climate over geologic
time, and will also help determine its future warming due to human activity. In
addition to providing a simple explanation of CO2 radiative forcing on Earth,
our results may have implications for understanding radiation and climate on
other planets.",http://arxiv.org/abs/2401.15177v1
"Dynamics of the temperature regime of permafrost soil in the vicinity of
  the main gas pipeline taking into account climate warming",2024-02-25T16:16:37Z,"A. A. Fedotov, P. V. Khrapov, A. E. Dengovskaya","An initial-boundary value problem for an unsteady two-dimensional heat
conduction equation in a bounded domain modeling the unsteady temperature
distribution of permafrost soil in the vicinity of a main gas pipeline, taking
into account climate warming, is investigated. The parameters of the
mathematical model are selected in accordance with experimental data on gas
transportation in permafrost areas. The problem is solved numerically by the
finite element method. Modeling of the temperature field has been carried out
for 30 years since the start of the gas pipeline operation. Calculations are
carried out until the periodic temperature regime of the soil around the gas
pipeline is practically established. Under the initial conditions adopted in
the work, a periodic temperature regime at the top and bottom of the pipe is
established in approximately 12 years, and a periodic temperature regime in
depth is established in approximately 22 years. Two scenarios of climate
warming are considered: moderate RCP2.6 and more negative RCP8.5. It is shown
that significant changes in the ground temperature regime occur in the vicinity
of the pipe under both warming scenarios. Nevertheless, the calculations
demonstrate the preservation of permafrost even in a negative scenario of
climate warming.",http://arxiv.org/abs/2402.16134v1
"Diffusion-Based Joint Temperature and Precipitation Emulation of Earth
  System Models",2024-04-12T20:13:19Z,"Katie Christensen, Lyric Otto, Seth Bassetti, Claudia Tebaldi, Brian Hutchinson","Earth system models (ESMs) are the principal tools used in climate science to
generate future climate projections under various atmospheric emissions
scenarios on a global or regional scale. Generative deep learning approaches
are suitable for emulating these tools due to their computational efficiency
and ability, once trained, to generate realizations in a fraction of the time
required by ESMs. We extend previous work that used a generative probabilistic
diffusion model to emulate ESMs by targeting the joint emulation of multiple
variables, temperature and precipitation, by a single diffusion model. Joint
generation of multiple variables is critical to generate realistic samples of
phenomena resulting from the interplay of multiple variables. The diffusion
model emulator takes in the monthly mean-maps of temperature and precipitation
and produces the daily values of each of these variables that exhibit
statistical properties similar to those generated by ESMs. Our results show the
outputs from our extended model closely resemble those from ESMs on various
climate metrics including dry spells and hot streaks, and that the joint
distribution of temperature and precipitation in our sample closely matches
those of ESMs.",http://arxiv.org/abs/2404.08797v1
Generative Diffusion-based Downscaling for Climate,2024-04-27T01:49:14Z,"Robbie A. Watt, Laura A. Mansfield","Downscaling, or super-resolution, provides decision-makers with detailed,
high-resolution information about the potential risks and impacts of climate
change, based on climate model output. Machine learning algorithms are proving
themselves to be efficient and accurate approaches to downscaling. Here, we
show how a generative, diffusion-based approach to downscaling gives accurate
downscaled results. We focus on an idealised setting where we recover ERA5 at
$0.25\degree$~resolution from coarse grained version at $2\degree$~resolution.
The diffusion-based method provides superior accuracy compared to a standard
U-Net, particularly at the fine scales, as highlighted by a spectral
decomposition. Additionally, the generative approach provides users with a
probability distribution which can be used for risk assessment. This research
highlights the potential of diffusion-based downscaling techniques in providing
reliable and detailed climate predictions.",http://arxiv.org/abs/2404.17752v1
Targeted marine cloud brightening can dampen El Ni√±o,2024-06-12T03:46:57Z,"Jessica S. Wan, John T. Fasullo, Nan Rosenbloom, Chih-Chieh Jack Chen, Katharine Ricke","Many record-breaking climate extremes arise from both greenhouse gas-induced
warming and natural climate variability. Marine cloud brightening, a solar
geoengineering strategy originally proposed to reduce long-term warming, could
potentially mitigate extreme events by instead targeting seasonal phenomena,
such as El Ni\~no-Southern Oscillation (ENSO). By exploiting the 2019-2020
Australian wildfire experiment-of-opportunity, we show that simulated marine
cloud brightening in the southeast Pacific reproduces observed cloud changes
and induces La Ni\~na-like responses. We then explore how cloud brightening
timing and duration modifies the 1997-1998 and 2015-2016 El Ni\~no events. We
find the earliest and longest interventions effectively restore neutral ENSO
conditions and dampen El Ni\~no's impacts. Solar geoengineering that targets
climate variability could complement tools such as ENSO forecasting and provide
a pathway for climate risk mitigation.",http://arxiv.org/abs/2406.07853v1
"ClimRetrieve: A Benchmarking Dataset for Information Retrieval from
  Corporate Climate Disclosures",2024-06-14T08:21:42Z,"Tobias Schimanski, Jingwei Ni, Roberto Spacey, Nicola Ranger, Markus Leippold","To handle the vast amounts of qualitative data produced in corporate climate
communication, stakeholders increasingly rely on Retrieval Augmented Generation
(RAG) systems. However, a significant gap remains in evaluating domain-specific
information retrieval - the basis for answer generation. To address this
challenge, this work simulates the typical tasks of a sustainability analyst by
examining 30 sustainability reports with 16 detailed climate-related questions.
As a result, we obtain a dataset with over 8.5K unique question-source-answer
pairs labeled by different levels of relevance. Furthermore, we develop a use
case with the dataset to investigate the integration of expert knowledge into
information retrieval with embeddings. Although we show that incorporating
expert knowledge works, we also outline the critical limitations of embeddings
in knowledge-intensive downstream domains like climate change communication.",http://arxiv.org/abs/2406.09818v3
Generative Debunking of Climate Misinformation,2024-07-08T04:21:58Z,"Francisco Zanartu, Yulia Otmakhova, John Cook, Lea Frermann","Misinformation about climate change causes numerous negative impacts,
necessitating corrective responses. Psychological research has offered various
strategies for reducing the influence of climate misinformation, such as the
fact-myth-fallacy-fact-structure. However, practically implementing corrective
interventions at scale represents a challenge. Automatic detection and
correction of misinformation offers a solution to the misinformation problem.
This study documents the development of large language models that accept as
input a climate myth and produce a debunking that adheres to the
fact-myth-fallacy-fact (``truth sandwich'') structure, by incorporating
contrarian claim classification and fallacy detection into an LLM prompting
framework. We combine open (Mixtral, Palm2) and proprietary (GPT-4) LLMs with
prompting strategies of varying complexity. Experiments reveal promising
performance of GPT-4 and Mixtral if combined with structured prompts. We
identify specific challenges of debunking generation and human evaluation, and
map out avenues for future work. We release a dataset of high-quality
truth-sandwich debunkings, source code and a demo of the debunking system.",http://arxiv.org/abs/2407.05599v1
"AQ-PINNs: Attention-Enhanced Quantum Physics-Informed Neural Networks
  for Carbon-Efficient Climate Modeling",2024-09-03T05:52:04Z,"Siddhant Dutta, Nouhaila Innan, Sadok Ben Yahia, Muhammad Shafique","The growing computational demands of artificial intelligence (AI) in
addressing climate change raise significant concerns about inefficiencies and
environmental impact, as highlighted by the Jevons paradox. We propose an
attention-enhanced quantum physics-informed neural networks model (AQ-PINNs) to
tackle these challenges. This approach integrates quantum computing techniques
into physics-informed neural networks (PINNs) for climate modeling, aiming to
enhance predictive accuracy in fluid dynamics governed by the Navier-Stokes
equations while reducing the computational burden and carbon footprint. By
harnessing variational quantum multi-head self-attention mechanisms, our
AQ-PINNs achieve a 51.51% reduction in model parameters compared to classical
multi-head self-attention methods while maintaining comparable convergence and
loss. It also employs quantum tensor networks to enhance representational
capacity, which can lead to more efficient gradient computations and reduced
susceptibility to barren plateaus. Our AQ-PINNs represent a crucial step
towards more sustainable and effective climate modeling solutions.",http://arxiv.org/abs/2409.01626v1
The belief in Moore's Law is undermining ICT climate action,2024-11-26T12:50:05Z,"Adrian Friday, Christina Bremer, Oliver Bates, Christian Remy, Srinjoy Mitra, Jan Tobias Muehlberg","The growth of semiconductor technology is unprecedented, with profound
transformational consequences for society. This includes feeding an
over-reliance on digital solutions to systemic problems such as climate change
('techno-solutionism'). Such technologies come at a cost: environmental, social
and material. We unpack topics arising from ""The True Cost of ICT: From
Materiality to Techno-Solutionism (TCICT)"", a workshop held at the
International ICT for Sustainability (ICT4S) conference 2024 in Stockholm,
Sweden -- exploring, as a matter of global climate injustice, the drivers and
material dependencies of these technologies. We point to the importance of
addressing ICT's impacts as a system, rather than purely in terms of efficiency
and energy use. We conclude by calling to build a community of like-minded and
critical colleagues to address the intersectional climate impacts of the
semiconductor industry and the techno-solutionism it embodies.",http://arxiv.org/abs/2411.17391v2
"High-resolution boreal winter precipitation projections over Tropical
  America from CMIP5 models",2024-01-14T08:15:34Z,"Reiner Palomino-Lemus, Samir C√≥rdoba-Machado, Sonia Raquel G√°miz-Fortis, Yolanda Castro-D√≠ez, Mar√≠a Jes√∫s Esteban-Parra","Climate change projections for boreal winter precipitation in Tropical
America has beenaddressed by statistical downscaling (SD) using the principal
component regression with sea-level pressure (SLP) as the predictor variable.
The SD model developed from the reanalysis of SLP and gridded precipitation
GPCC data, has been applied to SLP outputs from 20 CGMS of CMIP5, both from the
present climate (1971-2000) and for the future (2071-2100) under the RCP2.6,
RCP4.5, and RCP8.5 scenarios. The SD model shows a suitable performance over
large regions, presenting a strong bias only in small areas characterized by
very dry climate conditions or poor data coverage. The difference in percentage
between the projected SD precipitation and the simulated SD precipitation for
present climate, ranges from moderate to intense changes in rainfall (positive
or negative, depending on the region and the SD GCM model considered), as the
radiative forcing increases from the RCP2.6 to RCP8.5. The disparity in the
GCMs outputs seems to be the major source of uncertainty in the projected
changes, while the scenario considered appears less decisive. Mexico and
eastern Brazil are the areas showing the most coherent decreases between SD
GCMs, while northwestern and southeastern South America show consistently
significant increases. This coherence is corroborated by the results of the
ensemble mean which projects positive changes from 10N towards the south, with
exceptions such as eastern Brazil, northern Chile and some smaller areas, such
as the center of Colombia, while projected negative changes are the majority
found in the northernmost part.",http://arxiv.org/abs/2401.07226v1
"Mapping Land Naturalness from Sentinel-2 using Deep Contextual and
  Geographical Priors",2024-06-27T16:17:33Z,"Burak Ekim, Michael Schmitt","In recent decades, the causes and consequences of climate change have
accelerated, affecting our planet on an unprecedented scale. This change is
closely tied to the ways in which humans alter their surroundings. As our
actions continue to impact natural areas, using satellite images to observe and
measure these effects has become crucial for understanding and combating
climate change. Aiming to map land naturalness on the continuum of modern human
pressure, we have developed a multi-modal supervised deep learning framework
that addresses the unique challenges of satellite data and the task at hand. We
incorporate contextual and geographical priors, represented by corresponding
coordinate information and broader contextual information, including and
surrounding the immediate patch to be predicted. Our framework improves the
model's predictive performance in mapping land naturalness from Sentinel-2
data, a type of multi-spectral optical satellite imagery. Recognizing that our
protective measures are only as effective as our understanding of the
ecosystem, quantifying naturalness serves as a crucial step toward enhancing
our environmental stewardship.",http://arxiv.org/abs/2406.19302v1
"Constructing a High Temporal Resolution Global Lakes Dataset via
  Swin-Unet with Applications to Area Prediction",2024-08-20T13:17:07Z,"Yutian Han, Baoxiang Huang, He Gao","Lakes provide a wide range of valuable ecosystem services, such as water
supply, biodiversity habitats, and carbon sequestration. However, lakes are
increasingly threatened by climate change and human activities. Therefore,
continuous global monitoring of lake dynamics is crucial, but remains
challenging on a large scale. The recently developed Global Lakes Area Database
(GLAKES) has mapped over 3.4 million lakes worldwide, but it only provides data
at decadal intervals, which may be insufficient to capture rapid or short-term
changes.This paper introduces an expanded lake database, GLAKES-Additional,
which offers biennial delineations and area measurements for 152,567 lakes
globally from 1990 to 2021. We employed the Swin-Unet model, replacing
traditional convolution operations, to effectively address the challenges posed
by the receptive field requirements of high spatial resolution satellite
imagery. The increased biennial time resolution helps to quantitatively
attribute lake area changes to climatic and hydrological drivers, such as
precipitation and temperature changes.For predicting lake area changes, we used
a Long Short-Term Memory (LSTM) neural network and an extended time series
dataset for preliminary modeling. Under climate and land use scenarios, our
model achieved an RMSE of 0.317 km^2 in predicting future lake area changes.",http://arxiv.org/abs/2408.10821v1
"Efficient Aspect-Based Summarization of Climate Change Reports with
  Small Language Models",2024-11-21T16:28:32Z,"Iacopo Ghinassi, Leonardo Catalano, Tommaso Colella","The use of Natural Language Processing (NLP) for helping decision-makers with
Climate Change action has recently been highlighted as a use case aligning with
a broader drive towards NLP technologies for social good. In this context,
Aspect-Based Summarization (ABS) systems that extract and summarize relevant
information are particularly useful as they provide stakeholders with a
convenient way of finding relevant information in expert-curated reports. In
this work, we release a new dataset for ABS of Climate Change reports and we
employ different Large Language Models (LLMs) and so-called Small Language
Models (SLMs) to tackle this problem in an unsupervised way. Considering the
problem at hand, we also show how SLMs are not significantly worse for the
problem while leading to reduced carbon footprint; we do so by applying for the
first time an existing framework considering both energy efficiency and task
performance to the evaluation of zero-shot generative models for ABS. Overall,
our results show that modern language models, both big and small, can
effectively tackle ABS for Climate Change reports but more research is needed
when we frame the problem as a Retrieval Augmented Generation (RAG) problem and
our work and dataset will help foster efforts in this direction.",http://arxiv.org/abs/2411.14272v1
Generative Adversarial Models for Extreme Geospatial Downscaling,2024-02-21T18:25:04Z,"Guiye Li, Guofeng Cao","Addressing the challenges of climate change requires accurate and
high-resolution mapping of geospatial data, especially climate and weather
variables. However, many existing geospatial datasets, such as the gridded
outputs of the state-of-the-art numerical climate models (e.g., general
circulation models), are only available at very coarse spatial resolutions due
to the model complexity and extremely high computational demand.
Deep-learning-based methods, particularly generative adversarial networks
(GANs) and their variants, have proved effective for refining natural images
and have shown great promise in improving geospatial datasets. This paper
describes a conditional GAN-based stochastic geospatial downscaling method that
can accommodates very high scaling factors. Compared to most existing methods,
the method can generate high-resolution accurate climate datasets from very
low-resolution inputs. More importantly, the method explicitly considers the
uncertainty inherent to the downscaling process that tends to be ignored in
existing methods. Given an input, the method can produce a multitude of
plausible high-resolution samples instead of one single deterministic result.
These samples allow for an empirical exploration and inferences of model
uncertainty and robustness. With a case study of gridded climate datasets (wind
velocity and solar irradiance), we demonstrate the performances of the
framework in downscaling tasks with large scaling factors (up to $64\times$)
and highlight the advantages of the framework with a comprehensive comparison
with commonly used and most recent downscaling methods, including area-to-point
(ATP) kriging, deep image prior (DIP), enhanced super-resolution generative
adversarial networks (ESRGAN), physics-informed resolution-enhancing GAN (PhIRE
GAN), and an efficient diffusion model for remote sensing image
super-resolution (EDiffSR).",http://arxiv.org/abs/2402.14049v2
"Impact of Topography and Climate on Post-fire Vegetation Recovery Across
  Different Burn Severity and Land Cover Types through Machine Learning",2024-01-16T15:14:54Z,"Faria Tuz Zahura, Gautam Bisht, Zhi Li, Sarah McKnight, Xingyuan Chen","Wildfire significantly disturb ecosystems by altering forest structure,
vegetation ecophysiology, and soil properties. Understanding the complex
interactions between topographic and climatic conditions in post-wildfire
recovery is crucial. This study investigates the interplay between topography,
climate, burn severity, and years after fire on vegetation recovery across
dominant land cover types (evergreen forest, shrubs, and grassland) in the
Pacific Northwest region. Using Moderate Resolution Imaging Spectroradiometer
data, we estimated vegetation recovery by calculating the incremental enhanced
vegetation index (EVI) change during post-fire years. A machine learning
technique, random forest (RF), was employed to map relationships between the
input features (elevation, slope, aspect, precipitation, temperature, burn
severity, and years after fire) and the target (incremental EVI recovery) for
each land cover type. Variable importance analysis and partial dependence plots
were generated to understand the influence of individual features. The observed
and predicted incremental EVI values showed good matches, with R2 values of
0.99 for training and between 0.89 and 0.945 for testing. The study found that
climate variables, specifically precipitation and temperature, were the most
important features overall, while elevation played the most significant role
among the topographic factors. Partial dependence plots revealed that lower
precipitation tended to cause a reduction in vegetation recovery for varying
temperature ranges across land cover types. These findings can aid in
developing targeted strategies for post-wildfire forest management, considering
the varying responses of different land cover types to topographic, climatic,
and burn severity factors.",http://arxiv.org/abs/2404.16834v1
Milankoviƒá Forcing in Deep Time,2024-05-02T01:12:01Z,"Richard E. Zeebe, Margriet L. Lantink","Astronomical (or Milankovi\'c) forcing of the Earth system is key to
understanding rhythmic climate change on time scales >~ 10 kyr.
Paleoceanographic and paleoclimatological applications concerned with past
astronomical forcing rely on astronomical calculations (solutions), which
represent the backbone of cyclostratigraphy and astrochronology. Here we
present state-of-the-art astronomical solutions over the past 3.5 Gyr. Our goal
is to provide tuning targets and templates for interpreting deep-time
cyclostratigraphic records and designing external forcing functions in climate
models. Our approach yields internally consistent orbital and precession-tilt
solutions, including fundamental solar system frequencies, orbital eccentricity
and inclination, lunar distance, luni-solar precession rate, Earth's obliquity,
and climatic precession. Contrary to expectations, we find that the long
eccentricity cycle (previously assumed stable and labeled ''metronome'', recent
period ~405 kyr), can become unstable on long time scales. Our results reveal
episodes during which the long eccentricity cycle is very weak or absent and
Earth's orbital eccentricity and climate-forcing spectrum are unrecognizable
compared to the recent past. For the ratio of eccentricity-to-inclination
amplitude modulation (frequently observable in paleorecords) we find a wide
distribution around the recent 2:1 ratio, i.e., the system is not restricted to
a 2:1 or 1:1 resonance state. Our computations show that Earth's obliquity was
lower and its amplitude (variation around the mean) significantly reduced in
the past. We therefore predict weaker climate forcing at obliquity frequencies
in deep time and a trend toward reduced obliquity power with age in
stratigraphic records. For deep-time stratigraphic and modeling applications,
the orbital parameters of our 3.5-Gyr integrations are made available at
400-year resolution.",http://arxiv.org/abs/2405.00931v1
"User Archetypes and Information Dynamics on Telegram: COVID-19 and
  Climate Change Discourse in Singapore",2024-06-10T18:26:35Z,"Val Alvern Cueco Ligo, Lam Yin Cheung, Roy Ka-Wei Lee, Koustuv Saha, Edson C. Tandoc Jr., Navin Kumar","Social media platforms, particularly Telegram, play a pivotal role in shaping
public perceptions and opinions on global and national issues. Unlike
traditional news media, Telegram allows for the proliferation of user-generated
content with minimal oversight, making it a significant venue for the spread of
controversial and misinformative content. During the COVID-19 pandemic,
Telegram's popularity surged in Singapore, a country with one of the highest
rates of social media use globally. We leverage Singapore-based Telegram data
to analyze information flows within groups focused on COVID-19 and climate
change. Using k-means clustering, we identified distinct user archetypes,
including Strategic Disruptor, Empirical Enthusiast, Inquisitive Moderate, and
Critical Examiner, each contributing uniquely to the discourse. We developed a
model to classify users into these clusters (Precision: Climate change: 0.99;
COVID-19: 0.95).",http://arxiv.org/abs/2406.06717v3
"Contemporaneous and lagged spillovers between agriculture, crude oil,
  carbon emission allowance, and climate change",2024-08-19T03:06:11Z,"Yan-Hong Yang, Ying-Hui Shao, Wei-Xing Zhou","In this paper, we examine the dynamic spillovers among the crude oil, carbon
emission allowance, climate change, and agricultural markets. Adopting a novel
$R^2$ decomposed connectedness approach, our empirical analysis reveals several
key findings. The overall TCI dynamics have been mainly dominated by
contemporaneous dynamics rather than the lagged dynamics. We also find climate
change has significant spillovers to other markets. Moreover, there are
heterogeneous spillover effects among agricultural markets. Specially, corn is
the biggest risk contributor to this system, while barley is the major risk
receiver of shocks.",http://arxiv.org/abs/2408.09669v2
Climate Change in Austria: Precipitation and Dry Spells over 50 years,2024-08-21T10:12:56Z,"Corinna Perchtold, Evelyn Buckwar","We propose a spatio-temporal generalised additive model (GAM) to study if
precipitation patterns have changed between two 10-year time periods in the
last 50 years in Austria. In particular, we model three scenarios: monthly mean
and monthly maximum precipitation as well as the maximum length of a dry spell
per month with a gamma, blended generalised extreme value and negative binomial
distribution, respectively, over the periods 1973-1982 and 2013-2022. In order
to model the spatial dependencies in the data more realistically, we intend to
take the mountainous landscape of Austria into account. Therefore, we have
chosen a non-stationary version of the Mat\'ern covariance function, which
accounts for elevation differences, as a spatial argument of the latent field
in the GAM. The temporal part of the latent field is captured by an AR(1)
process. We use the stochastic partial differential equation approach in
combination with integrated nested Laplace approximation to perform inference
computationally efficient. The model outputs are visualised and support
existing climate change studies in the Alpine region obtained with, for
example, projections from regional climate models.",http://arxiv.org/abs/2408.11497v1
"DivShift: Exploring Domain-Specific Distribution Shift in
  Volunteer-Collected Biodiversity Datasets",2024-10-17T23:56:30Z,"Elena Sierra, Lauren E. Gillespie, Salim Soltani, Moises Exposito-Alonso, Teja Kattenborn","Climate change is negatively impacting the world's biodiversity. To build
automated systems to monitor these negative biodiversity impacts, large-scale,
volunteer-collected datasets like iNaturalist are built from
community-identified, natural imagery. However, such volunteer-based data are
opportunistic and lack a structured sampling strategy, resulting in geographic,
temporal, observation quality, and socioeconomic, biases that stymie uptake of
these models for downstream biodiversity monitoring tasks. Here we introduce
DivShift North American West Coast (DivShift-NAWC), a curated dataset of almost
8 million iNaturalist plant images across the western coast of North America,
for exploring the effects of these biases on deep learning model performance.
We compare model performance across four known biases and observe that they
indeed confound model performance. We suggest practical strategies for curating
datasets to train deep learning models for monitoring climate change's impacts
on the world's biodiversity.",http://arxiv.org/abs/2410.19816v2
"ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on
  Climate Change",2024-01-17T23:29:46Z,"David Thulke, Yingbo Gao, Petrus Pelser, Rein Brune, Rricha Jalota, Floris Fok, Michael Ramos, Ian van Wyk, Abdallah Nasir, Hayden Goldstein, Taylor Tragemann, Katie Nguyen, Ariana Fowler, Andrew Stanco, Jon Gabriel, Jordan Taylor, Dean Moro, Evgenii Tsymbalov, Juliette de Waal, Evgeny Matusov, Mudar Yaghi, Mohammad Shihadah, Hermann Ney, Christian Dugast, Jonathan Dotan, Daniel Erasmus","This paper introduces ClimateGPT, a model family of domain-specific large
language models that synthesize interdisciplinary research on climate change.
We trained two 7B models from scratch on a science-oriented dataset of 300B
tokens. For the first model, the 4.2B domain-specific tokens were included
during pre-training and the second was adapted to the climate domain after
pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously
pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each
model is instruction fine-tuned on a high-quality and human-generated
domain-specific dataset that has been created in close cooperation with climate
scientists. To reduce the number of hallucinations, we optimize the model for
retrieval augmentation and propose a hierarchical retrieval strategy. To
increase the accessibility of our model to non-English speakers, we propose to
make use of cascaded machine translation and show that this approach can
perform comparably to natively multilingual models while being easier to scale
to a large number of languages. Further, to address the intrinsic
interdisciplinary aspect of climate change we consider different research
perspectives. Therefore, the model can produce in-depth answers focusing on
different perspectives in addition to an overall answer. We propose a suite of
automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks,
ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model
while not degrading results on general domain benchmarks. Our human evaluation
confirms the trends we saw in our benchmarks. All models were trained and
evaluated using renewable energy and are released publicly.",http://arxiv.org/abs/2401.09646v1
"Future changes in land and atmospheric variables: An analysis of their
  couplings in the Iberian Peninsula",2024-01-14T08:57:34Z,"Matilde Garc√≠a-Valdecasas Ojeda, Patricio Yeste, Sonia Raquel G√°miz-Fortis, Yolanda Castro-D√≠ez, Mar√≠a Jes√∫s Esteban-Parra","This work investigates climate--projections over a transitional region
between dry and wet climates, the Iberian Peninsula (IP). With this purpose,
the Weather Research and Forecasting (WRF) model, driven CCSM4 and MPI-ESM-LR
GCMs previously bias-corrected, was used. Simulations were carried out for two
periods, 1980-2014 and 2071-2100, and under RCP4.5 and RCP8.5. The analysis
focused on changes in land-surface processes, their causes, and the potential
impact on the climate system. To achieve this, seasonal projected changes of
land-surface (soil moisture and surface evapotranspiration) and atmospheric
variables involved in the hydrologic (precipitation and runoff) and energy
balance (temperature and solar incoming radiation) were investigated. The
results reveal that the IP is likely to experience a soil dryness by the end of
the 21st century, particularly during summer and fall, more apparent in the
southern IP, and stronger under the RCP8.5. However, such trends would have
different implications throughout the year and directly affect the surface
evapotranspiration. Moreover, soil-drying trends are mainly associated with
reductions in the large-scale precipitation during spring, summer, and fall and
by enhanced evapotranspiration particularly in spring over the northwestern IP.
In addition, the results show notably changes in soil conditions at high
altitude, particularly during winter, which may alter the land-atmosphere
processes that currently occur in these regions. In this context, noteworthy
changes in the climate system are expected, leading to adverse impacts on water
resources and temperature. The results highlight the complex and nonlinear
nature of land-atmosphere interactions in regions such as the IP, which is a
tremendous challenge for adequately developing mitigation and adaptation
strategies to anthropogenic climate change.",http://arxiv.org/abs/2401.10164v1
Towards A Comprehensive Assessment of AI's Environmental Impact,2024-05-22T21:19:35Z,Srija Chakraborty,"Artificial Intelligence, machine learning (AI/ML) has allowed exploring
solutions for a variety of environmental and climate questions ranging from
natural disasters, greenhouse gas emission, monitoring biodiversity,
agriculture, to weather and climate modeling, enabling progress towards climate
change mitigation. However, the intersection of AI/ML and environment is not
always positive. The recent surge of interest in ML, made possible by
processing very large volumes of data, fueled by access to massive compute
power, has sparked a trend towards large-scale adoption of AI/ML. This interest
places tremendous pressure on natural resources, that are often overlooked and
under-reported. There is a need for a framework that monitors the environmental
impact and degradation from AI/ML throughout its lifecycle for informing
policymakers, stakeholders to adequately implement standards and policies and
track the policy outcome over time. For these policies to be effective, AI's
environmental impact needs to be monitored in a spatially-disaggregated, timely
manner across the globe at the key activity sites. This study proposes a
methodology to track environmental variables relating to the multifaceted
impact of AI around datacenters using openly available energy data and globally
acquired satellite observations. We present a case study around Northern
Virginia, United States that hosts a growing number of datacenters and observe
changes in multiple satellite-based environmental metrics. We then discuss the
steps to expand this methodology for comprehensive assessment of AI's
environmental impact across the planet. We also identify data gaps and
formulate recommendations for improving the understanding and monitoring
AI-induced changes to the environment and climate.",http://arxiv.org/abs/2405.14004v1
"Ensuring resilience to extreme weather events increases the ambition of
  mitigation scenarios on solar power and storage uptake: a study on the
  Italian power system",2024-09-05T14:51:22Z,"Alice Di Bella, Francesco Pietro Colelli","This study explores compounding impacts of climate change on power system's
load and generation, emphasising the need to integrate adaptation and
mitigation strategies into investment planning. We combine existing and novel
empirical evidence to model impacts on: i) air-conditioning demand; ii) thermal
power outages; iii) hydro-power generation shortages. Using a power dispatch
and capacity expansion model, we analyse the Italian power system's response to
these climate impacts in 2030, integrating mitigation targets and optimising
for cost-efficiency at an hourly resolution. We outline different
meteorological scenarios to explore the impacts of both average climatic
changes and the intensification of extreme weather events. We find that
addressing extreme weather in power system planning will require an extra 5-8
GW of photovoltaic (PV) capacity, on top of the 50 GW of the additional solar
PV capacity required by the mitigation target alone. Despite the higher initial
investments, we find that the adoption of renewable technologies, especially
PV, alleviates the power system's vulnerability to climate change and extreme
weather events. Furthermore, enhancing short-term storage with lithium-ion
batteries is crucial to counterbalance the reduced availability of dispatchable
hydro generation.",http://arxiv.org/abs/2409.03593v1
"Assessing the physical risks of climate change for the financial sector:
  a case study from Mexico's Central Bank",2024-11-28T00:45:09Z,"Francisco Estrada, Miguel A. Altamirano del Carmen, Oscar Calderon-Bustamante, W. J. Wouter Botzen, Serafin Martinez-Jaramillo, Stefano Battiston","The financial sector is increasingly concerned with the physical risks of
climate change, but economic and financial impact representations are still
developing, particularly for chronic risks. Mexico's Central Bank conducted a
comprehensive assessment using a suite of global models to evaluate both
physical and transition risks. We present the analysis concerning with chronic
physical risks, underlining innovations such as the use of a recent integrated
assessment model that enables grid-cell level analysis and differentiates urban
and non-urban areas, capturing the local effects of climate change more
accurately. The model includes multiple damage functions and a probabilistic
climate model for encompassing analyses and detailed economic impact insights.
Under the Current Policies scenario, economic losses could exceed 35 percent of
Mexico's GDP by 2100. Accounting for the urban heat island effect, losses could
surpass 20 trillion (USD) in present value, over ten times Mexico's 2024 GDP.
However, implementing a scenario aligned with the Paris Agreement significantly
reduces these losses, showcasing the benefits of international mitigation
efforts, though substantial residual impacts persist. This study emphasizes
integrating chronic physical risks into financial evaluations, proposing new
approaches, metrics, and methods that exploit detailed, spatially explicit
measures to improve risk and loss estimation and facilitate communication.",http://arxiv.org/abs/2411.18834v1
Urban and non-urban contributions to the social cost of carbon,2024-01-01T08:35:11Z,"Francisco Estrada, Veronica Lupi, Wouter Botzen, Richard S. J. Tol","The social cost of carbon (SCC) serves as a concise gauge of climate change's
economic impact, often reported at the global and country level. SCC values are
disproportionately high for less-developed, populous countries. Assessing the
contributions of urban and non-urban areas to the SCC can provide additional
insights for climate policy. Cities are essential for defining global
emissions, influencing warming levels and associated damages. High exposure and
concurrent socioenvironmental problems exacerbate climate change risks in
cities. Using a spatially explicit integrated assessment model, the SCC is
estimated at USD$137-USD$579/tCO2, rising to USD$262-USD$1,075/tCO2 when
including urban heat island (UHI) warming. Urban SCC dominates, with both urban
exposure and the UHI contributing significantly. A permanent 1% reduction of
the UHI in urban areas yields net present benefits of USD$484-USD$1,562 per
urban dweller. Global cities have significant leverage and incentives for a
swift transition to a low-carbon economy, and for reducing local warming.",http://arxiv.org/abs/2401.00919v1
"Diffusion Model-based Probabilistic Downscaling for 180-year East Asian
  Climate Reconstruction",2024-02-02T01:34:33Z,"Fenghua Ling, Zeyu Lu, Jing-Jia Luo, Lei Bai, Swadhin K. Behera, Dachao Jin, Baoxiang Pan, Huidong Jiang, Toshio Yamagata","As our planet is entering into the ""global boiling"" era, understanding
regional climate change becomes imperative. Effective downscaling methods that
provide localized insights are crucial for this target. Traditional approaches,
including computationally-demanding regional dynamical models or statistical
downscaling frameworks, are often susceptible to the influence of downscaling
uncertainty. Here, we address these limitations by introducing a diffusion
probabilistic downscaling model (DPDM) into the meteorological field. This
model can efficiently transform data from 1{\deg} to 0.1{\deg} resolution.
Compared with deterministic downscaling schemes, it not only has more accurate
local details, but also can generate a large number of ensemble members based
on probability distribution sampling to evaluate the uncertainty of
downscaling. Additionally, we apply the model to generate a 180-year dataset of
monthly surface variables in East Asia, offering a more detailed perspective
for understanding local scale climate change over the past centuries.",http://arxiv.org/abs/2402.06646v2
"How causal inference concepts can guide research into the effects of
  climate on infectious diseases",2024-02-19T20:17:48Z,"Laura Andrea Barrero Guevara, Sarah C Kramer, Tobias Kurth, Matthieu Domenech de Cell√®s","A pressing question resulting from global warming is how infectious diseases
will be affected by climate change. Answering this question requires research
into the effects of weather on the population dynamics of transmission and
infection; elucidating these effects, however, has proven difficult due to the
challenges of assessing causality from the predominantly observational data
available in epidemiological research. Here, we show how concepts from causal
inference -- the sub-field of statistics aiming at inferring causality from
data -- can guide that research. Through a series of case studies, we
illustrate how such concepts can help assess study design and strategically
choose a study's location, evaluate and reduce the risk of bias, and interpret
the multifaceted effects of meteorological variables on transmission. More
broadly, we argue that interdisciplinary approaches based on explicit causal
frameworks are crucial for reliably estimating the effect of weather and
accurately predicting the consequences of climate change.",http://arxiv.org/abs/2402.12507v1
Joint Liability Model with Adaptation to Climate Change,2024-04-22T01:40:37Z,"Jiayue Zhang, Ken Seng Tan, Tony S. Wirjanto, Lysa Porth","This paper extends the application of ESG score assessment methodologies from
large corporations to individual farmers' production, within the context of
climate change. Our proposal involves the integration of crucial agricultural
sustainability variables into conventional personal credit evaluation
frameworks, culminating in the formulation of a holistic sustainable credit
rating referred to as the Environmental, Social, Economics (ESE) score. This
ESE score is integrated into theoretical joint liability models, to gain
valuable insights into optimal group sizes and individual-ESE score
relationships. Additionally, we adopt a mean-variance utility function for
farmers to effectively capture the risk associated with anticipated profits.
Through a set of simulation exercises, the paper investigates the implications
of incorporating ESE scores into credit evaluation systems, offering a nuanced
comprehension of the repercussions under various climatic conditions.",http://arxiv.org/abs/2404.13818v2
"Evapotranspiration trends over the last 300 years reconstructed from
  historical weather station observations via machine learning",2024-07-23T08:12:38Z,Haiyang Shi,"Estimating historical evapotranspiration (ET) is essential for understanding
the effects of climate change and human activities on the water cycle. This
study used historical weather station data to reconstruct ET trends over the
past 300 years with machine learning. A Random Forest model, trained on
FLUXNET2015 flux stations' monthly data using precipitation, temperature,
aridity index, and rooting depth as predictors, achieved an R2 of 0.66 and a
KGE of 0.76 through 10-fold cross-validation. Applied to 5267 weather stations,
the model produced monthly ET data showing a general increase in global ET from
1700 to the present, with a notable acceleration after 1900 due to warming.
Regional differences were observed, with higher ET increases in mid-to-high
latitudes of the Northern Hemisphere and decreases in some mid-to-low latitudes
and the Southern Hemisphere. In drylands, ET and temperature were weakly
correlated, while in humid areas, the correlation was much higher. The
correlation between ET and precipitation has remained stable over the
centuries. This study extends the ET data time span, providing valuable
insights into long-term historical ET trends and their drivers, aiding in
reassessing the impact of historical climate change and human activities on the
water cycle and supporting future climate adaptation strategies.",http://arxiv.org/abs/2407.16265v2
FATE: Focal-modulated Attention Encoder for Temperature Prediction,2024-08-21T04:40:18Z,"Tajamul Ashraf, Janibul Bashir","One of the major challenges of the twenty-first century is climate change,
evidenced by rising sea levels, melting glaciers, and increased storm
frequency. Accurate temperature forecasting is vital for understanding and
mitigating these impacts. Traditional data-driven models often use recurrent
neural networks (RNNs) but face limitations in parallelization, especially with
longer sequences. To address this, we introduce a novel approach based on the
FocalNet Transformer architecture. Our Focal modulation Attention Encoder
(FATE) framework operates in a multi-tensor format, utilizing tensorized
modulation to capture spatial and temporal nuances in meteorological data.
Comparative evaluations against existing transformer encoders, 3D CNNs, LSTM,
and ConvLSTM models show that FATE excels at identifying complex patterns in
temperature data. Additionally, we present a new labeled dataset, the Climate
Change Parameter dataset (CCPD), containing 40 years of data from Jammu and
Kashmir on seven climate-related parameters. Experiments with real-world
temperature datasets from the USA, Canada, and Europe show accuracy
improvements of 12\%, 23\%, and 28\%, respectively, over current
state-of-the-art models. Our CCPD dataset also achieved a 24\% improvement in
accuracy. To support reproducible research, we have released the source code
and pre-trained FATE model at
\href{https://github.com/Tajamul21/FATE}{https://github.com/Tajamul21/FATE}.",http://arxiv.org/abs/2408.11336v1
Reimagining Data Visualization to Address Sustainability Goals,2024-09-05T15:16:37Z,Narges Mahyar,"Information visualization holds significant potential to support
sustainability goals such as environmental stewardship, and climate resilience
by transforming complex data into accessible visual formats that enhance public
understanding of complex climate change data and drive actionable insights.
While the field has predominantly focused on analytical orientation of
visualization, challenging traditional visualization techniques and goals,
through critical visualization research expands existing assumptions and
conventions in the field. In this paper, I explore how reimagining overlooked
aspects of data visualization, such as engagement, emotional resonance,
communication, and community empowerment, can contribute to achieving
sustainability objectives. I argue that by focusing on inclusive data
visualization that promotes clarity, understandability, and public
participation, we can make complex data more relatable and actionable,
fostering broader connections and mobilizing collective action on critical
issues like climate change. Moreover, I discuss the role of emotional
receptivity in environmental data communication, stressing the need for
visualizations that respect diverse cultural perspectives and emotional
responses to achieve impactful outcomes. Drawing on insights from a decade of
research in public participation and community engagement, I aim to highlight
how data visualization can democratize data access and increase public
involvement in order to contribute to a more sustainable and resilient future.",http://arxiv.org/abs/2409.03611v1
Part 1: Disruption of Water-Carbon Cycle under Wet Climate Extremes,2024-10-14T19:54:59Z,"Maheshwari Neelam, Christopher Hain","Modern climate change presents unprecedented challenges, posing critical
crises that threaten sustainable development, human well-being, and planetary
health. A significant concern is the potential for global warming to cause
irreversible disruptions to the water-carbon cycle, a topic that remains
underexplored. This study seeks to address a crucial knowledge gap by examining
how increasing wet extremes impact ecosystem productivity. The research agenda
focuses on three primary questions: 1) How do the intensity and duration of
various wet extremes affect evapotranspiration across different watersheds and
terrestrial biomes? 2) How do immediate and lagged responses to wet extremes
vary across different biomes, and what insights do these temporal patterns
provide about the causal and predictive relationships between wet extreme and
evapotranspiration? 3) To what extent do watershed characteristics (such as
soil properties, hydrological conditions, and vegetation factors) modulate the
relationship between wet extremes and ecosystem productivity? As climate change
alters precipitation patterns, understanding these complex ecosystem responses
becomes crucial for developing adaptive strategies and improving food and water
resource management.",http://arxiv.org/abs/2410.11049v1
"Deep Learning for Weather Forecasting: A CNN-LSTM Hybrid Model for
  Predicting Historical Temperature Data",2024-10-19T03:38:53Z,"Yuhao Gong, Yuchen Zhang, Fei Wang, Chi-Han Lee","As global climate change intensifies, accurate weather forecasting has become
increasingly important, affecting agriculture, energy management, environmental
protection, and daily life. This study introduces a hybrid model combining
Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks
to predict historical temperature data. CNNs are utilized for spatial feature
extraction, while LSTMs handle temporal dependencies, resulting in
significantly improved prediction accuracy and stability. By using Mean
Absolute Error (MAE) as the loss function, the model demonstrates excellent
performance in processing complex meteorological data, addressing challenges
such as missing data and high-dimensionality. The results show a strong
alignment between the prediction curve and test data, validating the model's
potential in climate prediction. This study offers valuable insights for fields
such as agriculture, energy management, and urban planning, and lays the
groundwork for future applications in weather forecasting under the context of
global climate change.",http://arxiv.org/abs/2410.14963v1
"Strengthening Power System Resilience to Extreme Weather Events Through
  Grid Enhancing Technologies",2024-11-25T22:00:29Z,Joseph Nyangon,"Climate change significantly increases risks to power systems, exacerbating
issues such as aging infrastructure, evolving regulations, cybersecurity
threats, and fluctuating demand. This paper focuses on the utilization of Grid
Enhancing Technologies (GETs) to strengthen power system resilience in the face
of extreme weather events. GETs are pivotal in optimizing energy distribution,
enabling predictive maintenance, ensuring reliable electricity supply,
facilitating renewable energy integration, and automating responses to power
instabilities and outages. Drawing insights from resilience theory, the paper
reviews recent grid resilience literature, highlighting increasing
vulnerabilities due to severe weather events. It demonstrates how GETs are
crucial in optimizing smart grid operations, thereby not only mitigating
climate-related impacts but also promoting industrial transformation.
  Keywords: Climate change, power systems, grid enhancing technologies (GETs),
power system resilience, extreme weather",http://arxiv.org/abs/2411.16962v1
"Counter-Geoengineering: Feasibility and Policy Implications for a
  Geoengineered World",2024-12-02T22:36:51Z,"Felipe de Bolle, Egemen Kolemen","With the increasing urgency of climate change's impacts and limited success
in reducing emissions, ""geoengineering,"" or the artificial manipulation of the
climate to reduce warming rates, has been proposed as an alternative short-term
solution. Options range from taking carbon out of the atmosphere through carbon
sinks and brightening clouds to increasing the planet's albedo through the
release of reflective particles into the atmosphere. While still controversial,
geoengineering has been proposed by some as a promising and low-cost way of
combating climate change. In particular, so-called 'moderate' geoengineering is
claimed to be achievable with few potential side effects or other
ramifications. However, this paper argues that the effect of moderate
geoengineering can easily be nullified by 'counter-geoengineering,' and any
impactful geoengineering would require a global governance framework to prevent
countries which benefit from warming temperatures from deploying
counter-geoengineering. In this paper, we take Russia as an example due to its
potential interest in counteracting geoengineering and its significant ability
to release a great amount of methane, a viable counter-geoengineering pathway
in the short term.",http://arxiv.org/abs/2412.03598v1
"Exploring Physics-Informed Neural Networks for Crop Yield Loss
  Forecasting",2024-12-31T15:21:50Z,"Miro Miranda, Marcela Charfuelan, Andreas Dengel","In response to climate change, assessing crop productivity under extreme
weather conditions is essential to enhance food security. Crop simulation
models, which align with physical processes, offer explainability but often
perform poorly. Conversely, machine learning (ML) models for crop modeling are
powerful and scalable yet operate as black boxes and lack adherence to crop
growths physical principles. To bridge this gap, we propose a novel method that
combines the strengths of both approaches by estimating the water use and the
crop sensitivity to water scarcity at the pixel level. This approach enables
yield loss estimation grounded in physical principles by sequentially solving
the equation for crop yield response to water scarcity, using an enhanced loss
function. Leveraging Sentinel-2 satellite imagery, climate data, simulated
water use data, and pixel-level yield data, our model demonstrates high
accuracy, achieving an R2 of up to 0.77, matching or surpassing
state-of-the-art models like RNNs and Transformers. Additionally, it provides
interpretable and physical consistent outputs, supporting industry,
policymakers, and farmers in adapting to extreme weather conditions.",http://arxiv.org/abs/2501.00502v1
"Augmenting Ground-Level PM2.5 Prediction via Kriging-Based Pseudo-Label
  Generation",2024-01-16T02:42:45Z,"Lei Duan, Ziyang Jiang, David Carlson","Fusing abundant satellite data with sparse ground measurements constitutes a
major challenge in climate modeling. To address this, we propose a strategy to
augment the training dataset by introducing unlabeled satellite images paired
with pseudo-labels generated through a spatial interpolation technique known as
ordinary kriging, thereby making full use of the available satellite data
resources. We show that the proposed data augmentation strategy helps enhance
the performance of the state-of-the-art convolutional neural network-random
forest (CNN-RF) model by a reasonable amount, resulting in a noteworthy
improvement in spatial correlation and a reduction in prediction error.",http://arxiv.org/abs/2401.08061v1
Aligning long-term climate mitigation with enhanced methane action,2024-02-07T11:06:34Z,"Katsumasa Tanaka, Kushal Tibrewal, Philippe Ciais, Olivier Boucher","The Global Methane Pledge and other methane measures may potentially
undermine CO2 mitigation in certain countries, unless they are considered as
additional to the existing Nationally Determined Contributions to strengthen
overall greenhouse gas emission targets. Maintaining the progress on CO2
mitigation in the revision of Nationally Determined Contributions after the
first Global Stocktake, while pursuing the immediate benefits from methane
mitigation, is necessary to address climate change in the long-term.",http://arxiv.org/abs/2402.04749v1
"Analyzing the Impact of Climate Change With Major Emphasis on Pollution:
  A Comparative Study of ML and Statistical Models in Time Series Data",2024-05-24T09:18:17Z,"Anurag Mishra, Ronen Gold, Sanjeev Vijayakumar","Industrial operations have grown exponentially over the last century, driving
advancements in energy utilization through vehicles and machinery.This growth
has significant environmental implications, necessitating the use of
sophisticated technology to monitor and analyze climate data.The surge in
industrial activities presents a complex challenge in forecasting its diverse
environmental impacts, which vary greatly across different regions.Aim to
understand these dynamics more deeply to predict and mitigate the environmental
impacts of industrial activities.",http://arxiv.org/abs/2405.15835v1
Artificial Intelligence Approaches for Energy Efficiency: A Review,2024-07-31T16:24:52Z,"Alberto Pasqualetto, Lorenzo Serafini, Michele Sprocatti","United Nations set Sustainable Development Goals and this paper focuses on
7th (Affordable and Clean Energy), 9th (Industries, Innovation and
Infrastructure), and 13th (Climate Action) goals. Climate change is a major
concern in our society; for this reason, a current global objective is to
reduce energy waste. This work summarizes all main approaches towards energy
efficiency using Artificial Intelligence with a particular focus on multi-agent
systems to create smart buildings. It mentions the tight relationship between
AI, especially IoT, and Big Data. It explains the application of AI to anomaly
detection in smart buildings and a possible classification of Intelligent
Energy Management Systems: Direct and Indirect. Finally, some drawbacks of AI
approaches and some possible future research focuses are proposed.",http://arxiv.org/abs/2407.21726v1
"Multi-model evidence of future tropical Atlantic precipitation change
  modulated by AMOC decline",2024-11-28T13:52:39Z,"Giada Cerato, Katinka Bellomo, Roberta D Agostino, Jost von Hardenberg","Projections from global climate models reveal a significant inter-model
spread in future rainfall changes in the tropical Atlantic by the end of the
21st century, including alterations to the Intertropical Convergence Zone
(ITCZ) and monsoonal regions. While existing studies have identified various
sources of uncertainty, our research uncovers a prominent role played by the
decline of the Atlantic Meridional Overturning Circulation (AMOC) for the
inter-model spread. Firstly we examine 30 climate model simulations (using the
ssp5-8.5 scenario) from the CMIP6 archive and show that models that present a
more substantial AMOC decline exhibit an equatorward shift of the ascending
branch of the Atlantic regional Hadley circulation, resulting in a southward
displacement of the ITCZ. Conversely, models characterized by a smaller AMOC
decline do not indicate any ITCZ displacement. Secondly, we use targeted
experiments (using the abrupt 4xCO2 experiment) to specifically isolate the
effects of a weakened AMOC from the changes in precipitation that would occur
if, under continuous global warming, the AMOC did not weaken. Our results
demonstrate that net precipitation anomalies in the abrupt 4xCO2 experiments
are displaced southwards compared to the simulation with fixed AMOC strength,
corroborating our previous findings. Our study has implications for
understanding the mechanisms driving future changes in tropical Atlantic
precipitation, and underscores the central role played by the AMOC in future
climate change.",http://arxiv.org/abs/2411.19151v1
"Climate Trends of Tropical Cyclone Intensity and Energy Extremes
  Revealed by Deep Learning",2024-02-01T06:02:29Z,"Buo-Fu Chen, Boyo Chen, Chun-Min Hsiao, Hsu-Feng Teng, Cheng-Shang Lee, Hung-Chi Kuo","Anthropogenic influences have been linked to tropical cyclone (TC) poleward
migration, TC extreme precipitation, and an increased proportion of major
hurricanes [1, 2, 3, 4]. Understanding past TC trends and variability is
critical for projecting future TC impacts on human society considering the
changing climate [5]. However, past trends of TC structure/energy remain
uncertain due to limited observations; subjective-analyzed and
spatiotemporal-heterogeneous ""best-track"" datasets lead to reduced confidence
in the assessed TC repose to climate change [6, 7]. Here, we use deep learning
to reconstruct past ""observations"" and yield an objective global TC wind
profile dataset during 1981 to 2020, facilitating a comprehensive examination
of TC structure/energy. By training with uniquely labeled data integrating best
tracks and numerical model analysis of 2004 to 2018 TCs, our model converts
multichannel satellite imagery to a 0-750-km wind profile of axisymmetric
surface winds. The model performance is verified to be sufficient for climate
studies by comparing it to independent satellite-radar surface winds. Based on
the new homogenized dataset, the major TC proportion has increased by ~13% in
the past four decades. Moreover, the proportion of extremely high-energy TCs
has increased by ~25%, along with an increasing trend (> one standard deviation
of the 40-y variability) of the mean total energy of high-energy TCs. Although
the warming ocean favors TC intensification, the TC track migration to higher
latitudes and altered environments further affect TC structure/energy. This new
deep learning method/dataset reveals novel trends regarding TC structure
extremes and may help verify simulations/studies regarding TCs in the changing
climate.",http://arxiv.org/abs/2402.00362v1
"Forecasting hospital discharges for respiratory conditions in Costa Rica
  using climate and pollution data",2024-01-06T00:47:44Z,"Shu Wei Chou-Chen, Luis A. Barboza","Respiratory diseases represent one of the most significant economic burdens
on healthcare systems worldwide. The variation in the increasing number of
cases depends greatly on climatic seasonal effects, socioeconomic factors, and
pollution. Therefore, understanding these variations and obtaining precise
forecasts allows health authorities to make correct decisions regarding the
allocation of limited economic and human resources. This study aims to model
and forecast weekly hospitalizations due to respiratory conditions in seven
regional hospitals in Costa Rica using four statistical learning techniques
(Random Forest, XGboost, Facebook's Prophet forecasting model, and an ensemble
method combining the above methods), along with 22 climate change indices and
aerosol optical depth as an indicator of pollution. Models are trained using
data from 2000 to 2018 and are evaluated using data from 2019 as testing data.
Reliable predictions are obtained for each of the seven regional hospitals",http://arxiv.org/abs/2401.03101v1
"MasonPerplexity at ClimateActivism 2024: Integrating Advanced Ensemble
  Techniques and Data Augmentation for Climate Activism Stance and Hate Event
  Identification",2024-02-03T01:06:33Z,"Al Nahian Bin Emran, Amrita Ganguly, Sadiya Sayara Chowdhury Puspo, Dhiman Goswami, Md Nishat Raihan","The task of identifying public opinions on social media, particularly
regarding climate activism and the detection of hate events, has emerged as a
critical area of research in our rapidly changing world. With a growing number
of people voicing either to support or oppose to climate-related issues -
understanding these diverse viewpoints has become increasingly vital. Our team,
MasonPerplexity, participates in a significant research initiative focused on
this subject. We extensively test various models and methods, discovering that
our most effective results are achieved through ensemble modeling, enhanced by
data augmentation techniques like back-translation. In the specific components
of this research task, our team achieved notable positions, ranking 5th, 1st,
and 6th in the respective sub-tasks, thereby illustrating the effectiveness of
our approach in this important field of study.",http://arxiv.org/abs/2402.01976v1
"Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System
  Model Fields with Generative Machine Learning",2024-03-05T08:41:41Z,"Philipp Hess, Michael Aich, Baoxiang Pan, Niklas Boers","Accurate and high-resolution Earth system model (ESM) simulations are
essential to assess the ecological and socio-economic impacts of anthropogenic
climate change, but are computationally too expensive to be run at sufficiently
high spatial resolution. Recent machine learning approaches have shown
promising results in downscaling ESM simulations, outperforming
state-of-the-art statistical approaches. However, existing methods require
computationally costly retraining for each ESM and extrapolate poorly to
climates unseen during training. We address these shortcomings by learning a
consistency model (CM) that efficiently and accurately downscales arbitrary ESM
simulations without retraining in a zero-shot manner. Our approach yields
probabilistic downscaled fields at a resolution only limited by the
observational reference data. We show that the CM outperforms state-of-the-art
diffusion models at a fraction of computational cost while maintaining high
controllability on the downscaling task. Further, our method generalizes to
climate states unseen during training without explicitly formulated physical
constraints.",http://arxiv.org/abs/2403.02774v3
"Physics-based deep learning reveals rising heating demand heightens air
  pollution in Norwegian cities",2024-05-07T23:43:46Z,"Cong Cao, Ramit Debnath, R. Michael Alvarez","Policymakers frequently analyze air quality and climate change in isolation,
disregarding their interactions. This study explores the influence of specific
climate factors on air quality by contrasting a regression model with K-Means
Clustering, Hierarchical Clustering, and Random Forest techniques. We employ
Physics-based Deep Learning (PBDL) and Long Short-Term Memory (LSTM) to examine
the air pollution predictions. Our analysis utilizes ten years (2009-2018) of
daily traffic, weather, and air pollution data from three major cities in
Norway. Findings from feature selection reveal a correlation between rising
heating degree days and heightened air pollution levels, suggesting increased
heating activities in Norway are a contributing factor to worsening air
quality. PBDL demonstrates superior accuracy in air pollution predictions
compared to LSTM. This paper contributes to the growing literature on PBDL
methods for more accurate air pollution predictions using environmental
variables, aiding policymakers in formulating effective data-driven climate
policies.",http://arxiv.org/abs/2405.04716v1
Data-driven Global Ocean Modeling for Seasonal to Decadal Prediction,2024-05-24T10:23:17Z,"Zijie Guo, Pumeng Lyu, Fenghua Ling, Lei Bai, Jing-Jia Luo, Niklas Boers, Toshio Yamagata, Takeshi Izumo, Sophie Cravatte, Antonietta Capotondi, Wanli Ouyang","Accurate ocean dynamics modeling is crucial for enhancing understanding of
ocean circulation, predicting climate variability, and tackling challenges
posed by climate change. Despite improvements in traditional numerical models,
predicting global ocean variability over multi-year scales remains challenging.
Here, we propose ORCA-DL (Oceanic Reliable foreCAst via Deep Learning), the
first data-driven 3D ocean model for seasonal to decadal prediction of global
ocean circulation. ORCA-DL accurately simulates three-dimensional ocean
dynamics and outperforms state-of-the-art dynamical models in capturing extreme
events, including El Ni\~no-Southern Oscillation and upper ocean heatwaves.
This demonstrates the high potential of data-driven models for efficient and
accurate global ocean forecasting. Moreover, ORCA-DL stably emulates ocean
dynamics at decadal timescales, demonstrating its potential even for skillful
decadal predictions and climate projections.",http://arxiv.org/abs/2405.15412v2
"Black carbon plumes from gas flaring in North Africa identified from
  multi-spectral imagery with deep learning",2024-06-10T11:27:46Z,"Tuel Alexandre, Kerdreux Thomas, Thiry Louis","Black carbon (BC) is an important pollutant aerosol emitted by numerous human
activities, including gas flaring. Improper combustion in flaring activities
can release large amounts of BC, which is harmful to human health and has a
strong climate warming effect. To our knowledge, no study has ever directly
monitored BC emissions from satellite imagery. Previous works quantified BC
emissions indirectly, by applying emission coefficients to flaring volumes
estimated from satellite imagery. Here, we develop a deep learning framework
and apply it to Sentinel-2 imagery over North Africa during 2022 to detect and
quantify BC emissions from gas flaring. We find that BC emissions in this
region amount to about 1 million tCO$_{2,\mathrm{eq}}$, or 1 million passenger
cars, more than a quarter of which are due to 10 sites alone. This work
demonstrates the operational monitoring of BC emissions from flaring, a key
step in implementing effective mitigation policies to reduce the climate impact
of oil and gas operations.",http://arxiv.org/abs/2406.06183v1
"AI, Climate, and Transparency: Operationalizing and Improving the AI Act",2024-08-28T07:57:39Z,"Nicolas Alder, Kai Ebert, Ralf Herbrich, Philipp Hacker","This paper critically examines the AI Act's provisions on climate-related
transparency, highlighting significant gaps and challenges in its
implementation. We identify key shortcomings, including the exclusion of energy
consumption during AI inference, the lack of coverage for indirect greenhouse
gas emissions from AI applications, and the lack of standard reporting
methodology. The paper proposes a novel interpretation to bring
inference-related energy use back within the Act's scope and advocates for
public access to climate-related disclosures to foster market accountability
and public scrutiny. Cumulative server level energy reporting is recommended as
the most suitable method. We also suggests broader policy changes, including
sustainability risk assessments and renewable energy targets, to better address
AI's environmental impact.",http://arxiv.org/abs/2409.07471v1
"Bidirectional Topic Matching: Quantifying Thematic Overlap Between
  Corpora Through Topic Modelling",2024-12-24T12:02:43Z,"Raven Adam, Marie Lisa Kogler","This study introduces Bidirectional Topic Matching (BTM), a novel method for
cross-corpus topic modeling that quantifies thematic overlap and divergence
between corpora. BTM is a flexible framework that can incorporate various topic
modeling approaches, including BERTopic, Top2Vec, and Latent Dirichlet
Allocation (LDA). BTM employs a dual-model approach, training separate topic
models for each corpus and applying them reciprocally to enable comprehensive
cross-corpus comparisons. This methodology facilitates the identification of
shared themes and unique topics, providing nuanced insights into thematic
relationships. Validation against cosine similarity-based methods demonstrates
the robustness of BTM, with strong agreement metrics and distinct advantages in
handling outlier topics. A case study on climate news articles showcases BTM's
utility, revealing significant thematic overlaps and distinctions between
corpora focused on climate change and climate action. BTM's flexibility and
precision make it a valuable tool for diverse applications, from political
discourse analysis to interdisciplinary studies. By integrating shared and
unique topic analyses, BTM offers a comprehensive framework for exploring
thematic relationships, with potential extensions to multilingual and dynamic
datasets. This work highlights BTM's methodological contributions and its
capacity to advance discourse analysis across various domains.",http://arxiv.org/abs/2412.18376v1
"Optimal dynamic climate adaptation pathways: a case study of New York
  City",2024-02-05T05:59:34Z,"Chi Truong, Matteo Malavasi, Han Li, Stefan Trueck, Pavel V. Shevchenko","Assessing climate risk and its potential impacts on our cities and economies
is of fundamental importance. Extreme weather events, such as hurricanes,
floods, and storm surges can lead to catastrophic damages. We propose a
flexible approach based on real options analysis and extreme value theory,
which enables the selection of optimal adaptation pathways for a portfolio of
climate adaptation projects. We model the severity of extreme sea level events
using the block maxima approach from extreme value theory, and then develop a
real options framework, factoring in climate change, sea level rise
uncertainty, and the growth in asset exposure. We then apply the proposed
framework to a real-world problem, considering sea level data as well as
different adaptation investment options for New York City. Our research can
assist governments and policy makers in taking informed decisions about optimal
adaptation pathways and more specifically about reducing flood and storm surge
risk in a dynamic settings.",http://arxiv.org/abs/2402.02745v1
"A non-intrusive machine learning framework for debiasing long-time
  coarse resolution climate simulations and quantifying rare events statistics",2024-02-28T17:06:19Z,"Benedikt Barthel Sorensen, Alexis Charalampopoulos, Shixuan Zhang, Bryce Harrop, Ruby Leung, Themistoklis Sapsis","Due to the rapidly changing climate, the frequency and severity of extreme
weather is expected to increase over the coming decades. As fully-resolved
climate simulations remain computationally intractable, policy makers must rely
on coarse-models to quantify risk for extremes. However, coarse models suffer
from inherent bias due to the ignored ""sub-grid"" scales. We propose a framework
to non-intrusively debias coarse-resolution climate predictions using
neural-network (NN) correction operators. Previous efforts have attempted to
train such operators using loss functions that match statistics. However, this
approach falls short with events that have longer return period than that of
the training data, since the reference statistics have not converged. Here, the
scope is to formulate a learning method that allows for correction of dynamics
and quantification of extreme events with longer return period than the
training data. The key obstacle is the chaotic nature of the underlying
dynamics. To overcome this challenge, we introduce a dynamical systems approach
where the correction operator is trained using reference data and a coarse
model simulation nudged towards that reference. The method is demonstrated on
debiasing an under-resolved quasi-geostrophic model and the Energy Exascale
Earth System Model (E3SM). For the former, our method enables the
quantification of events that have return period two orders longer than the
training data. For the latter, when trained on 8 years of ERA5 data, our
approach is able to correct the coarse E3SM output to closely reflect the
36-year ERA5 statistics for all prognostic variables and significantly reduce
their spatial biases.",http://arxiv.org/abs/2402.18484v1
"Carbon cycle instability for high-$\mathrm{CO_2}$ exoplanets:
  Implications for habitability",2024-05-08T19:55:13Z,"R. J. Graham, R. T. Pierrehumbert","Implicit in the definition of the classical circumstellar habitable zone (HZ)
is the hypothesis that the carbonate-silicate cycle can maintain clement
climates on exoplanets with land and surface water across a range of
instellations by adjusting atmospheric $\mathrm{CO_2}$ partial pressure
($p\mathrm{CO_2}$). This hypothesis is made by analogy to the Earth system, but
it is an open question whether silicate weathering can stabilize climate on
planets in the outer reaches of the HZ, where instellations are lower than
those received by even the Archean Earth and $\mathrm{CO_2}$ is thought likely
to dominate atmospheres. Since weathering products are carried from land to
ocean by the action of water, silicate weathering is intimately coupled to the
hydrologic cycle, which intensifies with hotter temperatures under Earth-like
conditions. Here, we use global climate model (GCM) simulations to demonstrate
that the hydrologic cycle responds counterintuitively to changes in climate on
planets with $\mathrm{CO_2}$-$\mathrm{H_2O}$ atmospheres at low instellations
and high $p\mathrm{CO_2}$, with global evaporation and precipitation decreasing
as $p\mathrm{CO_2}$ and temperatures increase at a given instellation. Within
the MAC weathering formulation, weathering then decreases with increasing
$p\mathrm{CO_2}$ for a range of instellations and $p\mathrm{CO_2}$ typical of
the outer reaches of the HZ, resulting in an unstable carbon cycle that may
lead to either runaway $\mathrm{CO_2}$ accumulation or depletion of
$\mathrm{CO_2}$ to colder (possibly Snowball) conditions. While the behavior of
the system has not been completely mapped out, the results suggest that
silicate weathering could fail to maintain habitable conditions in the outer
reaches of the nominal HZ.",http://arxiv.org/abs/2405.05396v1
How optimal control of polar sea-ice depends on its tipping points,2024-07-24T15:29:20Z,"Parvathi Kooloth, Jian Lu, Craig Bakker, Derek DeSantis, Adam Rupe","Several Earth system components are at a high risk of undergoing rapid and
irreversible qualitative changes or `tipping', due to increasing climate
warming. Potential tipping elements include Arctic sea-ice, Atlantic meridional
overturning circulation, and tropical coral reefs. Amidst such immediate
concerns, it has become necessary to investigate the feasibility of arresting
or even reversing the crossing of tipping thresholds using feedback control. In
this paper, we study the control of an idealized diffusive energy balance model
(EBM) for the Earth's climate; this model has two tipping points due to strong
co-albedo feedback. One of these tipping points is a `small icecap' instability
responsible for a rapid transition to an ice-free climate state under
increasing greenhouse gas (GHG) forcing. We develop an optimal control strategy
for the EBM under different climate forcing scenarios with the goal of
reversing sea ice loss while minimizing costs. We find that effective control
is achievable for such a system, but the cost of reversing sea-ice loss nearly
quadruples for an initial state that has just tipped as compared to a state
before reaching the tipping point. We also show that thermal inertia may delay
tipping leading to an overshoot of the critical GHG forcing threshold. This may
offer a short intervention window (overshoot window) during which the control
required to reverse sea-ice loss only scales linearly with intervention time.
While systems with larger system inertia may have longer overshoot windows,
this increased elbow room comes with a steeper rise in the requisite control
once the intervention is delayed past this window. Additionally, we find that
the requisite control to restore sea-ice is localized in the polar region.",http://arxiv.org/abs/2407.17357v1
"Transit Rider Heat Stress in Atlanta, GA under Current and Future
  Climate Scenarios",2024-08-06T21:37:38Z,"Huiying Fan, Geyu Lyu, Hongyu Lu, Angshuman Guin, Randall Guensler","Transit is a crucial mode of transportation, especially in urban areas and
for urban and rural disadvantaged communities. Because extreme temperatures
often pose threats to the elderly, members of the disability community, and
other vulnerable populations, this study seeks to understand the level of
influence that extreme temperatures may have on transit users across different
demographic groups. In this case study for Atlanta, GA, heat stress is
predicted for 2019 transit riders (using transit rider activity survey data)
and for three future climate scenarios, SSP245, SSP370, and SSP585, into the
year 2100. The HeatPath Analyzer and TransitSim 4.0 models were applied to
predict cumulative heat exposure and trip-level risk for 35,999 trip
equivalents for an average Atlanta area weekday in the summer of 2019. The
analyses show that under 2019 weather conditions, 8.33% of summer trips were
estimated to be conducted under extreme heat. With the projected future climate
conditions, the percentage of trips under extreme heat risk grows steadily. By
2100, 37.1%, 56.1%, and 76.4% are projected to be under extreme heat risk for
scenarios SSP245, SSP370, and SSP585, respectively. Under current weather
conditions, Atlanta transit riders that own no vehicles and transit riders that
are African American are disproportionately influenced by extreme heat. The
disparity between these two groups and other groups of transit riders becomes
wider as climate change continues to exacerbate. The findings of the study
highlight an urgent need to implement heat mitigation and adaptation strategies
in urban transit networks.",http://arxiv.org/abs/2408.03457v1
"Representation Learning of Complex Assemblies, An Effort to Improve
  Corporate Scope 3 Emissions Calculation",2024-08-21T06:21:31Z,"Ajay Chatterjee, Srikanth Ranganathan","Climate change is a pressing global concern for governments, corporations,
and citizens alike. This concern underscores the necessity for these entities
to accurately assess the climate impact of manufacturing goods and providing
services. Tools like process life cycle analysis (pLCA) are used to evaluate
the climate impact of production, use, and disposal, from raw material mining
through end-of-life. pLCA further enables practitioners to look deeply into
material choices or manufacturing processes for individual parts,
sub-assemblies, assemblies, and the final product. Reliable and detailed data
on the life cycle stages and processes of the product or service under study
are not always available or accessible, resulting in inaccurate assessment of
climate impact. To overcome the data limitation and enhance the effectiveness
of pLCA to generate an improved environmental impact profile, we are adopting
an innovative strategy to identify alternative parts, products, and components
that share similarities in terms of their form, function, and performance to
serve as qualified substitutes. Focusing on enterprise electronics hardware, we
propose a semi-supervised learning-based framework to identify substitute parts
that leverages product bill of material (BOM) data and a small amount of
component-level qualified substitute data (positive samples) to generate
machine knowledge graph (MKG) and learn effective embeddings of the components
that constitute electronic hardware. Our methodology is grounded in attributed
graph embeddings and introduces a strategy to generate biased negative samples
to significantly enhance the training process. We demonstrate improved
performance and generalization over existing published models.",http://arxiv.org/abs/2409.03769v1
"Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs:
  Thematic Insights and Fairness Evaluation",2024-10-07T18:07:56Z,"Tunazzina Islam, Dan Goldwasser","Climate change communication on social media increasingly employs
microtargeting strategies to effectively reach and influence specific
demographic groups. This study presents a post-hoc analysis of microtargeting
practices within climate campaigns by leveraging large language models (LLMs)
to examine Facebook advertisements. Our analysis focuses on two key aspects:
demographic targeting and fairness. We evaluate the ability of LLMs to
accurately predict the intended demographic targets, such as gender and age
group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the
LLMs to generate explanations for their classifications, providing transparent
reasoning behind each decision. These explanations reveal the specific thematic
elements used to engage different demographic segments, highlighting distinct
strategies tailored to various audiences. Our findings show that young adults
are primarily targeted through messages emphasizing activism and environmental
consciousness, while women are engaged through themes related to caregiving
roles and social advocacy. In addition to evaluating the effectiveness of LLMs
in detecting microtargeted messaging, we conduct a comprehensive fairness
analysis to identify potential biases in model predictions. Our findings
indicate that while LLMs perform well overall, certain biases exist,
particularly in the classification of senior citizens and male audiences. By
showcasing the efficacy of LLMs in dissecting and explaining targeted
communication strategies and by highlighting fairness concerns, this study
provides a valuable framework for future research aimed at enhancing
transparency, accountability, and inclusivity in social media-driven climate
campaigns.",http://arxiv.org/abs/2410.05401v1
"AI, Climate, and Regulation: From Data Centers to the AI Act",2024-10-09T08:43:53Z,"Kai Ebert, Nicolas Alder, Ralf Herbrich, Philipp Hacker","We live in a world that is experiencing an unprecedented boom of AI
applications that increasingly penetrate and enhance all sectors of private and
public life, from education, media, medicine, and mobility to the industrial
and professional workspace, and -- potentially particularly consequentially --
robotics. As this world is simultaneously grappling with climate change, the
climate and environmental implications of the development and use of AI have
become an important subject of public and academic debate. In this paper, we
aim to provide guidance on the climate-related regulation for data centers and
AI specifically, and discuss how to operationalize these requirements. We also
highlight challenges and room for improvement, and make a number of policy
proposals to this end. In particular, we propose a specific interpretation of
the AI Act to bring reporting on the previously unadressed energy consumption
from AI inferences back into the scope. We also find that the AI Act fails to
address indirect greenhouse gas emissions from AI applications. Furthermore,
for the purpose of energy consumption reporting, we compare levels of
measurement within data centers and recommend measurement at the cumulative
server level. We also argue for an interpretation of the AI Act that includes
environmental concerns in the mandatory risk assessment (sustainability risk
assessment, SIA), and provide guidance on its operationalization. The EU data
center regulation proves to be a good first step but requires further
development by including binding renewable energy and efficiency targets for
data centers. Overall, we make twelve concrete policy proposals, in four main
areas: Energy and Environmental Reporting Obligations; Legal and Regulatory
Clarifications; Transparency and Accountability Mechanisms; and Future
Far-Reaching Measures beyond Transparency.",http://arxiv.org/abs/2410.06681v1
Origin and Limits of Invariant Warming Patterns in Climate Models,2024-11-21T14:47:44Z,"Paolo Giani, Arlene M. Fiore, Glenn Flierl, Raffaele Ferrari, Noelle E. Selin","Climate models exhibit an approximately invariant surface warming pattern in
typical end-of-century projections. This observation has been used extensively
in climate impact assessments for fast calculations of local temperature
anomalies, with a linear procedure known as pattern scaling. At the same time,
emerging research has also shown that time-varying warming patterns are
necessary to explain the time evolution of effective climate sensitivity in
coupled models, a mechanism that is known as the pattern effect and that
seemingly challenges the pattern scaling understanding. Here we present a
simple theory based on local energy balance arguments to reconcile this
apparent contradiction. Specifically, we show that the pattern invariance is an
inherent feature of exponential forcing, linear feedbacks, a constant forcing
pattern and diffusive dynamics. These conditions are approximately met in most
CMIP6 Shared Socioeconomic Pathways (SSP), except in the Arctic where nonlinear
feedbacks are important and in regions where aerosols considerably alter the
forcing pattern. In idealized experiments where concentrations of CO2 are
abruptly increased, such as those used to study the pattern effect, the warming
pattern can change considerably over time because of spatially inhomogeneous
ocean heat uptake, even in the absence of nonlinear feedbacks. Our results
illustrate why typical future projections are amenable to pattern scaling, and
provide a plausible explanation of why more complicated approaches, such as
nonlinear emulators, have only shown marginal improvements in accuracy over
simple linear calculations.",http://arxiv.org/abs/2411.14183v1
"Kolmogorov Modes and Linear Response of Jump-Diffusion Models:
  Applications to Stochastic Excitation of the ENSO Recharge Oscillator",2024-11-22T07:20:13Z,"Micka√´l D. Chekroun, Niccol√≤ Zagli, Valerio Lucarini","We introduce a generalization of linear response theory for mixed
jump-diffusion models, combining both Gaussian and L\'evy noise forcings that
interact with the nonlinear dynamics. This class of models covers a broad range
of stochastic chaos and complexity for which the jump-diffusion processes are a
powerful tool to parameterize the missing physics or effects of the unresolved
scales onto the resolved ones.
  By generalizing concepts such as Kolmogorov operators and Green's functions
to this context, we derive fluctuation-dissipation relationships for such
models. The system response can then be interpreted in terms of contributions
from the eigenmodes of the Kolmogorov operator (Kolmogorov modes) decomposing
the time-lagged correlation functions of the unperturbed dynamics. The
underlying formulas offer a fresh look on the intimate relationships between
the system's natural variability and its forced variability.
  We apply our theory to a paradigmatic El Ni\~no-Southern Oscillation (ENSO)
subject to state-dependent jumps and additive white noise parameterizing
intermittent and nonlinear feedback mechanisms, key factors in the actual ENSO
phenomenon. Such stochastic parameterizations are shown to produce stochastic
chaos with an enriched time-variability. The Kolmogorov modes encoding the
latter are then computed, and our Green's functions formulas are shown to
achieve a remarkable accuracy to predict the system's response to
perturbations.
  This work enriches Hasselmann's program by providing a more comprehensive
approach to climate modeling and prediction, allowing for accounting the
effects of both continuous and discontinuous stochastic forcing. Our results
have implications for understanding climate sensitivity, detection and
attributing climate change, and assessing the risk of climate tipping points.",http://arxiv.org/abs/2411.14769v1
"Advancing Marine Heatwave Forecasts: An Integrated Deep Learning
  Approach",2024-11-19T06:11:52Z,"Ding Ning, Varvara Vetrova, Yun Sing Koh, Karin R. Bryan","Marine heatwaves (MHWs), an extreme climate phenomenon, pose significant
challenges to marine ecosystems and industries, with their frequency and
intensity increasing due to climate change. This study introduces an integrated
deep learning approach to forecast short-to-long-term MHWs on a global scale.
The approach combines graph representation for modeling spatial properties in
climate data, imbalanced regression to handle skewed data distributions, and
temporal diffusion to enhance forecast accuracy across various lead times. To
the best of our knowledge, this is the first study that synthesizes three
spatiotemporal anomaly methodologies to predict MHWs. Additionally, we
introduce a method for constructing graphs that avoids isolated nodes and
provide a new publicly available sea surface temperature anomaly graph dataset.
We examine the trade-offs in the selection of loss functions and evaluation
metrics for MHWs. We analyze spatial patterns in global MHW predictability by
focusing on historical hotspots, and our approach demonstrates better
performance compared to traditional numerical models in regions such as the
middle south Pacific, equatorial Atlantic near Africa, south Atlantic, and
high-latitude Indian Ocean. We highlight the potential of temporal diffusion to
replace the conventional sliding window approach for long-term forecasts,
achieving improved prediction up to six months in advance. These insights not
only establish benchmarks for machine learning applications in MHW forecasting
but also enhance understanding of general climate forecasting methodologies.",http://arxiv.org/abs/2412.04475v1
Breaching 1.5¬∞C: Give me the odds,2024-12-18T13:51:46Z,"J. Eduardo Vera-Vald√©s, Olivia Kvist","Climate change communication is crucial to raising awareness and motivating
action. In the context of breaching the limits set out by the Paris Agreement,
we argue that climate scientists should move away from point estimates and
towards reporting probabilities. Reporting probabilities will provide
policymakers with a range of possible outcomes and will allow them to make
informed timely decisions. To achieve this goal, we propose a method to
calculate the probability of breaching the limits set out by the Paris
Agreement. The method can be summarized as predicting future temperatures under
different scenarios and calculating the number of possible outcomes that breach
the limits as a proportion of the total number of outcomes. The probabilities
can be computed for different time horizons and can be updated as new data
become available. As an illustration, we performed a simulation study to
investigate the probability of breaching the limits in a statistical model. Our
results show that the probability of breaching the 1.5{\deg}C limit is already
greater than zero for 2024. Moreover, the probability of breaching the limit is
greater than 99% by 2042 if no action is taken to reduce greenhouse gas
emissions. Our methodology is simple to implement and can easily be extended to
more complex models of the climate system. We encourage climate model
developers to include the probabilities of breaching the limits in their
reports.",http://arxiv.org/abs/2412.13855v1
DroughtSet: Understanding Drought Through Spatial-Temporal Learning,2024-12-19T17:24:15Z,"Xuwei Tan, Qian Zhao, Yanlan Liu, Xueru Zhang","Drought is one of the most destructive and expensive natural disasters,
severely impacting natural resources and risks by depleting water resources and
diminishing agricultural yields. Under climate change, accurately predicting
drought is critical for mitigating drought-induced risks. However, the
intricate interplay among the physical and biological drivers that regulate
droughts limits the predictability and understanding of drought, particularly
at a subseasonal to seasonal (S2S) time scale. While deep learning has been
demonstrated with potential in addressing climate forecasting challenges, its
application to drought prediction has received relatively less attention. In
this work, we propose a new dataset, DroughtSet, which integrates relevant
predictive features and three drought indices from multiple remote sensing and
reanalysis datasets across the contiguous United States (CONUS). DroughtSet
specifically provides the machine learning community with a new real-world
dataset to benchmark drought prediction models and more generally, time-series
forecasting methods. Furthermore, we propose a spatial-temporal model SPDrought
to predict and interpret S2S droughts. Our model learns from the spatial and
temporal information of physical and biological features to predict three types
of droughts simultaneously. Multiple strategies are employed to quantify the
importance of physical and biological features for drought prediction. Our
results provide insights for researchers to better understand the
predictability and sensitivity of drought to biological and physical
conditions. We aim to contribute to the climate field by proposing a new tool
to predict and understand the occurrence of droughts and provide the AI
community with a new benchmark to study deep learning applications in climate
science.",http://arxiv.org/abs/2412.15075v1
"Predicting Coastal Water Levels in the Context of Climate Change Using
  Kolmogorov-Zurbenko Time Series Analysis Methods",2024-12-12T16:25:41Z,"Barry Loneck, Igor Zurbenko, Edward Valachovic","Given recent increases in ocean water levels brought on by climate change,
this investigation decomposed changes in coastal water levels into its
fundamental components to predict maximum water levels for a given coastal
location. The study focused on Virginia Key, Florida, in the United States,
located near the coast of Miami. Hourly mean lower low water (MLLW) levels were
obtained from the National Data Buoy Center from January 28, 1994, through
December 31, 2023. In the temporal dimension, Kolmogorov-Zurbenko filters were
used to extract long-term trends, annual and daily tides, and higher frequency
harmonics, while in the spectral dimension, Kolmogorov-Zurbenko periodograms
with DiRienzo-Zurbenko algorithm smoothing were used to confirm known tidal
frequencies and periods. A linear model predicted that the long-term trend in
water level will rise 2.02 feet from January 1994 to December 2050, while a
quadratic model predicted a rise of 5.91 during the same period. In addition,
the combined crests of annual tides, daily tides, and higher frequency
harmonics increase water levels up to 2.16 feet, yielding a combined total of
4.18 feet as a lower bound and a combined total of 8.09 feet as an upper bound.
These findings provide a foundation for more accurate prediction of coastal
flooding during severe weather events and provide an impetus for policy choices
with respect to residential communities, businesses, and wildlife habitats.
Further, using Kolmogorov-Zurbenko analytic methods to study coastal sites
throughout the world could draw a more comprehensive picture of the impact
climate change is having on coastal waters globally.",http://arxiv.org/abs/2412.09419v1
"Modelling Species Distributions with Deep Learning to Predict Plant
  Extinction Risk and Assess Climate Change Impacts",2024-01-10T15:24:27Z,"Joaquim Estopinan, Pierre Bonnet, Maximilien Servajean, Fran√ßois Munoz, Alexis Joly","The post-2020 global biodiversity framework needs ambitious, research-based
targets. Estimating the accelerated extinction risk due to climate change is
critical. The International Union for Conservation of Nature (IUCN) measures
the extinction risk of species. Automatic methods have been developed to
provide information on the IUCN status of under-assessed taxa. However, these
compensatory methods are based on current species characteristics, mainly
geographical, which precludes their use in future projections. Here, we
evaluate a novel method for classifying the IUCN status of species benefiting
from the generalisation power of species distribution models based on deep
learning. Our method matches state-of-the-art classification performance while
relying on flexible SDM-based features that capture species' environmental
preferences. Cross-validation yields average accuracies of 0.61 for status
classification and 0.78 for binary classification. Climate change will reshape
future species distributions. Under the species-environment equilibrium
hypothesis, SDM projections approximate plausible future outcomes. Two extremes
of species dispersal capacity are considered: unlimited or null. The projected
species distributions are translated into features feeding our IUCN
classification method. Finally, trends in threatened species are analysed over
time and i) by continent and as a function of average ii) latitude or iii)
altitude. The proportion of threatened species is increasing globally, with
critical rates in Africa, Asia and South America. Furthermore, the proportion
of threatened species is predicted to peak around the two Tropics, at the
Equator, in the lowlands and at altitudes of 800-1,500 m.",http://arxiv.org/abs/2401.05470v1
"Kicking the Can Down the Road: Understanding the Effects of Delaying the
  Deployment of Stratospheric Aerosol Injection",2024-02-19T09:35:18Z,"Ezra Brody, Daniele Visioni, Ewa M. Bednarz, Ben Kravitz, Douglas G. MacMartin, Jadwiga H. Richter, Mari R. Tye","Climate change is a prevalent threat, and it is unlikely that current
mitigation efforts will be enough to avoid unwanted impacts. One potential
option to reduce climate change impacts is the use of stratospheric aerosol
injection (SAI). Even if SAI is ultimately deployed, it might be initiated only
after some temperature target is exceeded. The consequences of such a delay are
assessed herein. This study compares two cases, with the same target global
mean temperature of 1.5C above preindustrial, but start dates of 2035 or a
delayed start in 2045. We make use of simulations in the Community Earth System
Model version 2 with the Whole Atmosphere Coupled Chemistry Model version 6
(CESM2-WACCM6), using SAI under the SSP2-4.5 emissions pathway. We find that
delaying the start of deployment (relative to the target temperature)
necessitates lower net radiative forcing (-30%) and thus larger sulfur dioxide
injection rates (+20%), even after surface temperatures converge, to compensate
for the extra energy absorbed by the Earth system. However, many of the surface
climate differences between the 2035 and 2045 start simulations appear to be
small during the 10-25 years following the delayed SAI start, although longer
simulations would be needed to assess any longer-term impacts in this model. In
addition, irreversibilities and tipping points that might be triggered during
the period of increased warming may not be adequately represented in the model
but could change this conclusion in the real world.",http://arxiv.org/abs/2402.11992v1
"S{√©}curit{√©} alimentaire de l'agriculture indig{√®}ne
  guat{√©}malt{√®}que face {√†} l'incertitude sociale et climatique",2024-03-28T08:56:25Z,"Julien Malard-Adam, Jan Adamowski, H√©ctor Tuy, Hugo Melgar-Qui√±onez","Given the increasing pressures exerted by climate change on small-scale
agriculture, the importance of participatory modelling methodologies that can
consider both the human and environmental components of these systems has
become more and more evident. The current study presents a socioeconomic system
dynamics model of the food and environmental systems of the predominantly
Indigenous region of Tz'ol{\""o}j Ya', Guatemala. The model was built in a
participatory manner with stakeholders from the region and was then coupled to
an external crop growth model before being applied to the analysis of the
impact of future climate change and its potential interactions with various
stakeholder-proposed policies. The analysis identified several feedback loops
between environmental and human components of the system that can lead to
counterintuitive responses to the proposed policies. At the same time, the use
of an external crop growth model allowed for a more realistic, yet still easily
implementable, representation of the impacts of climate change on crop
production. This analysis is the first to use a socioeconomic system dynamics
model coupled with an external crop growth model to analyse food security in
the context of a local socio-environmental food system.",http://arxiv.org/abs/2404.02168v1
"An Open and Large-Scale Dataset for Multi-Modal Climate Change-aware
  Crop Yield Predictions",2024-06-10T07:54:56Z,"Fudong Lin, Kaleb Guillot, Summer Crawford, Yihe Zhang, Xu Yuan, Nian-Feng Tzeng","Precise crop yield predictions are of national importance for ensuring food
security and sustainable agricultural practices. While AI-for-science
approaches have exhibited promising achievements in solving many scientific
problems such as drug discovery, precipitation nowcasting, etc., the
development of deep learning models for predicting crop yields is constantly
hindered by the lack of an open and large-scale deep learning-ready dataset
with multiple modalities to accommodate sufficient information. To remedy this,
we introduce the CropNet dataset, the first terabyte-sized, publicly available,
and multi-modal dataset specifically targeting climate change-aware crop yield
predictions for the contiguous United States (U.S.) continent at the county
level. Our CropNet dataset is composed of three modalities of data, i.e.,
Sentinel-2 Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset, for over
2200 U.S. counties spanning 6 years (2017-2022), expected to facilitate
researchers in developing versatile deep learning models for timely and
precisely predicting crop yields at the county-level, by accounting for the
effects of both short-term growing season weather variations and long-term
climate change on crop yields. Besides, we develop the CropNet package,
offering three types of APIs, for facilitating researchers in downloading the
CropNet data on the fly over the time and region of interest, and flexibly
building their deep learning models for accurate crop yield predictions.
Extensive experiments have been conducted on our CropNet dataset via employing
various types of deep learning solutions, with the results validating the
general applicability and the efficacy of the CropNet dataset in climate
change-aware crop yield predictions.",http://arxiv.org/abs/2406.06081v2
"Influence of climate variability on the potential forage production of a
  mown permanent grassland in the French Massif Central",2024-01-25T10:17:49Z,"I√±igo G√≥mara, Gianni Bellocchi, Rapha√´l Martin, Bel√©n Rodr√≠guez-Fonseca, Margarita Ruiz-Ramos","Climate Services (CS) provide support to decision makers across
socio-economic sectors. In the agricultural sector, one of the most important
CS applications is to provide timely and accurate yield forecasts based on
climate prediction. In this study, the Pasture Simulation model (PaSim) was
used to simulate, for the period 1959-2015, the forage production of a mown
grassland system (Laqueuille, Massif Central of France) under different
management conditions, with meteorological inputs extracted from the SAFRAN
atmospheric database. The aim was to generate purely climate-dependent
timeseries of optimal forage production, a variable that was maximized by
brighter and warmer weather conditions at the grassland. A long-term increase
was observed in simulated forage yield, with the 1995-2015 average being 29%
higher than the 1959-1979 average. Such increase seems consistent with observed
rising trends in temperature and CO2, and multi-decadal changes in incident
solar radiation. At interannual timescales, sea surface temperature anomalies
of the Mediterranean (MED), Tropical North Atlantic (TNA), equatorial Pacific
(El Ni\~no Southern Oscillation) and the North Atlantic Oscillation (NAO) index
were found robustly correlated with annual forage yield values. Relying only on
climatic predictors, we developed a stepwise statistical multi-regression model
with leave-one-out cross-validation. Under specific management conditions
(e.g., three annual cuts) and from one to five months in advance, the generated
model successfully provided a p-value<0.01 in correlation (t-test), a root mean
square error percentage (%RMSE) of 14.6% and a 71.43% hit rate predicting
above/below average years in terms of forage yield collection.",http://arxiv.org/abs/2401.14053v1
Optimal Power Grid Operations with Foundation Models,2024-09-03T09:06:13Z,"Alban Puech, Jonas Weiss, Thomas Brunschwiler, Hendrik F. Hamann","The energy transition, crucial for tackling the climate crisis, demands
integrating numerous distributed, renewable energy sources into existing grids.
Along with climate change and consumer behavioral changes, this leads to
changes and variability in generation and load patterns, introducing
significant complexity and uncertainty into grid planning and operations. While
the industry has already started to exploit AI to overcome computational
challenges of established grid simulation tools, we propose the use of AI
Foundation Models (FMs) and advances in Graph Neural Networks to efficiently
exploit poorly available grid data for different downstream tasks, enhancing
grid operations. For capturing the grid's underlying physics, we believe that
building a self-supervised model learning the power flow dynamics is a critical
first step towards developing an FM for the power grid. We show how this
approach may close the gap between the industry needs and current grid analysis
capabilities, to bring the industry closer to optimal grid operation and
planning.",http://arxiv.org/abs/2409.02148v1
"Uncertainty-enabled machine learning for emulation of regional sea-level
  change caused by the Antarctic Ice Sheet",2024-06-21T18:27:09Z,"Myungsoo Yoo, Giri Gopalan, Matthew J. Hoffman, Sophie Coulson, Holly Kyeore Han, Christopher K. Wikle, Trevor Hillebrand","Projecting sea-level change in various climate-change scenarios typically
involves running forward simulations of the Earth's gravitational, rotational
and deformational (GRD) response to ice mass change, which requires high
computational cost and time. Here we build neural-network emulators of
sea-level change at 27 coastal locations, due to the GRD effects associated
with future Antarctic Ice Sheet mass change over the 21st century. The
emulators are based on datasets produced using a numerical solver for the
static sea-level equation and published ISMIP6-2100 ice-sheet model simulations
referenced in the IPCC AR6 report. We show that the neural-network emulators
have an accuracy that is competitive with baseline machine learning emulators.
In order to quantify uncertainty, we derive well-calibrated prediction
intervals for simulated sea-level change via a linear regression postprocessing
technique that uses (nonlinear) machine learning model outputs, a technique
that has previously been applied to numerical climate models. We also
demonstrate substantial gains in computational efficiency: a feedforward
neural-network emulator exhibits on the order of 100 times speedup in
comparison to the numerical sea-level equation solver that is used for
training.",http://arxiv.org/abs/2406.17729v1
"Microwave Remote Sensing of Soil Moisture, Above Ground Biomass and
  Freeze-Thaw Dynamic: Modeling and Empirical Approaches",2024-12-04T18:10:00Z,"Laura Angeloni, Domenico Daniele Bloisi, Paolo Burghignoli, Davide Comite, Danilo Costarelli, Michele Piconi, Anna Rita Sambucini, Alessio Troiani, Alessandro Veneri","Human actions have accelerated changes in global temperature, precipitation
patterns, and other critical Earth systems. Key markers of these changes can be
linked to the dynamic of Essential Climate Variables (ECVs) and related
quantities, such as Soil Moisture (SM), Above Ground Biomass (AGB), and
Freeze-Thaw (FT) Dynamics. These variables are crucial for understanding global
climate changes, hydrological and carbon cycles included. Monitoring these
variables helps to validate climate models and inform policy decisions.
Technologies like microwave remote sensing provide critical tools for
monitoring the effects of human activities on these variables at a global
scale. Other than proper tachenological developments, the study of ECVs
requires suitable theoretical retrieval tools, which leads to the solutions of
inverse problems. In this brief survey, we analyze and summarize the main
retrieval techniques available in the literature for SM, AGB, and FT, performed
on data collected with microwave remote sensing sensors. Such methods will be
some of the fundamental algorithms that can find applications in the research
activities of the interdisciplinary, curiosity-driven, project {\it REmote
sensing daTa INversion with multivariate functional modeling for essential
climAte variables characterization (RETINA)}, recently funded by the European
Union under the Italian National Recovery and Resilience Plan of
NextGenerationEU, under the Italian Ministry of University and Research. The
main goal of RETINA, in which three research units from three different italian
universities are involved, is to create innovative techniques for analyzing
data generated by the interaction of electromagnetic waves with the Earth's
surface, applying theoretical insights to address real-world challenges.",http://arxiv.org/abs/2412.03523v1
"Modelling Global Fossil CO2 Emissions with a Lognormal Distribution: A
  Climate Policy Tool",2024-03-01T16:34:10Z,"Faustino Prieto, Catalina B. Garc√≠a-Garc√≠a, Rom√°n Salmer√≥n G√≥mez","Carbon dioxide (CO2) emissions have emerged as a critical issue with profound
impacts on the environment, human health, and the global economy. The steady
increase in atmospheric CO2 levels, largely due to human activities such as
burning fossil fuels and deforestation, has become a major contributor to
climate change and its associated catastrophic effects. To tackle this pressing
challenge, a coordinated global effort is needed, which necessitates a deep
understanding of emissions patterns and trends. In this paper, we explore the
use of statistical modelling, specifically the lognormal distribution, as a
framework for comprehending and predicting CO2 emissions. We build on prior
research that suggests a complex distribution of emissions and seek to test the
hypothesis that a simpler distribution can still offer meaningful insights for
policy-makers. We utilize data from three comprehensive databases and analyse
six candidate distributions (exponential, Fisk, gamma, lognormal, Lomax,
Weibull) to identify a suitable model for global fossil CO2 emissions. Our
findings highlight the adequacy of the lognormal distribution in characterizing
emissions across all countries and years studied. Furthermore, to provide
additional support for this distribution, we provide statistical evidence
supporting the applicability of Gibrat's law to those CO2 emissions. Finally,
we employ the lognormal model to predict emission parameters for the coming
years and propose two policies for reducing total fossil CO2 emissions. Our
research aims to provide policy-makers with accurate and detailed information
to support effective climate change mitigation strategies.",http://arxiv.org/abs/2403.00653v1
"A Comprehensive Approach to Carbon Dioxide Emission Analysis in High
  Human Development Index Countries using Statistical and Machine Learning
  Techniques",2024-05-01T21:00:02Z,"Hamed Khosravi, Ahmed Shoyeb Raihan, Farzana Islam, Ashish Nimbarte, Imtiaz Ahmed","Reducing Carbon dioxide (CO2) emission is vital at both global and national
levels, given their significant role in exacerbating climate change. CO2
emission, stemming from a variety of industrial and economic activities, are
major contributors to the greenhouse effect and global warming, posing
substantial obstacles in addressing climate issues. It's imperative to forecast
CO2 emission trends and classify countries based on their emission patterns to
effectively mitigate worldwide carbon emission. This paper presents an in-depth
comparative study on the determinants of CO2 emission in twenty countries with
high Human Development Index (HDI), exploring factors related to economy,
environment, energy use, and renewable resources over a span of 25 years. The
study unfolds in two distinct phases: initially, statistical techniques such as
Ordinary Least Squares (OLS), fixed effects, and random effects models are
applied to pinpoint significant determinants of CO2 emission. Following this,
the study leverages supervised and unsupervised machine learning (ML) methods
to further scrutinize and understand the factors influencing CO2 emission.
Seasonal AutoRegressive Integrated Moving Average with eXogenous variables
(SARIMAX), a supervised ML model, is first used to predict emission trends from
historical data, offering practical insights for policy formulation.
Subsequently, Dynamic Time Warping (DTW), an unsupervised learning approach, is
used to group countries by similar emission patterns. The dual-phase approach
utilized in this study significantly improves the accuracy of CO2 emission
predictions while also providing a deeper insight into global emission trends.
By adopting this thorough analytical framework, nations can develop more
focused and effective carbon reduction policies, playing a vital role in the
global initiative to combat climate change.",http://arxiv.org/abs/2405.02340v1
Anticipatory Understanding of Resilient Agriculture to Climate,2024-11-07T22:29:05Z,"David Willmes, Nick Krall, James Tanis, Zachary Terner, Fernando Tavares, Chris Miller, Joe Haberlin III, Matt Crichton, Alexander Schlichting","With billions of people facing moderate or severe food insecurity, the
resilience of the global food supply will be of increasing concern due to the
effects of climate change and geopolitical events. In this paper we describe a
framework to better identify food security hotspots using a combination of
remote sensing, deep learning, crop yield modeling, and causal modeling of the
food distribution system. While we feel that the methods are adaptable to other
regions of the world, we focus our analysis on the wheat breadbasket of
northern India, which supplies a large percentage of the world's population. We
present a quantitative analysis of deep learning domain adaptation methods for
wheat farm identification based on curated remote sensing data from France. We
model climate change impacts on crop yields using the existing crop yield
modeling tool WOFOST and we identify key drivers of crop simulation error using
a longitudinal penalized functional regression. A description of a system
dynamics model of the food distribution system in India is also presented,
along with results of food insecurity identification based on seeding this
model with the predicted crop yields.",http://arxiv.org/abs/2411.05219v2
"Harnessing Network Science for Urban Resilience: The CASA Model's
  Approach to Social and Environmental Challenges",2024-11-12T18:45:32Z,"Miguel Fuentes, Juan Pablo C√°rdenas, Gast√≥n Olivares, Eric Rasmussen, Carolina Urbina, Soledad Salazar, Gerardo Vidal","Resilience in social systems is crucial for mitigating the impacts of crises,
such as climate change, which poses an existential threat to communities
globally. As disasters become more frequent and severe, enhancing community
resilience has become imperative. This study introduces a cutting-edge
framework, quantitative network-based modeling called Complex Analysis for
Socio-environmental Adaptation (CASA) to evaluate and strengthen social
resilience. CASA transforms resilience models' linear and static structure into
a complex network that integrates complexity and systems thinking, utilizing
global scientific knowledge and complex network methodologies. The resulting
resilience framework features rich interdependencies, and subsequent
dimensionality reduction produces robust resilience indicators. This innovative
application of network sciences is then demonstrated by quantitatively
assessing what are known as ""Sacrifice Zones,"" socio-environmentally sensitive
areas. Results unveil the potential of this novel application of complex
network methodologies as tools for systemic diagnostics, identifying
vulnerabilities, and guiding policies and practices to enhance climate
resilience and adaptation. The CASA framework represents a pioneering tool for
assessing territorial resilience, leveraging network science applications, big
data analytics, and artificial intelligence. CASA serves as a systemic
diagnostic tool for urban resilience and a guide for policymakers, urban
planners, and other professionals to promote sustainable, healthy cities in an
era of climate change.",http://arxiv.org/abs/2411.08015v1
"Spatio-temporal patterns of diurnal temperature: a random matrix
  approach I-case of India",2024-04-17T20:57:19Z,"Madhuchhanda Bhattacharjee, Arup Bose","We consider the spatio-temporal gridded daily diurnal temperature range (DTR)
data across India during the 72-year period 1951--2022. We augment this data
with information on the El Nino-Southern Oscillation (ENSO) and on the climatic
regions (Stamp's and Koeppen's classification) and four seasons of India.
  We use various matrix theory approaches to trim out strong but routine
signals, random matrix theory to remove noise, and novel empirical generalised
singular-value distributions to establish retention of essential signals in the
trimmed data. We make use of the spatial Bergsma statistics to measure spatial
association and identify temporal change points in the spatial-association.
  In particular, our investigation captures a yet unknown change-point over the
72 years under study with drastic changes in spatial-association of DTR in
India. It also brings out changes in spatial association with regard to ENSO.
  We conclude that while studying/modelling Indian DTR data, due consideration
should be granted to the strong spatial association that is being persistently
exhibited over decades, and provision should be kept for potential change
points in the temporal behaviour, which in turn can bring moderate to dramatic
changes in the spatial association pattern.
  Some of our analysis also reaffirms the conclusions made by other authors,
regarding spatial and temporal behavior of DTR, adding our own insights. We
consider the data from the yearly, seasonal and climatic zones points of view,
and discover several new and interesting statistical structures which should be
of interest, especially to climatologists and statisticians. Our methods are
not country specific and could be used profitably for DTR data from other
geographical areas.",http://arxiv.org/abs/2404.11747v1
"Accurate and Efficient Urban Street Tree Inventory with Deep Learning on
  Mobile Phone Imagery",2024-01-02T12:16:01Z,"Asim Khan, Umair Nawaz, Anwaar Ulhaq, Iqbal Gondal, Sajid Javed","Deforestation, a major contributor to climate change, poses detrimental
consequences such as agricultural sector disruption, global warming, flash
floods, and landslides. Conventional approaches to urban street tree inventory
suffer from inaccuracies and necessitate specialised equipment. To overcome
these challenges, this paper proposes an innovative method that leverages deep
learning techniques and mobile phone imaging for urban street tree inventory.
Our approach utilises a pair of images captured by smartphone cameras to
accurately segment tree trunks and compute the diameter at breast height (DBH).
Compared to traditional methods, our approach exhibits several advantages,
including superior accuracy, reduced dependency on specialised equipment, and
applicability in hard-to-reach areas. We evaluated our method on a
comprehensive dataset of 400 trees and achieved a DBH estimation accuracy with
an error rate of less than 2.5%. Our method holds significant potential for
substantially improving forest management practices. By enhancing the accuracy
and efficiency of tree inventory, our model empowers urban management to
mitigate the adverse effects of deforestation and climate change.",http://arxiv.org/abs/2401.01180v1
"GeoFormer: A Vision and Sequence Transformer-based Approach for
  Greenhouse Gas Monitoring",2024-02-11T11:20:29Z,"Madhav Khirwar, Ankur Narang","Air pollution represents a pivotal environmental challenge globally, playing
a major role in climate change via greenhouse gas emissions and negatively
affecting the health of billions. However predicting the spatial and temporal
patterns of pollutants remains challenging. The scarcity of ground-based
monitoring facilities and the dependency of air pollution modeling on
comprehensive datasets, often inaccessible for numerous areas, complicate this
issue. In this work, we introduce GeoFormer, a compact model that combines a
vision transformer module with a highly efficient time-series transformer
module to predict surface-level nitrogen dioxide (NO2) concentrations from
Sentinel-5P satellite imagery. We train the proposed model to predict
surface-level NO2 measurements using a dataset we constructed with Sentinel-5P
images of ground-level monitoring stations, and their corresponding NO2
concentration readings. The proposed model attains high accuracy (MAE 5.65),
demonstrating the efficacy of combining vision and time-series transformer
architectures to harness satellite-derived data for enhanced GHG emission
insights, proving instrumental in advancing climate change monitoring and
emission regulation efforts globally.",http://arxiv.org/abs/2402.07164v1
"From Spectra to Biophysical Insights: End-to-End Learning with a Biased
  Radiative Transfer Model",2024-03-05T12:38:54Z,"Yihang She, Clement Atzberger, Andrew Blake, Srinivasan Keshav","Advances in machine learning have boosted the use of Earth observation data
for climate change research. Yet, the interpretability of machine-learned
representations remains a challenge, particularly in understanding forests'
biophysical reactions to climate change. Traditional methods in remote sensing
that invert radiative transfer models (RTMs) to retrieve biophysical variables
from spectral data often fail to account for biases inherent in the RTM,
especially for complex forests. We propose to integrate RTMs into an
auto-encoder architecture, creating an end-to-end learning approach. Our method
not only corrects biases in RTMs but also outperforms traditional techniques
for variable retrieval like neural network regression. Furthermore, our
framework has potential generally for inverting biased physical models. The
code is available on https://github.com/yihshe/ai-refined-rtm.git.",http://arxiv.org/abs/2403.02922v1
"Imbalance-aware Presence-only Loss Function for Species Distribution
  Modeling",2024-03-12T10:08:36Z,"Robin Zbinden, Nina van Tiel, Marc Ru√üwurm, Devis Tuia","In the face of significant biodiversity decline, species distribution models
(SDMs) are essential for understanding the impact of climate change on species
habitats by connecting environmental conditions to species occurrences.
Traditionally limited by a scarcity of species observations, these models have
significantly improved in performance through the integration of larger
datasets provided by citizen science initiatives. However, they still suffer
from the strong class imbalance between species within these datasets, often
resulting in the penalization of rare species--those most critical for
conservation efforts. To tackle this issue, this study assesses the
effectiveness of training deep learning models using a balanced presence-only
loss function on large citizen science-based datasets. We demonstrate that this
imbalance-aware loss function outperforms traditional loss functions across
various datasets and tasks, particularly in accurately modeling rare species
with limited observations.",http://arxiv.org/abs/2403.07472v1
Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting,2024-03-25T10:42:48Z,"Busra Asan, Abdullah Akg√ºl, Alper Unal, Melih Kandemir, Gozde Unal","Seasonal forecasting is a crucial task when it comes to detecting the extreme
heat and colds that occur due to climate change. Confidence in the predictions
should be reliable since a small increase in the temperatures in a year has a
big impact on the world. Calibration of the neural networks provides a way to
ensure our confidence in the predictions. However, calibrating regression
models is an under-researched topic, especially in forecasters. We calibrate a
UNet++ based architecture, which was shown to outperform physics-based models
in temperature anomalies. We show that with a slight trade-off between
prediction error and calibration error, it is possible to get more reliable and
sharper forecasts. We believe that calibration should be an important part of
safety-critical machine learning applications such as weather forecasters.",http://arxiv.org/abs/2403.16612v2
Predicting Species Occurrence Patterns from Partial Observations,2024-03-26T18:29:39Z,"Hager Radi Abdelwahed, M√©lisande Teng, David Rolnick","To address the interlinked biodiversity and climate crises, we need an
understanding of where species occur and how these patterns are changing.
However, observational data on most species remains very limited, and the
amount of data available varies greatly between taxonomic groups. We introduce
the problem of predicting species occurrence patterns given (a) satellite
imagery, and (b) known information on the occurrence of other species. To
evaluate algorithms on this task, we introduce SatButterfly, a dataset of
satellite images, environmental data and observational data for butterflies,
which is designed to pair with the existing SatBird dataset of bird
observational data. To address this task, we propose a general model, R-Tran,
for predicting species occurrence patterns that enables the use of partial
observational data wherever found. We find that R-Tran outperforms other
methods in predicting species encounter rates with partial information both
within a taxon (birds) and across taxa (birds and butterflies). Our approach
opens new perspectives to leveraging insights from species with abundant data
to other species with scarce data, by modelling the ecosystems in which they
co-occur.",http://arxiv.org/abs/2403.18028v2
Modelling the evolution of an ice sheet's weathering crust,2024-05-03T14:12:36Z,"Tilly Woods, Ian J. Hewitt","The weathering crust is a layer of porous ice that can form at the surface of
an ice sheet. It grows and decays in response changing weather and climate
conditions, affecting the albedo, the melt rate, and the transport of meltwater
across the surface. To understand this behaviour, we seek time-dependent
solutions to a continuum, thermodynamic model for the porosity, temperature and
thickness of the weathering crust, and the internal and surface melt rates. We
find solutions using a numerical enthalpy method, presented in this study. We
use idealised `switching' and sinusoidal forcings to explore the different
dynamics exhibited during growth and decay, the timescales involved, and the
impact of diurnal vs. annual variations. The results demonstrate qualitative
agreement with observations, and provide insight into the relative importance
of different surface heat fluxes during the growth and decay of the crust. The
model therefore provides a useful tool for exploring the response of the
weathering crust to climate change.",http://arxiv.org/abs/2405.02111v1
Catastrophe Insurance: An Adaptive Robust Optimization Approach,2024-05-11T18:35:54Z,"Dimitris Bertsimas, Cynthia Zeng","The escalating frequency and severity of natural disasters, exacerbated by
climate change, underscore the critical role of insurance in facilitating
recovery and promoting investments in risk reduction. This work introduces a
novel Adaptive Robust Optimization (ARO) framework tailored for the calculation
of catastrophe insurance premiums, with a case study applied to the United
States National Flood Insurance Program (NFIP). To the best of our knowledge,
it is the first time an ARO approach has been applied to for disaster insurance
pricing. Our methodology is designed to protect against both historical and
emerging risks, the latter predicted by machine learning models, thus directly
incorporating amplified risks induced by climate change. Using the US flood
insurance data as a case study, optimization models demonstrate effectiveness
in covering losses and produce surpluses, with a smooth balance transition
through parameter fine-tuning. Among tested optimization models, results show
ARO models with conservative parameter values achieving low number of insolvent
states with the least insurance premium charged. Overall, optimization
frameworks offer versatility and generalizability, making it adaptable to a
variety of natural disaster scenarios, such as wildfires, droughts, etc. This
work not only advances the field of insurance premium modeling but also serves
as a vital tool for policymakers and stakeholders in building resilience to the
growing risks of natural catastrophes.",http://arxiv.org/abs/2405.07068v1
Breeding Programs Optimization with Reinforcement Learning,2024-06-06T10:17:51Z,"Omar G. Younis, Luca Corinzia, Ioannis N. Athanasiadis, Andreas Krause, Joachim M. Buhmann, Matteo Turchetta","Crop breeding is crucial in improving agricultural productivity while
potentially decreasing land usage, greenhouse gas emissions, and water
consumption. However, breeding programs are challenging due to long turnover
times, high-dimensional decision spaces, long-term objectives, and the need to
adapt to rapid climate change. This paper introduces the use of Reinforcement
Learning (RL) to optimize simulated crop breeding programs. RL agents are
trained to make optimal crop selection and cross-breeding decisions based on
genetic information. To benchmark RL-based breeding algorithms, we introduce a
suite of Gym environments. The study demonstrates the superiority of RL
techniques over standard practices in terms of genetic gain when simulated in
silico using real-world genomic maize data.",http://arxiv.org/abs/2406.03932v1
A machine learning pipeline for automated insect monitoring,2024-06-18T19:51:16Z,"Aditya Jain, Fagner Cunha, Michael Bunsen, L√©onard Pasi, Anna Viklund, Maxim Larriv√©e, David Rolnick","Climate change and other anthropogenic factors have led to a catastrophic
decline in insects, endangering both biodiversity and the ecosystem services on
which human society depends. Data on insect abundance, however, remains
woefully inadequate. Camera traps, conventionally used for monitoring
terrestrial vertebrates, are now being modified for insects, especially moths.
We describe a complete, open-source machine learning-based software pipeline
for automated monitoring of moths via camera traps, including object detection,
moth/non-moth classification, fine-grained identification of moth species, and
tracking individuals. We believe that our tools, which are already in use
across three continents, represent the future of massively scalable data
collection in entomology.",http://arxiv.org/abs/2406.13031v1
"Methodology for Calculating CO2 Absorption by Tree Planting for Greening
  Projects",2024-07-08T04:18:39Z,"Kento Ichii, Toshiki Muraoka, Nobumichi Shinohara, Shunsuke Managi, Shutaro Takeda","In order to explore the possibility of carbon credits for greening projects,
which play an important role in climate change mitigation, this paper examines
a formula for estimating the amount of carbon fixation for greening activities
in urban areas through tree planting. The usefulness of the formula studied was
examined by conducting calculations based on actual data through measurements
made by on-site surveys of a greening companie. A series of calculation results
suggest that this formula may be useful. Recognizing carbon credits for green
businesses for the carbon sequestration of their projects is an important
incentive not only as part of environmental improvement and climate change
action, but also to improve the health and well-being of local communities and
to generate economic benefits. This study is a pioneering exploration of the
methodology.",http://arxiv.org/abs/2407.05596v1
Reconciling risk-based and storyline attribution with Bayes theorem,2024-07-15T14:55:17Z,"Sebastian Buschow, Petra Friederichs, Andreas Hense","The question to what extent climate change is responsible for extreme weather
events has been at the forefront of public and scholarly discussion for years.
Proponents of the ""risk-based"" approach to attribution attempt to give an
unconditional answer based on the probability of some class of events in a
world with and without human influences. As an alternative, so-called
""storyline"" studies investigate the impact of a warmer world on a single,
specific weather event. This can be seen as a conditional attribution
statement. In this study, we connect conditional to unconditional attribution
using Bayes theorem: in essence, the conditional statement is composed of two
unconditional statements, one based on all available data (event and
conditions) and one based on the conditions alone. We explore the effects of
the conditioning in a simple statistical toy model and a real-world attribution
of European summer temperatures conditional on blocking. The resulting
attribution statement is generally strengthened if the conditions are not
affected by climate change. Conversely, if part of the trend is contained in
the conditions, a weaker attribution statement may result.",http://arxiv.org/abs/2407.10776v1
"Assessing the Effectiveness of GPT-4o in Climate Change Evidence
  Synthesis and Systematic Assessments: Preliminary Insights",2024-07-02T13:14:57Z,"Elphin Tom Joe, Sai Dileep Koneru, Christine J Kirchhoff","In this research short, we examine the potential of using GPT-4o, a
state-of-the-art large language model (LLM) to undertake evidence synthesis and
systematic assessment tasks. Traditional workflows for such tasks involve large
groups of domain experts who manually review and synthesize vast amounts of
literature. The exponential growth of scientific literature and recent advances
in LLMs provide an opportunity to complementing these traditional workflows
with new age tools. We assess the efficacy of GPT-4o to do these tasks on a
sample from the dataset created by the Global Adaptation Mapping Initiative
(GAMI) where we check the accuracy of climate change adaptation related feature
extraction from the scientific literature across three levels of expertise. Our
results indicate that while GPT-4o can achieve high accuracy in low-expertise
tasks like geographic location identification, their performance in
intermediate and high-expertise tasks, such as stakeholder identification and
assessment of depth of the adaptation response, is less reliable. The findings
motivate the need for designing assessment workflows that utilize the strengths
of models like GPT-4o while also providing refinements to improve their
performance on these tasks.",http://arxiv.org/abs/2407.12826v1
"Assessing Generative Language Models in Classification Tasks:
  Performance and Self-Evaluation Capabilities in the Environmental and Climate
  Change Domain",2024-08-30T15:52:41Z,"Francesca Grasso, Stefano Locci","This paper examines the performance of two Large Language Models (LLMs),
GPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three
different classification tasks within the climate change (CC) and environmental
domain. Employing BERT-based models as a baseline, we compare their efficacy
against these transformer-based models. Additionally, we assess the models'
self-evaluation capabilities by analyzing the calibration of verbalized
confidence scores in these text classification tasks. Our findings reveal that
while BERT-based models generally outperform both the LLMs and SLM, the
performance of the large generative models is still noteworthy. Furthermore,
our calibration analysis reveals that although Gemma is well-calibrated in
initial tasks, it thereafter produces inconsistent results; Llama is reasonably
calibrated, and GPT consistently exhibits strong calibration. Through this
research, we aim to contribute to the ongoing discussion on the utility and
effectiveness of generative LMs in addressing some of the planet's most urgent
issues, highlighting their strengths and limitations in the context of ecology
and CC.",http://arxiv.org/abs/2408.17362v1
Mapping earth mounds from space,2024-08-31T18:08:37Z,"Baki Uzun, Shivam Pande, Gwendal Cachin-Bernard, Minh-Tan Pham, S√©bastien Lef√®vre, Rumais Blatrix, Doyle McKey","Regular patterns of vegetation are considered widespread landscapes, although
their global extent has never been estimated. Among them, spotted landscapes
are of particular interest in the context of climate change. Indeed, regularly
spaced vegetation spots in semi-arid shrublands result from extreme resource
depletion and prefigure catastrophic shift of the ecosystem to a homogeneous
desert, while termite mounds also producing spotted landscapes were shown to
increase robustness to climate change. Yet, their identification at large scale
calls for automatic methods, for instance using the popular deep learning
framework, able to cope with a vast amount of remote sensing data, e.g.,
optical satellite imagery. In this paper, we tackle this problem and benchmark
some state-of-the-art deep networks on several landscapes and geographical
areas. Despite the promising results we obtained, we found that more research
is needed to be able to map automatically these earth mounds from space.",http://arxiv.org/abs/2409.00518v1
MultiClimate: Multimodal Stance Detection on Climate Change Videos,2024-09-26T23:48:08Z,"Jiawen Wang, Longfei Zuo, Siyao Peng, Barbara Plank","Climate change (CC) has attracted increasing attention in NLP in recent
years. However, detecting the stance on CC in multimodal data is understudied
and remains challenging due to a lack of reliable datasets. To improve the
understanding of public opinions and communication strategies, this paper
presents MultiClimate, the first open-source manually-annotated stance
detection dataset with $100$ CC-related YouTube videos and $4,209$
frame-transcript pairs. We deploy state-of-the-art vision and language models,
as well as multimodal models for MultiClimate stance detection. Results show
that text-only BERT significantly outperforms image-only ResNet50 and ViT.
Combining both modalities achieves state-of-the-art, $0.747$/$0.749$ in
accuracy/F1. Our 100M-sized fusion models also beat CLIP and BLIP, as well as
the much larger 9B-sized multimodal IDEFICS and text-only Llama3 and Gemma2,
indicating that multimodal stance detection remains challenging for large
language models. Our code, dataset, as well as supplementary materials, are
available at https://github.com/werywjw/MultiClimate.",http://arxiv.org/abs/2409.18346v1
Green bubbles: a four-stage paradigm for detection and propagation,2024-10-09T05:58:29Z,"Gian Luca Vriz, Luigi Grossi","Climate change has emerged as a significant global concern, attracting
increasing attention worldwide. While green bubbles may be examined through a
social bubble hypothesis, it is essential not to neglect a Climate Minsky
moment triggered by sudden asset price changes. The significant increase in
green investments highlights the urgent need for a comprehensive understanding
of these market dynamics. Therefore, the current paper introduces a novel
paradigm for studying such phenomena. Focusing on the renewable energy sector,
Statistical Process Control (SPC) methodologies are employed to identify green
bubbles within time series data. Furthermore, search volume indexes and social
factors are incorporated into established econometric models to reveal
potential implications for the financial system. Inspired by Joseph
Schumpeter's perspectives on business cycles, this study recognizes green
bubbles as a necessary evil for facilitating a successful transition towards a
more sustainable future.",http://arxiv.org/abs/2410.06564v1
"Design and Performance Evaluation of an Elbow-Based Biomechanical Energy
  Harvester",2024-10-11T17:53:10Z,"Hubert Huang, Jeffrey Huang","Carbon emissions have long been attributed to the increase in climate change.
With the effects of climate change escalating in the past few years, there has
been an increased effort to find green alternatives to power generation, which
has been a major contributor to carbon emissions. One prominent way that has
arisen is biomechanical energy, or harvesting energy based on natural human
movement. This study will evaluate the feasibility of electric generation using
a gear and generator-based biomechanical energy harvester in the elbow joint.
The joint was chosen using kinetic arm analysis through MediaPipe, in which the
elbow joint showed much higher angular velocity during walking, thus showing
more potential as a place to construct the harvester. Leg joints were excluded
to not obstruct daily movement. The gear and generator type was decided to
maximize energy production in the elbow joint. The device was constructed using
a gearbox and a generator. The results show that it generated as much as 0.16
watts using the optimal resistance. This demonstrates the feasibility of
electric generation with an elbow joint gear and generator-type biomechanical
energy harvester.",http://arxiv.org/abs/2410.09036v1
"Underutilized land and sustainable development: effects on employment,
  economic output, and mitigation of CO2 emissions",2024-10-11T17:10:10Z,"Seymur Garibov, Wadim Strielkowski","Climate change, deforestation, and biodiversity loss are calling for
innovative approaches to effective reforestation and afforestation. This paper
explores the integration of artificial intelligence and remote sensing
technologies for optimizing tree planting strategies, estimating labor
requirements, and determining space needs for various tree species in Gabala
District of Azerbaijan. The study employs YOLOv8 for precise identification of
potential planting sites and a Retrieval-Augmented Generation approach,
combined with the Gemini API, to provide tailored species recommendations. The
methodology incorporates time-series modeling to forecast the impact of
reforestation on CO2 emissions reduction, utilizing Holt-Winters for
predictions. Our results indicate that the AI model can effectively identify
suitable locations and species, offering valuable insights into the potential
economic and environmental benefits of large-scale tree planting thus fostering
sustainable economic development and helping to mitigate the adverse effects of
global warming and climate change.",http://arxiv.org/abs/2410.09136v1
"Design of Amine-Functionalized Materials for Direct Air Capture Using
  Integrated High-Throughput Calculations and Machine Learning",2024-10-17T19:26:12Z,"Megan C. Davis, Wilton J. M. Kort-Kamp, Ivana Matanovic, Piotr Zelenay, Edward F. Holby","Direct air capture (DAC) of carbon dioxide is a critical technology for
mitigating climate change, but current materials face limitations in efficiency
and scalability. We discover novel DAC materials using a combined machine
learning (ML) and high-throughput atomistic modeling approach. Our ML model
accurately predicts high-quality, density functional theory-computed CO$_{2}$
binding enthalpies for a wide range of nitrogen-bearing moieties. Leveraging
this model, we rapidly screen over 1.6 million binding sites from a
comprehensive database of theoretically feasible molecules to identify
materials with superior CO$_{2}$ binding properties. Additionally, we assess
the synthesizability and experimental feasibility of these structures using
established ML metrics, discovering nearly 2,500 novel materials suitable for
integration into DAC devices. Altogether, our high-fidelity database and ML
framework represent a significant advancement in the rational development of
scalable, cost-effective carbon dioxide capture technologies, offering a
promising pathway to meet key targets in the global initiative to combat
climate change.",http://arxiv.org/abs/2410.13982v1
"RL for Mitigating Cascading Failures: Targeted Exploration via
  Sensitivity Factors",2024-11-27T04:34:31Z,"Anmol Dwivedi, Ali Tajer, Santiago Paternain, Nurali Virani","Electricity grid's resiliency and climate change strongly impact one another
due to an array of technical and policy-related decisions that impact both.
This paper introduces a physics-informed machine learning-based framework to
enhance grid's resiliency. Specifically, when encountering disruptive events,
this paper designs remedial control actions to prevent blackouts. The proposed
Physics-Guided Reinforcement Learning (PG-RL) framework determines effective
real-time remedial line-switching actions, considering their impact on power
balance, system security, and grid reliability. To identify an effective
blackout mitigation policy, PG-RL leverages power-flow sensitivity factors to
guide the RL exploration during agent training. Comprehensive evaluations using
the Grid2Op platform demonstrate that incorporating physical signals into RL
significantly improves resource utilization within electric grids and achieves
better blackout mitigation policies - both of which are critical in addressing
climate change.",http://arxiv.org/abs/2411.18050v1
"Composing Open-domain Vision with RAG for Ocean Monitoring and
  Conservation",2024-12-03T08:34:42Z,"Sepand Dyanatkar, Angran Li, Alexander Dungate","Climate change's destruction of marine biodiversity is threatening
communities and economies around the world which rely on healthy oceans for
their livelihoods. The challenge of applying computer vision to niche,
real-world domains such as ocean conservation lies in the dynamic and diverse
environments where traditional top-down learning struggle with long-tailed
distributions, generalization, and domain transfer. Scalable species
identification for ocean monitoring is particularly difficult due to the need
to adapt models to new environments and identify rare or unseen species. To
overcome these limitations, we propose leveraging bottom-up, open-domain
learning frameworks as a resilient, scalable solution for image and video
analysis in marine applications. Our preliminary demonstration uses pretrained
vision-language models (VLMs) combined with retrieval-augmented generation
(RAG) as grounding, leaving the door open for numerous architectural, training
and engineering optimizations. We validate this approach through a preliminary
application in classifying fish from video onboard fishing vessels,
demonstrating impressive emergent retrieval and prediction capabilities without
domain-specific training or knowledge of the task itself.",http://arxiv.org/abs/2412.02262v1
"Hierarchical Classification for Automated Image Annotation of Coral Reef
  Benthic Structures",2024-12-11T09:28:30Z,"C√©lia Blondin, Joris Gu√©rin, Kelly Inagaki, Guilherme Longo, Laure Berti-√âquille","Automated benthic image annotation is crucial to efficiently monitor and
protect coral reefs against climate change. Current machine learning approaches
fail to capture the hierarchical nature of benthic organisms covering reef
substrata, i.e., coral taxonomic levels and health condition. To address this
limitation, we propose to annotate benthic images using hierarchical
classification. Experiments on a custom dataset from a Northeast Brazilian
coral reef show that our approach outperforms flat classifiers, improving both
F1 and hierarchical F1 scores by approximately 2\% across varying amounts of
training data. In addition, this hierarchical method aligns more closely with
ecological objectives.",http://arxiv.org/abs/2412.08228v1
"Climate Downscaling: A Deep-Learning Based Super-resolution Model of
  Precipitation Data with Attention Block and Skip Connections",2024-03-26T16:36:50Z,"Chia-Hao Chiang, Zheng-Han Huang, Liwen Liu, Hsin-Chien Liang, Yi-Chi Wang, Wan-Ling Tseng, Chao Wang, Che-Ta Chen, Ko-Chih Wang","Human activities accelerate consumption of fossil fuels and produce
greenhouse gases, resulting in urgent issues today: global warming and the
climate change. These indirectly cause severe natural disasters, plenty of
lives suffering and huge losses of agricultural properties. To mitigate impacts
on our lands, scientists are developing renewable, reusable, and clean energies
and climatologists are trying to predict the extremes. Meanwhile, governments
are publicizing resource-saving policies for a more eco-friendly society and
arousing environment awareness. One of the most influencing factors is the
precipitation, bringing condensed water vapor onto lands. Water resources are
the most significant but basic needs in society, not only supporting our
livings, but also economics. In Taiwan, although the average annual
precipitation is up to 2,500 millimeter (mm), the water allocation for each
person is lower than the global average due to drastically geographical
elevation changes and uneven distribution through the year. Thus, it is crucial
to track and predict the rainfall to make the most use of it and to prevent the
floods. However, climate models have limited resolution and require intensive
computational power for local-scale use. Therefore, we proposed a deep
convolutional neural network with skip connections, attention blocks, and
auxiliary data concatenation, in order to downscale the low-resolution
precipitation data into high-resolution one. Eventually, we compare with other
climate downscaling methods and show better performance in metrics of Mean
Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,
structural similarity index (SSIM), and forecast indicators.",http://arxiv.org/abs/2403.17847v1
"Using rare event algorithms to understand the statistics and dynamics of
  extreme heatwave seasons in South Asia",2024-04-11T14:32:32Z,"Cl√©ment Le Priol, Joy M. Monteiro, Freddy Bouchet","Computing the return times of extreme events and assessing the impact of
climate change on such return times is fundamental to extreme event attribution
studies. However, the rarity of such events in the observational record makes
this task a challenging one, even more so for ""record-shattering"" events that
have not been previously observed at all. While climate models could be used to
simulate such extremely rare events, such an approach entails a huge
computational cost: gathering robust statistics for events with return time of
centuries would require a few thousand years of simulation.
  In this study, we use an innovative tool, rare event algorithm, that allows
to sample numerous extremely rare events at a much lower cost than direct
simulations. We employ the algorithm to sample extreme heatwave seasons,
corresponding to large anomalies of the seasonal average temperature, in a
heatwave hotspot of South Asia using the global climate model Plasim. We show
that the algorithm estimates the return levels of extremely rare events with
much greater precision than traditional statistical fits. It also enables the
computation of various composite statistics, whose accuracy is demonstrated
through comparison with a very long control run. In particular, our results
reveal that extreme heatwave seasons are associated with an anticyclonic
anomaly embedded within a large-scale hemispheric quasi-stationary
wave-pattern. Additionally, the algorithm accurately represents the
intensity-duration-frequency statistics of sub-seasonal heatwaves, offering
insights into both seasonal and sub-seasonal aspects of extreme heatwave
seasons. This innovative approach could be used in extreme event attribution
studies to better constrain the changes in event's probability and intensity
with global warming, particularly for events with return times spanning
centuries or millennia.",http://arxiv.org/abs/2404.07791v3
"The cloud cover and meteorological parameters at the Lenghu site on the
  Tibetan Plateau",2024-10-17T08:05:56Z,"Ruiyue Li, Fei He, Licai Deng, Xiaodian Chen, Fan Yang, Yong Zhao, Bo Zhang, Chunguang Zhang, Chen Yang, Tian Lan","The cloud cover and meteorological parameters serve as fundamental criteria
for the qualification of an astronomical observatory working in optical and
infrared wavelengths. In this paper, we present a systematic assessment of key
meteorological parameters at the Lenghu site. The datasets adopted in this
study includes the meteorological parameters collected at the local weather
stations at the site and in the Lenghu Town, the sky brightness at the local
zenith acquired by the Sky Quality Meters and night sky all-sky images from a
digital camera, the ERA5 reanalysis database and global climate monitoring
data. From 2019 to 2023, the fractional observable time of photometric
condition is 69.70%, 74.97%, 70.26%, 74.27% and 65.12%, respectively. The
fractional observing time is inversely correlated with surface air temperature,
relative humidity, precipitable water vapor, and dew temperature, demonstrating
that the observing conditions are influenced by these meteorological
parameters. Large-scale air-sea interactions affect the climate at Lenghu site,
which in fact delivers a clue to understand the irregularity of 2023.
Specifically, precipitable water vapor at Lenghu site is correlated to both the
westerly wind index and the summer North Atlantic Oscillation index, the yearly
average temperature of Lenghu site is observed to increase significantly during
the occurrence of a strong El Ni\~no event and the relative humidity anomaly at
Lenghu site is correlated to the Pacific Decadal Oscillation index. The
decrease of fractional observing time in 2023 was due to the ongoing strong El
Ni\~no event and relevant global climate change. We underscore the substantial
role of global climate change in regulating astronomical observing conditions
and the necessity for long-term continuous monitoring of the astronomical
meteorological parameters at Lenghu site.",http://arxiv.org/abs/2410.13306v1
Path to Low-Cost Direct Air Capture,2024-11-22T22:43:07Z,"Peter Eisenberger, Matthew Realff","It is now accepted that gigatonnes of Carbon Dioxide Removal (CDR) from the
atmosphere are needed to avoid the threat of catastrophic climate change.
Direct Air Capture (DAC) is a promising scalable CDR with a relatively small
environmental footprint. But questions about DAC cost and energy use remain
that are delaying the needed DAC policy decisions to create a mobilization
effort like was done in the Manhattan Project and to address the Covid crisis.
Global Thermostat (GT) has publicly claimed costs of under 50 dollars per tonne
for mature GT technology deployed at a climate relevant scale. Why this low DAC
cost is achievable will be addressed by a simplified analysis of generic DAC
costs and using that analysis combined with experimental data to evaluate GT's
DAC technology. A detailed cost analysis of different approaches to DAC by the
National Academy of Sciences (NAS) found an approach to DAC that had a learning
cost limit as low as 25 dollars per tonne GT's DAC technology will be shown in
Appendix 1 to have the same performance characteristics of the lowest-cost DAC
identified in the NAS study. Thus, like solar costs, DAC costs can be reduced
by learning by doing, but in the case of DAC, only one order of magnitude in
cost reduction is needed. Therefore DAC technology can reach its low learning
by doing cost limit at a scale much smaller than necessary to address climate
change. From a climate perspective, current DAC embodiments costs and scale
have less relevance than their learning curve cost limit. While GT's technology
has demonstrated the crucial performance parameters to achieve a low-cost DAC,
no inference should be drawn that other approaches cannot achieve low or lower
cost, if they can demonstrate the crucial performance parameters. Continued R&D
on those performance parameters is needed.",http://arxiv.org/abs/2411.15369v1
"Global Mapping of Exposure and Physical Vulnerability Dynamics in Least
  Developed Countries using Remote Sensing and Machine Learning",2024-04-02T09:04:56Z,"Joshua Dimasaka, Christian Gei√ü, Emily So","As the world marked the midterm of the Sendai Framework for Disaster Risk
Reduction 2015-2030, many countries are still struggling to monitor their
climate and disaster risk because of the expensive large-scale survey of the
distribution of exposure and physical vulnerability and, hence, are not on
track in reducing risks amidst the intensifying effects of climate change. We
present an ongoing effort in mapping this vital information using machine
learning and time-series remote sensing from publicly available Sentinel-1 SAR
GRD and Sentinel-2 Harmonized MSI. We introduce the development of
""OpenSendaiBench"" consisting of 47 countries wherein most are least developed
(LDCs), trained ResNet-50 deep learning models, and demonstrated the region of
Dhaka, Bangladesh by mapping the distribution of its informal constructions. As
a pioneering effort in auditing global disaster risk over time, this paper aims
to advance the area of large-scale risk quantification in informing our
collective long-term efforts in reducing climate and disaster risk.",http://arxiv.org/abs/2404.01748v1
"Applications of machine learning to predict seasonal precipitation for
  East Africa",2024-09-10T06:16:03Z,"Michael Scheuerer, Claudio Heinrich-Mertsching, Titike K. Bahaga, Masilin Gudoshava, Thordis L. Thorarinsdottir","Seasonal climate forecasts are commonly based on model runs from fully
coupled forecasting systems that use Earth system models to represent
interactions between the atmosphere, ocean, land and other Earth-system
components. Recently, machine learning (ML) methods are increasingly being
investigated for this task where large-scale climate variability is linked to
local or regional temperature or precipitation in a linear or non-linear
fashion. This paper investigates the use of interpretable ML methods to predict
seasonal precipitation for East Africa in an operational setting. Dimension
reduction is performed by decomposing the precipitation fields via empirical
orthogonal functions (EOFs), such that only the respective factor loadings need
to the predicted. Indices of large-scale climate variability--including the
rate of change in individual indices as well as interactions between different
indices--are then used as potential features to obtain tercile forecasts from
an interpretable ML algorithm. Several research questions regarding the use of
data and the effect of model complexity are studied. The results are compared
against the ECMWF seasonal forecasting system (SEAS5) for three seasons--MAM,
JJAS and OND--over the period 1993-2020. Compared to climatology for the same
period, the ECMWF forecasts have negative skill in MAM and JJAS and significant
positive skill in OND. The ML approach is on par with climatology in MAM and
JJAS and a significantly positive skill in OND, if not quite at the level of
the OND ECMWF forecast.",http://arxiv.org/abs/2409.06238v1
"European summer weather regimes 1990-2019: Automatic classification and
  representation in a small global climate model ensemble",2024-09-25T14:50:57Z,"Sibille Wehrmann, Carolyne Pickler, Marlene Schramm, Thomas M√∂lg","In Central Europe, the occurrence of different weather regimes (WRs) plays a
major role in spatiotemporal temperature and precipitation patterns. In the
context of increasingly extreme summers, this study focuses on European summer
WRs (June-August, JJA) over the last three decades (1990-2019), and aims to
examine the changing characteristics of these WRs and their potential
implications. In addition, based on ERA5 reanalysis data, the WR representation
from a carefully preselected, small ensemble of global general circulation
models (GCMs) is analyzed. A methodological refinement concerns the combination
of Self-Organizing Maps (SOM) with a novel GCM selection technique, which
enhances the robustness of the simulated large-scale circulation patterns. WRs
are defined using daily sea level pressure (SLP) and wind in the upper
troposphere. Results reveal that the SOM captures predominant European summer
synoptic patterns, and the salient result is a positive trend in 2 m air
temperature across nearly all WRs. The selected GCMs - MPI-ESM1-2-LR r29i1p1f1,
CanESM5 r1i1p1f1 and MRI-ESM2-0 r5i1p1f1 - identify WRs correctly and
ERA5-based results are always within the range of this small GCM ensemble. Its
members clearly show skill in representing large-scale WRs accurately and,
thus, serve as valuable tools for studying synoptic weather patterns during
summer in Central Europe. Therefore, we can recommend these GCMs for studies on
WR-related climate projections of future summer conditions, particularly for
those interested in climate impacts in Central Europe from a synoptic-scale
perspective.",http://arxiv.org/abs/2409.16988v1
"FengWu-W2S: A deep learning model for seamless weather-to-subseasonal
  forecast of global atmosphere",2024-11-15T13:44:37Z,"Fenghua Ling, Kang Chen, Jiye Wu, Tao Han, Jing-Jia Luo, Wanli Ouyang, Lei Bai","Seamless forecasting that produces warning information at continuum
timescales based on only one system is a long-standing pursuit for
weather-climate service. While the rapid advancement of deep learning has
induced revolutionary changes in classical forecasting field, current efforts
are still focused on building separate AI models for weather and climate
forecasts. To explore the seamless forecasting ability based on one AI model,
we propose FengWu-Weather to Subseasonal (FengWu-W2S), which builds on the
FengWu global weather forecast model and incorporates an ocean-atmosphere-land
coupling structure along with a diverse perturbation strategy. FengWu-W2S can
generate 6-hourly atmosphere forecasts extending up to 42 days through an
autoregressive and seamless manner. Our hindcast results demonstrate that
FengWu-W2S reliably predicts atmospheric conditions out to 3-6 weeks ahead,
enhancing predictive capabilities for global surface air temperature,
precipitation, geopotential height and intraseasonal signals such as the
Madden-Julian Oscillation (MJO) and North Atlantic Oscillation (NAO). Moreover,
our ablation experiments on forecast error growth from daily to seasonal
timescales reveal potential pathways for developing AI-based integrated system
for seamless weather-climate forecasting in the future.",http://arxiv.org/abs/2411.10191v2
"Advancing Heatwave Forecasting via Distribution Informed-Graph Neural
  Networks (DI-GNNs): Integrating Extreme Value Theory with GNNs",2024-11-20T17:45:03Z,"Farrukh A. Chishtie, Dominique Brunet, Rachel H. White, Daniel Michelson, Jing Jiang, Vicky Lucas, Emily Ruboonga, Sayana Imaash, Melissa Westland, Timothy Chui, Rana Usman Ali, Mujtaba Hassan, Roland Stull, David Hudak","Heatwaves, prolonged periods of extreme heat, have intensified in frequency
and severity due to climate change, posing substantial risks to public health,
ecosystems, and infrastructure. Despite advancements in Machine Learning (ML)
modeling, accurate heatwave forecasting at weather scales (1--15 days) remains
challenging due to the non-linear interactions between atmospheric drivers and
the rarity of these extreme events. Traditional models relying on heuristic
feature engineering often fail to generalize across diverse climates and
capture the complexities of heatwave dynamics. This study introduces the
Distribution-Informed Graph Neural Network (DI-GNN), a novel framework that
integrates principles from Extreme Value Theory (EVT) into the graph neural
network architecture. DI-GNN incorporates Generalized Pareto Distribution
(GPD)-derived descriptors into the feature space, adjacency matrix, and loss
function to enhance its sensitivity to rare heatwave occurrences. By
prioritizing the tails of climatic distributions, DI-GNN addresses the
limitations of existing methods, particularly in imbalanced datasets where
traditional metrics like accuracy are misleading. Empirical evaluations using
weather station data from British Columbia, Canada, demonstrate the superior
performance of DI-GNN compared to baseline models. DI-GNN achieved significant
improvements in balanced accuracy, recall, and precision, with high AUC and
average precision scores, reflecting its robustness in distinguishing heatwave
events.",http://arxiv.org/abs/2411.13496v1
"Enabling Adoption of Regenerative Agriculture through Soil Carbon
  Copilots",2024-11-25T19:11:41Z,"Margaret Capetz, Swati Sharma, Rafael Padilha, Peder Olsen, Jessica Wolk, Emre Kiciman, Ranveer Chandra","Mitigating climate change requires transforming agriculture to minimize
environ mental impact and build climate resilience. Regenerative agricultural
practices enhance soil organic carbon (SOC) levels, thus improving soil health
and sequestering carbon. A challenge to increasing regenerative agriculture
practices is cheaply measuring SOC over time and understanding how SOC is
affected by regenerative agricultural practices and other environmental factors
and farm management practices. To address this challenge, we introduce an
AI-driven Soil Organic Carbon Copilot that automates the ingestion of complex
multi-resolution, multi-modal data to provide large-scale insights into soil
health and regenerative practices. Our data includes extreme weather event data
(e.g., drought and wildfire incidents), farm management data (e.g., cropland
information and tillage predictions), and SOC predictions. We find that
integrating public data and specialized models enables large-scale, localized
analysis for sustainable agriculture. In comparisons of agricultural practices
across California counties, we find evidence that diverse agricultural activity
may mitigate the negative effects of tillage; and that while extreme weather
conditions heavily affect SOC, composting may mitigate SOC loss. Finally,
implementing role-specific personas empowers agronomists, farm consultants,
policymakers, and other stakeholders to implement evidence-based strategies
that promote sustainable agriculture and build climate resilience.",http://arxiv.org/abs/2411.16872v2
A Water Efficiency Dataset for African Data Centers,2024-12-04T21:09:45Z,"Noah Shumba, Opelo Tshekiso, Pengfei Li, Giulia Fanti, Shaolei Ren","AI computing and data centers consume a large amount of freshwater, both
directly for cooling and indirectly for electricity generation. While most
attention has been paid to developed countries such as the U.S., this paper
presents the first-of-its-kind dataset that combines nation-level weather and
electricity generation data to estimate water usage efficiency for data centers
in 41 African countries across five different climate regions. We also use our
dataset to evaluate and estimate the water consumption of inference on two
large language models (i.e., Llama-3-70B and GPT-4) in 11 selected African
countries. Our findings show that writing a 10-page report using Llama-3-70B
could consume about \textbf{0.7 liters} of water, while the water consumption
by GPT-4 for the same task may go up to about 60 liters. For writing a
medium-length email of 120-200 words, Llama-3-70B and GPT-4 could consume about
\textbf{0.13 liters} and 3 liters of water, respectively. Interestingly, given
the same AI model, 8 out of the 11 selected African countries consume less
water than the global average, mainly because of lower water intensities for
electricity generation. However, water consumption can be substantially higher
in some African countries with a steppe climate than the U.S. and global
averages, prompting more attention when deploying AI computing in these
countries. Our dataset is publicly available on
\href{https://huggingface.co/datasets/masterlion/WaterEfficientDatasetForAfricanCountries/tree/main}{Hugging
Face}.",http://arxiv.org/abs/2412.03716v2
"Deep Learning for Hydroelectric Optimization: Generating Long-Term River
  Discharge Scenarios with Ensemble Forecasts from Global Circulation Models",2024-12-16T16:37:27Z,Julio Alberto Silva Dias,"Hydroelectric power generation is a critical component of the global energy
matrix, particularly in countries like Brazil, where it represents the majority
of the energy supply. However, its strong dependence on river discharges, which
are inherently uncertain due to climate variability, poses significant
challenges. River discharges are linked to precipitation patterns, making the
development of accurate probabilistic forecasting models crucial for improving
operational planning in systems heavily reliant on this resource.
Traditionally, statistical models have been used to represent river discharges
in energy optimization. Yet, these models are increasingly unable to produce
realistic scenarios due to structural shifts in climate behavior. Changes in
precipitation patterns have altered discharge dynamics, which traditional
approaches struggle to capture. Machine learning methods, while effective as
universal predictors for time series, often focus solely on historical data,
ignoring key external factors such as meteorological and climatic conditions.
Furthermore, these methods typically lack a probabilistic framework, which is
vital for representing the inherent variability of hydrological processes. The
limited availability of historical discharge data further complicates the
application of large-scale deep learning models to this domain. To address
these challenges, we propose a framework based on a modified recurrent neural
network architecture. This model generates parameterized probability
distributions conditioned on projections from global circulation models,
effectively accounting for the stochastic nature of river discharges.
Additionally, the architecture incorporates enhancements to improve its
generalization capabilities. We validate this framework within the Brazilian
Interconnected System, using projections from the SEAS5-ECMWF system as
conditional variables.",http://arxiv.org/abs/2412.12234v1
The Societal Implications of Blockchain Proliferation,2024-04-03T04:37:14Z,Cory Cherven,"Blockchain and its distributed ledger technology have far-reaching
implications for consumers across the world. Cryptocurrencies like XRP work to
solve key issues in the remittance industry, targeting corridors like Mexico
where foreign remittance fuels economies. Blockchain's libertarian principles
have the potential to change lives in the third world, replacing corrupt
infrastructure with trust-based solutions. While this technology can be used to
significantly improve lives, it has a wealth of destructive applications.
Bitcoin's blockchain and nefarious websites like the Silk Road have fueled an
underground market of drugs, money laundering, and terrorism, complicating
digital currency legislation. The negative environmental effects of
cryptocurrency may also contribute significantly to global climate change.
Negatives aside, cryptocurrency still proves to be a valuable commodity in
technological development.",http://arxiv.org/abs/2404.02451v1
"Harnessing AI data-driven global weather models for climate attribution:
  An analysis of the 2017 Oroville Dam extreme atmospheric river",2024-09-17T23:34:39Z,"Jorge Ba√±o-Medina, Agniv Sengupta, Allison Michaelis, Luca Delle Monache, Julie Kalansky, Duncan Watson-Parris","AI data-driven models (Graphcast, Pangu Weather, Fourcastnet, and SFNO) are
explored for storyline-based climate attribution due to their short inference
times, which can accelerate the number of events studied, and provide real time
attributions when public attention is heightened. The analysis is framed on the
extreme atmospheric river episode of February 2017 that contributed to the
Oroville dam spillway incident in Northern California. Past and future
simulations are generated by perturbing the initial conditions with the
pre-industrial and the late-21st century temperature climate change signals,
respectively. The simulations are compared to results from a dynamical model
which represents plausible pseudo-realities under both climate environments.
Overall, the AI models show promising results, projecting a 5-6 % increase in
the integrated water vapor over the Oroville dam in the present day compared to
the pre-industrial, in agreement with the dynamical model. Different
geopotential-moisture-temperature dependencies are unveiled for each of the
AI-models tested, providing valuable information for understanding the
physicality of the attribution response. However, the AI models tend to
simulate weaker attribution values than the pseudo-reality imagined by the
dynamical model, suggesting some reduced extrapolation skill, especially for
the late-21st century regime. Large ensembles generated with an AI model (>500
members) produced statistically significant present-day to pre-industrial
attribution results, unlike the >20-member ensemble from the dynamical model.
This analysis highlights the potential of AI models to conduct attribution
analysis, while emphasizing future lines of work on explainable artificial
intelligence to gain confidence in these tools, which can enable reliable
attribution studies in real-time.",http://arxiv.org/abs/2409.11605v1
"Agricultural 4.0 Leveraging on Technological Solutions: Study for Smart
  Farming Sector",2024-01-01T17:02:49Z,"Emmanuel Kojo Gyamfi, Zag ElSayed, Jess Kropczynski, Mustapha Awinsongya Yakubu, Nelly Elsayed","By 2050, it is predicted that there will be 9 billion people on the planet,
which will call for more production, lower costs, and the preservation of
natural resources. It is anticipated that atypical occurrences and climate
change will pose severe risks to agricultural output. It follows that a 70% or
more significant rise in food output is anticipated. Smart farming, often known
as agriculture 4.0, is a tech-driven revolution in agriculture with the goal of
raising industry production and efficiency. Four primary trends are responsible
for it: food waste, climate change, population shifts, and resource scarcity.
The agriculture industry is changing as a result of the adoption of emerging
technologies. Using cutting-edge technology like IoT, AI, and other sensors,
smart farming transforms traditional production methods and international
agricultural policies. The objective is to establish a value chain that is
optimized to facilitate enhanced monitoring and decreased labor expenses. The
agricultural sector has seen tremendous transformation as a result of the
fourth industrial revolution, which has combined traditional farming methods
with cutting-edge technology to increase productivity, sustainability, and
efficiency. To effectively utilize the potential of technology gadgets in the
agriculture sector, collaboration between governments, private sector entities,
and other stakeholders is necessary. This paper covers Agriculture 4.0, looks
at its possible benefits and drawbacks of the implementation methodologies,
compatibility, reliability, and investigates the several digital tools that are
being utilized to change the agriculture industry and how to mitigate the
challenges.",http://arxiv.org/abs/2401.00814v1
"Temporal genomics help in deciphering neutral and adaptive patterns in
  the contemporary evolution of kelp populations",2024-04-22T09:09:09Z,"Lauric Reynes, Louise Fouqueau, D. Aurelle, St√©phane Mauger, Christophe Destombe, Myriam Valero","The impact of climate change on populations will be contingent upon their
contemporary adaptive evolution. In this study, we investigated the
contemporary evolution of four populations of the cold-water kelp Laminaria
digitata by analysing their spatial and temporal genomic variation using
ddRAD-sequencing. These populations were sampled from the center to the
southern margin of its north-eastern Atlantic distribution at two-time points,
spanning at least two generations. Through genome scans for local adaptation at
a single time point, we identified candidate loci that showed clinal variation
correlated with changes in sea surface temperature (SST) along latitudinal
gradients. This finding suggests that SST may drive the adaptive response of
these kelp populations, although factors such as species' demographic history
should also be considered. Additionally, we performed a simulation approach to
distinguish the effect of selection from genetic drift in allele frequency
changes over time. This enabled the detection of loci in the southernmost
population that exhibited temporal differentiation beyond what would be
expected from genetic drift alone: these are candidate loci which could have
evolved under selection over time. In contrast, we did not detect any outlier
locus based on temporal differentiation in the population from the North Sea,
which also displayed low and decreasing levels of genetic diversity. The
diverse evolutionary scenarios observed among populations can be attributed to
variations in the prevalence of selection relative to genetic drift across
different environments. Therefore, our study highlights the potential of
temporal genomics to offer valuable insights into the contemporary evolution of
marine foundation species facing climate change.",http://arxiv.org/abs/2404.14003v1
"Long-term foehn reconstruction combining unsupervised and supervised
  learning",2024-06-03T22:15:57Z,"Reto Stauffer, Achim Zeileis, Georg J. Mayr","Foehn winds, characterized by abrupt temperature increases and wind speed
changes, significantly impact regions on the leeward side of mountain ranges,
e.g., by spreading wildfires. Understanding how foehn occurrences change under
climate change is crucial. Unfortunately, foehn cannot be measured directly but
has to be inferred from meteorological measurements employing suitable
classification schemes. Hence, this approach is typically limited to specific
periods for which the necessary data are available. We present a novel approach
for reconstructing historical foehn occurrences using a combination of
unsupervised and supervised probabilistic statistical learning methods. We
utilize in-situ measurements (available for recent decades) to train an
unsupervised learner (finite mixture model) for automatic foehn classification.
These labeled data are then linked to reanalysis data (covering longer periods)
using a supervised learner (lasso or boosting). This allows to reconstruct past
foehn probabilities based solely on reanalysis data. Applying this method to
ERA5 reanalysis data for six stations across Switzerland and Austria achieves
accurate hourly reconstructions of north and south foehn occurrence,
respectively, dating back to 1940. This paves the way for investigating how
seasonal foehn patterns have evolved over the past 83 years, providing valuable
insights into climate change impacts on these critical wind events.",http://arxiv.org/abs/2406.01818v2
"Changes in anthropogenic aerosols during the first wave of COVID-19
  lockdowns in the context of long-term historical trends at 51 AERONET
  stations",2024-08-21T16:30:31Z,"Robert Blaga, Delia Calinoiu, Gavrila Trif-Tordai","A quasi-consensus has steadily formed in the scientific literature on the
fact that the prevention measures implemented by most countries to curb the
2020 COVID-19 pandemic have led to significant reductions in pollution levels
around the world, especially in urban environments. Fewer studies have looked
at the how these reductions at ground level translate into variations in the
whole atmosphere. In this study, we examine the columnar values of aerosols at
51 mainland European stations of the Aerosol Robotic Network (AERONET). We show
that when considered in the context of the long-term trend over the last
decade, the columnar aerosol levels for 2020, at the regional level, do not
appear exceptional. Both the yearly means and the number of episodes with
extreme values for this period are within the one standard deviation of the
long-term trends. We conclude that the spatially and temporally very localized
reductions do not add up to statistically significant reductions in the global
levels of aerosols. Furthermore, considering that pandemic lockdowns can be
thought of as a simulation of a climate change mitigation scenario, we conclude
that such lifestyle-based changes present a very low potential as a global
climate change mitigation strategy.",http://arxiv.org/abs/2408.11757v1
"Data Assimilation using ERA5, ASOS, and the U-STN model for Weather
  Forecasting over the UK",2024-01-15T11:21:25Z,"Wenqi Wang, Jacob Bieker, Rossella Arcucci, C√©sar Quilodr√°n-Casas","In recent years, the convergence of data-driven machine learning models with
Data Assimilation (DA) offers a promising avenue for enhancing weather
forecasting. This study delves into this emerging trend, presenting our
methodologies and outcomes. We harnessed the UK's local ERA5 850 hPa
temperature data and refined the U-STN12 global weather forecasting model,
tailoring its predictions to the UK's climate nuances. From the ASOS network,
we sourced T2m data, representing ground observations across the UK. We
employed the advanced kriging method with a polynomial drift term for
consistent spatial resolution. Furthermore, Gaussian noise was superimposed on
the ERA5 T850 data, setting the stage for ensuing multi-time step synthetic
observations. Probing into the assimilation impacts, the ASOS T2m data was
integrated with the ERA5 T850 dataset. Our insights reveal that while global
forecast models can adapt to specific regions, incorporating atmospheric data
in DA significantly bolsters model accuracy. Conversely, the direct
assimilation of surface temperature data tends to mitigate this enhancement,
tempering the model's predictive prowess.",http://arxiv.org/abs/2401.07604v1
"Wind speed super-resolution and validation: from ERA5 to CERRA via
  diffusion models",2024-01-27T17:43:08Z,"Fabio Merizzi, Andrea Asperti, Stefano Colamonaco","The Copernicus Regional Reanalysis for Europe, CERRA, is a high-resolution
regional reanalysis dataset for the European domain. In recent years it has
shown significant utility across various climate-related tasks, ranging from
forecasting and climate change research to renewable energy prediction,
resource management, air quality risk assessment, and the forecasting of rare
events, among others. Unfortunately, the availability of CERRA is lagging two
years behind the current date, due to constraints in acquiring the requisite
external data and the intensive computational demands inherent in its
generation. As a solution, this paper introduces a novel method using diffusion
models to approximate CERRA downscaling in a data-driven manner, without
additional informations. By leveraging the lower resolution ERA5 dataset, which
provides boundary conditions for CERRA, we approach this as a super-resolution
task. Focusing on wind speed around Italy, our model, trained on existing CERRA
data, shows promising results, closely mirroring original CERRA data.
Validation with in-situ observations further confirms the model's accuracy in
approximating ground measurements.",http://arxiv.org/abs/2401.15469v2
"Explainable Global Wildfire Prediction Models using Graph Neural
  Networks",2024-02-11T10:44:41Z,"Dayou Chen, Sibo Cheng, Jinwei Hu, Matthew Kasoar, Rossella Arcucci","Wildfire prediction has become increasingly crucial due to the escalating
impacts of climate change. Traditional CNN-based wildfire prediction models
struggle with handling missing oceanic data and addressing the long-range
dependencies across distant regions in meteorological data. In this paper, we
introduce an innovative Graph Neural Network (GNN)-based model for global
wildfire prediction. We propose a hybrid model that combines the spatial
prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long
Short-Term Memory (LSTM) networks. Our approach uniquely transforms global
climate and wildfire data into a graph representation, addressing challenges
such as null oceanic data locations and long-range dependencies inherent in
traditional models. Benchmarking against established architectures using an
unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior
predictive accuracy. Furthermore, we emphasise the model's explainability,
unveiling potential wildfire correlation clusters through community detection
and elucidating feature importance via Integrated Gradient analysis. Our
findings not only advance the methodological domain of wildfire prediction but
also underscore the importance of model transparency, offering valuable
insights for stakeholders in wildfire management.",http://arxiv.org/abs/2402.07152v1
Global Vegetation Modeling with Pre-Trained Weather Transformers,2024-03-27T10:45:16Z,"Pascal Janetzky, Florian Gallusser, Simon Hentschel, Andreas Hotho, Anna Krause","Accurate vegetation models can produce further insights into the complex
interaction between vegetation activity and ecosystem processes. Previous
research has established that long-term trends and short-term variability of
temperature and precipitation affect vegetation activity. Motivated by the
recent success of Transformer-based Deep Learning models for medium-range
weather forecasting, we adapt the publicly available pre-trained FourCastNet to
model vegetation activity while accounting for the short-term dynamics of
climate variability. We investigate how the learned global representation of
the atmosphere's state can be transferred to model the normalized difference
vegetation index (NDVI). Our model globally estimates vegetation activity at a
resolution of \SI{0.25}{\degree} while relying only on meteorological data. We
demonstrate that leveraging pre-trained weather models improves the NDVI
estimates compared to learning an NDVI model from scratch. Additionally, we
compare our results to other recent data-driven NDVI modeling approaches from
machine learning and ecology literature. We further provide experimental
evidence on how much data and training time is necessary to turn FourCastNet
into an effective vegetation model. Code and models will be made available upon
publication.",http://arxiv.org/abs/2403.18438v1
"EcoVerse: An Annotated Twitter Dataset for Eco-Relevance Classification,
  Environmental Impact Analysis, and Stance Detection",2024-04-08T01:21:11Z,"Francesca Grasso, Stefano Locci, Giovanni Siragusa, Luigi Di Caro","Anthropogenic ecological crisis constitutes a significant challenge that all
within the academy must urgently face, including the Natural Language
Processing (NLP) community. While recent years have seen increasing work
revolving around climate-centric discourse, crucial environmental and
ecological topics outside of climate change remain largely unaddressed, despite
their prominent importance. Mainstream NLP tasks, such as sentiment analysis,
dominate the scene, but there remains an untouched space in the literature
involving the analysis of environmental impacts of certain events and
practices. To address this gap, this paper presents EcoVerse, an annotated
English Twitter dataset of 3,023 tweets spanning a wide spectrum of
environmental topics. We propose a three-level annotation scheme designed for
Eco-Relevance Classification, Stance Detection, and introducing an original
approach for Environmental Impact Analysis. We detail the data collection,
filtering, and labeling process that led to the creation of the dataset.
Remarkable Inter-Annotator Agreement indicates that the annotation scheme
produces consistent annotations of high quality. Subsequent classification
experiments using BERT-based models, including ClimateBERT, are presented.
These yield encouraging results, while also indicating room for a model
specifically tailored for environmental texts. The dataset is made freely
available to stimulate further research.",http://arxiv.org/abs/2404.05133v1
Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks,2024-04-09T16:28:54Z,"Dimitrios Michail, Lefki-Ioanna Panagiotou, Charalampos Davalas, Ioannis Prapas, Spyros Kondylatos, Nikolaos Ioannis Bountos, Ioannis Papoutsis","With climate change expected to exacerbate fire weather conditions, the
accurate anticipation of wildfires on a global scale becomes increasingly
crucial for disaster mitigation. In this study, we utilize SeasFire, a
comprehensive global wildfire dataset with climate, vegetation, oceanic
indices, and human-related variables, to enable seasonal wildfire forecasting
with machine learning. For the predictive analysis, we train deep learning
models with different architectures that capture the spatio-temporal context
leading to wildfires. Our investigation focuses on assessing the effectiveness
of these models in predicting the presence of burned areas at varying
forecasting time horizons globally, extending up to six months into the future,
and on how different spatial or/and temporal context affects the performance of
the models. Our findings demonstrate the great potential of deep learning
models in seasonal fire forecasting; longer input time-series leads to more
robust predictions across varying forecasting horizons, while integrating
spatial information to capture wildfire spatio-temporal dynamics boosts
performance. Finally, our results hint that in order to enhance performance at
longer forecasting horizons, a larger receptive field spatially needs to be
considered.",http://arxiv.org/abs/2404.06437v1
"DiffObs: Generative Diffusion for Global Forecasting of Satellite
  Observations",2024-04-04T05:24:22Z,"Jason Stock, Jaideep Pathak, Yair Cohen, Mike Pritchard, Piyush Garg, Dale Durran, Morteza Mardani, Noah Brenowitz","This work presents an autoregressive generative diffusion model (DiffObs) to
predict the global evolution of daily precipitation, trained on a satellite
observational product, and assessed with domain-specific diagnostics. The model
is trained to probabilistically forecast day-ahead precipitation. Nonetheless,
it is stable for multi-month rollouts, which reveal a qualitatively realistic
superposition of convectively coupled wave modes in the tropics. Cross-spectral
analysis confirms successful generation of low frequency variations associated
with the Madden--Julian oscillation, which regulates most subseasonal to
seasonal predictability in the observed atmosphere, and convectively coupled
moist Kelvin waves with approximately correct dispersion relationships. Despite
secondary issues and biases, the results affirm the potential for a next
generation of global diffusion models trained on increasingly sparse, and
increasingly direct and differentiated observations of the world, for practical
applications in subseasonal and climate prediction.",http://arxiv.org/abs/2404.06517v1
"ClimODE: Climate and Weather Forecasting with Physics-informed Neural
  ODEs",2024-04-15T06:38:21Z,"Yogesh Verma, Markus Heinonen, Vikas Garg","Climate and weather prediction traditionally relies on complex numerical
simulations of atmospheric physics. Deep learning approaches, such as
transformers, have recently challenged the simulation paradigm with complex
network forecasts. However, they often act as data-driven black-box models that
neglect the underlying physics and lack uncertainty quantification. We address
these limitations with ClimODE, a spatiotemporal continuous-time process that
implements a key principle of advection from statistical mechanics, namely,
weather changes due to a spatial movement of quantities over time. ClimODE
models precise weather evolution with value-conserving dynamics, learning
global weather transport as a neural flow, which also enables estimating the
uncertainty in predictions. Our approach outperforms existing data-driven
methods in global and regional forecasting with an order of magnitude smaller
parameterization, establishing a new state of the art.",http://arxiv.org/abs/2404.10024v1
"Quantifying seasonal hydrogen storage demands under cost and market
  uptake uncertainties in energy system transformation pathways",2024-04-19T16:02:15Z,"Felix Frischmuth, Mattis Berghoff, Martin Braun, Philipp Haertel","Climate neutrality paradigms put electricity systems at the core of a clean
energy supply. At the same time, indirect electrification, with a potential
uptake of hydrogen or derived fuel economy, plays a crucial role in
decarbonising the energy supply and industrial processes. Besides energy
markets coordinating the transition, climate and energy policy targets require
fundamental changes and expansions in the energy transmission, import,
distribution, and storage infrastructures. While existing studies identify
relevant demands for hydrogen, critical decisions involve imports versus
domestic fuel production and investments in new or repurposing existing
pipeline and storage infrastructure. Linking the pan-European energy system
planning model SCOPE SD with the multiperiod European gas market model IMAGINE,
the case study analysis and its transformation pathway results indicate
extensive network development of hydrogen infrastructure, including expansion
beyond refurbished methane infrastructure. However, the ranges of future
hydrogen storage costs and market uptake restrictions expose and quantify the
uncertainty of its role in Europes transformation. The study finds that rapidly
planning the construction of hydrogen storage and pipeline infrastructure is
crucial to achieving the required capacity by 2050.",http://arxiv.org/abs/2404.12974v1
"Large increases in public R&D investment are needed to avoid declines of
  US agricultural productivity",2024-05-13T19:59:25Z,"Ariel Ortiz-Bobea, Robert G. Chambers, Yurou He, David B. Lobell","Increasing agricultural productivity is a gradual process with significant
time lags between research and development (R&D) investment and the resulting
gains. We estimate the response of US agricultural Total Factor Productivity
(TFP) to both R&D investment and weather, and quantify the public R&D spending
required to offset the emerging impacts of climate change. We find that
offsetting the climate-induced productivity slowdown by 2050 alone requires a
sustained public R&D spending growth of 5.2-7.8% per year over 2021-2050. This
amounts to an additional $208-$434B investment over this period. These are
substantial requirements comparable to the public R&D spending growth that
followed the two World Wars.",http://arxiv.org/abs/2405.08159v2
"Marrying Causal Representation Learning with Dynamical Systems for
  Science",2024-05-22T18:00:41Z,"Dingling Yao, Caroline Muller, Francesco Locatello","Causal representation learning promises to extend causal models to hidden
causal variables from raw entangled measurements. However, most progress has
focused on proving identifiability results in different settings, and we are
not aware of any successful real-world application. At the same time, the field
of dynamical systems benefited from deep learning and scaled to countless
applications but does not allow parameter identification. In this paper, we
draw a clear connection between the two and their key assumptions, allowing us
to apply identifiable methods developed in causal representation learning to
dynamical systems. At the same time, we can leverage scalable differentiable
solvers developed for differential equations to build models that are both
identifiable and practical. Overall, we learn explicitly controllable models
that isolate the trajectory-specific parameters for further downstream tasks
such as out-of-distribution classification or treatment effect estimation. We
experiment with a wind simulator with partially known factors of variation. We
also apply the resulting model to real-world climate data and successfully
answer downstream causal questions in line with existing literature on climate
change.",http://arxiv.org/abs/2405.13888v3
A dynamical geography of observed trends in the global ocean,2024-05-29T10:54:33Z,"Bruno Buongiorno Nardelli, Daniele Iudicone","Revealing the ongoing changes in ocean dynamics and their impact on marine
ecosystems requires the joint analysis of multiple variables. Yet, global
observational records only cover a few decades, posing a challenge in the
separation of climatic trends from internal dynamical modes. Here, we apply an
empirical stochastic model to identify the emergent patterns of trends in six
fundamental components of upper ocean physics. We analyze a data-driven
reconstruction of the ocean state covering the 1993-2018 period. We found that
including temporal derivatives into the state vector enhances the description
of the ocean's dynamical system. Once Pacific oscillations are properly
accounted for, averaged surface warming appears >60% faster, and a deep
reshaping of the seascape is revealed. A clustering of the trend patterns
identifies the main factors that drive observed trends in chlorophyll-a
concentration. This data-driven approach opens new perspectives in empirical
climate modelling.",http://arxiv.org/abs/2405.18981v1
Reducing the climate impact of data portals: a case study,2024-06-06T08:45:36Z,"Noah Gie√üing, Madhurima Deb, Ankit Satpute, Moritz Schubotz, Olaf Teschke","The carbon footprint share of the information and communication technology
(ICT) sector has steadily increased in the past decade and is predicted to make
up as much as 23 \% of global emissions in 2030. This shows a pressing need for
developers, including the information retrieval community, to make their code
more energy-efficient. In this project proposal, we discuss techniques to
reduce the energy footprint of the MaRDI (Mathematical Research Data
Initiative) Portal, a MediaWiki-based knowledge base. In future work, we plan
to implement these changes and provide concrete measurements on the gain in
energy efficiency. Researchers developing similar knowledge bases can adapt our
measures to reduce their environmental footprint. In this way, we are working
on mitigating the climate impact of Information Retrieval research.",http://arxiv.org/abs/2406.03858v1
Net Zero Averted Temperature Increase,2024-06-11T15:58:23Z,"R. Lindzen, W. Happer, W. A. van Wijngaarden","Using feedback-free estimates of the warming by increased atmospheric carbon
dioxide (CO2) and observed rates of increase, we estimate that if the United
States (U.S.) eliminated net CO2 emissions by the year 2050, this would avert a
warming of 0.0084 C (0.015 F), which is below our ability to accurately
measure. If the entire world forced net zero CO2 emissions by the year 2050, a
warming of only 0.070 C (0.13 F) would be averted. If one assumes that the
warming is a factor of 4 larger because of positive feedbacks, as asserted by
the Intergovernmental Panel on Climate Change (IPCC), the warming averted by a
net zero U.S. policy would still be very small, 0.034 C (0.061 F). For
worldwide net zero emissions by 2050 and the 4-times larger IPCC climate
sensitivity, the averted warming would be 0.28 C (0.50 F).",http://arxiv.org/abs/2406.07392v1
"Rethinking Digitalization and Climate: Don't Predict, Mitigate",2024-06-16T19:40:05Z,"Daria Gritsenko, Jon Aaen, Bent Flyvbjerg","Digitalization is a core component of the green transition. Today's focus is
on quantifying and pre-dicting the climate effects of digitalization through
various life-cycle assessments and baseline sce-nario methodologies. Here we
argue that this is a mistake. Most attempts at prediction are based on three
implicit assumptions: (a) the digital carbon footprint can be quantified, (b)
business-as-usual with episodic change leading to a new era of stability, and
(c) investments in digitalization will be delivered within the cost, timeframe,
and benefits described in their business cases. We problema-tize each
assumption within the context of digitalization and argue that the digital
carbon footprint is inherently unpredictable. We build on uncertainty
literature to show that even if you cannot predict, you can still mitigate. On
that basis, we propose to rethink practice on the digital carbon footprint from
prediction to mitigation.",http://arxiv.org/abs/2407.15016v1
"A Multivariate Space-Time Dynamic Model for Characterizing the
  Atmospheric Impacts Following the Mt Pinatubo Eruptio",2024-08-23T22:07:29Z,"Robert Garrett, Lyndsay Shand, J. Gabriel Huerta","The June 1991 Mt. Pinatubo eruption resulted in a massive increase of sulfate
aerosols in the atmosphere, absorbing radiation and leading to global changes
in surface and stratospheric temperatures. A volcanic eruption of this
magnitude serves as a natural analog for stratospheric aerosol injection, a
proposed solar radiation modification method to combat the warming climate. The
impacts of such an event are multifaceted and region-specific. Our goal is to
characterize the multivariate and dynamic nature of the climate impacts
following the Mt. Pinatubo eruption. We developed a multivariate space-time
dynamic linear model to understand the full extent of the spatially- and
temporally-varying impacts. Specifically, spatial variation is modeled using a
flexible set of basis functions for which the basis coefficients are allowed to
vary in time through a vector autoregressive (VAR) structure. This novel model
is caste in a Dynamic Linear Model (DLM) framework and estimated via a
customized MCMC approach. We demonstrate how the model quantifies the
relationships between key atmospheric parameters following the Mt. Pinatubo
eruption with reanalysis data from MERRA-2 and highlight when such model is
advantageous over univariate models.",http://arxiv.org/abs/2408.13392v1
"Cartographie du confort thermique au sein d'une cours d'{√©}cole
  parisienne : couplage de mesures microclimatiques fixes et mobiles",2024-08-30T08:53:42Z,"Ghid Karam, Ma√Ølys Chanial, Maxime Chaumont, Martin Hendel, Laurent Royon","Climate change will result in more frequent, more intense and longer-lasting
heat waves by 2050. As part of its Climate Plan and its resilience strategy,
the City of Paris is deploying, through its Oasis program, a network of urban
cool islands to mitigate the urban heat island phenomena: schoolyards are
renovated in order to reduce the heat stress of users. We establish a
methodology aiming to quantify the microclimatic impact of the transformation.
Mobile measurements are carried out within a case courtyard under hot
conditions and coupled with fixed weather station data to evaluate heat stress
using UTCI. The heat stress mapping thus obtained allows a first microclimatic
diagnosis of the schoolyard.",http://arxiv.org/abs/2409.00148v1
"The Application of Green GDP and Its Impact on Global Economy and
  Environment: Analysis of GGDP based on SEEA model",2024-09-04T12:19:11Z,Mingpu Ma,"This paper presents an analysis of Green Gross Domestic Product (GGDP) using
the System of Environmental-Economic Accounting (SEEA) model to evaluate its
impact on global climate mitigation and economic health. GGDP is proposed as a
superior measure to tradi-tional GDP by incorporating natural resource
consumption, environmental pollution control, and degradation factors. The
study develops a GGDP model and employs grey correlation analysis and grey
prediction models to assess its relationship with these factors. Key findings
demonstrate that replacing GDP with GGDP can positively influence climate
change, partic-ularly in reducing CO2 emissions and stabilizing global
temperatures. The analysis further explores the implications of GGDP adoption
across developed and developing countries, with specific predictions for China
and the United States. The results indicate a potential increase in economic
levels for developing countries, while developed nations may experi-ence a
decrease. Additionally, the shift to GGDP is shown to significantly reduce
natural re-source depletion and population growth rates in the United States,
suggesting broader envi-ronmental and economic benefits. This paper highlights
the universal applicability of the GGDP model and its potential to enhance
environmental and economic policies globally.",http://arxiv.org/abs/2409.02642v1
"3D-SAR Tomography and Machine Learning for High-Resolution Tree Height
  Estimation",2024-09-09T14:07:38Z,"Grace Colverd, Jumpei Takami, Laura Schade, Karol Bot, Joseph A. Gallego-Mejia","Accurately estimating forest biomass is crucial for global carbon cycle
modelling and climate change mitigation. Tree height, a key factor in biomass
calculations, can be measured using Synthetic Aperture Radar (SAR) technology.
This study applies machine learning to extract forest height data from two SAR
products: Single Look Complex (SLC) images and tomographic cubes, in
preparation for the ESA Biomass Satellite mission. We use the TomoSense
dataset, containing SAR and LiDAR data from Germany's Eifel National Park, to
develop and evaluate height estimation models. Our approach includes classical
methods, deep learning with a 3D U-Net, and Bayesian-optimized techniques. By
testing various SAR frequencies and polarimetries, we establish a baseline for
future height and biomass modelling. Best-performing models predict forest
height to be within 2.82m mean absolute error for canopies around 30m,
advancing our ability to measure global carbon stocks and support climate
action.",http://arxiv.org/abs/2409.05636v1
Masked Autoregressive Model for Weather Forecasting,2024-09-30T09:17:04Z,"Doyi Kim, Minseok Seo, Hakjin Lee, Junghoon Seo","The growing impact of global climate change amplifies the need for accurate
and reliable weather forecasting. Traditional autoregressive approaches, while
effective for temporal modeling, suffer from error accumulation in long-term
prediction tasks. The lead time embedding method has been suggested to address
this issue, but it struggles to maintain crucial correlations in atmospheric
events. To overcome these challenges, we propose the Masked Autoregressive
Model for Weather Forecasting (MAM4WF). This model leverages masked modeling,
where portions of the input data are masked during training, allowing the model
to learn robust spatiotemporal relationships by reconstructing the missing
information. MAM4WF combines the advantages of both autoregressive and lead
time embedding methods, offering flexibility in lead time modeling while
iteratively integrating predictions. We evaluate MAM4WF across weather, climate
forecasting, and video frame prediction datasets, demonstrating superior
performance on five test datasets.",http://arxiv.org/abs/2409.20117v1
Advanced Resilience Planning for Distribution Systems,2024-09-30T11:56:33Z,"Ahmad Bin Afzal, Nabil Mohammed, Shehab Ahmed, Charalambos Konstantinou","Climate change has led to an increase in the frequency and severity of
extreme weather events, posing significant challenges for power distribution
systems. In response, this work presents a planning approach in order to
enhance the resilience of distribution systems against climatic hazards. The
framework systematically addresses uncertainties during extreme events,
including weather variability and line damage. Key strategies include line
hardening, backup diesel generators, and sectionalizers to strengthen
resilience. We model spatio-temporal dynamics and costs through a hybrid model
integrating stochastic processes with deterministic elements. A two-stage
stochastic mixed-integer linear approach is developed to optimize resilience
investments against load loss, generator operations, and repairs. Case studies
on the IEEE 15-bus benchmark system and a realistic distribution grid model in
Riyadh, Saudi Arabia demonstrate enhanced system robustness as well as cost
efficiency of 10% and 15%, respectively.",http://arxiv.org/abs/2409.20219v1
The Scope 4 Emission: Neutralized Carbon Emissions,2024-10-11T23:35:37Z,"Zhu Liu, Guangqian Wang","Assessing carbon negative and carbon neutrality is critical for mitigating
and adapting global climate change. Here we proposed a new framework to account
for carbon-negative and carbon-neutral actions by introducing the definition of
Carbon Negative (C0),Carbon Neutrality Stock (C1), Carbon Supply (C2) and
carbon-neutral emissions or Scope 4 emissions, which refers to the avoided
emission due to use of non-fossil energy or C1 products. For the first time, we
calculated the global neutralized carbon emissions or Scope 4 emission by
renewable electricity generation, and the results indicating the significant
contributions by China, with total neutralized carbon emissions (2.15 Mt C/day
) much higher than the U.S. (0.85 Mt C/day)and EU27 & UK (1.25 Mt C/day)
together. We show that China contributed to more than 36% of global neutralized
CO2 emissions, and such contributions are still increasing. This new framework
reflects remarkable contributions for China to the global climate mitigation
through the development of carbon neutrality energy system.",http://arxiv.org/abs/2410.10891v1
Regional Ocean Forecasting with Hierarchical Graph Neural Networks,2024-10-15T17:34:50Z,"Daniel Holmberg, Emanuela Clementi, Teemu Roos","Accurate ocean forecasting systems are vital for understanding marine
dynamics, which play a crucial role in environmental management and climate
adaptation strategies. Traditional numerical solvers, while effective, are
computationally expensive and time-consuming. Recent advancements in machine
learning have revolutionized weather forecasting, offering fast and
energy-efficient alternatives. Building on these advancements, we introduce
SeaCast, a neural network designed for high-resolution, medium-range ocean
forecasting. SeaCast employs a graph-based framework to effectively handle the
complex geometry of ocean grids and integrates external forcing data tailored
to the regional ocean context. Our approach is validated through experiments at
a high spatial resolution using the operational numerical model of the
Mediterranean Sea provided by the Copernicus Marine Service, along with both
numerical and data-driven atmospheric forcings.",http://arxiv.org/abs/2410.11807v2
A Norwegian Approach to Downscaling,2024-11-05T06:59:16Z,Rasmus E. Benestad,"A comprehensive geoscientific downscaling model strategy is presented
outlining an approach that has evolved over the last 20 years, together with an
explanation for its development, its technical aspects, and evaluation scheme.
This effort has resulted in an open-source and free R-based tool, 'esd', for
the benefit of sharing and improving the reproducibility of the downscaling
results. Furthermore, a set of new metrics was developed as an integral part of
the downscaling approach which assesses model performance with an emphasis on
regional information for society (RifS). These metrics involve novel ways of
comparing model results with observational data and have been developed for
downscaling large multi-model global climate model ensembles. This paper
presents for the first time an overview of the comprehensive framework adopted
by the Norwegian Meteorological Institute for downscaling aimed at supporting
climate change adaptation. A literature search suggests that this comprehensive
downscaling strategy and evaluation scheme are not widely used within the
downscaling community. In addition, this strategy involves a new convention for
storing large datasets of ensemble results that provides fast access to
information and drastically saves data volume.",http://arxiv.org/abs/2411.02856v1
"Spatially Regularized Graph Attention Autoencoder Framework for
  Detecting Rainfall Extremes",2024-11-12T12:24:48Z,"Mihir Agarwal, Progyan Das, Udit Bhatia","We introduce a novel Graph Attention Autoencoder (GAE) with spatial
regularization to address the challenge of scalable anomaly detection in
spatiotemporal rainfall data across India from 1990 to 2015. Our model
leverages a Graph Attention Network (GAT) to capture spatial dependencies and
temporal dynamics in the data, further enhanced by a spatial regularization
term ensuring geographic coherence. We construct two graph datasets employing
rainfall, pressure, and temperature attributes from the Indian Meteorological
Department and ERA5 Reanalysis on Single Levels, respectively. Our network
operates on graph representations of the data, where nodes represent geographic
locations, and edges, inferred through event synchronization, denote
significant co-occurrences of rainfall events. Through extensive experiments,
we demonstrate that our GAE effectively identifies anomalous rainfall patterns
across the Indian landscape. Our work paves the way for sophisticated
spatiotemporal anomaly detection methodologies in climate science, contributing
to better climate change preparedness and response strategies.",http://arxiv.org/abs/2411.07753v1
Flexible Thermoelectric Active Cooling Garment to Combat Extreme Heat,2024-11-13T05:40:49Z,"Tianshi Feng, Jiedong Wang, Ethan Sun, Antonio Di Buono, Renkun Chen","With the increasing frequency, intensity, and duration of extreme heat events
due to climate change, heat-related diseases or even mortality have become more
prevalent. An efficient personal cooling strategy can mitigate heat stress by
regulating the skin temperature within the thermal comfort zone. However,
lightweight, wearable, and sustainable cooling garments are unavailable today.
Here, we developed a TED-based cooling garment and demonstrated its
effectiveness in active personal cooling. The garment is shown to maintain the
skin temperature within its thermal comfort zone in a hot environment of up to
40 oC under mild forced convection conditions (air flow speed of 2.2 m s-1).
Furthermore, we demonstrated a portable cooling system with less than 700 grams
of total weight, which includes the TED-based garment, a battery pack, and a
temperature controller. The system showed long-term cooling on the skin with
varying ambient temperatures from 35 to 40 oC. With the advantages of
lightweight, flexible, controllable and long-term effective cooling, the TED
cooling garments described in this work can contribute to enhanced health and
comfort in an increasingly hotter climate.",http://arxiv.org/abs/2411.08349v2
A model-free test of the time-reversibility of climate change processes,2024-11-18T02:36:13Z,"Yuichi Goto, Marc Hallin","Time-reversibility is a crucial feature of many time series models, while
time-irreversibility is the rule rather than the exception in real-life data.
Testing the null hypothesis of time-reversibilty, therefore, should be an
important step preliminary to the identification and estimation of most
traditional time-series models. Existing procedures, however, mostly consist of
testing necessary but not sufficient conditions, leading to under-rejection, or
sufficient but non-necessary ones, which leads to over-rejection. Moreover,
they generally are model-besed. In contrast, the copula spectrum studied by
Goto et al. ($\textit{Ann. Statist.}$ 2022, $\textbf{50}$: 3563--3591) allows
for a model-free necessary and sufficient time-reversibility condition. A test
based on this copula-spectrum-based characterization has been proposed by
authors. This paper illustrates the performance of this test, with an
illustration in the analysis of climatic data.",http://arxiv.org/abs/2411.11248v1
"Integrating Expert Labels into LLM-based Emission Goal Detection:
  Example Selection vs Automatic Prompt Design",2024-12-09T12:20:33Z,"Marco Wrzalik, Adrian Ulges, Anne Uersfeld, Florian Faust","We address the detection of emission reduction goals in corporate reports, an
important task for monitoring companies' progress in addressing climate change.
Specifically, we focus on the issue of integrating expert feedback in the form
of labeled example passages into LLM-based pipelines, and compare the two
strategies of (1) a dynamic selection of few-shot examples and (2) the
automatic optimization of the prompt by the LLM itself. Our findings on a
public dataset of 769 climate-related passages from real-world business reports
indicate that automatic prompt optimization is the superior approach, while
combining both methods provides only limited benefit. Qualitative results
indicate that optimized prompts do indeed capture many intricacies of the
targeted emission goal extraction task.",http://arxiv.org/abs/2412.06432v1
"Accurate Prediction of Temperature Indicators in Eastern China Using a
  Multi-Scale CNN-LSTM-Attention model",2024-12-11T00:42:31Z,"Jiajiang Shen, Weiyan Wu, Qianyu Xu","In recent years, the importance of accurate weather forecasting has become
increasingly prominent due to the impacts of global climate change and the
rapid development of data science. Traditional forecasting methods often
struggle to handle the complexity and nonlinearity inherent in climate data. To
address these challenges, we propose a weather prediction model based on a
multi-scale convolutional CNN-LSTM-Attention architecture, specifically
designed for time series forecasting of temperature data in China. The model
integrates Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM)
networks, and attention mechanisms to leverage the strengths of spatial feature
extraction, temporal sequence modeling, and the ability to focus on important
features. The development process of the model includes data collection,
preprocessing, feature extraction, and model building. Experimental results
show that the model performs excellently in predicting temperature trends with
high accuracy. The final computed results indicate that the Mean Squared Error
(MSE) is 1.978295 and the Root Mean Squared Error (RMSE) is 0.8106562. This
work marks a significant advancement in applying deep learning techniques to
meteorological data, offering a valuable tool for improving weather forecasting
accuracy and providing essential support for decision-making in areas such as
urban planning, agriculture, and energy management.",http://arxiv.org/abs/2412.07997v1
"AgroXAI: Explainable AI-Driven Crop Recommendation System for
  Agriculture 4.0",2024-12-16T20:18:10Z,"Ozlem Turgut, Ibrahim Kok, Suat Ozdemir","Today, crop diversification in agriculture is a critical issue to meet the
increasing demand for food and improve food safety and quality. This issue is
considered to be the most important challenge for the next generation of
agriculture due to the diminishing natural resources, the limited arable land,
and unpredictable climatic conditions caused by climate change. In this paper,
we employ emerging technologies such as the Internet of Things (IoT), machine
learning (ML), and explainable artificial intelligence (XAI) to improve
operational efficiency and productivity in the agricultural sector.
Specifically, we propose an edge computing-based explainable crop
recommendation system, AgroXAI, which suggests suitable crops for a region
based on weather and soil conditions. In this system, we provide local and
global explanations of ML model decisions with methods such as ELI5, LIME,
SHAP, which we integrate into ML models. More importantly, we provide regional
alternative crop recommendations with the counterfactual explainability method.
In this way, we envision that our proposed AgroXAI system will be a platform
that provides regional crop diversity in the next generation agriculture.",http://arxiv.org/abs/2412.16196v1
"A global evidence map of human well-being and biodiversity co-benefits
  and trade-offs of natural climate solutions",2024-04-30T14:55:14Z,"Charlotte H. Chang, James T. Erbaugh, Paola Fajardo, Luci Lu, Istv√°n Moln√°r, D√°vid Papp, Brian E. Robinson, Kemen Austin, Susan Cook-Patton, Timm Kroeger, Lindsey Smart, Miguel Castro, Samantha H. Cheng, Peter W. Ellis, Rob I. McDonald, Teevrat Garg, Erin E. Poor, Preston Welker, Andrew R. Tilman, Stephen A. Wood, Yuta J. Masuda","Natural climate solutions (NCS) are critical for mitigating climate change
through ecosystem-based carbon removal and emissions reductions. NCS
implementation can also generate biodiversity and human well-being co-benefits
and trade-offs (""NCS co-impacts""), but the volume of evidence on NCS co-impacts
has grown rapidly across disciplines, is poorly understood, and remains to be
systematically collated and synthesized. A global evidence map of NCS
co-impacts would overcome key barriers to NCS implementation by providing
relevant information on co-benefits and trade-offs where carbon mitigation
potential alone does not justify NCS projects. We employ large language models
to assess over two million articles, finding 257,266 relevant articles on NCS
co-impacts. We analyze this large and dispersed body of literature using
innovative machine learning methods to extract relevant data (e.g., study
location, species, and other key variables), and create a global evidence map
on NCS co-impacts. Evidence on NCS co-impacts has grown approximately ten-fold
in three decades, although some of the most abundant evidence is associated
with pathways that have less mitigation potential. We find that studies often
examine multiple NCS pathways, indicating natural NCS pathway complements, and
each NCS is often associated with two or more coimpacts. Finally, NCS
co-impacts evidence and priority areas for NCS are often mismatched--some
countries with high mitigation potential from NCS have few published studies on
the broader co-impacts of NCS implementation. Our work advances and makes
available novel methods and systematic and representative data of NCS
co-impacts studies, thus providing timely insights to inform NCS research and
action globally.",http://arxiv.org/abs/2405.00079v1
"Relationship of temperature changes in the mesopause region with the
  climate changes at the surface from observations in 1960-2024",2024-05-07T22:15:58Z,"I. I. Mokhov, I. A. Fomina, V. I. Perminov","The results of an analysis of temperature variations in the mesopause region
based on long-term measurements of hydroxyl airglow at the Zvenigorod
Scientific Station of the A.M. Obukhov Institute of Atmospheric Physics RAS
(ZSS IAP RAS) in 1960-2024 in comparison with variations of surface temperature
characterizing global-scale climate changes are presented. Along with
temperature variations in the mesopause region, two versions of temperature
variations in the mesopause region, normalized to the same level of solar
activity, were analyzed. Quantitative estimates of a strong decrease in
temperature in the mesopause region over the past decades in winter against the
background of a global increase in surface temperature have been obtained. It
was noted that significant coherence of long-term variations for temperature in
the mesopause region with the surface temperature in the Northern Hemisphere
with the use of cross-wavelet analysis, what was not previously evident in data
for a shorter time interval. The possibility of such coherence was predicted in
(Mokhov et al., 2017) under the continuation of global warming based on the
results of model simulations for the 20-21 centuries, taking into account
anthropogenic forcing. It was not previously manifested from observational data
for a shorter time interval. Along with long-term trends, features of a sharp
decrease in temperature in the mesopause region in the 1970s with its
synchronicity with the known shift in surface climate regimes associated with
El Ni\~no events were analyzed. The results of cross-wavelet analysis using
data obtained at the ZSS IAP RAS for the time interval 1960-2024 indicate a
more significant connection between temperature variations in the mesopause
region and El Nino indices in recent decades.",http://arxiv.org/abs/2405.04695v1
"Contribution of Shorter-term Radiative Forcings of Aerosols and Ozone to
  Global Warming in the Last Two Decades",2024-06-07T20:34:04Z,Qing-Bin Lu,"This paper reports observations of regional and global upper stratosphere
temperature (UST) and surface temperature, as well as various climate drivers
including greenhouse gases (GHGs), ozone, aerosols, solar variability, snow
cover extent, and sea ice extent (SIE). We strikingly found warming trends of
0.77(+/-0.57) and 0.69(+/-0.22) K/decade in UST at altitudes of 35-40 km in the
Arctic and Antarctic respectively and no significant trends over non-polar
regions since 2002. These UST trends provide fingerprints of decreasing and no
significant trends in total GHG effect in polar and non-polar regions
respectively. Correspondingly, we made the first observation of surface cooling
trends in both the Antarctic since 2005 and the Arctic since 2016 once the SIE
started to recover. But surface warming remains at mid-latitudes, which causes
the recent rise in global mean surface temperature (GMST). These temperature
changing patterns are consistent with the characteristics of the
cosmic-ray-driven electron reaction (CRE) mechanism of halogen-containing GHGs
(halo-GHGs) with larger destruction rates at higher latitudes. Moreover, the
no-parameter physics model of warming caused by halo-GHGs reproduces closely
the observed GMSTs from 2000 to 2024, including the almost no warming during
2000-2012 and the significant warming by 0.2-0.3 deg C during 2013-2023, of
which 0.27 deg C was calculated to arise from the net radiative forcing of
aerosols and ozone due to improved air quality. The results also show that the
physics model captures 76% of the variance in the observed GMSTs, exhibiting a
warming peak in October 2023 and predicting a gradual GMST reversal thereafter.
The results from this study may greatly improve our understanding of global
climate change and lead to the identifying of the correct major culprit for
human contribution to changing the climate.",http://arxiv.org/abs/2406.05253v2
Climate Change Task Force Report for the American Astronomical Society,2024-06-15T00:17:32Z,"T. A. Rector, L. Barbier, A. Couperis, R. Danner, A. Egan, P. Green, G. Jacoby, J. Monkiewicz, R. Nikutta, K. Pitman, M. Rutkowski, S. Tuttle, A. Virkki, K. Volk","The AAS Strategic Plan for 2021-26 called for the creation of a task force to
identify how the AAS can meet the goals of the Paris Agreement. The AAS and its
membership recognize the danger climate change represents to humanity and our
world, and to astronomy -- as a profession, a hobby, and a cultural good. Our
profession in general -- and the AAS in particular -- should work to make it
possible for all astronomers to have an equal opportunity to be successful
without needing to incur high carbon emissions, and to preserve astronomy for
future generations.
  A study was completed of the carbon emissions associated with the AAS,
finding that 84% of total AAS-related emissions are from in-person conferences.
We also conducted a survey of AAS members to determine their attitudes about
climate change. Respondents overwhelmingly (97%) think that the AAS should
reduce its carbon footprint. Our task force created a list of fourteen
recommendations, with two ranked as top priorities: The AAS should not schedule
additional in-person meetings before 2030 and it should work to innovate the
AAS conference model. Based upon our analysis it is clear that online
interaction is the only way to increase participation while meaningfully
decreasing emissions.
  Our recommendations are aligned with the Astro2020 Decadal Survey as well as
AAS values to disseminate our scientific understanding of the universe, and to
do our work in an ethically responsible way. Because of their other benefits --
particularly in making our society more welcoming to those who traditionally
have been excluded -- we feel that these are sound decisions, worthy of
implementation even if the AAS wasn't trying to reduce its carbon footprint.
They simply make sense as steps towards a professional society that better
serves a broader membership, as our profession evolves to be greener, more
inclusive, and more productive.",http://arxiv.org/abs/2406.10451v1
"Bootstrap Pettitt test for detecting change point in hydroclimatological
  data: a case study for Itaipu hydroelectric plant in Brazil",2024-11-07T23:01:00Z,"Luiza Chiarelli Conte, D√©bora Missio Bayer, F√°bio M. Bayer","The Pettitt test has been widely used in climate change and hydrological
analyzes. However, studies evidence difficulties of this test in detecting
change points, especially in small samples. This study presents a bootstrap
application of the Pettitt test, which is numerically compared with the
classical Pettitt test by an extensive Monte Carlo simulation study. The
proposed test outperforms the classical test in all simulated scenarios. An
application of the tests is conducted in the historical series of naturalized
flows of the Itaipu Hydroelectric plant in Brazil, where several studies have
shown a change point in the 70s. When the series is split into shorter series,
to simulate small sample actual situations, the proposed test is more powerful
than the classical Pettitt test to detect the change point. The proposed test
can be an important tool to detect abrupt changes in water availability,
supporting hydroclimatological resources decision making.",http://arxiv.org/abs/2411.05233v1
"Ready for climate change? The importance of adaptive thermoregulatory
  flexibility for the Malagasy bat species Triaenops menamena",2024-01-23T11:57:30Z,Sina Remmers,"The balance between energy intake and expenditure is essential and crucial
for survival for all organisms. The energy management is closely linked to the
ecology. Thus, changes in environmental conditions can be challenging,
especially for the animals physiology. Different strategies of thermoregulation
have evolved and heterothermy seems to be the most efficient way for saving
energy. Daily torpor, a temporally controlled reduction of the metabolic rate
and body temperature, is one form of heterothermy and recent studies revealed
that this physiological strategy is used by many tropical and subtropical
species. Yet, little is known about torpor in bats and their intraspecific
thermoregulatory flexibility. Therefore, three populations of the Malagasy bat
species Triaenops menamena were investigated, to examine their metabolic rate,
skin temperature and related energy expenditure during normothermic and torpid
states in context of different microclimatic conditions. This study exposed
significant physiological differences among these three populations along a
gradient of fluctuation in environmental conditions. The greater the
fluctuations in ambient temperature and humidity, the higher was the general
resting metabolic rate and the rate of its reduction, but the lower was the
torpid metabolic rate. This species shows a highly adaptive flexibility in
their physiology and are able to cope with unfavorable environmental conditions
by using different strategies of thermoregulation and hypometabolism, which is
beneficial regarding ongoing climatic changes.",http://arxiv.org/abs/2401.13012v1
"Assessment of low-carbon tourism development from multi-aspect analysis:
  A case study of the Yellow River Basin, China",2024-02-18T12:53:17Z,"Xiaopeng Si, Zi Tang","Climate change has become an unavoidable problem in achieving sustainable
development. As one of the major industries worldwide, tourism can make a
significant contribution to mitigating climate change. The main objective of
the paper is to assess the development level of low-carbon tourism from
multi-aspect, using the Yellow River Basin as an example. Firstly, this study
quantified tourism carbon dioxide emissions and tourism economy, and analyzed
their evolution characteristics. The interaction and coordination degree
between tourism carbon dioxide emissions and tourism economy were then analyzed
using the improved coupling coordination degree model. Finally, this study
analyzed the change in total factor productivity of low-carbon tourism by
calculating the Malmquist-Luenberger productivity index. The results showed
that: (1) The tourism industry in the Yellow River Basin has the
characteristics of the initial environmental Kuznets curve. (2) There was a
strong interaction between tourism carbon dioxide emissions and tourism
economy, which was manifested as mutual promotion. (3) The total factor
productivity of low-carbon tourism was increasing. Based on the above results,
it could be concluded that the development level of low-carbon tourism in the
Yellow River Basin has been continuously improved from 2000 to 2019, but it is
still in the early development stage with the continuous growth of carbon
dioxide emissions.",http://arxiv.org/abs/2402.11579v1
"Vegetation clustering and self-organization in inhomogeneous
  environments",2024-06-18T13:12:19Z,"D. Pinto-Ramos, M. G. Clerc, A. Makhoute, M. Tlidi","Due to climatic changes, excessive grazing, and deforestation, semi-arid and
arid ecosystems are vulnerable to desertification and land degradation.
Adversely affected biological productivity has a negative impact on social,
economic, and environmental factors. The term 'arid ecosystems' refers not only
to water-scarce landscapes but also to nutrient-poor environments.
Specifically, the vegetation cover loses spatial homogeneity as aridity
increases, and the self-organized heterogeneous vegetation patterns developed
could eventually collapse into a bare state. It is still unclear whether this
transition would be gradual or abrupt, leading to the often-called catastrophic
shift of ecosystems. Several studies suggest that environmental inhomogeneities
in time or space can promote a gradual transition to bare soil, thus avoiding
catastrophic shifts. Environmental inhomogeneities include non-uniformities in
the spatial distribution of precipitation, spatial irregularities in topography
and other external factors. Employing a generic mathematical model including
environmental inhomogeneities in space, we show how two branches of vegetation
patterns create a hysteresis loop when the effective mortality level changes.
These two branches correspond to qualitatively distinct vegetation
self-organized responses. In an increasing mortality scenario, one observes an
equilibrium branch of high vegetation biomass forming self-organized patterns
with a well-defined wavelength. However, reversing the mortality trend, one
observes a low biomass branch lacking a wavelength. We call this phenomenon the
clustering of vegetation patches. This behavior can be connected to
historically significant trends of climate change in arid ecosystems.",http://arxiv.org/abs/2406.12581v1
"SEN12-WATER: A New Dataset for Hydrological Applications and its
  Benchmarking",2024-09-25T16:50:59Z,"Luigi Russo, Francesco Mauro, Alessandro Sebastianelli, Paolo Gamba, Silvia Liberata Ullo","Climate change and increasing droughts pose significant challenges to water
resource management around the world. These problems lead to severe water
shortages that threaten ecosystems, agriculture, and human communities. To
advance the fight against these challenges, we present a new dataset,
SEN12-WATER, along with a benchmark using a novel end-to-end Deep Learning (DL)
framework for proactive drought-related analysis. The dataset, identified as a
spatiotemporal datacube, integrates SAR polarization, elevation, slope, and
multispectral optical bands. Our DL framework enables the analysis and
estimation of water losses over time in reservoirs of interest, revealing
significant insights into water dynamics for drought analysis by examining
temporal changes in physical quantities such as water volume. Our methodology
takes advantage of the multitemporal and multimodal characteristics of the
proposed dataset, enabling robust generalization and advancing understanding of
drought, contributing to climate change resilience and sustainable water
resource management. The proposed framework involves, among the several
components, speckle noise removal from SAR data, a water body segmentation
through a U-Net architecture, the time series analysis, and the predictive
capability of a Time-Distributed-Convolutional Neural Network (TD-CNN). Results
are validated through ground truth data acquired on-ground via dedicated
sensors and (tailored) metrics, such as Precision, Recall, Intersection over
Union, Mean Squared Error, Structural Similarity Index Measure and Peak
Signal-to-Noise Ratio.",http://arxiv.org/abs/2409.17087v1
"Dependence of convective precipitation extremes on near-surface relative
  humidity",2024-12-20T19:40:24Z,"Robert J. van der Drift, Paul A. O'Gorman","Precipitation extremes produced by convection have been found to intensify
with near-surface temperatures at a Clausius-Clapeyron rate of $6$ to $7\%$
K$^{-1}$ in simulations of radiative-convective equilibrium (RCE). However,
these idealized simulations are typically performed over an ocean surface with
a high near-surface relative humidity (RH) that stays roughly constant with
warming. Over land, near-surface RH is lower than over ocean and is projected
to decrease by global climate models. Here, we investigate the dependence of
precipitation extremes on near-surface RH in convection-resolving simulations
of RCE. We reduce near-surface RH by increasing surface evaporative resistance
while holding free-tropospheric temperatures fixed by increasing surface
temperature. This ``top-down'' approach produces an RCE state with a deeper,
drier boundary layer, which weakens convective precipitation extremes in three
distinct ways. First, the lifted condensation level is higher, leading to a
small thermodynamic weakening of precipitation extremes. Second, the higher
lifted condensation level also reduces positive buoyancy in the lower
troposphere, leading to a dynamic weakening of precipitation extremes. Third,
precipitation re-evaporates more readily when falling through a deeper, drier
boundary layer, leading to a substantial decrease in precipitation efficiency.
These three effects all follow from changes in near-surface relative humidity
and are physically distinct from the mechanism that underpins the
Clausius-Clapeyron scaling rate. Overall, our results suggest that changes in
relative humidity must be taken into account when seeking to understand and
predict changes in convective precipitation extremes over land.",http://arxiv.org/abs/2412.16306v1
"Satellite Image Time Series Semantic Change Detection: Novel
  Architecture and Analysis of Domain Shift",2024-07-10T12:54:51Z,"Elliot Vincent, Jean Ponce, Mathieu Aubry","Satellite imagery plays a crucial role in monitoring changes happening on
Earth's surface and aiding in climate analysis, ecosystem assessment, and
disaster response. In this paper, we tackle semantic change detection with
satellite image time series (SITS-SCD) which encompasses both change detection
and semantic segmentation tasks. We propose a new architecture that improves
over the state of the art, scales better with the number of parameters, and
leverages long-term temporal information. However, for practical use cases,
models need to adapt to spatial and temporal shifts, which remains a challenge.
We investigate the impact of temporal and spatial shifts separately on global,
multi-year SITS datasets using DynamicEarthNet and MUDS. We show that the
spatial domain shift represents the most complex setting and that the impact of
temporal shift on performance is more pronounced on change detection than on
semantic segmentation, highlighting that it is a specific issue deserving
further attention.",http://arxiv.org/abs/2407.07616v1
Detection of Critical Events in Renewable Energy Production Time Series,2024-01-31T13:13:37Z,"Laurens P. Stoop, Erik Duijm, Ad J. Feelders, Machteld van den Broek","The introduction of more renewable energy sources into the energy system
increases the variability and weather dependence of electricity generation.
Power system simulations are used to assess the adequacy and reliability of the
electricity grid over decades, but often become computational intractable for
such long simulation periods with high technical detail. To alleviate this
computational burden, we investigate the use of outlier detection algorithms to
find periods of extreme renewable energy generation which enables detailed
modelling of the performance of power systems under these circumstances.
Specifically, we apply the Maximum Divergent Intervals (MDI) algorithm to power
generation time series that have been derived from ERA5 historical climate
reanalysis covering the period from 1950 through 2019. By applying the MDI
algorithm on these time series, we identified intervals of extreme low and high
energy production. To determine the outlierness of an interval different
divergence measures can be used. Where the cross-entropy measure results in
shorter and strongly peaking outliers, the unbiased Kullback-Leibler divergence
tends to detect longer and more persistent intervals. These intervals are
regarded as potential risks for the electricity grid by domain experts,
showcasing the capability of the MDI algorithm to detect critical events in
these time series. For the historical period analysed, we found no trend in
outlier intensity, or shift and lengthening of the outliers that could be
attributed to climate change. By applying MDI on climate model output, power
system modellers can investigate the adequacy and possible changes of risk for
the current and future electricity grid under a wider range of scenarios.",http://arxiv.org/abs/2401.17814v2
"Equity Implications of Net-Zero Emissions: A Multi-Model Analysis of
  Energy Expenditures Across Income Classes Under Economy-Wide Deep
  Decarbonization Policies",2024-05-29T04:21:30Z,"John Bistlinea, Chikara Onda, Morgan Browning, Johannes Emmerling, Gokul Iyer, Megan Mahajan, Jim McFarland, Haewon McJeon, Robbie Orvis, Francisco Ralston Fonseca, Christopher Roney, Noah Sandoval, Luis Sarmiento, John Weyant, Jared Woollacott, Mei Yuan","With companies, states, and countries targeting net-zero emissions around
midcentury, there are questions about how these targets alter household welfare
and finances, including distributional effects across income groups. This paper
examines the distributional dimensions of technology transitions and net-zero
policies with a focus on welfare impacts across household incomes. The analysis
uses a model intercomparison with a range of energy-economy models using
harmonized policy scenarios reaching economy-wide, net-zero CO2 emissions
across the United States in 2050. We employ a novel linking approach that
connects output from detailed energy system models with survey microdata on
energy expenditures across income classes to provide distributional analysis of
net-zero policies. Although there are differences in model structure and input
assumptions, we find broad agreement in qualitative trends in policy incidence
and energy burdens across income groups. Models generally agree that direct
energy expenditures for many households will likely decline over time with
reference and net-zero policies. However, there is variation in the extent of
changes relative to current levels, energy burdens relative to reference
levels, and electricity expenditures. Policy design, primarily how climate
policy revenues are used, has first-order impacts on distributional outcomes.
Net-zero policy costs, in both absolute and relative terms, are unevenly
distributed across households, and relative increases in energy expenditures
are higher for lowest-income households. However, we also find that recycled
revenues from climate policies have countervailing effects when rebated on a
per-capita basis, offsetting higher energy burdens and potentially even leading
to net progressive outcomes.",http://arxiv.org/abs/2405.18748v1
DinAR: Augmenting Reality for Sustainable Dining,2024-04-20T04:56:29Z,"MJ Johns, Eunsol Sol Choi, Derusha Baskaran","Sustainable food is among the many challenges associated with climate change.
The resources required to grow or gather the food and the distance it travels
to reach the consumer are two key factors of an ingredient's sustainability.
Food that is grown locally and is currently ""in-season"" will have a lower
carbon footprint, but when dining out these details unfortunately may not
affect one's ordering preferences. We introduce DinAR as an immersive
experience to make this information more accessible and to encourage better
dining choices through friendly competition with a leaderboard of
sustainability scores. Our study measures the effectiveness of immersive AR
experiences on impacting consumer preferences towards sustainability.",http://arxiv.org/abs/2404.13272v1
"Non-Destructive Peat Analysis using Hyperspectral Imaging and Machine
  Learning",2024-05-03T15:47:07Z,"Yijun Yan, Jinchang Ren, Barry Harrison, Oliver Lewis, Yinhe Li, Ping Ma","Peat, a crucial component in whisky production, imparts distinctive and
irreplaceable flavours to the final product. However, the extraction of peat
disrupts ancient ecosystems and releases significant amounts of carbon,
contributing to climate change. This paper aims to address this issue by
conducting a feasibility study on enhancing peat use efficiency in whisky
manufacturing through non-destructive analysis using hyperspectral imaging.
Results show that shot-wave infrared (SWIR) data is more effective for
analyzing peat samples and predicting total phenol levels, with accuracies up
to 99.81%.",http://arxiv.org/abs/2405.02191v1
Quantifying Influencer Impact on Affective Polarization,2024-05-24T19:28:11Z,"Rezaur Rashid, Joshua Melton, Ouldouz Ghorbani, Siddharth Krishnan, Shannon Reid, Gabriel Terejanu","In today's digital age, social media platforms play a crucial role in shaping
public opinion. This study explores how discussions led by influencers on
Twitter, now known as 'X', affect public sentiment and contribute to online
polarization. We developed a counterfactual framework to analyze the
polarization scores of conversations in scenarios both with and without the
presence of an influential figure. Two case studies, centered on the polarizing
issues of climate change and gun control, were examined. Our research
highlights the significant impact these figures have on public discourse,
providing valuable insights into how online discussions can influence societal
divisions.",http://arxiv.org/abs/2405.15893v2
"Unlocking the Potential of Renewable Energy Through Curtailment
  Prediction",2024-05-28T18:48:39Z,"Bilge Acun, Brent Morgan, Henry Richardson, Nat Steinsultz, Carole-Jean Wu","A significant fraction (5-15%) of renewable energy generated goes into waste
in the grids around the world today due to oversupply issues and transmission
constraints. Being able to predict when and where renewable curtailment occurs
would improve renewable utilization. The core of this work is to enable the
machine learning community to help decarbonize electricity grids by unlocking
the potential of renewable energy through curtailment prediction.",http://arxiv.org/abs/2405.18526v1
Flood Prediction Using Classical and Quantum Machine Learning Models,2024-07-01T06:31:41Z,"Marek Grzesiak, Param Thakkar","This study investigates the potential of quantum machine learning to improve
flood forecasting we focus on daily flood events along Germany's Wupper River
in 2023 our approach combines classical machine learning techniques with QML
techniques this hybrid model leverages quantum properties like superposition
and entanglement to achieve better accuracy and efficiency classical and QML
models are compared based on training time accuracy and scalability results
show that QML models offer competitive training times and improved prediction
accuracy this research signifies a step towards utilizing quantum technologies
for climate change adaptation we emphasize collaboration and continuous
innovation to implement this model in real-world flood management ultimately
enhancing global resilience against floods",http://arxiv.org/abs/2407.01001v1
Assessing Annotation Accuracy in Ice Sheets Using Quantitative Metrics,2024-06-26T04:43:51Z,"Bayu Adhi Tama, Vandana Janeja, Sanjay Purushotham","The increasing threat of sea level rise due to climate change necessitates a
deeper understanding of ice sheet structures. This study addresses the need for
accurate ice sheet data interpretation by introducing a suite of quantitative
metrics designed to validate ice sheet annotation techniques. Focusing on both
manual and automated methods, including ARESELP and its modified version,
MARESELP, we assess their accuracy against expert annotations. Our methodology
incorporates several computer vision metrics, traditionally underutilized in
glaciological research, to evaluate the continuity and connectivity of ice
layer annotations. The results demonstrate that while manual annotations
provide invaluable expert insights, automated methods, particularly MARESELP,
improve layer continuity and alignment with expert labels.",http://arxiv.org/abs/2407.09535v1
Can nuclear energy contribute to the energy transition?,2024-07-18T09:24:27Z,"Axel Kleidon, Harald Lesch","In the course of the energy transition, energy generation from nuclear power
- through nuclear fission and perhaps in the future through nuclear fusion - is
often proposed as an alternative or supplement to renewable energy sources.
There are already good reasons why electricity generation from nuclear energy
is significantly more expensive than other forms of generation, while
increasing dryness as a result of climate change is generally calling into
question the reliability of thermal power plants. Nuclear energy is therefore
unlikely to play a role in a future energy supply that relies on low costs and
reliability.",http://arxiv.org/abs/2407.13325v1
Boosting the clean energy transition through data science,2024-08-27T17:18:55Z,"A. Fronzetti Colladon, A. L. Pisello, L. F. Cabeza","The demand for research supporting the development of new policy frameworks
for energy saving and conservation has never been more critical. As climate
change accelerates and its impacts become increasingly severe, the need for
sustainable and resilient socioeconomic systems is increasingly pressing. In
response to this global challenge, the ten articles of this special issue seek
to explore how advances in Artificial Intelligence and Data Science can drive
the energy transition and enhance environmental sustainability.",http://arxiv.org/abs/2408.15211v1
Trends and biases in the social cost of carbon,2024-09-12T15:51:51Z,Richard S. J. Tol,"An updated and extended meta-analysis confirms that the central estimate of
the social cost of carbon is around $200/tC with a large, right-skewed
uncertainty and trending up. The pure rate of time preference and the inverse
of the elasticity of intertemporal substitution are key assumptions, the total
impact of 2.5K warming less so. The social cost of carbon is much higher if
climate change is assumed to affect economic growth rather than the level of
output and welfare. The literature is dominated by a relatively small network
of authors, based in a few countries. Publication and citation bias have pushed
the social cost of carbon up.",http://arxiv.org/abs/2409.08158v1
Temperature Variability and Natural Disasters,2024-09-23T11:44:17Z,"Aatishya Mohanty, Nattavudh Powdthavee, Cheng Keat Tang, Andrew J. Oswald","This paper studies natural disasters and the psychological costs of climate
change. It presents what we believe to be the first evidence that higher
temperature variability and not a higher level of temperature is what predicts
natural disasters. This conclusion holds whether or not we control for the
(incorrectly signed) impact of temperature. The analysis draws upon
long-differences regression equations using GDIS data from 1960-2018 for 176
countries and the contiguous states of the USA. Results are checked on FEMA
data. Wellbeing impact losses are calculated. To our knowledge, the paper's
results are unknown to natural and social scientists.",http://arxiv.org/abs/2409.14936v1
"Enhanced physics-informed neural networks (PINNs) for high-order power
  grid dynamics",2024-10-10T01:52:47Z,Vineet Jagadeesan Nair,"We develop improved physics-informed neural networks (PINNs) for high-order
and high-dimensional power system models described by nonlinear ordinary
differential equations. We propose some novel enhancements to improve PINN
training and accuracy and also implement several other recently proposed ideas
from the literature. We successfully apply these to study the transient
dynamics of synchronous generators. We also make progress towards applying
PINNs to advanced inverter models. Such enhanced PINNs can allow us to
accelerate high-fidelity simulations needed to ensure a stable and reliable
renewables-rich future grid.",http://arxiv.org/abs/2410.07527v1
"At the junction between deep learning and statistics of extremes:
  formalizing the landslide hazard definition",2024-01-25T14:48:08Z,"Ashok Dahal, Rapha√´l Huser, Luigi Lombardo","The most adopted definition of landslide hazard combines spatial information
about landslide location (susceptibility), threat (intensity), and frequency
(return period). Only the first two elements are usually considered and
estimated when working over vast areas. Even then, separate models constitute
the standard, with frequency being rarely investigated. Frequency and intensity
are intertwined and depend on each other because larger events occur less
frequently and vice versa. However, due to the lack of multi-temporal
inventories and joint statistical models, modelling such properties via a
unified hazard model has always been challenging and has yet to be attempted.
Here, we develop a unified model to estimate landslide hazard at the slope unit
level to address such gaps. We employed deep learning, combined with a model
motivated by extreme-value theory to analyse an inventory of 30 years of
observed rainfall-triggered landslides in Nepal and assess landslide hazard for
multiple return periods. We also use our model to further explore landslide
hazard for the same return periods under different climate change scenarios up
to the end of the century. Our results show that the proposed model performs
excellently and can be used to model landslide hazard in a unified manner.
Geomorphologically, we find that under both climate change scenarios (SSP245
and SSP885), landslide hazard is likely to increase up to two times on average
in the lower Himalayan regions while remaining the same in the middle Himalayan
region whilst decreasing slightly in the upper Himalayan region areas.",http://arxiv.org/abs/2401.14210v1
"A Cradle-to-Gate Life Cycle Analysis of Bitcoin Mining Equipment Using
  Sphera LCA and ecoinvent Databases",2024-01-31T00:13:31Z,"Ludmila Courtillat--Piazza, Thibault Pirson, Louis Golard, David Bol","Bitcoin mining is regularly pointed out for its massive energy consumption
and associated greenhouse gas emissions, hence contributing significantly to
climate change. However, most studies ignore the environmental impacts of
producing mining equipment, which is problematic given the short lifespan of
such highly specific hardware. In this study, we perform a cradle-to-gate life
cycle assessment (LCA) of dedicated Bitcoin mining equipment, considering their
specific architecture. Our results show that the application-specific
integrated circuit designed for Bitcoin mining is the main contributor to
production-related impacts. This observation applies to most impact categories,
including the global warming potential. In addition, this finding stresses out
the necessity to carefully consider the specificity of the hardware. By
comparing these results with several usage scenarios, we also demonstrate that
the impacts of producing this type of equipment can be significant (up to 80%
of the total life cycle impacts), depending on the sources of electricity
supply for the use phase. Therefore, we highlight the need to consider the
production phase when assessing the environmental impacts of Bitcoin mining
hardware. To test the validity of our results, we use the Sphera LCA and
ecoinvent databases for the background modeling of our system. Surprisingly, it
leads to results with variations of up to 4 orders of magnitude for
toxicity-related indicators, despite using the same foreground modeling. This
database mismatch phenomenon, already identified in previous studies, calls for
better understanding, consideration and discussion of environmental impacts in
the field of electronics, going well beyond climate change indicators.",http://arxiv.org/abs/2401.17512v2
"A secular solar system resonance that disrupts the dominant cycle in
  Earth's orbital eccentricity (g2-g5): Implications for astrochronology",2024-03-14T12:20:59Z,"Richard E. Zeebe, Margriet L. Lantink","The planets' gravitational interaction causes rhythmic changes in Earth's
orbital parameters (also called Milankovi\'c cycles), which have powerful
applications in geology and astrochronology. For instance, the primary
astronomical eccentricity cycle due to the secular frequency term (g2-g5) (~405
kyr in the recent past) utilized in deep-time analyses is dominated by Venus'
and Jupiter's orbits, aka long eccentricity cycle. The widely accepted and
long-held view is that (g2-g5) was practically stable in the past and may hence
be used as a ""metronome"" to reconstruct accurate ages and chronologies.
However, using state-of-the-art integrations of the solar system, we show here
that (g2-g5) can become unstable over long time scales, without major changes
in, or destabilization of, planetary orbits. The (g2-g5) disruption is due to
the secular resonance $\sigma_{12}$ = (g1 - g2) + (s1 - s2), a major
contributor to solar system chaos. We demonstrate that entering/exiting the
$\sigma_{12}$ resonance is a common phenomenon on long time scales, occurring
in ~40% of our solutions. During $\sigma_{12}$-resonance episodes, (g2-g5) is
very weak or absent and Earth's orbital eccentricity and climate-forcing
spectrum are unrecognizable compared to the recent past. Our results have
fundamental implications for geology and astrochronology, as well as climate
forcing because the paradigm that the longest Milankovi\'c cycle dominates
Earth's astronomical forcing, is stable, and has a period of ~405 kyr requires
revision.",http://arxiv.org/abs/2403.09332v1
"Two-Stage Stance Labeling: User-Hashtag Heuristics with Graph Neural
  Networks",2024-04-16T02:18:30Z,"Joshua Melton, Shannon Reid, Gabriel Terejanu, Siddharth Krishnan","The high volume and rapid evolution of content on social media present major
challenges for studying the stance of social media users. In this work, we
develop a two stage stance labeling method that utilizes the user-hashtag
bipartite graph and the user-user interaction graph. In the first stage, a
simple and efficient heuristic for stance labeling uses the user-hashtag
bipartite graph to iteratively update the stance association of user and
hashtag nodes via a label propagation mechanism. This set of soft labels is
then integrated with the user-user interaction graph to train a graph neural
network (GNN) model using semi-supervised learning. We evaluate this method on
two large-scale datasets containing tweets related to climate change from June
2021 to June 2022 and gun control from January 2022 to January 2023. Our
experiments demonstrate that enriching text-based embeddings of users with
network information from the user interaction graph using our semi-supervised
GNN method outperforms both classifiers trained on user textual embeddings and
zero-shot classification using LLMs such as GPT4. We discuss the need for
integrating nuanced understanding from social science with the scalability of
computational methods to better understand how polarization on social media
occurs for divisive issues such as climate change and gun control.",http://arxiv.org/abs/2404.10228v2
"Ocean-DC: An analysis ready data cube framework for environmental and
  climate change monitoring over the port areas",2024-05-10T15:43:47Z,"Ioannis Kavouras, Ioannis Rallis, Nikolaos Doulamis, Anastasios Doulamis","The environmental hazards and climate change effects causes serious problems
in land and coastal areas. A solution to this problem can be the periodic
monitoring over critical areas, like coastal region with heavy industrial
activity (i.e., ship-buildings) or areas where a disaster (i.e., oil-spill) has
occurred. Today there are several Earth and non-Earth Observation data
available from several data providers. These data are huge in size and usually
it is needed to combine several data from multiple sources (i.e., data with
format differences) for a more effective evaluation. For addressing these
issues, this work proposes the Ocean-DC framework as a solution in data
harmonization and homogenization. A strong advantage of this Data Cube
implementation is the generation of a single NetCDF product that contains Earth
Observation data of several data types (i.e., Landsat-8 and Sentinel-2). To
evaluate the effectiveness and efficiency of the Ocean-DC implementation, it is
examined a case study of an oil-spill in Saronic gulf in September of 2017. The
generated 4D Data Cube considers both Landsat-8,9 and Sentinel-2 products for a
time-series analysis, before, during, and after the oil-spill event. The
Ocean-DC framework successfully generated a NetCDF product, containing all the
necessary remote sensing products for monitoring the oil-spill disaster in the
Saronic gulf.",http://arxiv.org/abs/2405.06730v1
"Time-Varying Constraint-Aware Reinforcement Learning for Energy Storage
  Control",2024-05-17T04:28:54Z,"Jaeik Jeong, Tai-Yeon Ku, Wan-Ki Park","Energy storage devices, such as batteries, thermal energy storages, and
hydrogen systems, can help mitigate climate change by ensuring a more stable
and sustainable power supply. To maximize the effectiveness of such energy
storage, determining the appropriate charging and discharging amounts for each
time period is crucial. Reinforcement learning is preferred over traditional
optimization for the control of energy storage due to its ability to adapt to
dynamic and complex environments. However, the continuous nature of charging
and discharging levels in energy storage poses limitations for discrete
reinforcement learning, and time-varying feasible charge-discharge range based
on state of charge (SoC) variability also limits the conventional continuous
reinforcement learning. In this paper, we propose a continuous reinforcement
learning approach that takes into account the time-varying feasible
charge-discharge range. An additional objective function was introduced for
learning the feasible action range for each time period, supplementing the
objectives of training the actor for policy learning and the critic for value
learning. This actively promotes the utilization of energy storage by
preventing them from getting stuck in suboptimal states, such as continuous
full charging or discharging. This is achieved through the enforcement of the
charging and discharging levels into the feasible action range. The
experimental results demonstrated that the proposed method further maximized
the effectiveness of energy storage by actively enhancing its utilization.",http://arxiv.org/abs/2405.10536v1
"Comparing the influence of Atlantic Multidecadal Variability and spring
  soil moisture on European summer heat waves",2024-05-17T14:41:29Z,"Valeria Mascolo, Cl√©ment Le Priol, Fabio D'Andrea, Freddy Bouchet","In this work, we study and compare the influence of the Atlantic Multidecadal
Variability (AMV) and of spring soil moisture in Southern Europe on the
duration and intensity of European summer heat waves. We study common heat
waves with return times of a few years like in previous studies, but we also
propose a new methodological approach, return time maps, that allows us to
study rare heat waves with return times from 10 to 50 years. We use the outputs
from three climate models, namely IPSL-CM6A-LR, EC-Earth3, and CNRM-CM6-1, in
which North Atlantic sea surface temperatures are restored towards the observed
AMV anomalies. The three models give consistent results, with the exception of
EC-Earth simulating a much greater influence of soil moisture. Typical AMV or
spring soil moisture anomalies induce changes in the temperature and duration
of heat waves that are of comparable amplitude, but follow different regional
patterns. As might be expected, a positive AMV phase or low soil moisture
induces hotter and longer typical heat waves over most of Europe. However,
counter-intuitively, they also induce less heat wave days and cooler heat waves
over part of Northeast Europe. For more extreme events, the influence of the
AMV and soil moisture increase, according to rather similar regional patterns
as for typical heat waves. However, while the amplitude of the influence is
greater, the regions with decreased heat wave temperature and less heat wave
days extend in size.",http://arxiv.org/abs/2405.10821v2
"Symmetry and symmetry-breaking in soil pores and climate change
  mitigation: What fractal geometry can tell us?",2024-05-23T06:25:06Z,Abhijeet Das,"Soil is a critical component of terrestrial ecosystems, directly influencing
global biogeochemical cycles. Despite its importance, the complex architecture
of soil pores and their impact on greenhouse gas emissions remain poorly
understood. This perspective aims to address this gap by applying symmetry and
symmetry-breaking concepts through fractal geometry to elucidate the structural
and functional complexities of soil pores. We highlight how fractal parameters
can quantify the self-similar nature of soil pore structures, revealing their
size, shape, and connectivity. These geometric attributes influence soil
properties such as permeability and diffusivity, which are essential for
understanding gas exchange and microbial activity within the soil matrix.
Furthermore, we emphasize the effects of various land management practices,
including tillage and wetting-drying cycles, on soil pore complexity using
three-dimensional multi-fractal analysis. Literature indicates that different
agricultural practices significantly alter pore heterogeneity and connectivity,
affecting greenhouse gas emissions. Conventional tillage decreases pore
connectivity and increases randomness, whereas no-tillage preserves larger,
more complex pore structures. We propose that integrating combinatorial,
geometric, and functional symmetry concepts offers a comprehensive framework
for examining the structure-property-function relationships in soil. This novel
approach could enhance our understanding of soil's role in the global cycle of
greenhouse gases and provide insights into sustainable land management
practices aimed at mitigating climate change.",http://arxiv.org/abs/2405.14217v1
"Deep Vision-Based Framework for Coastal Flood Prediction Under Climate
  Change Impacts and Shoreline Adaptations",2024-06-06T19:54:34Z,"Areg Karapetyan, Aaron Chung Hin Chow, Samer Madanat","In light of growing threats posed by climate change in general and sea level
rise (SLR) in particular, the necessity for computationally efficient means to
estimate and analyze potential coastal flood hazards has become increasingly
pressing. Data-driven supervised learning methods serve as promising candidates
that can dramatically expedite the process, thereby eliminating the
computational bottleneck associated with traditional physics-based hydrodynamic
simulators. Yet, the development of accurate and reliable coastal flood
prediction models, especially those based on Deep Learning (DL) techniques, has
been plagued with two major issues: (1) the scarcity of training data and (2)
the high-dimensional output required for detailed inundation mapping. To remove
this barrier, we present a systematic framework for training high-fidelity Deep
Vision-based coastal flood prediction models in low-data settings. We test the
proposed workflow on different existing vision models, including a fully
transformer-based architecture and a Convolutional Neural Network (CNN) with
additive attention gates. Additionally, we introduce a deep CNN architecture
tailored specifically to the coastal flood prediction problem at hand. The
model was designed with a particular focus on its compactness so as to cater to
resource-constrained scenarios and accessibility aspects. The performance of
the developed DL models is validated against commonly adopted geostatistical
regression methods and traditional Machine Learning (ML) approaches,
demonstrating substantial improvement in prediction quality. Lastly, we round
up the contributions by providing a meticulously curated dataset of synthetic
flood inundation maps of Abu Dhabi's coast produced with a physics-based
hydrodynamic simulator, which can serve as a benchmark for evaluating future
coastal flood prediction models.",http://arxiv.org/abs/2406.15451v1
"Statements: Universal Information Extraction from Tables with Large
  Language Models for ESG KPIs",2024-06-27T11:28:50Z,"Lokesh Mishra, Sohayl Dhibi, Yusik Kim, Cesar Berrospi Ramis, Shubham Gupta, Michele Dolfi, Peter Staar","Environment, Social, and Governance (ESG) KPIs assess an organization's
performance on issues such as climate change, greenhouse gas emissions, water
consumption, waste management, human rights, diversity, and policies. ESG
reports convey this valuable quantitative information through tables.
Unfortunately, extracting this information is difficult due to high variability
in the table structure as well as content. We propose Statements, a novel
domain agnostic data structure for extracting quantitative facts and related
information. We propose translating tables to statements as a new supervised
deep-learning universal information extraction task. We introduce SemTabNet - a
dataset of over 100K annotated tables. Investigating a family of T5-based
Statement Extraction Models, our best model generates statements which are 82%
similar to the ground-truth (compared to baseline of 21%). We demonstrate the
advantages of statements by applying our model to over 2700 tables from ESG
reports. The homogeneous nature of statements permits exploratory data analysis
on expansive information found in large collections of ESG reports.",http://arxiv.org/abs/2406.19102v1
"A 72h exploration of the co-evolution of food insecurity and
  international migration",2024-07-03T14:03:21Z,"Duncan Cassells, Lorenzo Costantini, Ariel Flint Ashery, Shreyas Gadge, Diogo L. Pires, Miguel √Å. S√°nchez-Cort√©s, Arnaldo Santoro, Elisa Omodei","Food insecurity, defined as the lack of physical or economic access to safe,
nutritious and sufficient food, remains one of the main challenges of the 2030
Agenda for Sustainable Development. Food insecurity is a complex phenomenon,
resulting from the interplay of environmental, socio-demographic, and political
events. Previous work has investigated the nexus between climate change,
conflict, migration and food security at the household level, however these
relations are still largely unexplored at national scales. In this context,
during the Complexity72h workshop, held at the Universidad Carlos III de Madrid
in June 2024, we explored the co-evolution of international migration flows and
food insecurity at the national scale, accounting for remittances, as well as
for changes in the economic, conflict, and climate situation. To this aim, we
gathered data from several publicly available sources (Food and Agriculture
Organization, World Bank, and UN Department of Economic and Social Affairs) and
analyzed the association between food insecurity and migration, migration and
remittances, and remittances and food insecurity. We then propose a framework
linking together these associations to model the co-evolution of food
insecurity and international migrations.",http://arxiv.org/abs/2407.03117v1
"Mapping Local Green Hydrogen Cost-Potentials by a Multidisciplinary
  Approach",2024-07-10T12:01:38Z,"Shitab Ishmam, Heidi Heinrichs, Christoph Winkler, Bagher Bayat, Amin Lahnaoui, Solomon Agbo, Edgar Ubaldo Pena Sanchez, David Franzmann, Nathan Ojieabu, Celine Koerner, Youpele Micheal, Bamidele Oloruntoba, Carsten Montzka, Harry Vereecken, Harrie-Jan Hendricks-Franssen, Jeerawan Brendt, Simon Brauner, Wilhelm Kuckshinrichs, Sandra Venghaus, Daouda Kone, Bruno Korgo, Kehinde Ogunjobi, Vasco Chiteculo, Jane Olwoch, Zachary Getenga, Jochen Lin√üen, Detlef Stolten","For fast-tracking climate change response, green hydrogen is key for
achieving greenhouse gas neutral energy systems. Especially Sub-Saharan Africa
can benefit from it enabling an increased access to clean energy through
utilizing its beneficial conditions for renewable energies. However, developing
green hydrogen strategies for Sub-Saharan Africa requires highly detailed and
consistent information ranging from technical, environmental, economic, and
social dimensions, which is currently lacking in literature. Therefore, this
paper provides a comprehensive novel approach embedding the required range of
disciplines to analyze green hydrogen cost-potentials in Sub-Saharan Africa.
This approach stretches from a dedicated land eligibility based on local
preferences, a location specific renewable energy simulation, locally derived
sustainable groundwater limitations under climate change, an optimization of
local hydrogen energy systems, and a socio-economic indicator-based impact
analysis. The capability of the approach is shown for case study regions in
Sub-Saharan Africa highlighting the need for a unified, interdisciplinary
approach.",http://arxiv.org/abs/2407.07573v1
Reconstructing Global Daily CO2 Emissions via Machine Learning,2024-07-29T14:44:14Z,"Tao Li, Lixing Wang, Zihan Qiu, Philippe Ciais, Taochun Sun, Matthew W. Jones, Robbie M. Andrew, Glen P. Peters, Piyu ke, Xiaoting Huang, Robert B. Jackson, Zhu Liu","High temporal resolution CO2 emission data are crucial for understanding the
drivers of emission changes, however, current emission dataset is only
available on a yearly basis. Here, we extended a global daily CO2 emissions
dataset backwards in time to 1970 using machine learning algorithm, which was
trained to predict historical daily emissions on national scales based on
relationships between daily emission variations and predictors established for
the period since 2019. Variation in daily CO2 emissions far exceeded the
smoothed seasonal variations. For example, the range of daily CO2 emissions
equivalent to 31% of the year average daily emissions in China and 46% of that
in India in 2022, respectively. We identified the critical emission-climate
temperature (Tc) is 16.5 degree celsius for global average (18.7 degree celsius
for China, 14.9 degree celsius for U.S., and 18.4 degree celsius for Japan), in
which negative correlation observed between daily CO2 emission and ambient
temperature below Tc and a positive correlation above it, demonstrating
increased emissions associated with higher ambient temperature. The long-term
time series spanning over fifty years of global daily CO2 emissions reveals an
increasing trend in emissions due to extreme temperature events, driven by the
rising frequency of these occurrences. This work suggests that, due to climate
change, greater efforts may be needed to reduce CO2 emissions.",http://arxiv.org/abs/2407.20057v1
"Cluster-Segregate-Perturb (CSP): A Model-agnostic Explainability
  Pipeline for Spatiotemporal Land Surface Forecasting Models",2024-08-12T04:29:54Z,"Tushar Verma, Sudipan Saha","Satellite images have become increasingly valuable for modelling regional
climate change effects. Earth surface forecasting represents one such task that
integrates satellite images with meteorological data to capture the joint
evolution of regional climate change effects. However, understanding the
complex relationship between specific meteorological variables and land surface
evolution poses a significant challenge. In light of this challenge, our paper
introduces a pipeline that integrates principles from both perturbation-based
explainability techniques like LIME and global marginal explainability
techniques like PDP, besides addressing the constraints of using such
techniques when applying them to high-dimensional spatiotemporal deep models.
The proposed pipeline simplifies the undertaking of diverse investigative
analyses, such as marginal sensitivity analysis, marginal correlation analysis,
lag analysis, etc., on complex land surface forecasting models In this study we
utilised Convolutional Long Short-Term Memory (ConvLSTM) as the surface
forecasting model and did analyses on the Normalized Difference Vegetation
Index (NDVI) of the surface forecasts, since meteorological variables like
temperature, pressure, and precipitation significantly influence it. The study
area encompasses various regions in Europe. Our analyses show that
precipitation exhibits the highest sensitivity in the study area, followed by
temperature and pressure. Pressure has little to no direct effect on NDVI.
Additionally, interesting nonlinear correlations between meteorological
variables and NDVI have been uncovered.",http://arxiv.org/abs/2408.05916v1
"Machine Learning for Methane Detection and Quantification from Space - A
  survey",2024-08-27T15:03:20Z,"Enno Tiemann, Shanyu Zhou, Alexander Kl√§ser, Konrad Heidler, Rochelle Schneider, Xiao Xiang Zhu","Methane ($CH_4$) is a potent anthropogenic greenhouse gas, contributing 86
times more to global warming than Carbon Dioxide ($CO_2$) over 20 years, and it
also acts as an air pollutant. Given its high radiative forcing potential and
relatively short atmospheric lifetime (9$\pm$1 years), methane has important
implications for climate change, therefore, cutting methane emissions is
crucial for effective climate change mitigation. This work expands existing
information on operational methane point source detection sensors in the
Short-Wave Infrared (SWIR) bands. It reviews the state-of-the-art for
traditional as well as Machine Learning (ML) approaches. The architecture and
data used in such ML models will be discussed separately for methane plume
segmentation and emission rate estimation. Traditionally, experts rely on
labor-intensive manually adjusted methods for methane detection. However, ML
approaches offer greater scalability. Our analysis reveals that ML models
outperform traditional methods, particularly those based on convolutional
neural networks (CNN), which are based on the U-net and transformer
architectures. These ML models extract valuable information from
methane-sensitive spectral data, enabling a more accurate detection. Challenges
arise when comparing these methods due to variations in data, sensor
specifications, and evaluation metrics. To address this, we discuss existing
datasets and metrics, providing an overview of available resources and
identifying open research problems. Finally, we explore potential future
advances in ML, emphasizing approaches for model comparability, large dataset
creation, and the European Union's forthcoming methane strategy.",http://arxiv.org/abs/2408.15122v1
"Dogs on forest trails; Understanding ecology of Striped Hyena and wild
  Canids in the presence of free-ranging dogs in Udanti-Sitanadi Tiger Reserve,
  Central India using Joint Distribution and Deep Neural Networks",2024-08-30T18:04:59Z,"Chiranjib Chaudhuri, Krishnendu Basak, M Suraj, Moiz Ahmed, Amit Kumar","This study uses Joint Species Distribution Models (JSDMs) and Deep Neural
Networks (DNNs) to explore how wild carnivores and free-ranging dogs interact
in the Udanti-Sitanadi Tiger Reserve (USTR) in Central India. The research
focuses on key species like the Striped Hyena, Grey Wolf, Golden Jackal, and
Indian Fox, revealing significant overlaps in habitat with free-ranging dogs,
especially in densely populated areas like the Sitanadi region of the tiger
reserve. These overlaps pose serious risks to wildlife through competition for
resources, predation, and the spread of diseases. The study shows that the
Striped Hyena prefers gentle slopes and forested areas, while the Grey Wolf
tends to avoid cropland and thrives in regions with higher rainfall that
supports a stable prey base. The Golden Jackal, more adaptable than the others,
favors west-facing slopes and stable temperatures, whereas the Indian Fox is
mainly found in the less disturbed, mountainous Kuladighat region.
Additionally, the study highlights the potential impacts of climate change,
predicting that the Grey Wolf could face habitat extinction under more severe
scenarios. These findings underscore the urgent need for conservation
strategies tailored to address both dog wild carnivore interactions and the
growing challenges posed by climate change, focusing on protecting the critical
habitats of vulnerable species like the Striped Hyena and Grey Wolf.",http://arxiv.org/abs/2409.00185v1
"Trophic Cascades and Habitat Suitability in Udanti Sitnadi Tiger
  Reserve: Impacts of Prey Depletion and Climate Change on Predator Prey
  Dynamics",2024-08-30T18:14:13Z,"Krishnendu Basak, Chiranjib Chaudhuri, M Suraj, Moiz Ahmed","This study investigates the trophic cascades and habitat suitability in
Udanti Sitnadi Tiger Reserve (USTR), highlighting the roles of apex predators,
subordinate predators, and prey species in maintaining ecosystem balance. Using
the Trophic Species Distribution Model (SDM), we explored prey-predator
interactions and habitat suitability, revealing that tigers, due to prey
depletion, increasingly rely on cattle, while leopards adapt by preying on
smaller species. The study emphasizes the need for prey augmentation and
habitat restoration to support apex predators. Additionally, climate change
projections for 2021-2040 and 2081-2100 under CMIP6 scenarios SSP245 and SSP585
indicate significant regional habitat shifts, necessitating adaptive management
strategies. Kuladighat is projected to face habitat contraction, while Sitanadi
may experience habitat expansion. Effective conservation efforts such as
habitat restoration, prey augmentation and predator recovery are the most
important steps needed to maintain the purpose of a Tiger reserve and
conservation potential of Udanti-Sonabeda Tiger Conservation Unit (TCU). To
achieve these dynamics, focusing on community participation, anti-poaching
measures, and scientific recommendations are the most crucial components to
focus on. This comprehensive analysis underscores the critical role of targeted
conservation activities in prey-depleted landscapes to ensure the long-term
survival of tigers and the overall health of forest ecosystems, enhancing
biodiversity and mitigating human-wildlife conflicts in USTR.",http://arxiv.org/abs/2409.00193v1
Explainable Earth Surface Forecasting under Extreme Events,2024-10-02T17:27:13Z,"Oscar J. Pellicer-Valero, Miguel-√Ångel Fern√°ndez-Torres, Chaonan Ji, Miguel D. Mahecha, Gustau Camps-Valls","With climate change-related extreme events on the rise, high dimensional
Earth observation data presents a unique opportunity for forecasting and
understanding impacts on ecosystems. This is, however, impeded by the
complexity of processing, visualizing, modeling, and explaining this data. To
showcase how this challenge can be met, here we train a convolutional long
short-term memory-based architecture on the novel DeepExtremeCubes dataset.
DeepExtremeCubes includes around 40,000 long-term Sentinel-2 minicubes (January
2016-October 2022) worldwide, along with labeled extreme events, meteorological
data, vegetation land cover, and topography map, sampled from locations
affected by extreme climate events and surrounding areas. When predicting
future reflectances and vegetation impacts through kernel normalized difference
vegetation index, the model achieved an R$^2$ score of 0.9055 in the test set.
Explainable artificial intelligence was used to analyze the model's predictions
during the October 2020 Central South America compound heatwave and drought
event. We chose the same area exactly one year before the event as
counterfactual, finding that the average temperature and surface pressure are
generally the best predictors under normal conditions. In contrast, minimum
anomalies of evaporation and surface latent heat flux take the lead during the
event. A change of regime is also observed in the attributions before the
event, which might help assess how long the event was brewing before happening.
The code to replicate all experiments and figures in this paper is publicly
available at https://github.com/DeepExtremes/txyXAI",http://arxiv.org/abs/2410.01770v2
"Evacuation patterns and socioeconomic stratification in the context of
  wildfires in Chile",2024-10-08T13:18:49Z,"Timur Naushirvanov, Erick Elejalde, Kyriaki Kalimeri, Elisa Omodei, M√°rton Karsai, Leo Ferres","Climate change is altering the frequency and intensity of wildfires, leading
to increased evacuation events that disrupt human mobility and socioeconomic
structures. These disruptions affect access to resources, employment, and
housing, amplifying existing vulnerabilities within communities. Understanding
the interplay between climate change, wildfires, evacuation patterns, and
socioeconomic factors is crucial for developing effective mitigation and
adaptation strategies. To contribute to this challenge, we use high-definition
mobile phone records to analyse evacuation patterns during the wildfires in
Valpara\'iso, Chile, that took place between February 2-3, 2024. This data
allows us to track the movements of individuals in the disaster area, providing
insight into how people respond to large-scale evacuations in the context of
severe wildfires. We apply a causal inference approach that combines regression
discontinuity and difference-in-differences methodologies to observe evacuation
behaviours during wildfires, with a focus on socioeconomic stratification. This
approach allows us to isolate the impact of the wildfires on different
socioeconomic groups by comparing the evacuation patterns of affected
populations before and after the event, while accounting for underlying trends
and discontinuities at the threshold of the disaster. We find that many people
spent nights away from home, with those in the lowest socioeconomic segment
stayed away the longest. In general, people reduced their travel distance
during the evacuation, and the lowest socioeconomic group moved the least.
Initially, movements became more random, as people sought refuge in a rush, but
eventually gravitated towards areas with similar socioeconomic status. Our
results show that socioeconomic differences play a role in evacuation dynamics,
providing useful insights for response planning.",http://arxiv.org/abs/2410.06017v1
Hybrid Bayesian Smoothing on Surfaces,2024-10-28T19:14:55Z,"Matthew Hofkes, Douglas Nychka","Modeling spatial processes that exhibit both smooth and rough features poses
a significant challenge. This is especially true in fields where complex
physical variables are observed across spatial domains. Traditional spatial
techniques, such as Gaussian processes (GPs), are ill-suited to capture sharp
transitions and discontinuities in spatial fields. In this paper, we propose a
new approach incorporating non-Gaussian processes (NGPs) into a hybrid model
which identifies both smooth and rough components. Specifically, we model the
rough process using scaled mixtures of Gaussian distributions in a Bayesian
hierarchical model (BHM).
  Our motivation comes from the Community Earth System Model Large Ensemble
(CESM-LE), where we seek to emulate climate sensitivity fields that exhibit
complex spatial patterns, including abrupt transitions at ocean-land
boundaries. We demonstrate that traditional GP models fail to capture such
abrupt changes and that our proposed hybrid model, implemented through a full
Gibbs sampler. This significantly improves model interpretability and accurate
recovery of process parameters.
  Through a multi-factor simulation study, we evaluate the performance of
several scaled mixtures designed to model the rough process. The results
highlight the advantages of using these heavier tailed priors as a replacement
to the Bayesian fused LASSO. One prior in particular, the normal Jeffrey's
prior stands above the rest. We apply our model to the CESM-LE dataset,
demonstrating its ability to better represent the mean function and its
uncertainty in climate sensitivity fields.
  This work combines the strengths of GPs for smooth processes with the
flexibility of NGPs for abrupt changes. We provide a computationally efficient
Gibbs sampler and include additional strategies for accelerating Monte Carlo
Markov Chain (MCMC) sampling.",http://arxiv.org/abs/2410.21469v1
Is a Recent Surge in Global Warming Detectable?,2024-03-06T00:49:17Z,"Claudie Beaulieu, Colin Gallagher, Rebecca Killick, Robert Lund, Xueheng Shi","The global mean surface temperature is widely studied to monitor climate
change. A current debate centers around whether there has been a recent
(post-1970s) surge/acceleration in the warming rate. This paper addresses
whether an acceleration in the warming rate is detectable from a statistical
perspective. We use changepoint models, which are statistical techniques
specifically designed for identifying structural changes in time series. Four
global mean surface temperature records over 1850-2023 are scrutinized within.
Our results show limited evidence for a warming surge; in most surface
temperature time series, no change in the warming rate beyond the 1970s is
detected. As such, we estimate minimum changes in the warming trend for a surge
to be detectable in the near future.",http://arxiv.org/abs/2403.03388v1
Landslide mapping from Sentinel-2 imagery through change detection,2024-05-30T15:33:32Z,"Tommaso Monopoli, Fabio Montello, Claudio Rossi","Landslides are one of the most critical and destructive geohazards.
Widespread development of human activities and settlements combined with the
effects of climate change on weather are resulting in a high increase in the
frequency and destructive power of landslides, making them a major threat to
human life and the economy. In this paper, we explore methodologies to map
newly-occurred landslides using Sentinel-2 imagery automatically. All
approaches presented are framed as a bi-temporal change detection problem,
requiring only a pair of Sentinel-2 images, taken respectively before and after
a landslide-triggering event. Furthermore, we introduce a novel deep learning
architecture for fusing Sentinel-2 bi-temporal image pairs with Digital
Elevation Model (DEM) data, showcasing its promising performances w.r.t. other
change detection models in the literature. As a parallel task, we address
limitations in existing datasets by creating a novel geodatabase, which
includes manually validated open-access landslide inventories over
heterogeneous ecoregions of the world. We release both code and dataset with an
open-source license.",http://arxiv.org/abs/2405.20161v1
Bayesian Spatiotemporal Wombling,2024-07-25T06:22:34Z,"Aritra Halder, Didong Li, Sudipto Banerjee","Stochastic process models for spatiotemporal data underlying random fields
find substantial utility in a range of scientific disciplines. Subsequent to
predictive inference on the values of the random field (or spatial surface
indexed continuously over time) at arbitrary space-time coordinates, scientific
interest often turns to gleaning information regarding zones of rapid
spatial-temporal change. We develop Bayesian modeling and inference for
directional rates of change along a given surface. These surfaces, which
demarcate regions of rapid change, are referred to as ``wombling'' surface
boundaries. Existing methods for studying such changes have often been
associated with curves and are not easily extendable to surfaces resulting from
curves evolving over time. Our current contribution devises a fully model-based
inferential framework for analyzing differential behavior in spatiotemporal
responses by formalizing the notion of a ``wombling'' surface boundary using
conventional multi-linear vector analytic frameworks and geometry followed by
posterior predictive computations using triangulated surface approximations. We
illustrate our methodology with comprehensive simulation experiments followed
by multiple applications in environmental and climate science; pollutant
analysis in environmental health; and brain imaging.",http://arxiv.org/abs/2407.17804v1
"Resilient Infrastructure Network: Sparse Edge Change Identification via
  L1-Regularized Least Squares",2024-09-11T01:34:45Z,Rajasekhar Anguluri,"Adversarial actions and a rapid climate change are disrupting operations of
infrastructure networks (e.g., energy, water, and transportation systems).
Unaddressed disruptions lead to system-wide shutdowns, emphasizing the need for
quick and robust identification methods. One significant disruption arises from
edge changes (addition or deletion) in networks. We present an $\ell_1$-norm
regularized least-squares framework to identify multiple but sparse edge
changes using noisy data. We focus only on networks that obey equilibrium
equations, as commonly observed in the above sectors. The presence or lack of
edges in these networks is captured by the sparsity pattern of the weighted,
symmetric Laplacian matrix, while noisy data are node injections and
potentials. Our proposed framework systematically leverages the inherent
structure within the Laplacian matrix, effectively avoiding
overparameterization. We demonstrate the robustness and efficacy of the
proposed approach through a series of representative examples, with a primary
emphasis on power networks.",http://arxiv.org/abs/2409.08304v1
"ChaosBench: A Multi-Channel, Physics-Based Benchmark for
  Subseasonal-to-Seasonal Climate Prediction",2024-02-01T16:07:12Z,"Juan Nathaniel, Yongquan Qu, Tung Nguyen, Sungduk Yu, Julius Busecke, Aditya Grover, Pierre Gentine","Accurate prediction of climate in the subseasonal-to-seasonal scale is
crucial for disaster preparedness and robust decision making amidst climate
change. Yet, forecasting beyond the weather timescale is challenging because it
deals with problems other than initial condition, including boundary
interaction, butterfly effect, and our inherent lack of physical understanding.
At present, existing benchmarks tend to have shorter forecasting range of up-to
15 days, do not include a wide range of operational baselines, and lack
physics-based constraints for explainability. Thus, we propose ChaosBench, a
challenging benchmark to extend the predictability range of data-driven weather
emulators to S2S timescale. First, ChaosBench is comprised of variables beyond
the typical surface-atmospheric ERA5 to also include ocean, ice, and land
reanalysis products that span over 45 years to allow for full Earth system
emulation that respects boundary conditions. We also propose physics-based, in
addition to deterministic and probabilistic metrics, to ensure a
physically-consistent ensemble that accounts for butterfly effect. Furthermore,
we evaluate on a diverse set of physics-based forecasts from four national
weather agencies as baselines to our data-driven counterpart such as
ViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we find
methods originally developed for weather-scale applications fail on S2S task:
their performance simply collapse to an unskilled climatology. Nonetheless, we
outline and demonstrate several strategies that can extend the predictability
range of existing weather emulators, including the use of ensembles, robust
control of error propagation, and the use of physics-informed models. Our
benchmark, datasets, and instructions are available at
https://leap-stc.github.io/ChaosBench.",http://arxiv.org/abs/2402.00712v5
"Deciphering Super El Ni√±o: Development of a Novel Predictive Model
  Integrating Local and Global Climatic Signals",2024-09-10T02:14:00Z,"Chae-Hyun Yoon, Jubin Park, Myung-Ki Cheoun","In recent years, extreme weather events have surged, highlighting the urgent
need for action on the climate emergency. The year 2023 saw record-breaking
global temperatures, unprecedented heatwaves in Europe, devastating floods in
Asia, and severe wildfires in North America and Australia. Super El Ni\~no
events, known for their profound impact on global weather, play a critical role
in these changes, causing severe economic and environmental damage. This study
presents a novel predictive model that integrates systematically local and
global climatic signals to forecast Super El Ni\~no events, introducing the
Super El Ni\~no Index (SEI), which value of 80 or higher defines a Super El
Ni\~no event. Our analysis shows that the SEI accurately reflects past Super El
Ni\~no events, including those from 1982-83, 1997-98, and 2015-16, with SEI
values for these periods containing 80 within the 2-sigma standard deviation.
Using data up to 2022, our model predicted an SEI of around 80 for 2023,
indicating a Super El Ni\~no for the 2023-24 period. Recent observations
confirm that the 2023-24 El Ni\~no is among the five strongest recorded Super
El Ni\~no events in history. An analysis of SEI trends from 1982 to 2023
reveals a gradual increase, with recent El Ni\~no events consistently exceeding
SEI values of 70. This trend suggests that El Ni\~no events are increasingly
approaching Super El Ni\~no intensity, potentially due to more favorable
conditions in the equatorial Pacific. This increase in SEI values and the
frequency of stronger El Ni\~no events may be attributed to the ongoing effects
of global warming. These findings emphasize the need for heightened
preparedness and strategic planning to mitigate the impacts of future Super El
Ni\~no events, which are likely to become more frequent in the coming decades.",http://arxiv.org/abs/2409.06161v1
"Organizing Scientific Knowledge From Energy System Research Using the
  Open Research Knowledge Graph",2024-01-24T10:46:32Z,"Oliver Karras, Jan G√∂pfert, Patrick Kuckertz, Tristan Pelser, S√∂ren Auer","Engineering sciences, such as energy system research, play an important role
in developing solutions to technical, environmental, economic, and social
challenges of our modern society. In this context, the transformation of energy
systems into climate-neutral systems is one of the key strategies for
mitigating climate change. For the transformation of energy systems, engineers
model, simulate and analyze scenarios and transformation pathways to initiate
debates about possible transformation strategies. For these debates and
research in general, all steps of the research process must be traceable to
guarantee the trustworthiness of published results, avoid redundancies, and
ensure their social acceptance. However, the analysis of energy systems is an
interdisciplinary field as the investigations of large, complex energy systems
often require the use of different software applications and large amounts of
heterogeneous data. Engineers must therefore communicate, understand, and
(re)use heterogeneous scientific knowledge and data. Although the importance of
FAIR scientific knowledge and data in the engineering sciences and energy
system research is increasing, little research has been conducted on this
topic. When it comes to publishing scientific knowledge and data from
publications, software, and datasets (such as models, scenarios, and
simulations) openly available and transparent, energy system research lags
behind other research domains. According to Schmitt et al. and Nie{\ss}e et
al., engineers need technical support in the form of infrastructures, services,
and terminologies to improve communication, understanding, and (re)use of
scientific knowledge and data.",http://arxiv.org/abs/2401.13365v1
WildfireGPT: Tailored Large Language Model for Wildfire Analysis,2024-02-12T18:41:55Z,"Yangxinyu Xie, Bowen Jiang, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, Jordan Branham, M. Ross Alexander, Robert B. Ross, Yan Feng, Leslie-Anne Levy, Weijie Su, Camillo J. Taylor","Recent advancement of large language models (LLMs) represents a
transformational capability at the frontier of artificial intelligence.
However, LLMs are generalized models, trained on extensive text corpus, and
often struggle to provide context-specific information, particularly in areas
requiring specialized knowledge, such as wildfire details within the broader
context of climate change. For decision-makers focused on wildfire resilience
and adaptation, it is crucial to obtain responses that are not only precise but
also domain-specific. To that end, we developed WildfireGPT, a prototype LLM
agent designed to transform user queries into actionable insights on wildfire
risks. We enrich WildfireGPT by providing additional context, such as climate
projections and scientific literature, to ensure its information is current,
relevant, and scientifically accurate. This enables WildfireGPT to be an
effective tool for delivering detailed, user-specific insights on wildfire
risks to support a diverse set of end users, including but not limited to
researchers and engineers, for making positive impact and decision making.",http://arxiv.org/abs/2402.07877v2
Ozone Anomalies in Dry Intrusions Associated with Atmospheric Rivers,2024-02-15T18:58:00Z,"Kirsten R. Hall, Huiqun Wang, Amir H. Souri, Xiong Liu, Kelly Chance","As a result of their important role in weather and the global hydrological
cycle, understanding atmospheric rivers' (ARs) connection to synoptic-scale
climate patterns and atmospheric dynamics has become increasingly important. In
addition to case studies of two extreme AR events, we produce a December
climatology of the three-dimensional structure of water vapor and O3 (ozone)
distributions associated with ARs in the northeastern Pacific from 2004-2014
using MERRA-2 reanalysis products. Results show that positive O3 anomalies
reside in dry intrusions of stratospheric air due to
stratosphere-to-troposphere transport (STT) behind the intense water vapor
transport of the AR. In composites, we find increased excesses of O3
concentration, as well as in the total O3 flux within the dry intrusions, with
increased AR strength. We find that STT O3 flux associated with ARs over the NE
Pacific accounts for up to 13 percent of total Northern Hemisphere STT O3 flux
in December, and extrapolation indicates that AR-associated dry intrusions may
account for as much as 32 percent of total NH STT O3 flux. This study
quantifies STT of O3 in connection with ARs for the first time and improves
estimates of tropospheric ozone concentration due to STT in the identification
of this correlation. In light of predictions that ARs will become more intense
and/or frequent with climate change, quantifying AR-related STT O3 flux is
especially valuable for future radiative forcing calculations.",http://arxiv.org/abs/2402.10205v1
"Between Green Hills and Green Bills: Unveiling the Green Shades of
  Sustainability and Burden Shifting through Multi-Objective Optimization in
  Swiss Energy System Planning",2024-02-20T12:48:16Z,"Jonas Schnidrig, Matthieu Souttre, Arthur Chuat, Fran√ßois Mar√©chal, Manuele Margni","The Paris agreement is the first-ever universally accepted and legally
binding agreement on global climate change. It is a bridge between today's and
climate-neutrality policies and strategies before the end of the century.
Critical to this endeavor is energy system modeling, which, while adept at
devising cost-effective carbon-neutral strategies, often overlooks the broader
environmental and social implications. This study introduces an innovative
methodology that integrates life-cycle impact assessment indicators into energy
system modeling, enabling a comprehensive assessment of both economic and
environmental outcomes.
  Focusing on Switzerland's energy system as a case study, our model reveals
that optimizing key environomic indicators can lead to significant economic
advantages, with system costs potentially decreasing by 15% to 47% by
minimizing potential impacts from operating fossil technologies to the indirect
impact related to the construction of the renewable infrastructure. However, a
system optimized solely for economic efficiency, despite achieving 63%
reduction in carbon footprint compared to 2020, our results show a potential
risk of burden shift to other environmental issues.
  The adoption of multi-objective optimization in our approach nuances the
exploration of the complex interplay between environomic objectives and
technological choices. Our results illuminate pathways towards more
holistically optimized energy systems, effectively addressing trade-offs across
environmental problems and enhancing societal acceptance of the solutions to
this century's defining challenge.",http://arxiv.org/abs/2402.12973v1
Spatio-temporal modeling for record-breaking temperature events in Spain,2024-02-29T19:14:13Z,"Jorge Castillo-Mateo, Alan E. Gelfand, Zeus Gracia-Tabuenca, Jes√∫s As√≠n, Ana C. Cebri√°n","Record-breaking temperature events are now very frequently in the news,
viewed as evidence of climate change. With this as motivation, we undertake the
first substantial spatial modeling investigation of temperature record-breaking
across years for any given day within the year. We work with a dataset
consisting of over sixty years (1960-2021) of daily maximum temperatures across
peninsular Spain. Formal statistical analysis of record-breaking events is an
area that has received attention primarily within the probability community,
dominated by results for the stationary record-breaking setting with some
additional work addressing trends. Such effort is inadequate for analyzing
actual record-breaking data. Effective analysis requires rich modeling of the
indicator events which define record-breaking sequences. Resulting from novel
and detailed exploratory data analysis, we propose hierarchical conditional
models for the indicator events. After suitable model selection, we discover
explicit trend behavior, necessary autoregression, significance of distance to
the coast, useful interactions, helpful spatial random effects, and very strong
daily random effects. Illustratively, the model estimates that global warming
trends have increased the number of records expected in the past decade almost
two-fold, 1.93 (1.89,1.98), but also estimates highly differentiated climate
warming rates in space and by season.",http://arxiv.org/abs/2403.00080v1
"Decades matter: Agricultural diversification increases financial
  profitability, biodiversity, and ecosystem services over time",2024-03-07T22:14:15Z,"Estelle Raveloaritiana, Thomas Cherico Wanger","Sustainable agriculture in the 21st century requires the production of
sufficient food while reducing the environmental impact and safeguarding human
livelihoods. Many studies have confirmed agricultural diversification practices
such as intercropping, organic farming and soil inoculations as a suitable
pathway to achieve these goals, but long-term viability of socioeconomic and
ecological benefits is uncertain. Here, we quantified the long-term effects of
agricultural diversification practices on socioeconomic and ecological benefits
based on 50 years of data from 184 meta-analyses and 4,260 effect sizes. We
showed that, with neutral crop yield over time, financial profitability, most
variables related to biological communities, all aspects of soil quality, and
carbon sequestration benefits increased by up to 2823% over 20 years of
practice. Non-crop diversification practices and the use of organic amendments
increased benefits by up to 2000% after 50 years. A trade-off analysis between
yield and other services showed win-win outcomes during the first 25 years. Our
synthesis provides the urgently needed evidence for farmers and other
decision-makers that diversification increases long-term profitability,
biodiversity, and climate mitigation benefits, and therefore, allows upscaling
diversification for climate change mitigation and global food system
transformation.",http://arxiv.org/abs/2403.05599v1
Employing Federated Learning for Training Autonomous HVAC Systems,2024-05-01T08:42:22Z,"Fredrik Hagstr√∂m, Vikas Garg, Fabricio Oliveira","Buildings account for 40 % of global energy consumption. A considerable
portion of building energy consumption stems from heating, ventilation, and air
conditioning (HVAC), and thus implementing smart, energy-efficient HVAC systems
has the potential to significantly impact the course of climate change. In
recent years, model-free reinforcement learning algorithms have been
increasingly assessed for this purpose due to their ability to learn and adapt
purely from experience. They have been shown to outperform classical
controllers in terms of energy cost and consumption, as well as thermal
comfort. However, their weakness lies in their relatively poor data efficiency,
requiring long periods of training to reach acceptable policies, making them
inapplicable to real-world controllers directly. Hence, common research goals
are to improve the learning speed, as well as to improve their ability to
generalize, in order to facilitate transfer learning to unseen building
environments. In this paper, we take a federated learning approach to training
the reinforcement learning controller of an HVAC system. A global control
policy is learned by aggregating local policies trained on multiple data
centers located in different climate zones. The goal of the policy is to
simultaneously minimize energy consumption and maximize thermal comfort. The
federated optimization strategy indirectly increases both the rate at which
experience data is collected and the variation in the data. We demonstrate
through experimental evaluation that these effects lead to a faster learning
speed, as well as greater generalization capabilities in the federated policy
compared to any individually trained policy.",http://arxiv.org/abs/2405.00389v1
"Heat, Health, and Habitats: Analyzing the Intersecting Risks of Climate
  and Demographic Shifts in Austrian Districts",2024-05-01T14:27:15Z,"Hannah Schuster, Axel Polleres, Amin Anjomshoaa, Johannes Wachs","The impact of hot weather on health outcomes of a population is mediated by a
variety of factors, including its age profile and local green infrastructure.
The combination of warming due to climate change and demographic aging suggests
that heat-related health outcomes will deteriorate in the coming decades. Here,
we measure the relationship between weekly all-cause mortality and heat days in
Austrian districts using a panel dataset covering $2015-2022$. An additional
day reaching $30$ degrees is associated with a $2.4\%$ increase in mortality
per $1000$ inhabitants during summer. This association is roughly doubled in
districts with a two standard deviation above average share of the population
over $65$. Using forecasts of hot days (RCP) and demographics in $2050$, we
observe that districts will have elderly populations and hot days $2-5$
standard deviations above the current mean in just $25$ years. This predicts a
drastic increase in heat-related mortality. At the same time, district green
scores, measured using $10\times 10$ meter resolution satellite images of
residential areas, significantly moderate the relationship between heat and
mortality. Thus, although local policies likely cannot reverse warming or
demographic trends, they can take measures to mediate the health consequences
of these growing risks, which are highly heterogeneous across regions, even in
Austria.",http://arxiv.org/abs/2405.00540v1
Discrete element method model of soot aggregates,2024-07-16T03:31:47Z,"Egor V. Demidov, Gennady Y. Gor, Alexei F. Khalizov","Soot is a component of atmospheric aerosols that affects climate by
scattering and absorbing the sunlight. Soot particles are fractal aggregates
composed of elemental carbon. In the atmosphere, the aggregates acquire
coatings by condensation and coagulation, resulting in significant compaction
of the aggregates that changes the direct climate forcing of soot. Currently,
no models exist to rigorously describe the process of soot restructuring,
reducing prediction accuracy of atmospheric aerosol models. We develop a
discrete element method contact model to simulate restructuring of fractal soot
aggregates, represented as assemblies of spheres joined by cohesion and by
sintered necks. The model is parametrized based on atomic force spectroscopy
data and is used to simulate soot restructuring, showing that the fraction of
necks in aggregates determines the restructuring pathway. Aggregates with fewer
necks undergo local compaction, while aggregates with nearly-full necking
prefer global compaction. Additionally, full compaction occurs within tens of
nanoseconds, orders of magnitude faster than the time scale of soot aging
through condensation. An important implication is that in atmospheric soot
aggregates, the rate of condensation determines how many necks are fractured
simultaneously, affecting the restructuring pathway, e.g., producing highly
compact, thinly-coated soot as observed in recent studies.",http://arxiv.org/abs/2407.14254v3
Thermal and microclimatic behavior of OASIS schoolyard paving materials,2024-07-31T12:13:29Z,"Ghid Karam, Ma√Ølys Chanial, Arnaud Grados, Martin Hendel, Laurent Royon","As part of its Resilience Strategy, the City of Paris' OASIS program aims to
contribute ot its adaptation to heatwaves and climate change by transforming
schoolyards into climate shelters, namely via desealing and greening. In this
context, a variety of alternative pavement materials have been proposed to
replace the initial schoolyard pavement, composed of an asphalt sidewalk
structure. In the context of the EU-funded ERDF UIA OASIS project, the thermal
performance of these alternative materials and their impact in terms of urban
cooling was explored. To this aim, five samples of reference and innovative
schoolyard pavements were studied in the lab under heat-wave conditions.
Alternative green, biosourced, recycled and reflective pavement solutions were
compared to standard fine-aggregate asphalt concrete. Their performance was
evaluated with regards to their contribution to the urban heat island
phenomenon and to pedestrian heat stress, account for the typical use schedule
of schoolyards. Green and biosourced materials were found to perform well for
both indicators, while the standard and recycled solutions had poor UHI
performance but had limited negative effects on daytime heat stress. The
reflective pavement had better UHI performance but had high radiosity during
daytime which can negatively affect pedestrian heat stress.",http://arxiv.org/abs/2408.08317v1
Modeling Snow on Sea Ice using Physics Guided Machine Learning,2024-09-12T14:45:07Z,"Ayush Prasad, Ioanna Merkouriadi, Aleksi Nummelin","Snow is a crucial element of the sea ice system, affecting sea ice growth and
decay due to its low thermal conductivity and high albedo. Despite its
importance, present-day climate models have an idealized representation of
snow, often including only single-layer thermodynamics and omitting several
processes that shape its properties. Although advanced snow process models like
SnowModel exist, they are often excluded from climate modeling due to their
high computational costs. SnowModel simulates snow depth, density, blowing-snow
redistribution, sublimation, grain size, and thermal conductivity in a
multi-layer snowpack. It operates with high spatial (1 meter) and temporal (1
hour) resolution. However, for large regions like the Arctic Ocean, these
high-resolution simulations face challenges such as slow processing and large
resource requirements. Data-driven emulators are used to address these issues,
but they often lack generalizability and consistency with physical laws. In our
study, we address these challenges by developing a physics-guided emulator that
incorporates physical laws governing changes in snow density due to compaction.
We evaluated three machine learning models: Long Short-Term Memory (LSTM),
Physics-Guided LSTM, and Random Forest across five Arctic regions. All models
achieved high accuracy, with the Physics-Guided LSTM showing the best
performance in accuracy and generalizability. Our approach offers a faster way
to emulate SnowModel with a speedup of over 9000 times, maintaining high
fidelity.",http://arxiv.org/abs/2409.08092v1
"Seasonal Performance Evaluation of a Hybrid PV-Wind-Battery Power System
  for a Mars Base",2024-09-30T08:32:48Z,"Abdollah Masoud Darya, Ramesh C. Bansal, Omaima Anwar Jarndal","This work investigates a hybrid photovoltaic-wind-battery power system
designed to sustain a Mars base under varying seasonal and climatic conditions.
The Mars Climate Database was utilized to simulate the effects of seasonal
changes, diurnal cycles, and dust storms on the system's power generation. The
seasonal performance was analyzed across the Martian surface and at potential
habitation sites proposed in the ""First Landing Site/Exploration Zone Workshop
for Human Missions to the Surface of Mars (FLSW).'' Within the hybrid system,
the photovoltaic arrays serve as the primary energy source, with wind turbines
providing essential backup during nighttime and dust storms. A single
$1\,000\,\mathrm{m}^2$ photovoltaic array, a $33.4\,\mathrm{m}$ diameter wind
turbine, and a $312\,\mathrm{kWh}$ battery can support a six-person Mars base
at $32.1\%$ of the Martian surface during the equinoxes and solstices,
expanding to $51.7\%$ with three sets of arrays and turbines. Additionally,
$24$ FLSW sites can be supported throughout the solstices and equinoxes by a
single photovoltaic array, turbine, and battery, even during global dust
storms. Among the $24$ sites, Hebrus Valles, Huygens Crater, and Noctis
Labyrinthus had the highest energy production potential. These findings are
expected to guide further research on hybrid renewable power systems for Mars
exploration.",http://arxiv.org/abs/2410.00066v2
"Interpolation-Free Deep Learning for Meteorological Downscaling on
  Unaligned Grids Across Multiple Domains with Application to Wind Power",2024-10-04T22:04:40Z,"Jean-S√©bastien Giroux, Simon-Philippe Breton, Julie Carreau","As climate change intensifies, the shift to cleaner energy sources becomes
increasingly urgent. With wind energy production set to accelerate, reliable
wind probabilistic forecasts are essential to ensure its efficient use.
However, since numerical weather prediction models are computationally
expensive, probabilistic forecasts are produced at resolutions too coarse to
capture all mesoscale wind behaviors. Statistical downscaling, typically
applied to enchance the resolution of climate model simulations, presents a
viable solution with lower computational costs by learning a mapping from
low-resolution (LR) variables to high-resolution (HR) meteorological variables.
Leveraging deep learning, we evaluate a downscaling model based on a
state-of-the-art U-Net architecture, applied to an ensemble member from a
coarse-scale probabilistic forecast of wind velocity. The architecture is
modified to incorporate (1) a learned grid alignment strategy to resolve LR-HR
grid mismatches and (2) a processing module for multi-level atmospheric
predictors. To extend the downscaling model's applicability from fixed spatial
domains to the entire Canadian region, we assess a transfer learning approach.
Our results show that the learned grid alignment strategy performs as well as
conventional pre-processing interpolation steps and that LR wind speed at
multiple levels is sufficient as a predictor, enabling a more compact
architecture. Additionally, they suggest that extending to new spatial domains
using transfer learning is promising, and that downscaled wind velocities
demonstrate potential in improving the detection of wind power ramps, a
critical phenomenon for wind energy.",http://arxiv.org/abs/2410.03945v1
"A Comprehensive Review: Impacts of Extreme Temperatures due to Climate
  Change on Power Grid Infrastructure and Operation",2024-10-11T00:08:48Z,"Kishan Prudhvi Guddanti, Alok Kumar Bharati, Sameer Nekkalapu, Joseph McWheter, Scott Morris","The power grid is experiencing a multi-fold transformation while the global
climate evolves with record-breaking extreme temperatures during heat domes,
polar vortices, and severe ice. Over the decades, these extreme temperature
events have increased in frequency, duration, and intensity. The power grid
infrastructure is geographically spread over thousands of square miles with
millions of small and large components, and the impact of extreme temperature
operations on the grid infrastructure needs to be researched further. This
paper reviews academic literature, standards, industry articles, and federal
reports to identify the impacts of heat domes, polar vortices, and icing on all
the T&D grid equipment, including substations (assets owned and operated by the
utilities and independent system operators). This paper classifies the
equipment into primary and auxiliary equipment and determines its vulnerability
to extreme temperatures for a deeper analysis of a more critical and vulnerable
set of grid equipment. For each equipment under consideration, its fundamental
role in the system, the impact of extreme temperatures on its operation,
available monitoring, and mitigation of these impacts are discussed. The paper
develops insights on standards readiness and identifies gaps concerning extreme
temperature definitions. The paper also develops summary tables to identify the
critical failure modes for each type of equipment, failure influence diagrams,
and cascading influence diagrams to highlight and aid in translating the
equipment vulnerability information into power grid contingency definitions
that need to be considered in grid planning and operations.",http://arxiv.org/abs/2410.08425v2
"The Second Law of Thermodynamics, Life and Earth's Planetary Machinery
  Revisited",2024-10-14T07:12:06Z,Axel Kleidon,"Life is a planetary feature that depends on its environment, but it has also
strongly shaped the physical conditions on Earth, having created conditions
highly suitable for a productive biosphere. Clearly, the second law of
thermodynamics must apply to these dynamics as well, but how? What insights can
we gain by placing life and its effects on planetary functioning in the context
of the second law? In Kleidon (2010), I described a thermodynamic Earth system
perspective by placing the functioning of the Earth system in terms of the
second law. The Earth system is represented by a planetary hierarchy of energy
transformations that are driven predominantly by incoming solar radiation,
these transformations are constrained by the second law, but they are also
modified by the feedbacks from various dissipative activities. It was then
hypothesised that life evolves its dissipative activity to the limits imposed
by this hierarchy and evolves feedbacks aimed at pushing these limits to higher
levels of dissipative activity. Here I provide an update of this perspective. I
first review applications to climate and global climate change to demonstrate
its success in predicting magnitudes of physical processes, particularly
regarding temperatures, heat redistribution and hydrological cycling. I then
focus on the limits to dissipative activity of the biosphere. It would seem
that the limitations by thermodynamics act indirectly by imposing limitations
associated with transport and material exchange. I substantiate this
interpretation and discuss the broader implications for habitability, the
emergence and evolution of life, and the contemporary biosphere.",http://arxiv.org/abs/2410.10213v1
"The unrealized potential of agroforestry for an emissions-intensive
  agricultural commodity",2024-10-28T10:02:32Z,"Alexander Becker, Jan D. Wegner, Evans Dawoe, Konrad Schindler, William J. Thompson, Christian Bunn, Rachael D. Garrett, Fabio Castro, Simon P. Hart, Wilma J. Blaser-Hart","Reconciling agricultural production with climate-change mitigation and
adaptation is one of the most formidable problems in sustainability. One
proposed strategy for addressing this problem is the judicious retention of
trees in agricultural systems. However, the magnitude of the current and
future-potential benefit that trees contribute remains uncertain, particularly
in the agricultural sector where trees can also limit production. Here we help
to resolve these issues across a West African region responsible for producing
$\approx$60% of the world's cocoa, a crop that contributes one of the highest
per unit carbon footprints of all foods. We use machine learning to generate
spatially-explicit estimates of shade-tree cover and carbon stocks across the
region. We find that existing shade-tree cover is low, and not spatially
aligned with climate threat. But we also find enormous unrealized potential for
the sector to counterbalance a large proportion of their high carbon footprint
annually, without threatening production. Our methods can be applied to other
globally significant commodities that can be grown in agroforests, and align
with accounting requirements of carbon markets, and emerging legislative
requirements for sustainability reporting.",http://arxiv.org/abs/2410.20882v1
"Simulation and Data Assimilation in an Idealized Coupled
  Atmosphere-Ocean-Sea Ice Floe Model with Cloud Effects",2024-10-30T15:53:51Z,"Changhong Mou, Samuel N. Stechmann, Nan Chen","Sea ice plays a crucial role in the climate system, particularly in the
Marginal Ice Zone (MIZ), a transitional area consisting of fragmented ice
between the open ocean and consolidated pack ice. As the MIZ expands,
understanding its dynamics becomes essential for predicting climate change
impacts. However, the role of clouds in these processes has been largely
overlooked. This paper addresses that gap by developing an idealized coupled
atmosphere-ocean-ice model incorporating cloud and precipitation effects,
tackling both forward (simulation) and inverse (data assimilation) problems.
Sea ice dynamics are modeled using the discrete element method, which simulates
floes driven by atmospheric and oceanic forces. The ocean is represented by a
two-layer quasi-geostrophic (QG) model, capturing mesoscale eddies and
ice-ocean drag. The atmosphere is modeled using a two-layer saturated
precipitating QG system, accounting for variable evaporation over sea surfaces
and ice. Cloud cover affects radiation, influencing ice melting. The idealized
coupled modeling framework allows us to study the interactions between
atmosphere, ocean, and sea ice floes. Specifically, it focuses on how clouds
and precipitation affect energy balance, melting, and freezing processes. It
also serves as a testbed for data assimilation, which allows the recovery of
unobserved floe trajectories and ocean fields in cloud-induced uncertainties.
Numerical results show that appropriate reduced-order models help improve data
assimilation efficiency with partial observations, allowing the skillful
inference of missing floe trajectories and lower atmospheric winds. These
results imply the potential of integrating idealized models with data
assimilation to improve our understanding of Arctic dynamics and predictions.",http://arxiv.org/abs/2410.23138v1
"Differentiable Land Model Reveals Global Environmental Controls on
  Ecological Parameters",2024-11-14T18:21:10Z,"Jianing Fang, Kevin Bowman, Wenli Zhao, Xu Lian, Pierre Gentine","Accurate modeling of terrestrial carbon and water exchange requires robust
ecological parameters that capture vegetation responses and adaptations to the
local environment. The current generation of land models use Plant Functional
Types (PFTs) to discretize vegetation functional diversity, but these coarse
categorizations often overlook fine-scale variations shaped by local climate,
soil, and forest age factors. The lack of governing equations for plant
adaptation demands a paradigm shift in how we integrate diverse Earth
observations to uncover ecological functional dependence on changing
environments. To address this challenge, we developed DifferLand, a
differentiable, hybrid physics and machine learning model that infers the
spatial distributions of ecological parameters and their relationships with
environmental factors constrained by satellite and in-situ observations. Our
model unifies top-down and bottom-up observational constraints with
process-based knowledge to generate a global analysis of ecological functions
and their adaptation to environmental gradients. We found PFTs account for less
than half of the explainable spatial parameter variations controlling carbon
fluxes and vegetation states. The remaining parameter variability is largely
driven by local climate and forest demography factors, and the learned
environment-parameter relationships lead to enhanced spatial generalization at
unseen locations. DifferLand identified growing season length, leaf economics,
and agricultural intensity as the three orthogonal spatial gradients underlying
parameter variations. Our novel framework can lead to new insights on global
carbon cycling by learning directly from data and expanding our understanding
of local responses of ecosystems to environmental drivers.",http://arxiv.org/abs/2411.09654v1
"Advancing Large Language Models for Spatiotemporal and Semantic
  Association Mining of Similar Environmental Events",2024-11-19T21:57:22Z,"Yuanyuan Tian, Wenwen Li, Lei Hu, Xiao Chen, Michael Brook, Michael Brubaker, Fan Zhang, Anna K. Liljedahl","Retrieval and recommendation are two essential tasks in modern search tools.
This paper introduces a novel retrieval-reranking framework leveraging Large
Language Models (LLMs) to enhance the spatiotemporal and semantic associated
mining and recommendation of relevant unusual climate and environmental events
described in news articles and web posts. This framework uses advanced natural
language processing techniques to address the limitations of traditional manual
curation methods in terms of high labor cost and lack of scalability.
Specifically, we explore an optimized solution to employ cutting-edge embedding
models for semantically analyzing spatiotemporal events (news) and propose a
Geo-Time Re-ranking (GT-R) strategy that integrates multi-faceted criteria
including spatial proximity, temporal association, semantic similarity, and
category-instructed similarity to rank and identify similar spatiotemporal
events. We apply the proposed framework to a dataset of four thousand Local
Environmental Observer (LEO) Network events, achieving top performance in
recommending similar events among multiple cutting-edge dense retrieval models.
The search and recommendation pipeline can be applied to a wide range of
similar data search tasks dealing with geospatial and temporal data. We hope
that by linking relevant events, we can better aid the general public to gain
an enhanced understanding of climate change and its impact on different
communities.",http://arxiv.org/abs/2411.12880v1
"Long-term predictive models for mosquito borne diseases: a narrative
  review",2024-11-20T19:54:31Z,"Marcio Maciel Bastos, Luiz Max Carvalho, Eduardo Correa Araujo, Fl√°vio Code√ßo Coelho","In face of climate change and increasing urbanization, the predictive
mosquito-borne diseases (MBD) transmission models require constant updates.
Thus, is urgent to comprehend the driving forces of this non stationary
behavior, observed through spatial and incidence expansion. We observed that
temperature is a critical driver in predictive models for MBD transmission,
also being consistently used in multiple reviewed papers with considerable
incidence predictive capacity. Rainfall, however, have more subtle importance
as moderate precipitation creates breeding sites for mosquitoes, but excessive
rainfall can reduce larvae populations. We highlight the frequent use of
mechanistic models, particularly those that integrate temperature-dependent
biological parameters of disease transmission in incidence proxies as the
Vectorial Capacity (VC) and temperature-based basic reproduction number
$R_0(t)$, for example. These models show the importance of climate variables,
but the socio-demographic factors are often not considered. This gap is a
significant opportunity for future research to incorporate socio-demographic
data into long-term predictive models for more comprehensive and reliable
forecasts. With this survey, we outline the most promising paths to be followed
by long-term MBD transmission research and highlighting the potential facing
challenges. Thus, we offer a valuable foundation for enhancing disease
forecasting models and supporting more effective public health interventions,
specially in the long term.",http://arxiv.org/abs/2411.13680v1
DRUM: Diffusion-based runoff model for probabilistic flood forecasting,2024-12-16T16:25:29Z,"Zhigang Ou, Congyi Nai, Baoxiang Pan, Ming Pan, Chaopeng Shen, Peishi Jiang, Xingcai Liu, Qiuhong Tang, Wenqing Li, Yi Zheng","Reliable flood forecasting remains a critical challenge due to persistent
underestimation of peak flows and inadequate uncertainty quantification in
current approaches. We present DRUM (Diffusion-based Runoff Model), a
generative AI solution for probabilistic runoff prediction. DRUM builds up an
iterative refinement process that generates ensemble runoff estimates from
noise, guided by past meteorological conditions, present meteorological
forecasts, and static catchment attributes. This framework allows learning
complex hydrological behaviors without imposing explicit distributional
assumptions, particularly benefiting extreme event prediction and uncertainty
quantification. Using data from 531 representative basins across the contiguous
United States, DRUM outperforms state-of-the-art deep learning methods in
runoff forecasting regarding both deterministic and probabilistic skills, with
particular advantages in extreme flow (0.1%) predictions. DRUM demonstrates
superior flood early warning skill across all magnitudes and lead times (1-7
days), achieving F1 scores near 0.4 for extreme events under perfect forecasts
and maintaining robust performance with operational forecasts, especially for
longer lead times and high-magnitude floods. When applied to climate
projections through the 21st century, DRUM reveals increasing flood
vulnerability in 47.8-57.1% of basins across emission scenarios, with
particularly elevated risks along the West Coast and Southeast regions. These
advances demonstrate significant potential for improving both operational flood
forecasting and long-term risk assessment in a changing climate.",http://arxiv.org/abs/2412.11942v1
Downscaling Precipitation with Bias-informed Conditional Diffusion Model,2024-12-19T05:36:52Z,"Ran Lyu, Linhan Wang, Yanshen Sun, Hedanqiu Bai, Chang-Tien Lu","Climate change is intensifying rainfall extremes, making high-resolution
precipitation projections crucial for society to better prepare for impacts
such as flooding. However, current Global Climate Models (GCMs) operate at
spatial resolutions too coarse for localized analyses. To address this
limitation, deep learning-based statistical downscaling methods offer promising
solutions, providing high-resolution precipitation projections with a moderate
computational cost. In this work, we introduce a bias-informed conditional
diffusion model for statistical downscaling of precipitation. Specifically, our
model leverages a conditional diffusion approach to learn distribution priors
from large-scale, high-resolution precipitation datasets. The long-tail
distribution of precipitation poses a unique challenge for training diffusion
models; to address this, we apply gamma correction during preprocessing.
Additionally, to correct biases in the downscaled results, we employ a
guided-sampling strategy to enhance bias correction. Our experiments
demonstrate that the proposed model achieves highly accurate results in an 8
times downscaling setting, outperforming previous deterministic methods. The
code and dataset are available at
https://github.com/RoseLV/research_super-resolution",http://arxiv.org/abs/2412.14539v1
"MiTREE: Multi-input Transformer Ecoregion Encoder for Species
  Distribution Modelling",2024-12-25T22:20:47Z,"Theresa Chen, Yao-Yi Chiang","Climate change poses an extreme threat to biodiversity, making it imperative
to efficiently model the geographical range of different species. The
availability of large-scale remote sensing images and environmental data has
facilitated the use of machine learning in Species Distribution Models (SDMs),
which aim to predict the presence of a species at any given location.
Traditional SDMs, reliant on expert observation, are labor-intensive, but
advancements in remote sensing and citizen science data have facilitated
machine learning approaches to SDM development. However, these models often
struggle with leveraging spatial relationships between different inputs -- for
instance, learning how climate data should inform the data present in satellite
imagery -- without upsampling or distorting the original inputs. Additionally,
location information and ecological characteristics at a location play a
crucial role in predicting species distribution models, but these aspects have
not yet been incorporated into state-of-the-art approaches. In this work, we
introduce MiTREE: a multi-input Vision-Transformer-based model with an
ecoregion encoder. MiTREE computes spatial cross-modal relationships without
upsampling as well as integrates location and ecological context. We evaluate
our model on the SatBird Summer and Winter datasets, the goal of which is to
predict bird species encounter rates, and we find that our approach improves
upon state-of-the-art baselines.",http://arxiv.org/abs/2412.18995v1
Projected changes in the Iberian Peninsula drought characteristics,2024-01-13T16:03:19Z,"M. Garc√≠a-Valdecasas Ojeda, S. R. G√°miz-Fortis, J. J. Rosa-C√°novas, E. Romero-Jim√©nez, P. Yeste, Y. Castro-D√≠ez, M. J. Esteban-Parra","High spatial resolution drought projections for the Iberian Peninsula (IP)
have been examined in terms of duration, frequency, and severity of drought
events. For this end, a set of regional climate simulations was completed using
the Weather Research and Forecasting (WRF) model driven by two global climate
models (GCMs), the CCSM4 and the MPI-ESM-LR, for a near (2021-2050) and a far
(2071-2100) future, and under two representative concentration pathway (RCP)
scenarios (RCP4.5 and RCP8.5). Projected changes for these simulations were
analyzed using two drought indices, the Standardized Precipitation
Evapotranspiration Index (SPEI) and the Standardized Precipitation Index (SPI),
considering different timescales (3- and 12-months). The results showed that
the IP is very likely to undergo longer and more severe drought events.
Substantial changes in drought parameters (i.e., frequency, duration, and
severity) were projected by both indices and at both time scales in most of the
IP. These changes are particularly strong by the end of the century under
RCP8.5. Meanwhile, the intensification of drought conditions is expected to be
more moderate for the near future. However, the results also indicated key
differences between indices. Projected drought conditions by using the SPEI
showed more severe increases in drought events than those from SPI by the end
of the century and, especially, for the high-emission scenario. The most
extreme conditions were projected in terms of the duration of the events.
Specifically, results from the 12-month SPEI analysis suggested a significant
risk of megadrought events (drought events longer than 15 years) in many areas
of IP by the end of the century under RCP8.5.",http://arxiv.org/abs/2401.07104v1
"A fluctuation-dissipation theorem perspective on radiative responses to
  temperature perturbations",2024-08-22T17:52:08Z,"Fabrizio Falasca, Aurora Basinski-Ferris, Laure Zanna, Ming Zhao","Radiative forcing drives warming in the Earth system, leading to changes in
sea surface temperatures (SSTs) and associated radiative feedbacks. The link
between changes in the top-of-the-atmosphere (TOA) net radiative flux and SST
patterns, known as the ""pattern effect"", is typically diagnosed by studying the
response of atmosphere-only models to SST perturbations. In this work, we
diagnose the pattern effect through response theory, by performing idealized
warming perturbation experiments from unperturbed data alone. First, by
studying the response at short time scales, where the response is dominated by
atmospheric variability, we recover results that agree with the literature.
Second, by extending the framework to longer time scales, we capture coupled
interactions between the slow ocean component and the atmosphere, yielding a
novel ""sensitivity map"" quantifying the response of the net radiative flux to
SST perturbations in the coupled system. Here, feedbacks are captured by a
spatiotemporal response operator, rather than time-independent maps as in
traditional studies. Both formulations skillfully reconstruct changes in
externally forced simulations and provide practical strategies for climate
studies. The key distinction lies in their perspectives on climate feedbacks.
The first formulation, closely aligned with prediction tasks, follows the
traditional view in which slow variables, such as SSTs, exert a one-way
influence on fast variables. The second formulation broadens this perspective
by incorporating spatiotemporal interactions across state variables. This
alternative approach explores how localized SST perturbations can alter the
coupled dynamics, leading to temperature changes in remote areas and further
impacting the radiative fluxes at later times.",http://arxiv.org/abs/2408.12585v4
The generic temperature response of large biochemical networks,2024-03-25T21:27:38Z,"Julian B. Voits, Ulrich S. Schwarz","Biological systems are remarkably susceptible to relatively small temperature
changes. The most obvious example is fever, when a modest rise in body
temperature of only few Kelvin has strong effects on our immune system and how
it fights pathogens. Another very important example is climate change, when
even smaller temperature changes lead to dramatic shifts in ecosystems.
Although it is generally accepted that the main effect of an increase in
temperature is the acceleration of biochemical reactions according to the
Arrhenius equation, it is not clear how it effects large biochemical networks
with complicated architectures. For developmental systems like fly and frog, it
has been shown that the system response to temperature deviates in a
characteristic manner from the linear Arrhenius plot of single reactions, but a
rigorous explanation has not been given yet. Here we use a graph theoretical
interpretation of the mean first passage times of a biochemical master equation
to give a statistical description. We find that in the limit of large system
size and if the network has a bias towards a target state, then the Arrhenius
plot is generically quadratic, in excellent agreement with experimental data
for developmental times in fly and frog.",http://arxiv.org/abs/2403.17202v1
Oligopoly Game Stabilisation Through Multilayer Congestion Dynamics,2024-06-27T11:07:12Z,"Toby Willis, Giuliano Punzo","International trade and logistics are subject to factors including
geopolitical instability, climate change, and black swan events such as the
unforeseen closure of the Suez Canal. The problem of predicting local price
change under modification of an underlying transport network or change in
supply characteristics unites elements of game theory, network theory and
transport. The Cournot Oligopoly models economic actors as rational players
attempting to maximise profit by optimising supply quantities with analytical
results now consolidated about equilibrium characteristics where transport
conditions are fixed. Similarly, where supply and demand are fixed, the routing
of goods in a transport network can be analytically solved through a traffic
assignment problem. Hence we can solve the coupled Cournot-congestion problem
by means of a 2-layer network. Where the layers are linked, inter-layer
feedback wherein players attempt to maximise their utility occurs. In this
respect we find players benefit from taking advantage of non-simultaneous
responses to the market rather than moving to a new equilibrium. We draw
conclusions about the nature of equilibria, finding that the concave utility
curve property results in unique and stable equilibrium for each uncoupled
layer, while linked layers have a non-unique stable equilibria for which
general solutions are stated.",http://arxiv.org/abs/2406.19079v1
"The temperature affects the impact levels of synthetic insecticides on a
  parasitoid wasp used in the biological control of pentatomid pests in soybean
  crops",2024-03-08T17:43:13Z,"Matheus Rakes, Ma√≠ra Chagas Morais, Leandro do Prado Ribeiro, Gabriel Rodrigues Palma, Rafael de Andrade Moral, Daniel Bernardi, Anderson Dionei Gr√ºtzmacher","The impact of climate change has led to growing global concern about the
interaction of temperature and xenobiotics in agricultural toxicological
studies. Thus, for the first time, we evaluated the lethal, sublethal and
transgerational effects of six insecticides used in the management of stink bug
complex in soybean crops on the different life stages of the parasitoid
Telenomus podisi (Hymenoptera: Scelionidae) in three temperature levels (15, 25
and 30 {\deg}C). Telenomus podisi adults (F0 generation), when exposed to
insecticides based on acephate, spinosad and thiamethoxam + lambda-cyhalothrin,
showed accumulated mortality of 100% at all temperature levels tested. On the
other hand, methoxyfenozide + spinetoram caused average mortalities of 88.75%
at 15 {\deg}C and 38.75% at 25 and 30 {\deg}C. In contrast, the mortality rates
caused by chlorfenapyr at 15, 25 and 30 {\deg}C were 1.25, 71.25 and 71.25%. On
the other hand, surviving adults in lethal toxicity bioassay did not show
differences in egg parasitism (F0 generation) and emergence of F1 generation in
all temperature levels studied; however, the insecticide methoxyfenozide +
spinetoram showed the lowest level of parasitism and emergence of T. podisi. In
addition, our results demonstrated significant changes in the proportion of
emerged males and females as the temperature increased; however, we did not
find any differences when comparing the insecticides studied. Furthermore, we
detected a significant interaction between insecticides and temperatures by
contaminating the host's parasitized eggs (parasitoid pupal stage). Generally,
the highest emergence reduction values were found at the highest temperature
studied (30 {\deg}C). Our results highlighted the temperature-dependent impact
of synthetic insecticides on parasitoids, which should be considered in
toxicological risk assessments and under predicted climate change scenarios.",http://arxiv.org/abs/2403.05479v1
"The process of polarisation as a loss of dimensionality: measuring
  changes in polarisation using Singular Value Decomposition of network graphs",2024-03-27T01:55:57Z,"Sage Anastasi, Giulio Dalla Riva","In this paper we present new methods that extend Baldassari and Gelman's
theory of polarisation. They show that it is useful to define polarisation as
increasing correlation between positions in an ideological field, which reduces
political pluralism. We also draw from post-structuralist work which argues
that deliberate development of these correlations is a feature of polarised
regimes such as apartheid.
  To measure polarisation in social networks, we use Random Dot Product Graphs
to embed social networks in metric spaces. Singular Value Decomposition of a
social network provides an embedded dimensionality which corresponds to the
number of uncorrelated dimensions in the network. Each uncorrelated dimension
in a social network represents a part of that society which allows two people
from different groups to form a social connection, such as living in a racially
integrated neighbourhood. A decrease in the optimal dimensionality for the
embedding of the network graph means that the dimensions in the network are
becoming more correlated, and therefore the network is becoming more polarised.
  We apply this method to the communication interactions among New Zealand
Twitter users discussing climate change issues from 2017 to 2023. We find that
the discussion is more polarised after 2020 than before, as shown by a decrease
in the dimensionality of the communication network. Second, we apply this
method to discussions of the COP climate change conferences, showing that our
methods agree with other researchers' detections of polarisation in this space.
Finally, we use networks generated by stochastic block models to explore how an
increase of the isolation between distinct communities, or the increase of the
predominance of one community over the other, in the social networks decrease
the embedded dimensionality and are therefore identifiable as polarisation
processes.",http://arxiv.org/abs/2403.18191v2
"Domain Adaptation for Sustainable Soil Management using Causal and
  Contrastive Constraint Minimization",2024-01-13T23:51:42Z,"Somya Sharma, Swati Sharma, Rafael Padilha, Emre Kiciman, Ranveer Chandra","Monitoring organic matter is pivotal for maintaining soil health and can help
inform sustainable soil management practices. While sensor-based soil
information offers higher-fidelity and reliable insights into organic matter
changes, sampling and measuring sensor data is cost-prohibitive. We propose a
multi-modal, scalable framework that can estimate organic matter from remote
sensing data, a more readily available data source while leveraging sparse soil
information for improving generalization. Using the sensor data, we preserve
underlying causal relations among sensor attributes and organic matter.
Simultaneously we leverage inherent structure in the data and train the model
to discriminate among domains using contrastive learning. This causal and
contrastive constraint minimization ensures improved generalization and
adaptation to other domains. We also shed light on the interpretability of the
framework by identifying attributes that are important for improving
generalization. Identifying these key soil attributes that affect organic
matter will aid in efforts to standardize data collection efforts.",http://arxiv.org/abs/2401.07175v1
"Diffusive coupling facilitates and impedes noise-induced escape in
  interacting bistable elements",2024-01-19T04:29:43Z,"Hidemasa Ishii, Hiroshi Kori","Diverse complex systems often undergo sudden changes in their states, such as
epileptic seizures, climate changes, and social uprisings. Such behavior has
been modeled by noise-induced escape of bistable elements, which is the escape
from an attracting state driven by a fluctuation in the system's state. We
consider a system of interacting bistable elements and investigate the effect
of diffusive coupling among elements on the process of noise-induced escape. We
focus on the influence of the coupling strength over the escape time, which is
the time it takes for noise-induced escape to occur. We performed numerical
simulations and observed that weak coupling reduced the mean escape time,
whereas strong coupling impeded escape. We argue that, although diffusive
coupling both facilitates and impedes escape, the facilitating effect is
dominant when coupling is weak. For weak coupling cases, we develop an
approximate theory that can predict the mean and variance of escape times. In
contrast, strong coupling reduces the effective noise intensity to impede
escape. Our results suggest that diffusive coupling among multistable elements
contributes to regulating the rate of transitions among attracting states.",http://arxiv.org/abs/2401.10489v1
Learning Style Identification Using Semi-Supervised Self-Taught Labeling,2024-02-04T11:56:49Z,"Hani Y. Ayyoub, Omar S. Al-Kadi","Education is a dynamic field that must be adaptable to sudden changes and
disruptions caused by events like pandemics, war, and natural disasters related
to climate change. When these events occur, traditional classrooms with
traditional or blended delivery can shift to fully online learning, which
requires an efficient learning environment that meets students' needs. While
learning management systems support teachers' productivity and creativity, they
typically provide the same content to all learners in a course, ignoring their
unique learning styles. To address this issue, we propose a semi-supervised
machine learning approach that detects students' learning styles using a data
mining technique. We use the commonly used Felder Silverman learning style
model and demonstrate that our semi-supervised method can produce reliable
classification models with few labeled data. We evaluate our approach on two
different courses and achieve an accuracy of 88.83% and 77.35%, respectively.
Our work shows that educational data mining and semi-supervised machine
learning techniques can identify different learning styles and create a
personalized learning environment.",http://arxiv.org/abs/2402.14597v1
Towards an Edge Intelligence-Based Traffic Monitoring System,2024-02-05T17:08:18Z,"Vincenzo Barbuto, Claudio Savaglio, Roberto Minerva, Noel Crespi, Giancarlo Fortino","Cities have undergone significant changes due to the rapid increase in urban
population, heightened demand for resources, and growing concerns over climate
change. To address these challenges, digital transformation has become a
necessity. Recent advancements in Artificial Intelligence (AI) and sensing
techniques, such as synthetic sensing, can elevate Digital Twins (DTs) from
digital copies of physical objects to effective and efficient platforms for
data collection and in-situ processing. In such a scenario, this paper presents
a compre-hensive approach for developing a Traffic Monitoring System (TMS)
based on Edge Intelligence (EI), specifically designed for smart cities. Our
approach prioritizes the placement of intelligence as close as possible to data
sources, and leverages an ""opportunistic"" interpretation of DT (ODT), resulting
in a novel and interdisciplinary strategy to re-engineering large-scale
distributed smart systems. The preliminary results of the proposed system have
shown that moving computation to the edge of the network provides several
benefits, including (i) enhanced inference performance, (ii) reduced bandwidth
and power consumption, (iii) and decreased latencies with respect to the
classic cloud -centric approach.",http://arxiv.org/abs/2403.12976v1
Faraday: Synthetic Smart Meter Generator for the smart grid,2024-04-05T13:18:10Z,"Sheng Chai, Gus Chadney","Access to smart meter data is essential to rapid and successful transitions
to electrified grids, underpinned by flexibility delivered by low carbon
technologies, such as electric vehicles (EV) and heat pumps, and powered by
renewable energy. Yet little of this data is available for research and
modelling purposes due consumer privacy protections. Whilst many are calling
for raw datasets to be unlocked through regulatory changes, we believe this
approach will take too long. Synthetic data addresses these challenges directly
by overcoming privacy issues. In this paper, we present Faraday, a Variational
Auto-encoder (VAE)-based model trained over 300 million smart meter data
readings from an energy supplier in the UK, with information such as property
type and low carbon technologies (LCTs) ownership. The model produces
household-level synthetic load profiles conditioned on these labels, and we
compare its outputs against actual substation readings to show how the model
can be used for real-world applications by grid modellers interested in
modelling energy grids of the future.",http://arxiv.org/abs/2404.04314v1
"Watching Grass Grow: Long-term Visual Navigation and Mission Planning
  for Autonomous Biodiversity Monitoring",2024-04-16T10:31:24Z,"Matthew Gadd, Daniele De Martini, Luke Pitt, Wayne Tubby, Matthew Towlson, Chris Prahacs, Oliver Bartlett, John Jackson, Man Qi, Paul Newman, Andrew Hector, Roberto Salguero-G√≥mez, Nick Hawes","We describe a challenging robotics deployment in a complex ecosystem to
monitor a rich plant community. The study site is dominated by dynamic
grassland vegetation and is thus visually ambiguous and liable to drastic
appearance change over the course of a day and especially through the growing
season. This dynamism and complexity in appearance seriously impact the
stability of the robotics platform, as localisation is a foundational part of
that control loop, and so routes must be carefully taught and retaught until
autonomy is robust and repeatable. Our system is demonstrated over a 6-week
period monitoring the response of grass species to experimental climate change
manipulations. We also discuss the applicability of our pipeline to monitor
biodiversity in other complex natural settings.",http://arxiv.org/abs/2404.10446v2
"Integrating behavioral experimental findings into dynamical models to
  inform social change interventions",2024-05-21T22:17:17Z,"Radu Tanase, Ren√© Algesheimer, Manuel S. Mariani","Addressing global challenges -- from public health to climate change -- often
involves stimulating the large-scale adoption of new products or behaviors.
Research traditions that focus on individual decision making suggest that
achieving this objective requires better identifying the drivers of individual
adoption choices. On the other hand, computational approaches rooted in
complexity science focus on maximizing the propagation of a given product or
behavior throughout social networks of interconnected adopters. The integration
of these two perspectives -- although advocated by several research communities
-- has remained elusive so far. Here we show how achieving this integration
could inform seeding policies to facilitate the large-scale adoption of a given
behavior or product. Drawing on complex contagion and discrete choice theories,
we propose a method to estimate individual-level thresholds to adoption, and
validate its predictive power in two choice experiments. By integrating the
estimated thresholds into computational simulations, we show that
state-of-the-art seeding methods for social influence maximization might be
suboptimal if they neglect individual-level behavioral drivers, which can be
corrected through the proposed experimental method.",http://arxiv.org/abs/2405.13224v1
Causal machine learning for sustainable agroecosystems,2024-08-23T15:25:50Z,"Vasileios Sitokonstantinou, Emiliano D√≠az Salas Porras, Jordi Cerd√† Bautista, Maria Piles, Ioannis Athanasiadis, Hannah Kerner, Giulia Martini, Lily-belle Sweet, Ilias Tsoumas, Jakob Zscheischler, Gustau Camps-Valls","In a changing climate, sustainable agriculture is essential for food security
and environmental health. However, it is challenging to understand the complex
interactions among its biophysical, social, and economic components. Predictive
machine learning (ML), with its capacity to learn from data, is leveraged in
sustainable agriculture for applications like yield prediction and weather
forecasting. Nevertheless, it cannot explain causal mechanisms and remains
descriptive rather than prescriptive. To address this gap, we propose causal
ML, which merges ML's data processing with causality's ability to reason about
change. This facilitates quantifying intervention impacts for evidence-based
decision-making and enhances predictive model robustness. We showcase causal ML
through eight diverse applications that benefit stakeholders across the
agri-food chain, including farmers, policymakers, and researchers.",http://arxiv.org/abs/2408.13155v1
"Forecasting and decisions in the birth-death-suppression Markov model
  for wildfires",2024-09-12T17:56:51Z,"George Hulsey, David L. Alderson, Jean Carlson","As changing climates transform the landscape of wildfire management and
suppression, agencies are faced with difficult resource allocation decisions.
We analyze trade-offs in temporal resource allocation using a simple but robust
Markov model of a wildfire under suppression: the birth-death-suppression
process. Though the model is not spatial, its stochastic nature and rich
temporal structure make it broadly applicable in describing the dynamic
evolution of a fire including ignition, the effect of adverse conditions, and
the effect of external suppression. With strong analytical and numerical
control of the probabilities of outcomes, we construct classes of processes
which analogize common wildfire suppression scenarios and determine aspects of
optimal suppression allocations. We model problems which include resource
management in changing conditions, the effect of resource mobilization delay,
and allocation under uncertainty about future events. Our results are
consistent with modern resource management and suppression practices in
wildland fire.",http://arxiv.org/abs/2410.02765v1
Spatio-Temporal Jump Model for Urban Thermal Comfort Monitoring,2024-11-14T15:36:19Z,"Federico P. Cortese, Antonio Pievatolo","Thermal comfort is essential for well-being in urban spaces, especially as
cities face increasing heat from urbanization and climate change. Existing
thermal comfort models usually overlook temporal dynamics alongside spatial
dependencies. We address this problem by introducing a spatio-temporal jump
model that clusters data with persistence across both spatial and temporal
dimensions. This framework enhances interpretability, minimizes abrupt state
changes, and easily handles missing data. We validate our approach through
extensive simulations, demonstrating its accuracy in recovering the true
underlying partition. When applied to hourly environmental data gathered from a
set of weather stations located across the city of Singapore, our proposal
identifies meaningful thermal comfort regimes, demonstrating its effectiveness
in dynamic urban settings and suitability for real-world monitoring. The
comparison of these regimes with feedback on thermal preference indicates the
potential of an unsupervised approach to avoid extensive surveys.",http://arxiv.org/abs/2411.09726v2
"Population dynamics in the global coral symbiont network under
  temperature variations",2024-11-28T20:14:55Z,"Maria Gabriella Cavalcante Bas√≠lio, Daniel Ratton Figueiredo","Coral reefs are crucial to marine biodiversity and rely on a delicate
symbiotic relationship between corals and zooxanthellae algae. Water
temperature variations, however, disrupt this association, leading to coral
bleaching events that severely affect marine ecosystems. This study presents a
mathematical model for the population dynamics of coral and symbiont species
considering the coral symbiont network and recurrent warming events. The model
incorporates thermal tolerances of species and coupled growth dynamics (between
corals and symbionts) to investigate how network structure and thermal
tolerance influence the species' growth. Using real data from different ocean
regions, results reveal that network connectivity plays a significant role in
population growth after successive warming events, with generalist species
demonstrating greater growth across all regions analyzed. The comparatively
higher correlation between node degree and final population also emphasizes the
impact of ecological network structure on species growth, offering valuable
insights into coral reef population dynamics under climate change. This
research highlights the need to consider network structure beyond species'
thermal tolerances when evaluating the ecological responses of corals to
environmental changes.",http://arxiv.org/abs/2411.19361v2
"Assessing The Spatially Heterogeneous Transportation Impacts of
  Recurrent Flooding in The Hampton Roads Region: Part 1 Auto Accessibility",2024-01-12T23:18:47Z,"Luwei Zeng, T. Donna Chen, John S. Miller, Jonathan L. Goodall, Faria Tuz Zahura","Recurrent flooding has increased rapidly in coastal regions due to sea level
rise and climate change. A key metric for evaluating transportation system
degradation is accessibility, yet the lack of temporally and spatially
disaggregate data means that the impact of recurrent flooding on accessibility,
and hence transportation system performance: is not well understood. Using
crowdsourced WAZE flood incident data from the Hampton Roads region in
Virginia, this study (Part 1) examines changes in the roadway network
accessibility for travelers residing in 1,113 traffic analysis zones (TAZs)
across five time of day periods. Additionally, a social vulnerability index
framework is developed to understand the socioeconomic characteristics of TAZs
that experience high accessibility reduction under recurrent flooding.
  Results show that TAZs experience the most accessibility reduction under
recurrent flooding during the morning peak period (6 to 9am) with large
differences across different zones, ranging from 0 to 49.6 (percentage) for
work trips (with population weighted mean reduction of 1.71 percent) and 0 to
87.9 (percentage) for nonwork trips (with population weighted mean reduction of
0.81 percent). Furthermore, the social vulnerability analysis showed that zones
with higher percentages of lower socioeconomic status, unemployed, less
educated, and limited English proficiency residents experience greater
accessibility reduction for work trips. In contrast to previous studies that
aggregate the effects of recurrent flooding across a city, these results
demonstrate that there exists large spatial and temporal variation in recurrent
floodings impacts on accessibility. This study also highlights the need to
include social vulnerability analysis in assessing impacts of climate events,
to ensure equitable outcomes as investments are made to create resilient
transportation infrastructure.",http://arxiv.org/abs/2402.03334v1
"Mathematical modelling and uncertainty quantification for analysis of
  biphasic coral reef recovery patterns",2024-06-28T01:09:13Z,"David J. Warne, Kerryn Crossman, Grace E. M. Heron, Jesse A. Sharp, Wang Jin, Paul Pao-Yen Wu, Matthew J. Simpson, Kerrie Mengersen, Juan-Carlos Ortiz","Coral reefs are increasingly subjected to major disturbances threatening the
health of marine ecosystems. Substantial research underway to develop
intervention strategies that assist reefs in recovery from, and resistance to,
inevitable future climate and weather extremes. To assess potential benefits of
interventions, mechanistic understanding of coral reef recovery and resistance
patterns is essential. Recent evidence suggests that more than half of the
reefs surveyed across the Great Barrier Reef (GBR) exhibit deviations from
standard recovery modelling assumptions when the initial coral cover is low
($\leq 10$\%). New modelling is necessary to account for these observed
patterns to better inform management strategies. We consider a new model for
reef recovery at the coral cover scale that accounts for biphasic recovery
patterns. The model is based on a multispecies Richards' growth model that
includes a change point in the recovery patterns. Bayesian inference is applied
for uncertainty quantification of key parameters for assessing reef health and
recovery patterns. This analysis is applied to benthic survey data from the
Australian Institute of Marine Sciences (AIMS). We demonstrate agreement
between model predictions and data across every recorded recovery trajectory
with at least two years of observations following disturbance events occurring
between 1992--2020. This new approach will enable new insights into the
biological, ecological and environmental factors that contribute to the
duration and severity of biphasic coral recovery patterns across the GBR. These
new insights will help to inform managements and monitoring practice to
mitigate the impacts of climate change on coral reefs.",http://arxiv.org/abs/2406.19591v1
"Unveiling the Role of Artificial Intelligence and Stock Market Growth in
  Achieving Carbon Neutrality in the United States: An ARDL Model Analysis",2024-12-04T17:07:04Z,"Azizul Hakim Rafi, Abdullah Al Abrar Chowdhury, Adita Sultana, Abdulla All Noman","Given the fact that climate change has become one of the most pressing
problems in many countries in recent years, specialized research on how to
mitigate climate change has been adopted by many countries. Within this
discussion, the influence of advanced technologies in achieving carbon
neutrality has been discussed. While several studies investigated how AI and
Digital innovations could be used to reduce the environmental footprint, the
actual influence of AI in reducing CO2 emissions (a proxy measuring carbon
footprint) has yet to be investigated. This paper studies the role of advanced
technologies in general, and Artificial Intelligence (AI) and ICT use in
particular, in advancing carbon neutrality in the United States, between 2021.
Secondly, this paper examines how Stock Market Growth, ICT use, Gross Domestic
Product (GDP), and Population affect CO2 emissions using the STIRPAT model.
After examining stationarity among the variables using a variety of unit root
tests, this study concluded that there are no unit root problems across all the
variables, with a mixed order of integration. The ARDL bounds test for
cointegration revealed that variables in this study have a long-run
relationship. Moreover, the estimates revealed from the ARDL model in the
short- and long-run indicated that economic growth, stock market
capitalization, and population significantly contributed to the carbon
emissions in both the short-run and long-run. Conversely, AI and ICT use
significantly reduced carbon emissions over both periods. Furthermore, findings
were confirmed to be robust using FMOLS, DOLS, and CCR estimations.
Furthermore, diagnostic tests indicated the absence of serial correlation,
heteroscedasticity, and specification errors and, thus, the model was robust.",http://arxiv.org/abs/2412.16166v1
"Projected hydrologic changes over the north of the Iberian Peninsula
  using a Euro-CORDEX multi-model ensemble",2024-01-13T18:34:43Z,"Patricio Yeste, Juan Jos√© Rosa-C√°novas, Emilio Romero-Jim√©nez, Matilde Garc√≠a-Valdecasas Ojeda, Sonia R. G√°miz-Fortis, Yolanda Castro-D√≠ez, Mar√≠a Jes√∫s Esteban-Parra","This study explores the impacts of climate change on the hydrology of the
headwater areas of the Duero River Basin, the largest basin of the Iberian
Peninsula. To this end, an ensemble of 18 Euro-CORDEX model experiments was
gathered for 1975-2005 and 2021-2100, under the RCP4.5 and RCP8.5, and were
used as the meteorological forcings of the Variable Infiltration Capacity (VIC)
during the hydrological modelling exercise. The projected hydrologic changes
for the future period were analyzed at annual and seasonal scales using several
evaluation metrics, such as the delta changes of the atmospheric and land
variables, the runoff and evapotranspiration ratios of the overall water
balance, the snowmelt contribution to the total streamflow and the centroid
position for the daily hydrograph of the average hydrologic year. Annual
streamflow reductions of up to 40% were attained in various parts of the basin
for 2071-2100 under RCP8.5 scenario, and resulted from the precipitation
decreases in the southern subwatersheds and the combined effect of the
precipitation decreases and evapotranspiration increases in the north. The
runoff and the evapotranspiration ratios evinced a tendency towards an
evaporative regime in the north part of the basin and a strengthening of the
evaporative response in the south. Seasonal streamflow changes were mostly
negative and dependent on the season considered, with greater detriments in
spring and summer, and less intense ones in autumn and winter. The snowmelt
contribution to the total streamflow was strongly diminished with decreases
reaching -80% in autumn and spring, thus pointing to a change in the snow
regime for the Duero mountains. Finally, the annual and seasonal changes of the
centroid position accounted for the shape changes of the hydrograph,
constituting a measure of seasonality and reflecting high correlations degrees
with the streamflow delta changes.",http://arxiv.org/abs/2401.07136v1
"The Impact of Cometary 'impacts' on the Chemistry, Climate, and Spectra
  of Hot Jupiter Atmospheres",2024-02-08T09:39:42Z,"Felix Sainsbury-Martinez, Catherine Walsh","Impacts from icy and rocky bodies have helped shape the composition of solar
system objects, for example the Earth-Moon system, or the recent impact of
comet Shoemaker-Levy 9 with Jupiter. It is likely that such impacts also shape
the composition of exoplanetary systems. Here we investigate how cometary
impacts might affect the atmospheric composition/chemistry of hot Jupiters,
which are prime targets for characterisation. We introduce a parametrised
cometary impact model that includes thermal ablation and pressure driven
breakup, which we couple with the 1D `radiative-convective' atmospheric model
ATMO, including disequilibrium chemistry. We use this model to investigate a
wide range of impactor masses and compositions, including those based on
observations of Solar System comets, and interstellar ices (with JWST). We find
that even a small impactor (R = 2.5 km) can lead to significant short-term
changes in the atmospheric chemistry, including a factor $>10$ enhancement in
H$_2$O, CO, CO$_2$ abundances, and atmospheric opacity more generally, and the
near complete removal of observable hydrocarbons, such as CH$_4$, from the
upper atmosphere. These effects scale with the change in atmospheric C/O ratio
and metallicity. Potentially observable changes are possible for a body that
has undergone significant/continuous bombardment, such that the global
atmospheric chemistry has been impacted. Our works reveals that cometary
impacts can significantly alter or pollute the atmospheric
composition/chemistry of hot Jupiters. These changes have the potential to
mute/break the proposed link between atmospheric C/O ratio and planet formation
location relative to key snowlines in the natal protoplanetary disc.",http://arxiv.org/abs/2402.05509v1
"Atmospheric Centers of Action: current features and expected changes
  from simulations with CMIP5 and CMIP6 models",2024-06-11T06:53:39Z,"I. I. Mokhov, A. M. Osipov, A. V. Chernokulsky","The results of an analysis of changes in the characteristics of atmospheric
centers of action (ACAs) in the Northern (NH) and Southern (SH) hemispheres
using results of simulations with the CMIP5 and CMIP6 ensembles of climate
models are presented. The ability of models to simulate ACA features is
estimated for the historical scenario in comparison with ERA5 reanalysis data.
The projected changes are evaluated under RCP8.5 and SSP5-8.5 scenarios for
CMIP5 and CMIP6 models, respectively. The ACA intensity is evaluated that
defined as the difference in sea level pressure averaged over the ACA region
and the entire hemisphere. In NH, reanalysis and models show greater intensity
of subtropical oceanic anticyclonic ACAs in summer than in winter. The opposite
is found for the intensity of NH subpolar oceanic cyclonic ACAs. The
interannual variability of the ACA intensity in winter is generally greater
than in summer. In SH, the season with greater intensity of oceanic
anticyclonic and cyclonic ACAs and its interannual variability varies from
ocean to ocean. CMIP5 and CMIP6 models show substantial changes of ACAs
characteristics in the XXI century. More significant trends in the
strengthening of ACAs in the 21st century appear in the SH, especially in the
winter seasons. The most consistent weakening trends are found over continents
for winter North American maximum and the summer Asian minimum. For the winter
Siberian maximum, the weakening trend is found more pronounced in CMIP6 models
than in CMIP5.",http://arxiv.org/abs/2406.07002v1
VegeDiff: Latent Diffusion Model for Geospatial Vegetation Forecasting,2024-07-17T14:15:52Z,"Sijie Zhao, Hao Chen, Xueliang Zhang, Pengfeng Xiao, Lei Bai, Wanli Ouyang","In the context of global climate change and frequent extreme weather events,
forecasting future geospatial vegetation states under these conditions is of
significant importance. The vegetation change process is influenced by the
complex interplay between dynamic meteorological variables and static
environmental variables, leading to high levels of uncertainty. Existing
deterministic methods are inadequate in addressing this uncertainty and fail to
accurately model the impact of these variables on vegetation, resulting in
blurry and inaccurate forecasting results. To address these issues, we propose
VegeDiff for the geospatial vegetation forecasting task. To our best knowledge,
VegeDiff is the first to employ a diffusion model to probabilistically capture
the uncertainties in vegetation change processes, enabling the generation of
clear and accurate future vegetation states. VegeDiff also separately models
the global impact of dynamic meteorological variables and the local effects of
static environmental variables, thus accurately modeling the impact of these
variables. Extensive experiments on geospatial vegetation forecasting tasks
demonstrate the effectiveness of VegeDiff. By capturing the uncertainties in
vegetation changes and modeling the complex influence of relevant variables,
VegeDiff outperforms existing deterministic methods, providing clear and
accurate forecasting results of future vegetation states. Interestingly, we
demonstrate the potential of VegeDiff in applications of forecasting future
vegetation states from multiple aspects and exploring the impact of
meteorological variables on vegetation dynamics. The code of this work will be
available at https://github.com/walking-shadow/ Official_VegeDiff.",http://arxiv.org/abs/2407.12592v1
Monitoring the Venice Lagoon: an IoT Cloud-Based Sensor Nerwork Approach,2024-03-11T17:04:04Z,"Filippo Campagnaro, Matin Ghalkhani, Riccardo Tumiati, Federico Marin, Matteo Del Grande, Alessandro Pozzebon, Davide De Battisti, Roberto Francescon, Michele Zorzi","Monitoring the coastal area of the Venice Lagoon is of significant
importance. While the impact of global warming is felt worldwide, coastal and
littoral regions bear the brunt more prominently. These areas not only face the
threat of rising sea levels but also contend with the escalating occurrence of
seaquakes and floods. Additionally, the intricate ecosystems of rivers, seas,
and lakes undergo profound transformations due to climate change and
pollutants.
  Employing devices like the SENSWICH floating wireless sensor presented in
this article and similar measurement instruments proves invaluable to automate
environmental monitoring, hence eliminating the need for manual sampling
campaigns. The utilization of wireless measurement devices offers
cost-effectiveness, real-time analysis, and a reduction in human resource
requirements. Storing data in cloud services further enhances the ability to
monitor parameter changes over extended time intervals.
  In this article, we present an enhanced sensing device aimed at automating
water quality assessment, while considering power consumption and reducing
circuit complexity. Specifically, we will introduce the new schematic and
circuit of SENSWICH which had changes in circuit and electronic aspects.
Furthermore, we outline the methodology for aggregating data in a cloud service
environment, such as Amazon Web Service (AWS), and using Grafana for
visualization.",http://arxiv.org/abs/2403.06915v2
Social bots sour activist sentiment without eroding engagement,2024-03-19T16:58:45Z,"Linda Li, Orsolya Vasarhelyi, Balazs Vedres","Social media platforms have witnessed a substantial increase in social bot
activity, significantly affecting online discourse. Our study explores the
dynamic nature of bot engagement related to Extinction Rebellion climate change
protests from 18 November 2019 to 10 December 2019. We find that bots exert a
greater influence on human behavior than vice versa during heated online
periods. To assess the causal impact of human-bot communication, we compared
communication histories between human users who directly interacted with bots
and matched human users who did not. Our findings demonstrate a consistent
negative impact of bot interactions on subsequent human sentiment, with exposed
users displaying significantly more negative sentiment than their counterparts.
Furthermore, the nature of bot interaction influences human tweeting activity
and the sentiment towards protests. Political astroturfing bots increase
activity, whereas other bots decrease it. Sentiment changes towards protests
depend on the user's original support level, indicating targeted manipulation.
However, bot interactions do not change activists' engagement towards protests.
Despite the seemingly minor impact of individual bot encounters, the cumulative
effect is profound due to the large volume of bot communication. Our findings
underscore the importance of unrestricted access to social media data for
studying the prevalence and influence of social bots, as with new technological
advancements distinguishing between bots and humans becomes nearly impossible.",http://arxiv.org/abs/2403.12904v1
"Tropical cyclone genesis potential using a ventilated potential
  intensity",2024-04-02T02:07:13Z,"Daniel R Chavas, Suzana J Camargo, Michael K Tippett","Genesis potential indices (GPIs) are widely used to understand the
climatology of tropical cyclones (TCs). However, the sign of projected future
changes depends on how they incorporate environmental moisture. Recent theory
combines potential intensity and mid-tropospheric moisture into a single
quantity called the ventilated potential intensity, which removes this
ambiguity. This work proposes a new GPI ($GPI_v$) that is proportional to the
product of the ventilated potential intensity and the absolute vorticity raised
to a power. This power is estimated to be approximately 5 by fitting observed
tropical cyclone best-track and ECMWF Reanalysis v5 (ERA5) data. Fitting the
model with separate exponents yields nearly identical values, indicating that
their product likely constitutes a single joint parameter. Likewise, results
are nearly identical for a Poisson model as for the power law. $GPI_v$ performs
comparably well to existing indices in reproducing the climatological
distribution of tropical cyclone genesis and its covariability with El
Ni\~no-Southern Oscillation, while only requiring a single fitting exponent.
When applied to Coupled Model Intercomparison Project Phase 6 (CMIP6)
projections, $GPI_v$ predicts that environments globally will become gradually
more favorable for TC genesis with warming, consistent with prior work based on
the normalized entropy deficit, though significant changes emerge only at
higher latitudes under relatively strong warming. $GPI_v$ helps resolve the
debate over the treatment of the moisture term and its implication for changes
in TC genesis favorability with warming, and its clearer physical
interpretation may offer a step forward towards a theory for genesis across
climate states.",http://arxiv.org/abs/2404.01572v1
"Unveiling land use dynamics: Insights from a hierarchical Bayesian
  spatio-temporal modelling of Compositional Data",2024-07-31T15:39:44Z,"Mario Figueira, Carmen Guarner, David Conesa, Antonio L√≥pez-Qu√≠lez, Tam√°s Krisztin","Changes in land use patterns have significant environmental and socioeconomic
impacts, making it crucial for policymakers to understand their causes and
consequences. This study, part of the European LAMASUS (Land Management for
Sustainability) project, aims to support the EU's climate neutrality target by
developing a governance model through collaboration between policymakers, land
users, and researchers. We present a methodological synthesis for treating land
use data using a Bayesian approach within spatial and spatio-temporal modeling
frameworks.
  The study tackles the challenges of analyzing land use changes, particularly
the presence of zero values and computational issues with large datasets. It
introduces joint model structures to address zeros and employs sequential
inference and consensus methods for Big Data problems. Spatial downscaling
models approximate smaller scales from aggregated data, circumventing
high-resolution data complications.
  We explore Beta regression and Compositional Data Analysis (CoDa) for land
use data, review relevant spatial and spatio-temporal models, and present
strategies for handling zeros. The paper demonstrates the implementation of key
models, downscaling techniques, and solutions to Big Data challenges with
examples from simulated data and the LAMASUS project, providing a comprehensive
framework for understanding and managing land use changes.",http://arxiv.org/abs/2407.21695v2
Deep Learning for predicting rate-induced tipping,2024-09-11T19:40:57Z,"Yu Huang, Sebastian Bathiany, Peter Ashwin, Niklas Boers","Nonlinear dynamical systems exposed to changing forcing can exhibit
catastrophic transitions between alternative and often markedly different
states. The phenomenon of critical slowing down (CSD) can be used to anticipate
such transitions if caused by a bifurcation and if the change in forcing is
slow compared to the internal time scale of the system. However, in many
real-world situations, these assumptions are not met and transitions can be
triggered because the forcing exceeds a critical rate. For example, given the
pace of anthropogenic climate change in comparison to the internal time scales
of key Earth system components, such as the polar ice sheets or the Atlantic
Meridional Overturning Circulation, such rate-induced tipping poses a severe
risk. Moreover, depending on the realisation of random perturbations, some
trajectories may transition across an unstable boundary, while others do not,
even under the same forcing. CSD-based indicators generally cannot distinguish
these cases of noise-induced tipping versus no tipping. This severely limits
our ability to assess the risks of tipping, and to predict individual
trajectories. To address this, we make a first attempt to develop a deep
learning framework to predict transition probabilities of dynamical systems
ahead of rate-induced transitions. Our method issues early warnings, as
demonstrated on three prototypical systems for rate-induced tipping, subjected
to time-varying equilibrium drift and noise perturbations. Exploiting
explainable artificial intelligence methods, our framework captures the
fingerprints necessary for early detection of rate-induced tipping, even in
cases of long lead times. Our findings demonstrate the predictability of
rate-induced and noise-induced tipping, advancing our ability to determine safe
operating spaces for a broader class of dynamical systems than possible so far.",http://arxiv.org/abs/2409.07590v1
Super Resolution On Global Weather Forecasts,2024-09-17T19:07:13Z,"Lawrence Zhang, Adam Yang, Rodz Andrie Amor, Bryan Zhang, Dhruv Rao","Weather forecasting is a vitally important tool for tasks ranging from
planning day to day activities to disaster response planning. However, modeling
weather has proven to be challenging task due to its chaotic and unpredictable
nature. Each variable, from temperature to precipitation to wind, all influence
the path the environment will take. As a result, all models tend to rapidly
lose accuracy as the temporal range of their forecasts increase. Classical
forecasting methods use a myriad of physics-based, numerical, and stochastic
techniques to predict the change in weather variables over time. However, such
forecasts often require a very large amount of data and are extremely
computationally expensive. Furthermore, as climate and global weather patterns
change, classical models are substantially more difficult and time-consuming to
update for changing environments. Fortunately, with recent advances in deep
learning and publicly available high quality weather datasets, deploying
learning methods for estimating these complex systems has become feasible. The
current state-of-the-art deep learning models have comparable accuracy to the
industry standard numerical models and are becoming more ubiquitous in practice
due to their adaptability. Our group seeks to improve upon existing deep
learning based forecasting methods by increasing spatial resolutions of global
weather predictions. Specifically, we are interested in performing super
resolution (SR) on GraphCast temperature predictions by increasing the global
precision from 1 degree of accuracy to 0.5 degrees, which is approximately
111km and 55km respectively.",http://arxiv.org/abs/2409.11502v2
Emulating the Global Change Analysis Model with Deep Learning,2024-12-12T01:12:55Z,"Andrew Holmes, Matt Jensen, Sarah Coffland, Hidemi Mitani Shen, Logan Sizemore, Seth Bassetti, Brenna Nieva, Claudia Tebaldi, Abigail Snyder, Brian Hutchinson","The Global Change Analysis Model (GCAM) simulates complex interactions
between the coupled Earth and human systems, providing valuable insights into
the co-evolution of land, water, and energy sectors under different future
scenarios. Understanding the sensitivities and drivers of this multisectoral
system can lead to more robust understanding of the different pathways to
particular outcomes. The interactions and complexity of the coupled human-Earth
systems make GCAM simulations costly to run at scale - a requirement for large
ensemble experiments which explore uncertainty in model parameters and outputs.
A differentiable emulator with similar predictive power, but greater
efficiency, could provide novel scenario discovery and analysis of GCAM and its
outputs, requiring fewer runs of GCAM. As a first use case, we train a neural
network on an existing large ensemble that explores a range of GCAM inputs
related to different relative contributions of energy production sources, with
a focus on wind and solar. We complement this existing ensemble with
interpolated input values and a wider selection of outputs, predicting 22,528
GCAM outputs across time, sectors, and regions. We report a median $R^2$ score
of 0.998 for the emulator's predictions and an $R^2$ score of 0.812 for its
input-output sensitivity.",http://arxiv.org/abs/2412.08850v1
"Urban Street Network Design and Transport-Related Greenhouse Gas
  Emissions around the World",2024-01-02T19:20:54Z,"Geoff Boeing, Clemens Pilgram, Yougeng Lu","This study estimates the relationships between street network characteristics
and transport-sector CO2 emissions across every urban area in the world and
investigates whether they are the same across development levels and urban
design paradigms. The prior literature has estimated relationships between
street network design and transport emissions -- including greenhouse gases
implicated in climate change -- primarily through case studies focusing on
certain world regions or relatively small samples of cities, complicating
generalizability and applicability for evidence-informed practice. Our
worldwide study finds that straighter, more-connected, and less-overbuilt
street networks are associated with lower transport emissions, all else equal.
Importantly, these relationships vary across development levels and design
paradigms -- yet most prior literature reports findings from urban areas that
are outliers by global standards. Planners need a better empirical base for
evidence-informed practice in under-studied regions, particularly the rapidly
urbanizing Global South.",http://arxiv.org/abs/2401.01411v1
"Interpretable Time Series Models for Wastewater Modeling in Combined
  Sewer Overflows",2024-01-04T11:48:27Z,"Teodor Chiaburu, Felix Biessmann","Climate change poses increasingly complex challenges to our society. Extreme
weather events such as floods, wild fires or droughts are becoming more
frequent, spontaneous and difficult to foresee or counteract. In this work we
specifically address the problem of sewage water polluting surface water bodies
after spilling over from rain tanks as a consequence of heavy rain events. We
investigate to what extent state-of-the-art interpretable time series models
can help predict such critical water level points, so that the excess can
promptly be redistributed across the sewage network. Our results indicate that
modern time series models can contribute to better waste water management and
prevention of environmental pollution from sewer systems. All the code and
experiments can be found in our repository:
https://github.com/TeodorChiaburu/RIWWER_TimeSeries.",http://arxiv.org/abs/2401.02465v1
Unified Momentum Model for Rotor Aerodynamics Across Operating Regimes,2024-01-17T22:27:24Z,"Jaime Liew, Kirby S. Heck, Michael F. Howland","Despite substantial growth in wind energy technology in recent decades,
aerodynamic modeling of wind turbines relies on momentum models derived in the
late 19th and early 20th centuries, which are well-known to break down under
flow regimes in which wind turbines often operate. This gap in theoretical
modeling for rotors that are misaligned with the inflow and also for
high-thrust rotors has resulted in the development of numerous empirical
corrections which are widely applied in textbooks, research articles, and
open-source and industry design codes. This work reports a unified momentum
model to efficiently predict power production, thrust force, and wake dynamics
of rotors under arbitrary inflow angles and thrust coefficients without
empirical corrections. This unified momentum model can form a new basis for
wind turbine modeling, design, and control tools from first-principles and may
enable further development of innovations necessary for increased wind
production and reliability to respond to 21st century climate change
challenges.",http://arxiv.org/abs/2401.09623v1
Diffusion Representation for Asymmetric Kernels,2024-01-20T19:56:42Z,"Alvaro Almeida Gomez, Antonio Silva Neto, Jorge zubelli","We extend the diffusion-map formalism to data sets that are induced by
asymmetric kernels. Analytical convergence results of the resulting expansion
are proved, and an algorithm is proposed to perform the dimensional reduction.
In this work we study data sets in which its geometry structure is induced by
an asymmetric kernel. We use a priori coordinate system to represent this
geometry and, thus, be able to improve the computational complexity of reducing
the dimensionality of data sets. A coordinate system connected to the tensor
product of Fourier basis is used to represent the underlying geometric
structure obtained by the diffusion-map, thus reducing the dimensionality of
the data set and making use of the speedup provided by the two-dimensional Fast
Fourier Transform algorithm (2-D FFT). We compare our results with those
obtained by other eigenvalue expansions, and verify the efficiency of the
algorithms with synthetic data, as well as with real data from applications
including climate change studies.",http://arxiv.org/abs/2401.12251v1
"Abrupt and persistent atmospheric circulation changes in the North
  Atlantic under La Ni√±a conditions",2024-01-25T10:45:48Z,"Marina Garc√≠a-Burgos, I√±igo G√≥mara, Bel√©n Rodr√≠guez-Fonseca, Juan Jes√∫s Gonz√°lez-Alem√°n, Pablo Zurita-Gotor, Blanca Ayarzag√ºena","Several recent studies have linked the exceptional North Atlantic and
Eurasian atmospheric evolution during late February and March 2018 to the
Sudden Stratospheric Warming (SSW) that took place a few weeks earlier.
February 2018 was characterized by an abrupt transition from the positive to
the negative phase of the North Atlantic Oscillation (NAO) and a subsequent
persistence of the negative NAO for several weeks. This paper investigates the
contribution of atmospheric and oceanic phenomena to both the 2018 event and a
set of 19 identified analogues (including the former) for the period 1959-2022.
Evidence is given that La Ni\~na conditions in the tropical Pacific and
upstream North Atlantic cyclones play an important role as a trigger for these
events. Ensuing two-way tropospheric-stratospheric coupling and eddy feedbacks
provide extended-range persistence for negative NAO conditions. These results
may help improve the prediction of such exceptional events.",http://arxiv.org/abs/2401.14070v1
"Unraveling how winds and surface heat fluxes control the Atlantic
  Ocean's meridional heat transport",2024-01-25T15:15:38Z,"Dhruv Bhagtani, Andrew McC. Hogg, Ryan M. Holmes, Navid C. Constantinou","The North Atlantic Ocean circulation, fueled by winds and surface buoyancy
fluxes, carries 1.25 PettaWatts of heat poleward in the subtropics, and helps
in regulating global weather and climate patterns. Here, we assess the impacts
of changes in winds and surface heat fluxes on the Atlantic Ocean circulation
and heat transport using ocean simulations. We decompose the circulation and
heat transport into warm and cold cells (resembling a subtropical gyre and the
dense overturning circulation respectively), and a mixed cell capturing waters
transitioning between warm and cold regions. Warm and mixed cells transport
more heat poleward as wind stress increases; however, these anomalies are
compensated by reductions in the cold cell's heat transport. Warm and cold
cells transport more heat poleward when we increase meridional heat flux
gradients. Our findings underscore the distinct roles of winds and surface heat
fluxes in controlling the Atlantic Ocean's meridional heat transport.",http://arxiv.org/abs/2401.14230v3
"Using Multi-Temporal Sentinel-1 and Sentinel-2 data for water bodies
  mapping",2024-01-05T18:11:08Z,"Luigi Russo, Francesco Mauro, Babak Memar, Alessandro Sebastianelli, Paolo Gamba, Silvia Liberata Ullo","Climate change is intensifying extreme weather events, causing both water
scarcity and severe rainfall unpredictability, and posing threats to
sustainable development, biodiversity, and access to water and sanitation. This
paper aims to provide valuable insights for comprehensive water resource
monitoring under diverse meteorological conditions. An extension of the
SEN2DWATER dataset is proposed to enhance its capabilities for water basin
segmentation. Through the integration of temporally and spatially aligned radar
information from Sentinel-1 data with the existing multispectral Sentinel-2
data, a novel multisource and multitemporal dataset is generated. Benchmarking
the enhanced dataset involves the application of indices such as the Soil Water
Index (SWI) and Normalized Difference Water Index (NDWI), along with an
unsupervised Machine Learning (ML) classifier (k-means clustering). Promising
results are obtained and potential future developments and applications arising
from this research are also explored.",http://arxiv.org/abs/2402.00023v1
"Time-Series Analysis Approach for Improving Energy Efficiency of a
  Fixed-Route Vessel in Short-Sea Shipping",2024-02-01T15:55:25Z,"Mohamed Abuella, Hadi Fanaee, Slawomir Nowaczyk, Simon Johansson, Ethan Faghani","Several approaches have been developed for improving the ship energy
efficiency, thereby reducing operating costs and ensuring compliance with
climate change mitigation regulations. Many of these approaches will heavily
depend on measured data from onboard IoT devices, including operational and
environmental information, as well as external data sources for additional
navigational data. In this paper, we develop a framework that implements
time-series analysis techniques to optimize the vessel's speed profile for
improving the vessel's energy efficiency. We present a case study involving a
real-world data from a passenger vessel that was collected over a span of 15
months in the south of Sweden. The results indicate that the implemented models
exhibit a range of outcomes and adaptability across different scenarios. The
findings highlight the effectiveness of time-series analysis approach for
optimizing vessel voyages within the context of constrained landscapes, as
often seen in short-sea shipping.",http://arxiv.org/abs/2402.00698v1
"Spiking CenterNet: A Distillation-boosted Spiking Neural Network for
  Object Detection",2024-02-02T10:23:03Z,"Lennard Bodden, Franziska Schwaiger, Duc Bach Ha, Lars Kreuzberg, Sven Behnke","In the era of AI at the edge, self-driving cars, and climate change, the need
for energy-efficient, small, embedded AI is growing. Spiking Neural Networks
(SNNs) are a promising approach to address this challenge, with their
event-driven information flow and sparse activations. We propose Spiking
CenterNet for object detection on event data. It combines an SNN CenterNet
adaptation with an efficient M2U-Net-based decoder. Our model significantly
outperforms comparable previous work on Prophesee's challenging GEN1 Automotive
Detection Dataset while using less than half the energy. Distilling the
knowledge of a non-spiking teacher into our SNN further increases performance.
To the best of our knowledge, our work is the first approach that takes
advantage of knowledge distillation in the field of spiking object detection.",http://arxiv.org/abs/2402.01287v2
Managing Household Waste through Transfer Learning,2024-01-27T05:24:55Z,Suman Kunwar,"As the world continues to face the challenges of climate change, it is
crucial to consider the environmental impact of the technologies we use. In
this study, we investigate the performance and computational carbon emissions
of various transfer learning models for garbage classification. We examine the
MobileNet, ResNet50, ResNet101, and EfficientNetV2S and EfficientNetV2M models.
Our findings indicate that the EfficientNetV2 family achieves the highest
accuracy, recall, f1-score, and IoU values. However, the EfficientNetV2M model
requires more time and produces higher carbon emissions. ResNet50 outperforms
ResNet110 in terms of accuracy, recall, f1-score, and IoU, but it has a larger
carbon footprint. We conclude that EfficientNetV2S is the most sustainable and
accurate model with 96.41% accuracy. Our research highlights the significance
of considering the ecological impact of machine learning models in garbage
classification.",http://arxiv.org/abs/2402.09437v1
Dynamic nowcast of the New Zealand greenhouse gas inventory,2024-02-16T22:19:43Z,"Malcolm Jones, Hannah Chorley, Flynn Owen, Tamsyn Hilder, Holly Trowland, Paul Bracewell","As efforts to mitigate the effects of climate change grow, reliable and
thorough reporting of greenhouse gas emissions are essential for measuring
progress towards international and domestic emissions reductions targets. New
Zealand's national emissions inventories are currently reported between 15 to
27 months out-of-date. We present a machine learning approach to nowcast
(dynamically estimate) national greenhouse gas emissions in New Zealand in
advance of the national emissions inventory's release, with just a two month
latency due to current data availability. Key findings include an estimated
0.2% decrease in national gross emissions since 2020 (as at July 2022). Our
study highlights the predictive power of a dynamic view of emissions intensive
activities. This methodology is a proof of concept that a machine learning
approach can make sub-annual estimates of national greenhouse gas emissions by
sector with a relatively low error that could be of value for policy makers.",http://arxiv.org/abs/2402.11107v1
"Machine-learning prediction of tipping with applications to the Atlantic
  Meridional Overturning Circulation",2024-02-21T20:59:19Z,"Shirin Panahi, Ling-Wei Kong, Mohammadamin Moradi, Zheng-Meng Zhai, Bryan Glaz, Mulugeta Haile, Ying-Cheng Lai","Anticipating a tipping point, a transition from one stable steady state to
another, is a problem of broad relevance due to the ubiquity of the phenomenon
in diverse fields. The steady-state nature of the dynamics about a tipping
point makes its prediction significantly more challenging than predicting other
types of critical transitions from oscillatory or chaotic dynamics. Exploiting
the benefits of noise, we develop a general data-driven and machine-learning
approach to predicting potential future tipping in nonautonomous dynamical
systems and validate the framework using examples from different fields. As an
application, we address the problem of predicting the potential collapse of the
Atlantic Meridional Overturning Circulation (AMOC), possibly driven by
climate-induced changes in the freshwater input to the North Atlantic. Our
predictions based on synthetic and currently available empirical data place a
potential collapse window spanning from 2040 to 2065, in consistency with the
results in the current literature.",http://arxiv.org/abs/2402.14877v2
"Crisis talk: analysis of the public debate around the energy crisis and
  cost of living",2024-02-28T04:42:59Z,"Rrubaa Panchendrarajan, Geri Popova, Tony Russell-Rose","A prominent media topic in the UK in the early 2020s is the energy crisis
affecting the UK and most of Europe. It brings into a single public debate
issues of energy dependency and sustainability, fair distribution of economic
burdens and cost of living, as well as climate change, risk, and
sustainability. In this paper, we investigate the public discourse around the
energy crisis and cost of living to identify how these pivotal and
contradictory issues are reconciled in this debate and to identify which social
actors are involved and the role they play. We analyse a document corpus
retrieved from UK newspapers from January 2014 to March 2023. We apply a
variety of natural language processing and data visualisation techniques to
identify key topics, novel trends, critical social actors, and the role they
play in the debate, along with the sentiment associated with those actors and
topics. We combine automated techniques with manual discourse analysis to
explore and validate the insights revealed in this study. The findings verify
the utility of these techniques by providing a flexible and scalable pipeline
for discourse analysis and providing critical insights for cost of living -
energy crisis nexus research.",http://arxiv.org/abs/2402.18043v1
"Rare events in a stochastic vegetation-water dynamical system based on
  machine learning",2024-02-28T13:22:42Z,"Yang Li, Shenglan Yuan, Shengyuan Xu","Stochastic vegetation-water dynamical systems play a pivotal role in
ecological stability, biodiversity, water resource management, and adaptation
to climate change. This research proposes a machine learning-based method for
analyzing rare events in stochastic vegetation-water dynamical systems with
multiplicative Gaussian noise. Utilizing the Freidlin-Wentzell large deviation
theory, we derive the asymptotic expressions for the quasipotential and the
mean first exit time. Based on the decomposition of vector field, we design a
neural network architecture to compute the most probable transition paths and
the mean first exit time for both non-characteristic and characteristic
boundary scenarios. The results indicate that this method can effectively
predict early warnings of vegetation degradation, providing new theoretical
foundations and mathematical tools for ecological management and conservation.
Moreover, the method offers new possibilities for exploring more complex and
higher-dimensional stochastic dynamical systems.",http://arxiv.org/abs/2402.18315v1
"Perspective: Challenges and opportunities for high-quality battery
  production at scale",2024-03-02T02:10:57Z,"Peter M. Attia, Eric Moch, Patrick K. Herring","As the impacts of climate change become increasingly apparent, the need for
widespread electrification is now internationally recognized. As a result,
global battery production is set to dramatically expand over the next decade.
Unfortunately, however, batteries are both immensely difficult to produce at
the gigawatt-hour scale and inordinately sensitive to minor manufacturing
variation. As a result, the battery industry has already experienced a number
of both highly-visible safety incidents and under-the-radar reliability issues
-- a trend that will only worsen if left unaddressed. In this perspective, we
highlight both the challenges and opportunities to enable battery quality at
scale. We first describe the interplay between various battery failure modes
and their numerous root causes. We then discuss the tensions at play to manage
and improve battery quality during cell production. We hope our perspective
brings greater visibility to the battery quality challenge to enable safe
global electrification.",http://arxiv.org/abs/2403.01065v3
Tree Counting by Bridging 3D Point Clouds with Imagery,2024-03-04T11:02:17Z,"Lei Li, Tianfang Zhang, Zhongyu Jiang, Cheng-Yen Yang, Jenq-Neng Hwang, Stefan Oehmcke, Dimitri Pierre Johannes Gominski, Fabian Gieseke, Christian Igel","Accurate and consistent methods for counting trees based on remote sensing
data are needed to support sustainable forest management, assess climate change
mitigation strategies, and build trust in tree carbon credits. Two-dimensional
remote sensing imagery primarily shows overstory canopy, and it does not
facilitate easy differentiation of individual trees in areas with a dense
canopy and does not allow for easy separation of trees when the canopy is
dense. We leverage the fusion of three-dimensional LiDAR measurements and 2D
imagery to facilitate the accurate counting of trees. We compare a deep
learning approach to counting trees in forests using 3D airborne LiDAR data and
2D imagery. The approach is compared with state-of-the-art algorithms, like
operating on 3D point cloud and 2D imagery. We empirically evaluate the
different methods on the NeonTreeCount data set, which we use to define a
tree-counting benchmark. The experiments show that FuseCountNet yields more
accurate tree counts.",http://arxiv.org/abs/2403.01932v3
"An Adaptive Hydropower Management Approach for Downstream Ecosystem
  Preservation",2024-03-05T09:44:51Z,"C. Coelho, M. Jing, M. Fernanda P. Costa, L. L. Ferr√°s","Hydropower plants play a pivotal role in advancing clean and sustainable
energy production, contributing significantly to the global transition towards
renewable energy sources. However, hydropower plants are currently perceived
both positively as sources of renewable energy and negatively as disruptors of
ecosystems. In this work, we highlight the overlooked potential of using
hydropower plant as protectors of ecosystems by using adaptive ecological
discharges. To advocate for this perspective, we propose using a neural network
to predict the minimum ecological discharge value at each desired time.
Additionally, we present a novel framework that seamlessly integrates it into
hydropower management software, taking advantage of the well-established
approach of using traditional constrained optimisation algorithms. This novel
approach not only protects the ecosystems from climate change but also
contributes to potentially increase the electricity production.",http://arxiv.org/abs/2403.02821v2
"Political polarisation in turbulent times: Tracking polarisation trends
  and partisan news link sharing on Finnish Twitter, 2015-2023",2024-03-06T16:36:11Z,"Antti Gronow, Arttu Malkam√§ki","The study analyses polarisation on Finnish social media with data from the
platform X, which was known as Twitter during the time of data collection
(during the Sipil\""a and Marin governments, 2015-2023). The users were
clustered into three different ideological groups - the Conservative Right, the
Moderate Right, and the Liberal Left - based on their retweeting of tweets
referring to the different political parties in Finland. Trends in polarisation
of several topics encompassing the most recent political crises - immigration,
climate change, COVID-19, and security policy - between these ideological
groups is analysed using network methods. To what extent the polarisation of
each topic aligns with the polarisation of the other topics is also studied. In
addition, the sharing of news links is examined in relation to the ideological
groups of the users as well as to the sentiment and the virality of the tweets
in which news links are shared.",http://arxiv.org/abs/2403.03842v1
"Identification of socioeconomic factors influencing global food price
  security using machine learning",2024-03-07T05:24:11Z,Shan Shan,"Global concern over food prices and security has been exacerbated by the
impacts of armed conflicts such as the Russia Ukraine War, pandemic diseases,
and climate change. Traditionally, analyzing global food prices and their
associations with socioeconomic factors has relied on static linear regression
models. However, the complexity of socioeconomic factors and their implications
extend beyond simple linear relationships. By incorporating determinants,
critical characteristics identification, and comparative model analysis, this
study aimed to identify the critical socioeconomic characteristics and
multidimensional relationships associated with the underlying factors of food
prices and security. Machine learning tools were used to uncover the
socioeconomic factors influencing global food prices from 2000 to 2022. A total
of 105 key variables from the World Development Indicators and the Food and
Agriculture Organization of the United Nations were selected. Machine learning
identified four key dimensions of food price security: economic and population
metrics, military spending, health spending, and environmental factors. The top
30 determinants were selected for feature extraction using data mining. The
efficiency of the support vector regression model allowed for precise
prediction making and correlation analysis.
  Keywords: environment and growth, global economics, price fluctuation,
support vector regression",http://arxiv.org/abs/2403.04231v1
"A Logarithmic Mean Divisia Index Decomposition of CO$_2$ Emissions from
  Energy Use in Romania",2024-03-07T09:37:31Z,"Mariana Carmelia Balanica-Dragomir, Gabriel Murariu, Lucian Puiu Georgescu","Carbon emissions have become a specific alarming indicators and intricate
challenges that lead an extended argue about climate change. The growing trend
in the utilization of fossil fuels for the economic progress and simultaneously
reducing the carbon quantity has turn into a substantial and global challenge.
The aim of this paper is to examine the driving factors of CO$_2$ emissions
from energy sector in Romania during the period 2008-2022 emissions using the
log mean Divisia index (LMDI) method and takes into account five items: CO$_2$
emissions, primary energy resources, energy consumption, gross domestic product
and population, the driving forces of CO$_2$ emissions, based on which it was
calculated the contribution of carbon intensity, energy mixes, generating
efficiency, economy, and population. The results indicate that generating
efficiency effect -90968.57 is the largest inhibiting index while economic
effect is the largest positive index 69084.04 having the role of increasing
CO$_2$ emissions.",http://arxiv.org/abs/2403.04354v1
Reasons behind the Water Crisis and its Potential Health Outcomes,2024-03-09T19:06:39Z,"Md. Galib Ishraq Emran, Rhidi Barma, Akram Hussain Khan, Mrinmoy Roy","Globally, the water crisis has become a significant problem that affects
developing and industrialized nations. Water shortage can harm public health by
increasing the chance of contracting water-borne diseases, dehydration, and
malnutrition. This study aims to examine the causes of the water problem and
its likely effects on human health. The study scrutinizes the reasons behind
the water crisis, including population increase, climate change, and
inefficient water management techniques. The results of a lack of water on
human health, such as the spread of infectious diseases, a higher risk of
starvation and dehydration, and psychological stress, are also concealed in the
study. The research further suggests several ways to deal with the water
situation and lessen its potential outcomes on human health. These remedies
include enhanced sanitation and hygiene procedures, water management, and
conservation techniques like rainwater gathering and wastewater recycling.",http://arxiv.org/abs/2403.07019v1
"Joint Planning of Charging Stations and Power Systems for Heavy-Duty
  Drayage Trucks",2024-03-21T22:21:46Z,"Zuzhao Ye, Nanpeng Yu, Ran Wei","As global concerns about climate change intensify, the transition towards
zero-emission freight is becoming increasingly vital. Drayage is an important
segment of the freight system, typically involving the transport of goods from
seaports or intermodal terminals to nearby warehouses. This sector
significantly contributes to not only greenhouse gas emissions, but also
pollution in densely populated areas. This study presents a holistic
optimization model designed for an efficient transition to zero-emission
drayage, offering cost-effective strategies for the coordinated investment
planning for power systems, charging infrastructure, and electric drayage
trucks. The model is validated in the Greater Los Angeles area, where
regulatory goals are among the most ambitious. Furthermore, the model's design
allows for easy adaptation to other regions. By focusing on drayage trucks,
this study also paves the way for future research into other freight
categories, establishing a foundation for a more extensive exploration in this
field.",http://arxiv.org/abs/2403.14866v1
"STEntConv: Predicting Disagreement with Stance Detection and a Signed
  Graph Convolutional Network",2024-03-23T16:45:22Z,"Isabelle Lorge, Li Zhang, Xiaowen Dong, Janet B. Pierrehumbert","The rise of social media platforms has led to an increase in polarised online
discussions, especially on political and socio-cultural topics such as
elections and climate change. We propose a simple and novel unsupervised method
to predict whether the authors of two posts agree or disagree, leveraging user
stances about named entities obtained from their posts. We present STEntConv, a
model which builds a graph of users and named entities weighted by stance and
trains a Signed Graph Convolutional Network (SGCN) to detect disagreement
between comment and reply posts. We run experiments and ablation studies and
show that including this information improves disagreement detection
performance on a dataset of Reddit posts for a range of controversial subreddit
topics, without the need for platform-specific features or user history.",http://arxiv.org/abs/2403.15885v2
Generalized Policy Learning for Smart Grids: FL TRPO Approach,2024-03-27T10:47:06Z,"Yunxiang Li, Nicolas Mauricio Cuadrado, Samuel Horv√°th, Martin Tak√°ƒç","The smart grid domain requires bolstering the capabilities of existing energy
management systems; Federated Learning (FL) aligns with this goal as it
demonstrates a remarkable ability to train models on heterogeneous datasets
while maintaining data privacy, making it suitable for smart grid applications,
which often involve disparate data distributions and interdependencies among
features that hinder the suitability of linear models. This paper introduces a
framework that combines FL with a Trust Region Policy Optimization (FL TRPO)
aiming to reduce energy-associated emissions and costs. Our approach reveals
latent interconnections and employs personalized encoding methods to capture
unique insights, understanding the relationships between features and optimal
strategies, allowing our model to generalize to previously unseen data.
Experimental results validate the robustness of our approach, affirming its
proficiency in effectively learning policy models for smart grid challenges.",http://arxiv.org/abs/2403.18439v1
"Measuring Political Bias in Large Language Models: What Is Said and How
  It Is Said",2024-03-27T18:22:48Z,"Yejin Bang, Delong Chen, Nayeon Lee, Pascale Fung","We propose to measure political bias in LLMs by analyzing both the content
and style of their generated content regarding political issues. Existing
benchmarks and measures focus on gender and racial biases. However, political
bias exists in LLMs and can lead to polarization and other harms in downstream
applications. In order to provide transparency to users, we advocate that there
should be fine-grained and explainable measures of political biases generated
by LLMs. Our proposed measure looks at different political issues such as
reproductive rights and climate change, at both the content (the substance of
the generation) and the style (the lexical polarity) of such bias. We measured
the political bias in eleven open-sourced LLMs and showed that our proposed
framework is easily scalable to other topics and is explainable.",http://arxiv.org/abs/2403.18932v1
"BEACON: Bayesian Experimental design Acceleration with Conditional
  Normalizing flows $-$ a case study in optimal monitor well placement for
  CO$_2$ sequestration",2024-03-28T20:17:58Z,"Rafael Orozco, Abhinav Gahlot, Felix J. Herrmann","CO$_2$ sequestration is a crucial engineering solution for mitigating climate
change. However, the uncertain nature of reservoir properties, necessitates
rigorous monitoring of CO$_2$ plumes to prevent risks such as leakage, induced
seismicity, or breaching licensed boundaries. To address this, project managers
use borehole wells for direct CO$_2$ and pressure monitoring at specific
locations. Given the high costs associated with drilling, it is crucial to
strategically place a limited number of wells to ensure maximally effective
monitoring within budgetary constraints. Our approach for selecting well
locations integrates fluid-flow solvers for forecasting plume trajectories with
generative neural networks for plume inference uncertainty. Our methodology is
extensible to three-dimensional domains and is developed within a Bayesian
framework for optimal experimental design, ensuring scalability and
mathematical optimality. We use a realistic case study to verify these claims
by demonstrating our method's application in a large scale domain and optimal
performance as compared to baseline well placement.",http://arxiv.org/abs/2404.00075v1
Semi-Supervised Domain Adaptation for Wildfire Detection,2024-04-02T11:03:13Z,"JooYoung Jang, Youngseo Cha, Jisu Kim, SooHyung Lee, Geonu Lee, Minkook Cho, Young Hwang, Nojun Kwak","Recently, both the frequency and intensity of wildfires have increased
worldwide, primarily due to climate change. In this paper, we propose a novel
protocol for wildfire detection, leveraging semi-supervised Domain Adaptation
for object detection, accompanied by a corresponding dataset designed for use
by both academics and industries. Our dataset encompasses 30 times more diverse
labeled scenes for the current largest benchmark wildfire dataset, HPWREN, and
introduces a new labeling policy for wildfire detection. Inspired by CoordConv,
we propose a robust baseline, Location-Aware Object Detection for
Semi-Supervised Domain Adaptation (LADA), utilizing a teacher-student based
framework capable of extracting translational variance features characteristic
of wildfires. With only using 1% target domain labeled data, our framework
significantly outperforms our source-only baseline by a notable margin of 3.8%
in mean Average Precision on the HPWREN wildfire dataset. Our dataset is
available at https://github.com/BloomBerry/LADA.",http://arxiv.org/abs/2404.01842v1
"Enabling Clean Energy Resilience with Machine Learning-Empowered
  Underground Hydrogen Storage",2024-04-04T06:10:57Z,"Alvaro Carbonero, Shaowen Mao, Mohamed Mehana","To address the urgent challenge of climate change, there is a critical need
to transition away from fossil fuels towards sustainable energy systems, with
renewable energy sources playing a pivotal role. However, the inherent
variability of renewable energy, without effective storage solutions, often
leads to imbalances between energy supply and demand. Underground Hydrogen
Storage (UHS) emerges as a promising long-term storage solution to bridge this
gap, yet its widespread implementation is impeded by the high computational
costs associated with high fidelity UHS simulations. This paper introduces UHS
from a data-driven perspective and outlines a roadmap for integrating machine
learning into UHS, thereby facilitating the large-scale deployment of UHS.",http://arxiv.org/abs/2404.03222v1
"Generalizable Temperature Nowcasting with Physics-Constrained RNNs for
  Predictive Maintenance of Wind Turbine Components",2024-04-05T14:23:43Z,"Johannes Exenberger, Matteo Di Salvo, Thomas Hirsch, Franz Wotawa, Gerald Schweiger","Machine learning plays an important role in the operation of current wind
energy production systems. One central application is predictive maintenance to
increase efficiency and lower electricity costs by reducing downtimes.
Integrating physics-based knowledge in neural networks to enforce their
physical plausibilty is a promising method to improve current approaches, but
incomplete system information often impedes their application in real world
scenarios. We describe a simple and efficient way for physics-constrained deep
learning-based predictive maintenance for wind turbine gearbox bearings with
partial system knowledge. The approach is based on temperature nowcasting
constrained by physics, where unknown system coefficients are treated as
learnable neural network parameters. Results show improved generalization
performance to unseen environments compared to a baseline neural network, which
is especially important in low data scenarios often encountered in real-world
applications.",http://arxiv.org/abs/2404.04126v1
YOLO based Ocean Eddy Localization with AWS SageMaker,2024-04-10T05:10:05Z,"Seraj Al Mahmud Mostafa, Jinbo Wang, Benjamin Holt, Jianwu Wang","Ocean eddies play a significant role both on the sea surface and beneath it,
contributing to the sustainability of marine life dependent on oceanic
behaviors. Therefore, it is crucial to investigate ocean eddies to monitor
changes in the Earth, particularly in the oceans, and their impact on climate.
This study aims to pinpoint ocean eddies using AWS cloud services, specifically
SageMaker. The primary objective is to detect small-scale (<20km) ocean eddies
from satellite remote images and assess the feasibility of utilizing SageMaker,
which offers tools for deploying AI applications. Moreover, this research not
only explores the deployment of cloud-based services for remote sensing of
Earth data but also evaluates several YOLO (You Only Look Once) models using
single and multi-GPU-based services in the cloud. Furthermore, this study
underscores the potential of these services, their limitations, challenges
related to deployment and resource management, and their user-riendliness for
Earth science projects.",http://arxiv.org/abs/2404.06744v2
Flow-Acoustics: Theory and Benchmarking,2024-03-26T13:31:37Z,Stefan Schoder,"The urgent need for transitioning to green energy solutions, particularly in
the context of house heating and urban redensification, has brought the issue
of fan noise aeroacoustics investigations to the forefront. As societies
worldwide strive to mitigate climate change and reduce carbon emissions,
adopting sustainable heating technologies such as air heat pumps has gained
significant traction. In Germany, renowned for its commitment to environmental
sustainability, the ""TA L\""arm"" regulations, derived from the
""Bundes-Immissionsschutzgesetz,"" impose stringent limits on noise levels both
inside and outside buildings across various applications. These regulations
delineate permissible noise levels during daytime (6 AM to 10 PM) and nighttime
(10 PM to 6 AM), with particular emphasis on protecting residential areas with
low noise limits. Moreover, the noise limits prescribed for indoor environments
are even more stringent. Given the necessity of maintaining acoustic comfort
and quality of life, compliance with these regulations necessitates meticulous
attention to noise generation sources, especially those associated with heating
and ventilation systems. Consequently, understanding and mitigating fan noise
through aeroacoustic investigations is essential to ensure the successful
adoption and integration of green energy solutions in residential and urban
settings. In the following, an experimental benchmark for a low-pressure rise
axial fan (FAN-01) is presented, and several prediction methods of the sound
pressure and sound power are evaluated.",http://arxiv.org/abs/2404.10634v1
"Gasformer: A Transformer-based Architecture for Segmenting Methane
  Emissions from Livestock in Optical Gas Imaging",2024-04-16T18:38:23Z,"Toqi Tahamid Sarker, Mohamed G Embaby, Khaled R Ahmed, Amer AbuGhazaleh","Methane emissions from livestock, particularly cattle, significantly
contribute to climate change. Effective methane emission mitigation strategies
are crucial as the global population and demand for livestock products
increase. We introduce Gasformer, a novel semantic segmentation architecture
for detecting low-flow rate methane emissions from livestock, and controlled
release experiments using optical gas imaging. We present two unique datasets
captured with a FLIR GF77 OGI camera. Gasformer leverages a Mix Vision
Transformer encoder and a Light-Ham decoder to generate multi-scale features
and refine segmentation maps. Gasformer outperforms other state-of-the-art
models on both datasets, demonstrating its effectiveness in detecting and
segmenting methane plumes in controlled and real-world scenarios. On the
livestock dataset, Gasformer achieves mIoU of 88.56%, surpassing other
state-of-the-art models. Materials are available at:
github.com/toqitahamid/Gasformer.",http://arxiv.org/abs/2404.10841v1
"A Configurable Pythonic Data Center Model for Sustainable Cooling and ML
  Integration",2024-04-18T20:25:33Z,"Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, Vineet Gundecha, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Soumyendu Sarkar","There have been growing discussions on estimating and subsequently reducing
the operational carbon footprint of enterprise data centers. The design and
intelligent control for data centers have an important impact on data center
carbon footprint. In this paper, we showcase PyDCM, a Python library that
enables extremely fast prototyping of data center design and applies
reinforcement learning-enabled control with the purpose of evaluating key
sustainability metrics including carbon footprint, energy consumption, and
observing temperature hotspots. We demonstrate these capabilities of PyDCM and
compare them to existing works in EnergyPlus for modeling data centers. PyDCM
can also be used as a standalone Gymnasium environment for demonstrating
sustainability-focused data center control.",http://arxiv.org/abs/2404.12498v1
"Exploration of Quantum Computing in Materials Discovery for Direct Air
  Capture Applications",2024-04-19T18:11:38Z,"Marco Antonio Barroca, Rodrigo Neumann Barros Ferreira, Mathias Steiner","Direct air capture (DAC) of carbon dioxide is a promising method for
mitigating climate change. Solid sorbents, such as metal-organic frameworks,
are currently being tested for DAC application. However, their potential for
deployment at scale has not been fully realized. The computational discovery of
solid sorbents is challenging, given the vast chemical search space and the DAC
requirements for molecular selectivity. Quantum computing can potentially
accelerate the discovery of solid sorbents for DAC by predicting molecular
binding energies. In this work, we explore simulation methods and algorithms
for predicting gas adsorption in metal-organic frameworks using a quantum
computer. Specifically, we simulate the potential energy surfaces of CO2, N2,
and H2O molecules at the Mg+2 metal center that represents the binding sites of
typical metal-organic frameworks. We apply the qubit-ADAPT-VQE technique to run
simulations on both classical computing and quantum computing hardware, and
achieve reasonable accuracy while maintaining hardware efficiency.",http://arxiv.org/abs/2404.13122v3
"Conditional diffusion models for downscaling & bias correction of Earth
  system model precipitation",2024-04-05T11:01:50Z,"Michael Aich, Philipp Hess, Baoxiang Pan, Sebastian Bathiany, Yu Huang, Niklas Boers","Climate change exacerbates extreme weather events like heavy rainfall and
flooding. As these events cause severe losses of property and lives, accurate
high-resolution simulation of precipitation is imperative. However, existing
Earth System Models (ESMs) struggle with resolving small-scale dynamics and
suffer from biases, especially for extreme events. Traditional statistical bias
correction and downscaling methods fall short in improving spatial structure,
while recent deep learning methods lack controllability over the output and
suffer from unstable training. Here, we propose a novel machine learning
framework for simultaneous bias correction and downscaling. We train a
generative diffusion model in a supervised way purely on observational data. We
map observational and ESM data to a shared embedding space, where both are
unbiased towards each other and train a conditional diffusion model to reverse
the mapping. Our method can be used to correct any ESM field, as the training
is independent of the ESM. Our approach ensures statistical fidelity, preserves
large-scale spatial patterns and outperforms existing methods especially
regarding extreme events and small-scale spatial features that are crucial for
impact assessments.",http://arxiv.org/abs/2404.14416v1
"Aviation sector decarbonization within the hydrogen economy A UAE case
  study",2024-04-20T13:28:47Z,"Ghassan Zubi, Maximilian Kuhn, Sofoklis Makridis, Savio Coutinho","The UAE aviation sector is vital for its economy and is forecast to grow
substantially in the coming decades, increasing thereby fuel consumption. At
the same time, the country is committed to cutting greenhouse gas emissions to
mitigate climate change. Liquid green hydrogen is expected to emerge as an
important aviation fuel in the future. The UAE can use its vast solar energy
resources to produce cost-competitive hydrogen at scale, securing so its
aviation fuel supply. This development, however, needs several decades to
materialize. The PV farms needed to produce the electricity for water
electrolysis need yet to be constructed. The infrastructure to produce,
liquify, store and transport hydrogen is yet to unfold. Hydrogen-powered
aircrafts need to yet evolve from the current small scale demonstration
projects to long-haul commercial airplanes. It is realistically by 2050 when
hydrogen gains momentum and by 2070 when it becomes the primary aviation fuel.
This paper details why and how liquid green hydrogen will find its gap as
aviation fuel in the UAE and provides the strategy and policy recommendations
to facilitate this development.",http://arxiv.org/abs/2404.15393v1
"Efficient Strategies on Supply Chain Network Optimization for Industrial
  Carbon Emission Reduction",2024-04-17T14:53:55Z,Jihu Lei,"This study investigates the efficient strategies for supply chain network
optimization, specifically aimed at reducing industrial carbon emissions.
Amidst escalating concerns about global climate change, industry sectors are
motivated to counteract the negative environmental implications of their supply
chain networks. This paper introduces a novel framework for optimizing these
networks via strategic approaches which lead to a definitive decrease in carbon
emissions. We introduce Adaptive Carbon Emissions Indexing (ACEI), utilizing
real-time carbon emissions data to drive instantaneous adjustments in supply
chain operations. This adaptability predicates on evolving environmental
regulations, fluctuating market trends and emerging technological advancements.
The empirical validations demonstrate our strategy's effectiveness in various
industrial sectors, indicating a significant reduction in carbon emissions and
an increase in operational efficiency. This method also evidences resilience in
the face of sudden disruptions and crises, reflecting its robustness.",http://arxiv.org/abs/2404.16863v1
"A hypergraph model shows the carbon reduction potential of effective
  space use in housing",2024-05-02T13:50:58Z,"Ramon Elias Weber, Caitlin Mueller, Christoph Reinhart","Humans spend over 90% of their time in buildings which account for 40% of
anthropogenic greenhouse gas (GHG) emissions, making buildings the leading
cause of climate change. To incentivize more sustainable construction, building
codes are used to enforce indoor comfort standards and maximum energy use.
However, they currently only reward energy efficiency measures such as
equipment or envelope upgrades and disregard the actual spatial configuration
and usage. Using a new hypergraph model that encodes building floorplan
organization and facilitates automatic geometry creation, we demonstrate that
space efficiency outperforms envelope upgrades in terms of operational carbon
emissions in 72%, 61% and 33% of surveyed buildings in Zurich, New York, and
Singapore. Automatically generated floorplans for a case study in Zurich
further increase access to daylight by up to 24%, revealing that auto-generated
floorplans have the potential to improve the quality of residential spaces in
terms of environmental performance and access to daylight.",http://arxiv.org/abs/2405.01290v1
Mid-latitude interactions expand the Hadley circulation,2024-05-04T21:52:52Z,"W. Moon, J. S. Wettlaufer","The Hadley circulation describes a planetary-scale tropical atmospheric flow,
which has a major influence on climate. Contemporary theoretical understanding
is based upon angular momentum conservation, the basic dynamical constraint
governing the state of the flow pattern. However, despite the degree of success
in representing the Hadley circulation, the canonical theoretical model does
not treat interactions with other regions, particularly the mid-latitudes.
Here, we extend the original model of Held and Hou (1980) to include the
influence of mid-latitude large-scale atmospheric dynamics, which we treat
using the planetary-scale heat equation with a parameterized poleward heat flux
driven by synoptic eddies. The energy flux balance within the Hadley cell
includes the poleward heat flux at the poleward edge of the cell, which is
controlled by the baroclinic instability of the sub-tropical jet. We find that
an increase (decrease) in the poleward heat flux leads to a strengthening
(weakening) of tropical convection, driving an equatorward (poleward) shift of
the edge of the Hadley cell. Thus, our theoretical solutions suggest that
global warming, which can reduce the baroclinicity of the subtropical jet, can
lead to the poleward expansion of the Hadley cell due to the change in energy
flux balance within it.",http://arxiv.org/abs/2405.02761v2
"EarthMatch: Iterative Coregistration for Fine-grained Localization of
  Astronaut Photography",2024-05-08T20:46:36Z,"Gabriele Berton, Gabriele Goletto, Gabriele Trivigno, Alex Stoken, Barbara Caputo, Carlo Masone","Precise, pixel-wise geolocalization of astronaut photography is critical to
unlocking the potential of this unique type of remotely sensed Earth data,
particularly for its use in disaster management and climate change research.
Recent works have established the Astronaut Photography Localization task, but
have either proved too costly for mass deployment or generated too coarse a
localization. Thus, we present EarthMatch, an iterative homography estimation
method that produces fine-grained localization of astronaut photographs while
maintaining an emphasis on speed. We refocus the astronaut photography
benchmark, AIMS, on the geolocalization task itself, and prove our method's
efficacy on this dataset. In addition, we offer a new, fair method for image
matcher comparison, and an extensive evaluation of different matching models
within our localization pipeline. Our method will enable fast and accurate
localization of the 4.5 million and growing collection of astronaut photography
of Earth. Webpage with code and data at
https://earthloc-and-earthmatch.github.io",http://arxiv.org/abs/2405.05422v2
Towards Invariant Time Series Forecasting in Smart Cities,2024-05-08T21:23:01Z,"Ziyi Zhang, Shaogang Ren, Xiaoning Qian, Nick Duffield","In the transformative landscape of smart cities, the integration of the
cutting-edge web technologies into time series forecasting presents a pivotal
opportunity to enhance urban planning, sustainability, and economic growth. The
advancement of deep neural networks has significantly improved forecasting
performance. However, a notable challenge lies in the ability of these models
to generalize well to out-of-distribution (OOD) time series data. The inherent
spatial heterogeneity and domain shifts across urban environments create
hurdles that prevent models from adapting and performing effectively in new
urban environments. To tackle this problem, we propose a solution to derive
invariant representations for more robust predictions under different urban
environments instead of relying on spurious correlation across urban
environments for better generalizability. Through extensive experiments on both
synthetic and real-world data, we demonstrate that our proposed method
outperforms traditional time series forecasting models when tackling domain
shifts in changing urban environments. The effectiveness and robustness of our
method can be extended to diverse fields including climate modeling, urban
planning, and smart city resource management.",http://arxiv.org/abs/2405.05430v1
Detecting Statements in Text: A Domain-Agnostic Few-Shot Solution,2024-05-09T12:03:38Z,"Sandrine Chausson, Bj√∂rn Ross","Many tasks related to Computational Social Science and Web Content Analysis
involve classifying pieces of text based on the claims they contain.
State-of-the-art approaches usually involve fine-tuning models on large
annotated datasets, which are costly to produce. In light of this, we propose
and release a qualitative and versatile few-shot learning methodology as a
common paradigm for any claim-based textual classification task. This
methodology involves defining the classes as arbitrarily sophisticated
taxonomies of claims, and using Natural Language Inference models to obtain the
textual entailment between these and a corpus of interest. The performance of
these models is then boosted by annotating a minimal sample of data points,
dynamically sampled using the well-established statistical heuristic of
Probabilistic Bisection. We illustrate this methodology in the context of three
tasks: climate change contrarianism detection, topic/stance classification and
depression-relates symptoms detection. This approach rivals traditional
pre-train/fine-tune approaches while drastically reducing the need for data
annotation.",http://arxiv.org/abs/2405.05705v1
"Narrative to Trajectory (N2T+): Extracting Routes of Life or Death from
  Human Trafficking Text Corpora",2024-05-09T22:21:40Z,"Saydeh N. Karabatis, Vandana P. Janeja","Climate change and political unrest in certain regions of the world are
imposing extreme hardship on many communities and are forcing millions of
vulnerable populations to abandon their homelands and seek refuge in safer
lands. As international laws are not fully set to deal with the migration
crisis, people are relying on networks of exploiting smugglers to escape the
devastation in order to live in stability. During the smuggling journey,
migrants can become victims of human trafficking if they fail to pay the
smuggler and may be forced into coerced labor. Government agencies and
anti-trafficking organizations try to identify the trafficking routes based on
stories of survivors in order to gain knowledge and help prevent such crimes.
In this paper, we propose a system called Narrative to Trajectory (N2T+), which
extracts trajectories of trafficking routes. N2T+ uses Data Science and Natural
Language Processing techniques to analyze trafficking narratives, automatically
extract relevant location names, disambiguate possible name ambiguities, and
plot the trafficking route on a map. In a comparative evaluation we show that
the proposed multi-dimensional approach offers significantly higher geolocation
detection than other state of the art techniques.",http://arxiv.org/abs/2405.06129v1
Advocating Feedback Control for Human-Earth System Applications,2024-05-12T20:45:52Z,Guido Cavraro,"This paper proposes a feedback control perspective for Human-Earth Systems
(HESs) which essentially are complex systems that capture the interactions
between humans and nature. Recent attention in HES research has been directed
towards devising strategies for climate change mitigation and adaptation, aimed
at achieving environmental and societal objectives. However, existing
approaches heavily rely on HES models, which inherently suffer from
inaccuracies due to the complexity of the system. Moreover, overly detailed
models often prove impractical for optimization tasks. We propose a framework
inheriting from feedback control strategies the robustness against model
errors, because inaccuracies are mitigated using measurements retrieved from
the field. The framework comprises two nested control loops. The outer loop
computes the optimal inputs to the HES, which are then implemented by actuators
controlled in the inner loop. Potential fields of applications are also
identified.",http://arxiv.org/abs/2405.07376v1
Requirements Engineering for Research Software: A Vision,2024-05-13T14:25:01Z,"Adrian Bajraktari, Michelle Binder, Andreas Vogelsang","Modern science is relying on software more than ever. The behavior and
outcomes of this software shape the scientific and public discourse on
important topics like climate change, economic growth, or the spread of
infections. Most researchers creating software for scientific purposes are not
trained in Software Engineering. As a consequence, research software is often
developed ad hoc without following stringent processes. With this paper, we
want to characterize research software as a new application domain that needs
attention from the Requirements Engineering community. We conducted an
exploratory study based on 8 interviews with 12 researchers who develop
software. We describe how researchers elicit, document, and analyze
requirements for research software and what processes they follow. From this,
we derive specific challenges and describe a vision of Requirements Engineering
for research software.",http://arxiv.org/abs/2405.07781v2
"Time-Varying Graph Signal Recovery Using High-Order Smoothness and
  Adaptive Low-rankness",2024-05-16T01:25:30Z,"Weihong Guo, Yifei Lou, Jing Qin, Ming Yan","Time-varying graph signal recovery has been widely used in many applications,
including climate change, environmental hazard monitoring, and epidemic
studies. It is crucial to choose appropriate regularizations to describe the
characteristics of the underlying signals, such as the smoothness of the signal
over the graph domain and the low-rank structure of the spatial-temporal signal
modeled in a matrix form. As one of the most popular options, the graph
Laplacian is commonly adopted in designing graph regularizations for
reconstructing signals defined on a graph from partially observed data. In this
work, we propose a time-varying graph signal recovery method based on the
high-order Sobolev smoothness and an error-function weighted nuclear norm
regularization to enforce the low-rankness. Two efficient algorithms based on
the alternating direction method of multipliers and iterative reweighting are
proposed, and convergence of one algorithm is shown in detail. We conduct
various numerical experiments on synthetic and real-world data sets to
demonstrate the proposed method's effectiveness compared to the
state-of-the-art in graph signal recovery.",http://arxiv.org/abs/2405.09752v1
"Global-Local Detail Guided Transformer for Sea Ice Recognition in
  Optical Remote Sensing Images",2024-05-21T21:02:20Z,"Zhanchao Huang, Wenjun Hong, Hua Su","The recognition of sea ice is of great significance for reflecting climate
change and ensuring the safety of ship navigation. Recently, many deep learning
based methods have been proposed and applied to segment and recognize sea ice
regions. However, the diverse scales of sea ice areas, the zigzag and fine edge
contours, and the difficulty in distinguishing different types of sea ice pose
challenges to existing sea ice recognition models. In this paper, a
Global-Local Detail Guided Transformer (GDGT) method is proposed for sea ice
recognition in optical remote sensing images. In GDGT, a global-local feature
fusiont mechanism is designed to fuse global structural correlation features
and local spatial detail features. Furthermore, a detail-guided decoder is
developed to retain more high-resolution detail information during feature
reconstruction for improving the performance of sea ice recognition.
Experiments on the produced sea ice dataset demonstrated the effectiveness and
advancement of GDGT.",http://arxiv.org/abs/2405.13197v1
"The impact of temporal hydrogen regulation on hydrogen exporters and
  their domestic energy transition",2024-05-23T15:48:24Z,"Leon Schumm, Hazem Abdel-Khalek, Tom Brown, Falko Ueckerdt, Michael Sterner, Davide Fioriti, Max Parzen","As global demand for green hydrogen rises, potential hydrogen exporters move
into the spotlight. However, the large-scale installation of on-grid hydrogen
electrolysis for export can have profound impacts on domestic energy prices and
energy-related emissions. Our investigation explores the interplay of hydrogen
exports, domestic energy transition and temporal hydrogen regulation, employing
a sector-coupled energy model in Morocco. We find substantial co-benets of
domestic climate change mitigation and hydrogen exports, whereby exports can
reduce domestic electricity prices while mitigation reduces hydrogen export
prices. However, increasing hydrogen exports quickly in a system that is still
dominated by fossil fuels can substantially raise domestic electricity prices,
if green hydrogen production is not regulated. Surprisingly, temporal matching
of hydrogen production lowers domestic electricity cost by up to 31% while the
effect on exporters is minimal. This policy instrument can steer the welfare
(re-)distribution between hydrogen exporting firms, hydrogen importers, and
domestic electricity consumers and hereby increases acceptance among actors.",http://arxiv.org/abs/2405.14717v1
The Merit of River Network Topology for Neural Flood Forecasting,2024-05-30T08:45:45Z,"Nikolas Kirschstein, Yixuan Sun","Climate change exacerbates riverine floods, which occur with higher frequency
and intensity than ever. The much-needed forecasting systems typically rely on
accurate river discharge predictions. To this end, the SOTA data-driven
approaches treat forecasting at spatially distributed gauge stations as
isolated problems, even within the same river network. However, incorporating
the known topology of the river network into the prediction model has the
potential to leverage the adjacency relationship between gauges. Thus, we model
river discharge for a network of gauging stations with GNNs and compare the
forecasting performance achieved by different adjacency definitions. Our
results show that the model fails to benefit from the river network topology
information, both on the entire network and small subgraphs. The learned edge
weights correlate with neither of the static definitions and exhibit no regular
pattern. Furthermore, the GNNs struggle to predict sudden, narrow discharge
spikes. Our work hints at a more general underlying phenomenon of neural
prediction not always benefitting from graphical structure and may inspire a
systematic study of the conditions under which this happens.",http://arxiv.org/abs/2405.19836v1
"ESG-FTSE: A corpus of news articles with ESG relevance labels and use
  cases",2024-05-30T16:19:02Z,"Mariya Pavlova, Bernard Casey, Miaosen Wang","We present ESG-FTSE, the first corpus comprised of news articles with
Environmental, Social and Governance (ESG) relevance annotations. In recent
years, investors and regulators have pushed ESG investing to the mainstream due
to the urgency of climate change. This has led to the rise of ESG scores to
evaluate an investment's credentials as socially responsible. While demand for
ESG scores is high, their quality varies wildly. Quantitative techniques can be
applied to improve ESG scores, thus, responsible investing. To contribute to
resource building for ESG and financial text mining, we pioneer the ESG-FTSE
corpus. We further present the first of its kind ESG annotation schema. It has
three levels: a binary classification (relevant versus irrelevant news
articles), ESG classification (ESG-related news articles), and target company.
Both supervised and unsupervised learning experiments for ESG relevance
detection were conducted to demonstrate that the corpus can be used in
different settings to derive accurate ESG predictions. Keywords: corpus
annotation, ESG labels, annotation schema, news article, natural language
processing",http://arxiv.org/abs/2405.20218v1
Programmable Multi-input Buck-Boost Converter for Photovoltaics Arrays,2024-06-03T10:55:28Z,"Zhongting Tang, Yi Zhang, Pooya Davari","This paper proposes a programmable multi-input buck-boost structure method,
which can enhance the operation tolerance for the PV array under extremely
harsh climatic conditions. The proposed structure based on a traditional two
switches buck-boost converter can connect PV panels in parallel and cascade
flexibly, and also enable the individual operation of each PV panel. The active
switches can be programmed to change the connection structures as well as
achieve the maximum power point track of PV panels simultaneously. The paper
presents the programming method for an exemplified scalable structure converter
for two PV panels. The simulation has been established in MATLAB/Simulink to
validate the performance of the proposed converter in terms of multiplexing
function, wide operating range of PV panels, and low switching stress.",http://arxiv.org/abs/2406.01193v1
"Beyond Data, Towards Sustainability: A Sydney Case Study on Urban
  Digital Twins",2024-06-07T12:53:32Z,"Ammar Sohail, Bojie Shen, Muhammad Aamir Cheema, Mohammed Eunus Ali, Anwaar Ulhaq, Muhammad Ali Babar, Asama Qureshi","As urban areas grapple with unprecedented challenges stemming from population
growth and climate change, the emergence of urban digital twins offers a
promising solution. This paper presents a case study focusing on Sydney's urban
digital twin, a virtual replica integrating diverse real-time and historical
data, including weather, crime, emissions, and traffic. Through advanced
visualization and data analysis techniques, the study explores some
applications of this digital twin in urban sustainability, such as spatial
ranking of suburbs and automatic identification of correlations between
variables. Additionally, the research delves into predictive modeling,
employing machine learning to forecast traffic crash risks using environmental
data, showcasing the potential for proactive interventions. The contributions
of this work lie in the comprehensive exploration of a city-scale digital twin
for sustainable urban planning, offering a multifaceted approach to data-driven
decision-making.",http://arxiv.org/abs/2406.04902v1
Carbon Market Simulation with Adaptive Mechanism Design,2024-06-12T05:08:51Z,"Han Wang, Wenhao Li, Hongyuan Zha, Baoxiang Wang","A carbon market is a market-based tool that incentivizes economic agents to
align individual profits with the global utility, i.e., reducing carbon
emissions to tackle climate change. Cap and trade stands as a critical
principle based on allocating and trading carbon allowances (carbon emission
credit), enabling economic agents to follow planned emissions and penalizing
excess emissions. A central authority is responsible for introducing and
allocating those allowances in cap and trade. However, the complexity of carbon
market dynamics makes accurate simulation intractable, which in turn hinders
the design of effective allocation strategies. To address this, we propose an
adaptive mechanism design framework, simulating the market using hierarchical,
model-free multi-agent reinforcement learning (MARL). Government agents
allocate carbon credits, while enterprises engage in economic activities and
carbon trading. This framework illustrates agents' behavior comprehensively.
Numerical results show MARL enables government agents to balance productivity,
equality, and carbon emissions. Our project is available at
https://github.com/xwanghan/Carbon-Simulator.",http://arxiv.org/abs/2406.07875v2
"BTS: Building Timeseries Dataset: Empowering Large-Scale Building
  Analytics",2024-06-13T10:38:38Z,"Arian Prabowo, Xiachong Lin, Imran Razzak, Hao Xue, Emily W. Yap, Matthew Amos, Flora D. Salim","Buildings play a crucial role in human well-being, influencing occupant
comfort, health, and safety. Additionally, they contribute significantly to
global energy consumption, accounting for one-third of total energy usage, and
carbon emissions. Optimizing building performance presents a vital opportunity
to combat climate change and promote human flourishing. However, research in
building analytics has been hampered by the lack of accessible, available, and
comprehensive real-world datasets on multiple building operations. In this
paper, we introduce the Building TimeSeries (BTS) dataset. Our dataset covers
three buildings over a three-year period, comprising more than ten thousand
timeseries data points with hundreds of unique ontologies. Moreover, the
metadata is standardized using the Brick schema. To demonstrate the utility of
this dataset, we performed benchmarks on two tasks: timeseries ontology
classification and zero-shot forecasting. These tasks represent an essential
initial step in addressing challenges related to interoperability in building
analytics. Access to the dataset and the code used for benchmarking are
available here: https://github.com/cruiseresearchgroup/DIEF_BTS .",http://arxiv.org/abs/2406.08990v2
"CoastTerm: a Corpus for Multidisciplinary Term Extraction in Coastal
  Scientific Literature",2024-06-13T14:01:08Z,"Julien Delaunay, Hanh Thi Hong Tran, Carlos-Emiliano Gonz√°lez-Gallardo, Georgeta Bordea, Mathilde Ducos, Nicolas Sidere, Antoine Doucet, Senja Pollak, Olivier De Viron","The growing impact of climate change on coastal areas, particularly active
but fragile regions, necessitates collaboration among diverse stakeholders and
disciplines to formulate effective environmental protection policies. We
introduce a novel specialized corpus comprising 2,491 sentences from 410
scientific abstracts concerning coastal areas, for the Automatic Term
Extraction (ATE) and Classification (ATC) tasks. Inspired by the ARDI
framework, focused on the identification of Actors, Resources, Dynamics and
Interactions, we automatically extract domain terms and their distinct roles in
the functioning of coastal systems by leveraging monolingual and multilingual
transformer models. The evaluation demonstrates consistent results, achieving
an F1 score of approximately 80\% for automated term extraction and F1 of 70\%
for extracting terms and their labels. These findings are promising and signify
an initial step towards the development of a specialized Knowledge Base
dedicated to coastal areas.",http://arxiv.org/abs/2406.09128v1
Global Crop-Specific Fertilization Dataset from 1961-2019,2024-06-14T13:16:34Z,"Fernando Coello, Thomas Decorte, Iris Janssens, Steven Mortier, Jordi Sardans, Josep Pe√±uelas, Tim Verdonck","As global fertilizer application rates increase, high-quality datasets are
paramount for comprehensive analyses to support informed decision-making and
policy formulation in crucial areas such as food security or climate change.
This study aims to fill existing data gaps by employing two machine learning
models, eXtreme Gradient Boosting and HistGradientBoosting algorithms to
produce precise country-level predictions of nitrogen ($N$), phosphorus
pentoxide ($P_2O_5$), and potassium oxide ($K_2O$) application rates.
Subsequently, we created a comprehensive dataset of 5-arcmin resolution maps
depicting the application rates of each fertilizer for 13 major crop groups
from 1961 to 2019. The predictions were validated by both comparing with
existing databases and by assessing the drivers of fertilizer application rates
using the model's SHapley Additive exPlanations. This extensive dataset is
poised to be a valuable resource for assessing fertilization trends,
identifying the socioeconomic, agricultural, and environmental drivers of
fertilizer application rates, and serving as an input for various applications,
including environmental modeling, causal analysis, fertilizer price
predictions, and forecasting.",http://arxiv.org/abs/2406.10001v2
"Arabian Sea high salinity core supplies oxygen to Bay of Bengal oxygen
  minimum zone",2024-06-15T09:17:37Z,"Anoop A. Nayak, P. N. Vinayachandran, Jenson V. George","The oxygen minimum zone (OMZ) in the Bay of Bengal (BoB) is unique owing to
its curious capability to maintain steady dissolved oxygen (DO) levels. In this
study, we identify a process by which the oxygen levels in BoB are sustained
above the tipping point, using DO and microstructure profiles in the southern
BoB and Argo profiles over the entire basin. High salinity core (HSC) rich in
DO is advected by the Summer Monsoon Current (SMC) into BoB. Vertical mixing
driven by turbulent and salt-fingering processes recharge DO concentration in
thermocline above OMZ. HSC identified in the Argo data, also rich in oxygen,
can be traced up to 19$^\circ$ N, confirming that HSC is a source of DO and
potentially prevents OMZ from moving to the denitrification regime. In changing
climate conditions, this might be the only significant oxygen source for the
BoB OMZ in the future.",http://arxiv.org/abs/2406.10571v1
"The Transformation Risk-Benefit Model of Artificial Intelligence:
  Balancing Risks and Benefits Through Practical Solutions and Use Cases",2024-04-11T19:19:57Z,"Richard Fulton, Diane Fulton, Nate Hayes, Susan Kaplan","This paper summarizes the most cogent advantages and risks associated with
Artificial Intelligence from an in-depth review of the literature. Then the
authors synthesize the salient risk-related models currently being used in AI,
technology and business-related scenarios. Next, in view of an updated context
of AI along with theories and models reviewed and expanded constructs, the
writers propose a new framework called ""The Transformation Risk-Benefit Model
of Artificial Intelligence"" to address the increasing fears and levels of AI
risk. Using the model characteristics, the article emphasizes practical and
innovative solutions where benefits outweigh risks and three use cases in
healthcare, climate change/environment and cyber security to illustrate unique
interplay of principles, dimensions and processes of this powerful AI
transformational model.",http://arxiv.org/abs/2406.11863v1
Crowdfunding for Equitable EV Charging Infrastructure,2024-06-20T13:23:38Z,"Abdolmajid Erfani, Qingbin Cui, Patrick DeCorla-Souza","The transportation sector significantly contributes to greenhouse gas
emissions, highlighting the need to transition to Electric Vehicles (EVs) to
reduce fossil fuel dependence and combat climate change. The US government has
set ambitious targets for 2030, aiming for half of all new vehicles sold to be
zero-emissions. Expanding EV charging stations is crucial for this transition,
but social equity presents a significant challenge. The Justice40 program
mandates that at least 40% of benefits be allocated to disadvantaged
communities, ensuring they benefit from federal investments. Given the current
concentration of EV ownership in affluent areas, merely installing charging
stations in disadvantaged neighborhoods may not suffice. This article explores
crowdfunding as a novel method to finance EV charging infrastructure, engaging,
and empowering underserved communities. The paper concludes with a hypothetical
case showing financing benefits for disadvantaged communities, exploring
crowdfunding variations, and scaling to develop equitable EV charging networks.",http://arxiv.org/abs/2406.14295v1
Determination of the mean center of a region: A physics-based approach,2024-06-07T09:04:59Z,Dipak Patra,"The mean center of a geographical region, including continents and countries,
has been mostly determined to study the trend of population migration, the
shift of economic hubs, and the spatial change of extreme climate events.
However, the determination of the mean center is a formidable task as it deals
with the curvature of the earth's surface. Here, we report a physics-based
model to determine the mean center of a region. Our method provides the
analytical expression for the location of the mean center for both flat and
curved spaces, such as straight lines, circles, planes, three-dimensional
space, cylinders, and spheres. Some of these expressions are often used to
compute the center of mass of the physical system. Therefore, the implication
of our model in the physical system extends the general validity of the model.
Furthermore, we have computed various mean centers of India, such as the
geographical, population, and crime centers. We have also assessed the
year-wise movement of the crime center and found a spatial trend towards the
north.",http://arxiv.org/abs/2406.15455v2
"Similarity-Based Analysis of Atmospheric Organic Compounds for Machine
  Learning Applications",2024-06-26T08:35:08Z,"Hilda Sandstr√∂m, Patrick Rinke","The formation of aerosol particles in the atmosphere impacts air quality and
climate change, but many of the organic molecules involved remain unknown.
Machine learning could aid in identifying these compounds through accelerated
analysis of molecular properties and detection characteristics. However, such
progress is hindered by the current lack of curated datasets for atmospheric
molecules and their associated properties. To tackle this challenge, we propose
a similarity analysis that connects atmospheric compounds to existing large
molecular datasets used for machine learning development. We find a small
overlap between atmospheric and non-atmospheric molecules using standard
molecular representations in machine learning applications. The identified
out-of-domain character of atmospheric compounds is related to their distinct
functional groups and atomic composition. Our investigation underscores the
need for collaborative efforts to gather and share more molecular-level
atmospheric chemistry data. The presented similarity based analysis can be used
for future dataset curation for machine learning development in the atmospheric
sciences.",http://arxiv.org/abs/2406.18171v1
"Vehicle-to-Grid Technology meets Packetized Energy Management: A
  Co-Simulation Study",2024-06-27T16:10:55Z,"Freddy Tuxworth, Adnan Aijaz","The global energy landscape is experiencing a significant transformation
driven by increased awareness of climate change and rapid technological
advancements in renewable energy and electric vehicles (EVs). Packetized energy
management (PEM) schemes are gaining attention as a potential solution for
power management for effective load control. This study presents the
development of a co-simulation platform to investigate integration of
vehicle-to-grid (V2G) with packetized energy trading (PET) in microgrid
scenarios. The platform facilitates the interaction between EVs and prosumers,
with a focus on responsive loads, and solar photovoltaic (PV) as intermittently
available resources. Using the developed co-simulation, this study evaluates
how V2G-capable EVs can enhance the stability and efficiency of PET-based
microgrids. The results demonstrate the capability of V2G EVs to act as an
energy reservoir, effectively managing demand-side load, thus mitigating its
fluctuation from available supply while maintaining quality-of-service.",http://arxiv.org/abs/2406.19296v1
"Wildfire Autonomous Response and Prediction Using Cellular Automata
  (WARP-CA)",2024-07-02T19:01:59Z,Abdelrahman Ramadan,"Wildfires pose a severe challenge to ecosystems and human settlements,
exacerbated by climate change and environmental factors. Traditional wildfire
modeling, while useful, often fails to adapt to the rapid dynamics of such
events. This report introduces the (Wildfire Autonomous Response and Prediction
Using Cellular Automata) WARP-CA model, a novel approach that integrates
terrain generation using Perlin noise with the dynamism of Cellular Automata
(CA) to simulate wildfire spread. We explore the potential of Multi-Agent
Reinforcement Learning (MARL) to manage wildfires by simulating autonomous
agents, such as UAVs and UGVs, within a collaborative framework. Our
methodology combines world simulation techniques and investigates emergent
behaviors in MARL, focusing on efficient wildfire suppression and considering
critical environmental factors like wind patterns and terrain features.",http://arxiv.org/abs/2407.02613v1
Causal Discovery in Semi-Stationary Time Series,2024-07-10T00:55:38Z,"Shanyun Gao, Raghavendra Addanki, Tong Yu, Ryan A. Rossi, Murat Kocaoglu","Discovering causal relations from observational time series without making
the stationary assumption is a significant challenge. In practice, this
challenge is common in many areas, such as retail sales, transportation
systems, and medical science. Here, we consider this problem for a class of
non-stationary time series. The structural causal model (SCM) of this type of
time series, called the semi-stationary time series, exhibits that a finite
number of different causal mechanisms occur sequentially and periodically
across time. This model holds considerable practical utility because it can
represent periodicity, including common occurrences such as seasonality and
diurnal variation. We propose a constraint-based, non-parametric algorithm for
discovering causal relations in this setting. The resulting algorithm,
PCMCI$_{\Omega}$, can capture the alternating and recurring changes in the
causal mechanisms and then identify the underlying causal graph with
conditional independence (CI) tests. We show that this algorithm is sound in
identifying causal relations on discrete time series. We validate the algorithm
with extensive experiments on continuous and discrete simulated data. We also
apply our algorithm to a real-world climate dataset.",http://arxiv.org/abs/2407.07291v1
"Quantum-Train Long Short-Term Memory: Application on Flood Prediction
  Problem",2024-07-11T15:56:00Z,"Chu-Hsuan Abraham Lin, Chen-Yu Liu, Kuan-Cheng Chen","Flood prediction is a critical challenge in the context of climate change,
with significant implications for ecosystem preservation, human safety, and
infrastructure protection. In this study, we tackle this problem by applying
the Quantum-Train (QT) technique to a forecasting Long Short-Term Memory (LSTM)
model trained by Quantum Machine Learning (QML) with significant parameter
reduction. The QT technique, originally successful in the A Matter of Taste
challenge at QHack 2024, leverages QML to reduce the number of trainable
parameters to a polylogarithmic function of the number of parameters in a
classical neural network (NN). This innovative framework maps classical NN
weights to a Hilbert space, altering quantum state probability distributions to
adjust NN parameters. Our approach directly processes classical data without
the need for quantum embedding and operates independently of quantum computing
resources post-training, making it highly practical and accessible for
real-world flood prediction applications. This model aims to improve the
efficiency of flood forecasts, ultimately contributing to better disaster
preparedness and response.",http://arxiv.org/abs/2407.08617v1
A Novel Lexicon for the Moral Foundation of Liberty,2024-07-16T15:49:05Z,"Oscar Araque, Lorenzo Gatti, Sergio Consoli, Kyriaki Kalimeri","The moral value of liberty is a central concept in our inference system when
it comes to taking a stance towards controversial social issues such as vaccine
hesitancy, climate change, or the right to abortion. Here, we propose a novel
Liberty lexicon evaluated on more than 3,000 manually annotated data both in
in- and out-of-domain scenarios. As a result of this evaluation, we produce a
combined lexicon that constitutes the main outcome of this work. This final
lexicon incorporates information from an ensemble of lexicons that have been
generated using word embedding similarity (WE) and compositional semantics
(CS). Our key contributions include enriching the liberty annotations,
developing a robust liberty lexicon for broader application, and revealing the
complexity of expressions related to liberty across different platforms.
Through the evaluation, we show that the difficulty of the task calls for
designing approaches that combine knowledge, in an effort of improving the
representations of learning systems.",http://arxiv.org/abs/2407.11862v1
Tree semantic segmentation from aerial image time series,2024-07-18T02:19:57Z,"Venkatesh Ramesh, Arthur Ouaknine, David Rolnick","Earth's forests play an important role in the fight against climate change,
and are in turn negatively affected by it. Effective monitoring of different
tree species is essential to understanding and improving the health and
biodiversity of forests. In this work, we address the challenge of tree species
identification by performing semantic segmentation of trees using an aerial
image dataset spanning over a year. We compare models trained on single images
versus those trained on time series to assess the impact of tree phenology on
segmentation performances. We also introduce a simple convolutional block for
extracting spatio-temporal features from image time series, enabling the use of
popular pretrained backbones and methods. We leverage the hierarchical
structure of tree species taxonomy by incorporating a custom loss function that
refines predictions at three levels: species, genus, and higher-level taxa. Our
findings demonstrate the superiority of our methodology in exploiting the time
series modality and confirm that enriching labels using taxonomic information
improves the semantic segmentation performance.",http://arxiv.org/abs/2407.13102v1
"Early Detection of Coffee Leaf Rust Through Convolutional Neural
  Networks Trained on Low-Resolution Images",2024-07-20T03:24:25Z,"Angelly Cabrera, Kleanthis Avramidis, Shrikanth Narayanan","Coffee leaf rust, a foliar disease caused by the fungus Hemileia vastatrix,
poses a major threat to coffee production, especially in Central America.
Climate change further aggravates this issue, as it shortens the latency period
between initial infection and the emergence of visible symptoms in diseases
like leaf rust. Shortened latency periods can lead to more severe plant
epidemics and faster spread of diseases. There is, hence, an urgent need for
effective disease management strategies. To address these challenges, we
explore the potential of deep learning models for enhancing early disease
detection. However, deep learning models require extensive processing power and
large amounts of data for model training, resources that are typically scarce.
To overcome these barriers, we propose a preprocessing technique that involves
convolving training images with a high-pass filter to enhance lesion-leaf
contrast, significantly improving model efficacy in resource-limited
environments. This method and our model demonstrated a strong performance,
achieving over 90% across all evaluation metrics--including precision, recall,
F1-score, and the Dice coefficient. Our experiments show that this approach
outperforms other methods, including two different image preprocessing
techniques and using unaltered, full-color images.",http://arxiv.org/abs/2407.14737v1
"Hurricane Evacuation Analysis with Large-scale Mobile Device Location
  Data during Hurricane Ian",2024-07-21T19:27:50Z,"Luyu Liu, Xiaojian Zhang, Shangkun Jiang, Xilei Zhao","Hurricane Ian is the deadliest and costliest hurricane in Florida's history,
with 2.5 million people ordered to evacuate. As we witness increasingly severe
hurricanes in the context of climate change, mobile device location data offers
an unprecedented opportunity to study hurricane evacuation behaviors. With a
terabyte-level GPS dataset, we introduce a holistic hurricane evacuation
behavior algorithm with a case study of Ian: we infer evacuees' departure time
and categorize them into different behavioral groups, including self,
voluntary, mandatory, shadow and in-zone evacuees. Results show the landfall
area (Fort Myers, Lee County) had lower out-of-zone but higher overall
evacuation rate, while the predicted landfall area (Tampa, Hillsborough County)
had the opposite, suggesting the effects of delayed evacuation order.
Out-of-zone evacuation rates would increase from shore to inland.
Spatiotemporal analysis identified three evacuation waves: during formation,
before landfall, and after landfall. These insights are valuable for enhancing
future disaster planning and management.",http://arxiv.org/abs/2407.15249v1
"Convection in the active layer speeds up permafrost thaw in coarse
  grained soils",2024-07-30T10:21:33Z,"Marta Magnani, Stefano Musacchio, Antonello Provenzale, Guido Boffetta","Permafrost thaw is a major concern raised by the ongoing climate change. An
understudied phenomenon possibly affecting the pace of permafrost thaw is the
onset of convective motions within the active layer caused by the density
anomaly of water. Here, we explore the effects of groundwater convection on
permafrost thawing using a model that accounts for ice - water phase
transitions, coupled with the dynamics of the temperature field transported by
the Darcy's flow across a porous matrix. Numerical simulations of this model
show that ice thawing in the presence of convection is much faster than in the
diffusive case and deepens at a constant velocity proportional to the soil
permeability. A scaling argument is able to predict correctly the asymptotic
velocity. Since in the convective regime the heat transport is mediated by the
coherent motion of thermal plumes across the thawed layer, we find that the
depth of the thawing interface becomes highly heterogeneous.",http://arxiv.org/abs/2407.20714v1
Matrix-Free Finite Volume Kernels on a Dataflow Architecture,2024-08-06T21:18:51Z,"Ryuichi Sai, Francois P. Hamon, John Mellor-Crummey, Mauricio Araya-Polo","Fast and accurate numerical simulations are crucial for designing large-scale
geological carbon storage projects ensuring safe long-term CO2 containment as a
climate change mitigation strategy. These simulations involve solving numerous
large and complex linear systems arising from the implicit Finite Volume (FV)
discretization of PDEs governing subsurface fluid flow. Compounded with highly
detailed geomodels, solving linear systems is computationally and memory
expensive, and accounts for the majority of the simulation time. Modern memory
hierarchies are insufficient to meet the latency and bandwidth needs of
large-scale numerical simulations. Therefore, exploring algorithms that can
leverage alternative and balanced paradigms, such as dataflow and in-memory
computing is crucial. This work introduces a matrix-free algorithm to solve
FV-based linear systems using a dataflow architecture to significantly minimize
memory latency and bandwidth bottlenecks. Our implementation achieves two
orders of magnitude speedup compared to a GPGPU-based reference implementation,
and up to 1.2 PFlops on a single dataflow device.",http://arxiv.org/abs/2408.03452v1
"Depth Any Canopy: Leveraging Depth Foundation Models for Canopy Height
  Estimation",2024-08-08T15:24:07Z,"Daniele Rege Cambrin, Isaac Corley, Paolo Garza","Estimating global tree canopy height is crucial for forest conservation and
climate change applications. However, capturing high-resolution ground truth
canopy height using LiDAR is expensive and not available globally. An efficient
alternative is to train a canopy height estimator to operate on single-view
remotely sensed imagery. The primary obstacle to this approach is that these
methods require significant training data to generalize well globally and
across uncommon edge cases. Recent monocular depth estimation foundation models
have show strong zero-shot performance even for complex scenes. In this paper
we leverage the representations learned by these models to transfer to the
remote sensing domain for measuring canopy height. Our findings suggest that
our proposed Depth Any Canopy, the result of fine-tuning the Depth Anything v2
model for canopy height estimation, provides a performant and efficient
solution, surpassing the current state-of-the-art with superior or comparable
performance using only a fraction of the computational resources and
parameters. Furthermore, our approach requires less than \$1.30 in compute and
results in an estimated carbon footprint of 0.14 kgCO2. Code, experimental
results, and model checkpoints are openly available at
https://github.com/DarthReca/depth-any-canopy.",http://arxiv.org/abs/2408.04523v1
"Spatial Microclimatic Characterization of a Parisian ""Oasis"" Schoolyard",2024-07-29T08:56:04Z,"Ghid Karam, Ma√Ølys Chanial, Sophie Parison, Martin Hendel, Laurent Royon","In the aftermath of the 2003 heatwave, and with growing concern over climate
change, Paris City Hall has been implementing several heat mitigation
strategies. One of these is the OASIS Schoolyard Strategy which aims to
transform Parisian schoolyards into cool islands. Within this framework, the
EU-funded ERDF UIA OASIS Project aims to study the transformation of ten
schoolyards, including an evaluation of their microclimatic performance.The
present article presents case study results from one schoolyard using GIS data
and fixed and mobile microclimatic measurements. An analysis method for mobile
measurement data is proposed, tested and discussed on the basis of this case
study.",http://arxiv.org/abs/2408.07284v1
Physical and geographic analysis of the urban cooling potential,2024-08-05T13:28:54Z,"Martin Hendel, Parison Sophie, Royon Laurent","The performance of a number of urban cooling techniques has been thoroughly
studied by the scientific community. However, decision-makers lack the tools to
spatialize their deployment as part of their urban cooling and climate change
adaptation strategies. Among other indicators, a spatial assessment of the
cooling potential for a technique in a given area is lacking. To this end, we
analyze the physical mechanisms on which these techniques are based and
identify corresponding geographical indicators that influence their cooling
performance. Solar irradiance, existing material properties and underground
infrastructure stand out as essential indicators for this purpose.",http://arxiv.org/abs/2408.08885v1
"Public Health in Disaster: Emotional Health and Life Incidents
  Extraction during Hurricane Harvey",2024-08-20T18:31:20Z,"Thomas Hoang, Quynh Anh Nguyen, Long Nguyen","Countless disasters have resulted from climate change, causing severe damage
to infrastructure and the economy. These disasters have significant societal
impacts, necessitating mental health services for the millions affected. To
prepare for and respond effectively to such events, it is important to
understand people's emotions and the life incidents they experience before and
after a disaster strikes. In this case study, we collected a dataset of
approximately 400,000 public tweets related to the storm. Using a BERT-based
model, we predicted the emotions associated with each tweet. To efficiently
identify these topics, we utilized the Latent Dirichlet Allocation (LDA)
technique for topic modeling, which allowed us to bypass manual content
analysis and extract meaningful patterns from the data. However, rather than
stopping at topic identification like previous methods \cite{math11244910}, we
further refined our analysis by integrating Graph Neural Networks (GNN) and
Large Language Models (LLM). The GNN was employed to generate embeddings and
construct a similarity graph of the tweets, which was then used to optimize
clustering. Subsequently, we used an LLM to automatically generate descriptive
names for each event cluster, offering critical insights for disaster
preparedness and response strategies.",http://arxiv.org/abs/2408.11133v1
"Satellite Sunroof: High-res Digital Surface Models and Roof Segmentation
  for Global Solar Mapping",2024-08-26T16:34:13Z,"Vishal Batchu, Alex Wilson, Betty Peng, Carl Elkin, Umangi Jain, Christopher Van Arsdale, Ross Goroshin, Varun Gulshan","The transition to renewable energy, particularly solar, is key to mitigating
climate change. Google's Solar API aids this transition by estimating solar
potential from aerial imagery, but its impact is constrained by geographical
coverage. This paper proposes expanding the API's reach using satellite
imagery, enabling global solar potential assessment. We tackle challenges
involved in building a Digital Surface Model (DSM) and roof instance
segmentation from lower resolution and single oblique views using deep learning
models. Our models, trained on aligned satellite and aerial datasets, produce
25cm DSMs and roof segments. With ~1m DSM MAE on buildings, ~5deg roof pitch
error and ~56% IOU on roof segmentation, they significantly enhance the Solar
API's potential to promote solar adoption.",http://arxiv.org/abs/2408.14400v2
"Efficient fine-tuning of 37-level GraphCast with the Canadian global
  deterministic analysis",2024-08-26T19:16:08Z,Christopher Subich,"This work describes a process for efficiently fine-tuning the GraphCast
data-driven forecast model to simulate another analysis system, here the Global
Deterministic Prediction System (GDPS) of Environment and Climate Change Canada
(ECCC). Using two years of training data (July 2019 -- December 2021) and 37
GPU-days of computation to tune the 37-level, quarter-degree version of
GraphCast, the resulting model significantly outperforms both the unmodified
GraphCast and operational forecast, showing significant forecast skill in the
troposphere over lead times from 1 to 10 days. This fine-tuning is accomplished
through abbreviating DeepMind's original training curriculum for GraphCast,
relying on a shorter single-step forecast stage to accomplish the bulk of the
adaptation work and consolidating the autoregressive stages into separate 12hr,
1d, 2d, and 3d stages with larger learning rates. Additionally, training over
3d forecasts is split into two sub-steps to conserve host memory while
maintaining a strong correlation with training over the full period.",http://arxiv.org/abs/2408.14587v1
"Visions of Destruction: Exploring a Potential of Generative AI in
  Interactive Art",2024-08-26T21:20:45Z,"Mar Canet Sola, Varvara Guljajeva","This paper explores the potential of generative AI within interactive art,
employing a practice-based research approach. It presents the interactive
artwork ""Visions of Destruction"" as a detailed case study, highlighting its
innovative use of generative AI to create a dynamic, audience-responsive
experience. This artwork applies gaze-based interaction to dynamically alter
digital landscapes, symbolizing the impact of human activities on the
environment by generating contemporary collages created with AI, trained on
data about human damage to nature, and guided by audience interaction. The
transformation of pristine natural scenes into human-made and industrialized
landscapes through viewer interaction serves as a stark reminder of
environmental degradation. The paper thoroughly explores the technical
challenges and artistic innovations involved in creating such an interactive
art installation, emphasizing the potential of generative AI to revolutionize
artistic expression, audience engagement, and especially the opportunities for
the interactive art field. It offers insights into the conceptual framework
behind the artwork, aiming to evoke a deeper understanding and reflection on
the Anthropocene era and human-induced climate change. This study contributes
significantly to the field of creative AI and interactive art, blending
technology and environmental consciousness in a compelling, thought-provoking
manner.",http://arxiv.org/abs/2408.14644v1
"Risk-Averse Resilient Operation of Electricity Grid Under the Risk of
  Wildfire",2024-08-28T13:12:28Z,"Muhammad Waseem, Arash F. Soofi, Saeed D. Manshadi","Wildfires and other extreme weather conditions due to climate change are
stressing the aging electrical infrastructure. Power utilities have implemented
public safety power shutoffs as a method to mitigate the risk of wildfire by
proactively de-energizing some power lines, which leaves customers without
power. System operators have to make a compromise between de-energizing of
power lines to avoid the wildfire risk and energizing those lines to serve the
demand. In this work, with a quantified wildfire ignition risk of each line, a
resilient operation problem is presented in power systems with a high
penetration level of renewable generation resources. A two-stage robust
optimization problem is formulated and solved using column-and-constraint
generation algorithm to find improved balance between the de-energization of
power lines and the customers served. Different penetration levels of renewable
generation to mitigate the impact of extreme fire hazard situations on the
energization of customers is assessed. The validity of the presented robust
optimization algorithm is demonstrated on various test cases.",http://arxiv.org/abs/2408.15774v3
"Chaotic uncertainty and statistical inference for natural chaotic
  systems: Choosing predictors for multiple season ahead prediction of
  precipitation, Extended and Annotated",2024-08-16T16:41:47Z,Michael LuValle,"Here we define natural chaotic systems, like the earths weather and climate
system, as chaotic systems which are open to the world so have constantly
changing boundary conditions, and measurements of their states are subject to
errors. In such systems the chaoticity, amplifying error exponentially fast, is
so confounded with the boundary condition fluctuations and the measurement
error, that it is impossible to consistently estimate the trajectory of the
system much less predict it. Although asymptotic theory exists for estimating
the conditional predictive distributions, it is hard to find where this theory
has been applied. Here the theory is reviewed, and applied to identifying
useful predictive variables for simultaneous multiseason prediction of
precipitation with potentially useful updating possible. This is done at two
locations, one midocean the other landlocked. The method appears to show
promise for fast exploration of variables for multiseason prediction.",http://arxiv.org/abs/2409.00023v2
Review of the EU ETS Literature: A Bibliometric Perspective,2024-09-03T09:30:19Z,Cristiano Salvagnin,"This study conducts a bibliometric review of scientific literature on the
European Union Emissions Trading System (EU ETS) from 2004 to 2024, using
research articles from the Scopus database. Using the Bibliometrix R package,
we analyze publication trends, key themes, influential authors, and prominent
journals related to the EU ETS. Our results indicate a notable increase in
research activity over the past two decades, particularly during significant
policy changes and economic events affecting carbon markets. Key research
focuses include carbon pricing, market volatility, and economic impacts,
highlighting a shift toward financial analysis and policy implications.
Thematic mapping shows cap-and-trade systems, and carbon leakage as central
topics linking various research areas. Additionally, we observe key areas where
further research could be beneficial, such as expanding non-parametric
methodologies, deepening the exploration of macroeconomic factors, and
enhancing the examination of financial market connections. Moreover, we
highlight recent and innovative papers that contribute new insights, showcasing
emerging trends and cutting-edge approaches within the field. This review
provides insights for researchers and policymakers, highlighting the evolving
landscape of EU ETS research and its relevance to global climate strategies.",http://arxiv.org/abs/2409.01739v3
"Targeted calibration to adjust stability biases in non-differentiable
  complex system models",2024-09-06T07:14:14Z,"Daniel Pals, Sebastian Bathiany, Richard Wood, Niklas Boers","Numerical models of complex systems like the Earth system are expensive to
run and involve many uncertain and typically hand-tuned parameters. In the
context of anthropogenic climate change, there is particular concern that
specific tipping elements, like the Atlantic Meridional Overturning
Circulation, might be overly stable in models due to imperfect parameter
choices. However, estimates of the critical forcing thresholds are highly
uncertain because the parameter spaces can practically not be explored. Here,
we introduce a method for efficient, systematic, and objective calibration of
process-based models. Our method drives the system toward parameter
configurations where it loses or gains stability, and scales much more
efficiently than a brute force approach. We successfully apply the method to a
simple bistable model and a conceptual but physically plausible model of the
global ocean circulation, demonstrating that our method can help find hidden
tipping points, and can calibrate complex models under user-defined
constraints.",http://arxiv.org/abs/2409.04063v1
"A Framework for Evaluating PM2.5 Forecasts from the Perspective of
  Individual Decision Making",2024-09-09T17:59:54Z,"Renato Berlinghieri, David R. Burt, Paolo Giani, Arlene M. Fiore, Tamara Broderick","Wildfire frequency is increasing as the climate changes, and the resulting
air pollution poses health risks. Just as people routinely use weather
forecasts to plan their activities around precipitation, reliable air quality
forecasts could help individuals reduce their exposure to air pollution. In the
present work, we evaluate several existing forecasts of fine particular matter
(PM2.5) within the continental United States in the context of individual
decision-making. Our comparison suggests there is meaningful room for
improvement in air pollution forecasting, which might be realized by
incorporating more data sources and using machine learning tools. To facilitate
future machine learning development and benchmarking, we set up a framework to
evaluate and compare air pollution forecasts for individual decision making. We
introduce a new loss to capture decisions about when to use mitigation
measures. We highlight the importance of visualizations when comparing
forecasts. Finally, we provide code to download and compare archived forecast
predictions.",http://arxiv.org/abs/2409.05866v1
"Advancing Towards a Marine Digital Twin Platform: Modeling the Mar Menor
  Coastal Lagoon Ecosystem in the South Western Mediterranean",2024-09-16T10:01:18Z,"Yu Ye, Aurora Gonz√°lez-Vidal, Alejandro Cisterna-Garc√≠a, Angel P√©rez-Ruzafa, Miguel A. Zamora Izquierdo, Antonio F. Skarmeta","Coastal marine ecosystems face mounting pressures from anthropogenic
activities and climate change, necessitating advanced monitoring and modeling
approaches for effective management. This paper pioneers the development of a
Marine Digital Twin Platform aimed at modeling the Mar Menor Coastal Lagoon
Ecosystem in the Region of Murcia. The platform leverages Artificial
Intelligence to emulate complex hydrological and ecological models,
facilitating the simulation of what-if scenarios to predict ecosystem responses
to various stressors. We integrate diverse datasets from public sources to
construct a comprehensive digital representation of the lagoon's dynamics. The
platform's modular design enables real-time stakeholder engagement and informed
decision-making in marine management. Our work contributes to the ongoing
discourse on advancing marine science through innovative digital twin
technologies.",http://arxiv.org/abs/2409.10134v1
"Managing Basis Risks in Weather Parametric Insurance: A Quantitative
  Study of Diversification and Key Influencing Factors",2024-09-25T03:53:45Z,"Hang Gao, Shuohua Yang, Xinli Liu","Weather parametric insurance relies on weather indices rather than actual
loss assessments, enhancing claims efficiency, reducing moral hazard, and
improving fairness. In the context of increasing climate change risks, despite
growing interest and demand,, weather parametric insurance's market share
remains limited due to inherent basis risk, which is the mismatch between
actual loss and payout, leading to loss without payout or payout without loss.
This paper proposes a novel empirical research using Monte Carlo simulations to
test whether basis risk can be managed through diversification and hedged like
other risks. Key findings include: Firstly, portfolio basis risk and volatility
decrease as the number of contracts increases. Secondly, spatial relationships
significantly impact basis risk, with risk levels correlating with the ratio
between insured location, weather station, and disaster footprint radius, and
thirdly, event severity does not significantly impact basis risk, suggesting
that catastrophic disaster severity should not hinder parametric insurance
development.",http://arxiv.org/abs/2409.16599v1
"Improving satellite imagery segmentation using multiple Sentinel-2
  revisits",2024-09-25T21:13:33Z,"Kartik Jindgar, Grace W. Lindsay","In recent years, analysis of remote sensing data has benefited immensely from
borrowing techniques from the broader field of computer vision, such as the use
of shared models pre-trained on large and diverse datasets. However, satellite
imagery has unique features that are not accounted for in traditional computer
vision, such as the existence of multiple revisits of the same location. Here,
we explore the best way to use revisits in the framework of fine-tuning
pre-trained remote sensing models. We focus on an applied research question of
relevance to climate change mitigation -- power substation segmentation -- that
is representative of applied uses of pre-trained models more generally. Through
extensive tests of different multi-temporal input schemes across diverse model
architectures, we find that fusing representations from multiple revisits in
the model latent space is superior to other methods of using revisits,
including as a form of data augmentation. We also find that a SWIN
Transformer-based architecture performs better than U-nets and ViT-based
models. We verify the generality of our results on a separate building density
estimation task.",http://arxiv.org/abs/2409.17363v2
"Multimodal Power Outage Prediction for Rapid Disaster Response and
  Resource Allocation",2024-09-14T21:35:29Z,"Alejandro Aparcedo, Christian Lopez, Abhinav Kotta, Mengjie Li","Extreme weather events are increasingly common due to climate change, posing
significant risks. To mitigate further damage, a shift towards renewable energy
is imperative. Unfortunately, underrepresented communities that are most
affected often receive infrastructure improvements last. We propose a novel
visual spatiotemporal framework for predicting nighttime lights (NTL), power
outage severity and location before and after major hurricanes. Central to our
solution is the Visual-Spatiotemporal Graph Neural Network (VST-GNN), to learn
spatial and temporal coherence from images. Our work brings awareness to
underrepresented areas in urgent need of enhanced energy infrastructure, such
as future photovoltaic (PV) deployment. By identifying the severity and
localization of power outages, our initiative aims to raise awareness and
prompt action from policymakers and community stakeholders. Ultimately, this
effort seeks to empower regions with vulnerable energy infrastructure,
enhancing resilience and reliability for at-risk communities.",http://arxiv.org/abs/2410.00017v1
"Development of the Complex Nexus of Socio-Techno-Economic-Environmental
  Parametric (STEEP) Metrics for Evaluating Coal-to-Clean Energy Transitions",2024-10-05T16:04:09Z,"Muhammad R. Abdussami, Aditi Verma","Transitioning from coal to clean energy, such as nuclear and renewables, is
essential for mitigating climate change, improving air quality, and ensuring
sustainable energy security. Reducing reliance on coal lowers greenhouse gas
emissions and pollution, which enhances public health and economic growth
through renewable energy investments. Clean energy also fosters energy
independence and long-term sustainability. This paper presents a Complex Nexus
of Socio-Techno-Economic-Environmental Parametric (STEEP) Metrics to
systematically evaluate and guide these transitions, facilitating informed
decision-making and optimizing resource allocation. The methodology is
classified into three approaches: optimal site selection using a multi-criteria
decision-making framework that ranks coal plant sites based on societal,
technical, economic, and environmental criteria; long-term planning evaluation
through performance indicators comparing Greenfield, Coal-to-Nuclear (C2N), and
Coal-to-Integrated Energy Systems (C2IES); and short-term operational benefits
assessment using the Unit Commitment Economic Dispatch (UCED) model to optimize
generator scheduling and minimize costs across various scenarios. This
framework enables practical analysis of coal-to-clean transitions, identifying
the best strategies for implementation.",http://arxiv.org/abs/2410.04212v1
A Hard-Science Approach to Kondratieff's Economic Cycle,2024-10-01T11:32:30Z,Theodore Modis,"In an effort to evidence the Kondratieff cycle more scientifically than the
way economists do, physical variables are studied rather than monetary
indicators. Previously published graphs are reproduced and updated here with
recent data. A cyclical rather regular variation of energy consumption reveals
a 56-year cycle. A dozen human endeavors/phenomena, such as bank failures,
homicides, hurricanes, feminism, and sunspot activity are shown to resonate
with this cycle. Possible explanations for this phenomenon may have to do with
a climatic variation or with the length of time any individual actively
influences the environment. There is some evidence that the cycle may be
getting shorter in amplitude and duration in recent years. All quantitative
confidence levels involved in these observations are poor by scientific
standards and permit critics to question the very existence of this phenomenon.",http://arxiv.org/abs/2410.05285v1
Multi-Source Temporal Attention Network for Precipitation Nowcasting,2024-10-11T09:09:07Z,"Rafael Pablos Sarabia, Joachim Nyborg, Morten Birk, Jeppe Liborius Sj√∏rup, Anders Lillevang Vesterholt, Ira Assent","Precipitation nowcasting is crucial across various industries and plays a
significant role in mitigating and adapting to climate change. We introduce an
efficient deep learning model for precipitation nowcasting, capable of
predicting rainfall up to 8 hours in advance with greater accuracy than
existing operational physics-based and extrapolation-based models. Our model
leverages multi-source meteorological data and physics-based forecasts to
deliver high-resolution predictions in both time and space. It captures complex
spatio-temporal dynamics through temporal attention networks and is optimized
using data quality maps and dynamic thresholds. Experiments demonstrate that
our model outperforms state-of-the-art, and highlight its potential for fast
reliable responses to evolving weather conditions.",http://arxiv.org/abs/2410.08641v2
"Scito2M: A 2 Million, 30-Year Cross-disciplinary Dataset for Temporal
  Scientometric Analysis",2024-10-12T12:16:57Z,"Yiqiao Jin, Yijia Xiao, Yiyang Wang, Jindong Wang","Understanding the creation, evolution, and dissemination of scientific
knowledge is crucial for bridging diverse subject areas and addressing complex
global challenges such as pandemics, climate change, and ethical AI.
Scientometrics, the quantitative and qualitative study of scientific
literature, provides valuable insights into these processes. We introduce
Scito2M, a longitudinal scientometric dataset with over two million academic
publications, providing comprehensive contents information and citation graphs
to support cross-disciplinary analyses. Using Scito2M, we conduct a temporal
study spanning over 30 years to explore key questions in scientometrics: the
evolution of academic terminology, citation patterns, and interdisciplinary
knowledge exchange. Our findings reveal critical insights, such as disparities
in epistemic cultures, knowledge production modes, and citation practices. For
example, rapidly developing, application-driven fields like LLMs exhibit
significantly shorter citation age (2.48 years) compared to traditional
theoretical disciplines like oral history (9.71 years).",http://arxiv.org/abs/2410.09510v1
Cybersecurity in Industry 5.0: Open Challenges and Future Directions,2024-10-12T13:56:17Z,"Bruno Santos, Rog√©rio Lu√≠s C. Costa, Leonel Santos","Unlocking the potential of Industry 5.0 hinges on robust cybersecurity
measures. This new Industrial Revolution prioritises human-centric values while
addressing pressing societal issues such as resource conservation, climate
change, and social stability. Recognising the heightened risk of cyberattacks
due to the new enabling technologies in Industry 5.0, this paper analyses
potential threats and corresponding countermeasures. Furthermore, it evaluates
the existing industrial implementation frameworks, which reveals their
inadequacy in ensuring a secure transition from Industry 4.0 to Industry 5.0.
Consequently, the paper underscores the necessity of developing a new framework
centred on cybersecurity to facilitate organisations' secure adoption of
Industry 5.0 principles. The creation of such a framework is emphasised as a
necessity for organisations.",http://arxiv.org/abs/2410.09538v1
"Learning from the past: predicting critical transitions with machine
  learning trained on surrogates of historical data",2024-10-13T03:25:49Z,"Zhiqin Ma, Chunhua Zeng, Yi-Cheng Zhang, Thomas M. Bury","Complex systems can undergo critical transitions, where slowly changing
environmental conditions trigger a sudden shift to a new, potentially
catastrophic state. Early warning signals for these events are crucial for
decision-making in fields such as ecology, biology and climate science. Generic
early warning signals motivated by dynamical systems theory have had mixed
success on real noisy data. More recent studies found that deep learning
classifiers trained on synthetic data could improve performance. However,
neither of these methods take advantage of historical, system-specific data.
Here, we introduce an approach that trains machine learning classifiers
directly on surrogate data of past transitions, namely surrogate data-based
machine learning (SDML). The approach provides early warning signals in
empirical and experimental data from geology, climatology, sociology, and
cardiology with higher sensitivity and specificity than two widely used generic
early warning signals -- variance and lag-1 autocorrelation. Since the approach
is trained directly on surrogates of historical data, it is not bound by the
restricting assumption of a local bifurcation like previous methods. This
system-specific approach can contribute to improved early warning signals to
help humans better prepare for or avoid undesirable critical transitions.",http://arxiv.org/abs/2410.09707v1
"Improving accuracy and convergence of federated learning edge computing
  methods for generalized DER forecasting applications in power grid",2024-10-13T21:34:00Z,"Vineet Jagadeesan Nair, Lucas Pereira","This proposal aims to develop more accurate federated learning (FL) methods
with faster convergence properties and lower communication requirements,
specifically for forecasting distributed energy resources (DER) such as
renewables, energy storage, and loads in modern, low-carbon power grids. This
will be achieved by (i) leveraging recently developed extensions of FL such as
hierarchical and iterative clustering to improve performance with non-IID data,
(ii) experimenting with different types of FL global models well-suited to
time-series data, and (iii) incorporating domain-specific knowledge from power
systems to build more general FL frameworks and architectures that can be
applied to diverse types of DERs beyond just load forecasting, and with
heterogeneous clients.",http://arxiv.org/abs/2410.10018v1
Time Series Viewmakers for Robust Disruption Prediction,2024-10-14T20:23:43Z,"Dhruva Chayapathy, Tavis Siebert, Lucas Spangher, Akshata Kishore Moharir, Om Manoj Patil, Cristina Rea","Machine Learning guided data augmentation may support the development of
technologies in the physical sciences, such as nuclear fusion tokamaks. Here we
endeavor to study the problem of detecting disruptions i.e. plasma
instabilities that can cause significant damages, impairing the reliability and
efficiency required for their real world viability. Machine learning (ML)
prediction models have shown promise in detecting disruptions for specific
tokamaks, but they often struggle in generalizing to the diverse
characteristics and dynamics of different machines. This limits the
effectiveness of ML models across different tokamak designs and operating
conditions, which is a critical barrier to scaling fusion technology. Given the
success of data augmentation in improving model robustness and generalizability
in other fields, this study explores the use of a novel time series viewmaker
network to generate diverse augmentations or ""views"" of training data. Our
results show that incorporating views during training improves AUC and F2
scores on DisruptionBench tasks compared to standard or no augmentations. This
approach represents a promising step towards developing more broadly applicable
ML models for disruption avoidance, which is essential for advancing fusion
technology and, ultimately, addressing climate change through reliable and
sustainable energy production.",http://arxiv.org/abs/2410.11065v1
"Challenges, Methods, Data -- a Survey of Machine Learning in Water
  Distribution Networks",2024-10-16T11:21:07Z,"Valerie Vaquet, Fabian Hinder, Andr√© Artelt, Inaam Ashraf, Janine Strotherm, Jonas Vaquet, Johannes Brinkrolf, Barbara Hammer","Research on methods for planning and controlling water distribution networks
gains increasing relevance as the availability of drinking water will decrease
as a consequence of climate change. So far, the majority of approaches is based
on hydraulics and engineering expertise. However, with the increasing
availability of sensors, machine learning techniques constitute a promising
tool. This work presents the main tasks in water distribution networks,
discusses how they relate to machine learning and analyses how the
particularities of the domain pose challenges to and can be leveraged by
machine learning approaches. Besides, it provides a technical toolkit by
presenting evaluation benchmarks and a structured survey of the exemplary task
of leakage detection and localization.",http://arxiv.org/abs/2410.12461v1
"An Exploration of Modeling Approaches for Capturing Seasonal
  Transmission in Stochastic Epidemic Models",2024-10-22T03:34:28Z,Mahmudul Bari Hridoy,"Seasonal variations in the incidence of infectious diseases are a
well-established phenomenon, driven by factors such as climate changes, social
behaviors, and ecological interactions that influence host susceptibility and
transmission rates. While seasonality plays a significant role in shaping
epidemiological dynamics, it is often overlooked in both empirical and
theoretical studies. Incorporating seasonal parameters into mathematical models
of infectious diseases is crucial for accurately capturing disease dynamics,
enhancing the predictive power of these models, and developing successful
control strategies. This paper highlights key modeling approaches for
incorporating seasonality into disease transmission, including sinusoidal
functions, periodic piecewise linear functions, Fourier series expansions,
Gaussian functions, and data-driven methods, accompanied by real-world
examples. Additionally, a stochastic Susceptible-Infected-Recovered (SIR) model
with seasonal transmission is demonstrated through numerical simulations.
Important outcome measures, such as the basic and instantaneous reproduction
numbers and the probability of a disease outbreak using branching process
approximation of the Markov chain, are also presented to illustrate the impact
of seasonality on disease dynamics.",http://arxiv.org/abs/2410.16664v1
ICT Sector Greenhouse Gas Emissions -- Issues and Trends,2024-10-22T19:51:37Z,"Peter Garraghan, John Hutchinson, Adrian Friday","As Information and Communication Technology (ICT) use has become more
prevalent, there has been a growing concern in how its associated greenhouse
gas emissions will impact the climate. Estimating such ICT emissions is a
difficult undertaking due to its complexity, its rapidly changing nature, and
the lack of accurate and up-to-date data on individual stakeholder emissions.
In this paper we provide a framework for estimating ICT's carbon footprint and
identify some of the issues that impede the task. We attempt to gain greater
insight into the factors affecting the ICT sector by drawing on a number of
interviews with industry experts. We conclude that more accurate emissions
estimates will only be possible with a more more detailed, industry informed,
understanding of the whole ICT landscape and much more transparent reporting of
energy usage and emissions data by ICT stakeholders.",http://arxiv.org/abs/2410.17388v1
Language-Agnostic Modeling of Source Reliability on Wikipedia,2024-10-24T14:52:21Z,"Jacopo D'Ignazi, Andreas Kaltenbrunner, Yelena Mejova, Michele Tizzani, Kyriaki Kalimeri, Mariano Beir√≥, Pablo Arag√≥n","Over the last few years, content verification through reliable sources has
become a fundamental need to combat disinformation. Here, we present a
language-agnostic model designed to assess the reliability of sources across
multiple language editions of Wikipedia. Utilizing editorial activity data, the
model evaluates source reliability within different articles of varying
controversiality such as Climate Change, COVID-19, History, Media, and Biology
topics. Crafting features that express domain usage across articles, the model
effectively predicts source reliability, achieving an F1 Macro score of
approximately 0.80 for English and other high-resource languages. For
mid-resource languages, we achieve 0.65 while the performance of low-resource
languages varies; in all cases, the time the domain remains present in the
articles (which we dub as permanence) is one of the most predictive features.
We highlight the challenge of maintaining consistent model performance across
languages of varying resource levels and demonstrate that adapting models from
higher-resource languages can improve performance. This work contributes not
only to Wikipedia's efforts in ensuring content verifiability but in ensuring
reliability across diverse user-generated content in various language
communities.",http://arxiv.org/abs/2410.18803v2
"Information Sharing with Social Image Concerns and the Spread of Fake
  News",2024-10-25T13:37:40Z,"Dana Sisak, Philipp Denter","We study how social image concerns affect information sharing patterns
between peers. An individual receives a signal (""news"") about the state of the
world and can either share it with a peer or not. This signal has two
attributes: a headline -- e.g., arguing for or against human-induced climate
change -- and a veracity status, indicating if the signal is based on facts or
made-up. The headline is observable at no cost by everyone, while observing the
veracity status is costly and the cost depends on an individual's type. We
study the sharing patterns induced by two different types of social image
concern: wanting to be perceived as talented, which implies being able to
distinguish proper from fake news, and wanting to signal one's worldview. Our
model can rationalize the empirical finding that fake news may be shared with a
higher propensity than proper news (e.g., Vosoughi et al., 2018). We show that
both a veracity and a worldview concern may rationalize this finding, though
sharing patterns are empirically distinguishable and welfare implications
differ.",http://arxiv.org/abs/2410.19557v2
Forecasting the Mix of World Energy Needs by mid-21st Century,2024-10-24T09:13:08Z,Theodore Modis,"The logistic function is used to forecast energy consumed worldwide. The
logistic substitution model is used to describe the energy mix since 1965
presenting a picture significantly different from the one covering the previous
100 years. In the new picture the share of heavy pollutants, i.e. coal plus
oil, keeps declining systematically in favor of natural gas and renewables
(wind, geothermal, solar, biomass, and waste), the share of which grows
rapidly. The shares of these three energy sources (coal plus oil, natural gas,
and renewables) are poised to reach around 30 percent each by the mid-21st
century. Nuclear and hydroelectric energy, both with rather stable shares, are
responsible for the remaining 10 percent, which goes mostly to hydroelectric.
  Zooming into the composition of renewables we find that today's dominant wind
power is about to begin losing share to solar energy, which will overtake wind
after 2024 and account for more than 90 percent of all renewables by mid-21st
century, by which time geothermal, biomass, and other renewable energy sources
will have dropped to insignificant levels.
  Forecasts in exajouls are given for all energy sources up to 2050.",http://arxiv.org/abs/2410.19873v1
"Shining a Light on Hurricane Damage Estimation via Nighttime Light Data:
  Pre-processing Matters",2024-10-29T15:48:55Z,"Nancy Thomas, Saba Rahimi, Annita Vapsi, Cathy Ansell, Elizabeth Christie, Daniel Borrajo, Tucker Balch, Manuela Veloso","Amidst escalating climate change, hurricanes are inflicting severe
socioeconomic impacts, marked by heightened economic losses and increased
displacement. Previous research utilized nighttime light data to predict the
impact of hurricanes on economic losses. However, prior work did not provide a
thorough analysis of the impact of combining different techniques for
pre-processing nighttime light (NTL) data. Addressing this gap, our research
explores a variety of NTL pre-processing techniques, including value
thresholding, built masking, and quality filtering and imputation, applied to
two distinct datasets, VSC-NTL and VNP46A2, at the zip code level. Experiments
evaluate the correlation of the denoised NTL data with economic damages of
Category 4-5 hurricanes in Florida. They reveal that the quality masking and
imputation technique applied to VNP46A2 show a substantial correlation with
economic damage data.",http://arxiv.org/abs/2410.22150v1
"Community-Based Resilience: Digital Technologies for Living within
  Planetary Boundaries",2024-11-01T15:41:27Z,"Catherine Mulligan, Giaime Berti, Seema Gadh Kumar","The world faces increasing challenges related to climate change and shifting
geopolitical situations. The COVID 19 pandemic clearly illustrated the need for
resilience not just within civil engineering but also within social and
economic systems. While significant work has focused on applying digital
technologies to solve the Sustainable Development Goals (SDGs), less effort has
been placed on the ability of digital technologies to enable humanity to
continue to live effectively within the Planetary Boundaries. Within this
paper, we perform a Systematic Literature Review using the Preferred Reporting
Items for Systematic Reviews and Meta Analyses (PRISMA) methodology. Nine
hundred and twenty papers were reviewed. Based on the Systematic Literature
Review (SLR), we have outlined specific methods to apply digital technologies
to enable humanity to live within PB. Critical capabilities provided by digital
technologies, specifically the ability to create dynamic networks for enabling
humanity to live within PB, are identified and illustrated in food and
agriculture. The paper closes with a brief assessment of the concept's
applicability to other Critical National Infrastructure.",http://arxiv.org/abs/2411.00682v1
Mapping Global Floods with 10 Years of Satellite Radar Data,2024-11-03T02:44:32Z,"Amit Misra, Kevin White, Simone Fobi Nsutezo, William Straka, Juan Lavista","Floods cause extensive global damage annually, making effective monitoring
essential. While satellite observations have proven invaluable for flood
detection and tracking, comprehensive global flood datasets spanning extended
time periods remain scarce. In this study, we introduce a novel deep learning
flood detection model that leverages the cloud-penetrating capabilities of
Sentinel-1 Synthetic Aperture Radar (SAR) satellite imagery, enabling
consistent flood extent mapping in any weather condition. By applying this
model to nearly 10 years of SAR data, we create a unique, longitudinal global
flood extent dataset with predictions unaffected by cloud coverage, offering
comprehensive and consistent insights into historically flood-prone areas over
the past decade. We use our model predictions to identify historically
flood-prone areas in Ethiopia and demonstrate real-time disaster response
capabilities during the May 2024 floods in Kenya. Additionally, our
longitudinal analysis reveals potential increasing trends in global flood
extent over time, although further validation is required to explore links to
climate change. To maximize impact, we provide public access to both our model
predictions and a code repository, empowering researchers and practitioners
worldwide to advance flood monitoring and enhance disaster response strategies.",http://arxiv.org/abs/2411.01411v1
First observations of the seiche that shook the world,2024-11-04T15:05:35Z,"Thomas Monahan, Tianning Tang, Stephen Roberts, Thomas A. A. Adcock","On September 16th, 2023, an anomalous 10.88 mHz seismic signal was observed
globally, persisting for 9 days. One month later an identical signal appeared,
lasting for another week. Several studies have theorized that these signals
were produced by seiches which formed after two landslide generated
mega-tsunamis in an East-Greenland fjord. This theory is supported by seismic
inversions, and analytical and numerical modeling, but no direct observations
have been made -- until now. Using data from the new Surface Water Ocean
Topography mission, we present the first observations of this phenomenon. By
ruling out other oceanographic processes, we validate the seiche theory of
previous authors and independently estimate its initial amplitude at 7.9 m
using Bayesian machine learning and seismic data. This study demonstrates the
value of satellite altimetry for studying extreme events, while also
highlighting the need for specialized methods to address the altimetric data's
limitations, namely temporal sparsity. These data and approaches will help in
understanding future unseen extremes driven by climate change.",http://arxiv.org/abs/2411.02469v1
"Towards more efficient agricultural practices via transformer-based crop
  type classification",2024-11-04T21:38:02Z,"E. Ulises Moya-S√°nchez, Yazid S. Mikail, Daisy Nyang'anyi, Michael J. Smith, Isabella Smythe","Machine learning has great potential to increase crop production and
resilience to climate change. Accurate maps of where crops are grown are a key
input to a number of downstream policy and research applications. In this
proposal, we present preliminary work showing that it is possible to accurately
classify crops from time series derived from Sentinel 1 and 2 satellite imagery
in Mexico using a pixel-based binary crop/non-crop time series transformer
model. We also find preliminary evidence that meta-learning approaches
supplemented with data from similar agro-ecological zones may improve model
performance. Due to these promising results, we propose further development of
this method with the goal of accurate multi-class crop classification in
Jalisco, Mexico via meta-learning with a dataset comprising similar
agro-ecological zones.",http://arxiv.org/abs/2411.02627v1
"Novel Simulation Framework for Analyzing Cosmic Ray Particle
  Distributions at a Global Scale",2024-11-05T14:32:44Z,"Olesya Sarajlic, Xiaochun He","Cosmic ray measurements have inspired numerous interesting applications over
several decades worldwide. These applications encompass non-invasive cosmic ray
muon tomography, which enables the imaging of concealed dense objects or
structures, the monitoring of area-averaged soil moisture with cosmic ray
neutrons in agriculture and climate studies, real-time monitoring of the
dynamical changes of the space and earth weather, etc. The demand for a
quantitative characterization of cosmic ray shower particles near the Earth's
surface is substantial, as it provides realistic particle spectra and rates for
these diverse applications. In this study, we introduce Earth Cosmic Ray Shower
(ECRS), a GEANT4-based software designed to simulate cosmic ray particle
interactions in the atmosphere. ECRS incorporates the U.S. Standard Atmospheric
Model and integrates a time-dependent geomagnetic field based on the Tsyganenko
and IGRF models. Additionally, we present two case studies illustrating
variations in the location-dependent average particle energy for muons,
electrons, neutrons, and gammas at sea level. An outlook of this project is
provided toward the conclusion.",http://arxiv.org/abs/2411.03142v1
"A Machine Learning Approach for the Efficient Estimation of Ground-Level
  Air Temperature in Urban Areas",2024-11-05T15:05:23Z,"I√±igo Delgado-Enales, Joshua Lizundia-Loiola, Patricia Molina-Costa, Javier Del Ser","The increasingly populated cities of the 21st Century face the challenge of
being sustainable and resilient spaces for their inhabitants. However, climate
change, among other problems, makes these objectives difficult to achieve. The
Urban Heat Island (UHI) phenomenon that occurs in cities, increasing their
thermal stress, is one of the stumbling blocks to achieve a more sustainable
city. The ability to estimate temperatures with a high degree of accuracy
allows for the identification of the highest priority areas in cities where
urban improvements need to be made to reduce thermal discomfort. In this work
we explore the usefulness of image-to-image deep neural networks (DNNs) for
correlating spatial and meteorological variables of a urban area with
street-level air temperature. The air temperature at street-level is estimated
both spatially and temporally for a specific use case, and compared with
existing, well-established numerical models. Based on the obtained results,
deep neural networks are confirmed to be faster and less computationally
expensive alternative for ground-level air temperature compared to numerical
models.",http://arxiv.org/abs/2411.03162v1
Sustainability and Carbon Emissions of Future Accelerators,2024-11-05T19:56:36Z,"Kenneth Bloom, V√©ronique Boisvert","Future accelerators and experiments for energy-frontier particle physics will
be built and operated during a period in which society must also address the
climate change emergency by significantly reducing emissions of carbon dioxide.
The carbon intensity of many particle-physics activities is potentially
significant, such that as a community particle physicists could create
substantially more emissions than average citizens, which is currently more
than budgeted to limit the increase in average global temperatures. We estimate
the carbon impacts of potential future accelerators, detectors, computing, and
travel, and find that while emissions from civil construction dominate by far,
some other activities make noticeable contributions. We discuss potential
mitigation strategies, and research and development activities that can be
pursued to make particle physics more sustainable.",http://arxiv.org/abs/2411.03473v3
"The New Dynamics of Open Source: Relicensing, Forks, & Community Impact",2024-11-07T14:21:45Z,Dawn Foster,"Many popular open source projects are owned and driven by vendors, and in
today's difficult economic climate, those vendors are under increasing pressure
from investors to deliver a strong return on their investments. One response to
this pressure has been the relicensing of popular open source projects to more
restrictive licenses in the hopes of generating more revenue, disrupting the
idea of open source as a digital commons. In some cases, relicensing has
resulted in a hard fork of the original project. These relicensing events and
resulting forks can be disruptive to the organizations and individuals using
these open source projects. This research compares and contrasts organizational
affiliation data from three case studies based on license changes that resulted
in forks: Elasticsearch / OpenSearch, Redis / Valkey, and Terraform / OpenTofu.
The research indicates that the forks resulting from these relicensing events
have more organizational diversity than the original projects, especially when
the forks are created under a neutral foundation, like the Linux Foundation,
rather than by a single company.",http://arxiv.org/abs/2411.04739v1
"Thermodynamic consistency and structure-preservation in summation by
  parts methods for the moist compressible Euler equations",2024-11-12T05:33:52Z,"Kieran Ricardo, David Lee, Kenneth Duru","Moist thermodynamics is a fundamental driver of atmospheric dynamics across
all scales, making accurate modeling of these processes essential for reliable
weather forecasts and climate change projections. However, atmospheric models
often make a variety of inconsistent approximations in representing moist
thermodynamics. These inconsistencies can introduce spurious sources and sinks
of energy, potentially compromising the integrity of the models.
  Here, we present a thermodynamically consistent and structure preserving
formulation of the moist compressible Euler equations. When discretised with a
summation by parts method, our spatial discretisation conserves: mass, water,
entropy, and energy. These properties are achieved by discretising a skew
symmetric form of the moist compressible Euler equations, using entropy as a
prognostic variable, and the summation-by-parts property of discrete derivative
operators. Additionally, we derive a discontinuous Galerkin spectral element
method with energy and tracer variance stable numerical fluxes, and
experimentally verify our theoretical results through numerical simulations.",http://arxiv.org/abs/2411.07562v2
"Uncertainty Quantification of Fluid Leakage and Fault Instability in
  Geologic CO2 Storage",2024-10-26T04:11:55Z,"Hannah Lu, Lluis Salo-Salgado, Youssef M. Marzouk, Ruben Juanes","Geologic CO$_2$ storage is an important strategy for reducing greenhouse gas
emissions to the atmosphere and mitigating climate change. In this process,
coupling between mechanical deformation and fluid flow in fault zones is a key
determinant of fault instability, induced seismicity, and CO$_2$ leakage. Using
a recently developed methodology, PREDICT, we obtain probability distributions
of the permeability tensor in faults from the stochastic placement of clay
smears that accounts for geologic uncertainty. We build a comprehensive set of
fault permeability scenarios from PREDICT and investigate the effects of
uncertainties from the fault zone internal structure and composition on
forecasts of CO$_2$ permanence and fault stability. To tackle the prohibitively
expensive computational cost of the large number of simulations required to
quantify uncertainty, we develop a deep-learning-based surrogate model capable
of predicting flow migration, pressure buildup, and geomechanical responses in
CO$_2$ storage operations. We also compare our probabilistic estimation of
CO$_2$ leakage and fault instability with previous studies based on
deterministic estimates of fault permeability. The results highlight the
importance of including uncertainty and anisotropy in modeling of complex fault
structures and improved management of geologic CO$_2$ storage projects.",http://arxiv.org/abs/2411.08039v1
"Leveraging LLMs for Predictive Insights in Food Policy and Behavioral
  Interventions",2024-11-13T12:21:13Z,"Micha Kaiser, Paul Lohmann, Peter Ochieng, Billy Shi, Cass R. Sunstein, Lucia A. Reisch","Food consumption and production contribute significantly to global greenhouse
gas emissions, making them crucial entry points for mitigating climate change
and maintaining a liveable planet. Over the past two decades, food policy
initiatives have explored interventions to reshape production and consumption
patterns, focusing on reducing food waste and curbing ruminant meat
consumption. While the evidence of ""what works"" improves, evaluating which
policies are appropriate and effective in specific contexts remains difficult
due to external validity challenges. This paper demonstrates that a fine-tuned
large language model (LLM) can accurately predict the direction of outcomes in
approximately 80\% of empirical studies measuring dietary-based impacts (e.g.
food choices, sales, waste) resulting from behavioral interventions and
policies. Approximately 75 prompts were required to achieve optimal results,
with performance showing signs of catastrophic loss beyond this point. Our
findings indicate that greater input detail enhances predictive accuracy,
although the model still faces challenges with unseen studies, underscoring the
importance of a representative training sample. As LLMs continue to improve and
diversify, they hold promise for advancing data-driven, evidence-based
policymaking.",http://arxiv.org/abs/2411.08563v1
"Identifying companies and financial actors exposed to marine tipping
  points",2024-11-15T16:03:33Z,"Juan C. Rocha, Jean-Baptiste Jouffray, Frida Bengtsson, Bianca-Ioana Voicu, Paula A. S√°nchez, Victor Galaz","Climate change and other anthropogenic pressures are likely to induce tipping
points in marine ecosystems, potentially leading to declines in primary
productivity and fisheries. Despite increasing attention to nature-related
financial risks and opportunities within the ocean economy, the extent to which
these tipping points could affect investors has remained largely unexplored.
Here we used satellite data to track fishing vessels operating in areas prone
to marine regime shifts, as identified by their loss of resilience and
vulnerability to marine heatwaves, and uncovered their corporate beneficial
owners and shareholders. Despite some data gaps, we identified key countries,
companies, and shareholders exposed to tipping risk. We also outline the
potential challenges and opportunities that these actors may face if marine
ecosystems shift to less productive states.",http://arxiv.org/abs/2411.10307v1
Generating Energy-efficient code with LLMs,2024-11-15T21:45:58Z,"Tom Cappendijk, Pepijn de Reus, Ana Oprescu","The increasing electricity demands of personal computers, communication
networks, and data centers contribute to higher atmospheric greenhouse gas
emissions, which in turn lead to global warming and climate change. Therefore
the energy consumption of code must be minimized. Code can be generated by
large language models. We look at the influence of prompt modification on the
energy consumption of the code generated. We use three different Python code
problems of varying difficulty levels. Prompt modification is done by adding
the sentence ``Give me an energy-optimized solution for this problem'' or by
using two Python coding best practices. The large language models used are
CodeLlama-70b, CodeLlama-70b-Instruct, CodeLlama-70b-Python,
DeepSeek-Coder-33b-base, and DeepSeek-Coder-33b-instruct. We find a decrease in
energy consumption for a specific combination of prompt optimization, LLM, and
Python code problem. However, no single optimization prompt consistently
decreases energy consumption for the same LLM across the different Python code
problems.",http://arxiv.org/abs/2411.10599v1
"Flood Risk Assessment of the National Harbor at Maryland, United States",2024-11-17T09:33:15Z,"Neftalem Negussie, Addis Yesserie, Chinchu Harris, Abou Keita, Huthaifa I. Ashqar","Over the past few decades, floods have become one of the costliest natural
hazards and losses have sharply escalated. Floods are an increasing problem in
urban areas due to increased residential settlement along the coastline and
climate change is a contributing factor to this increased frequency. In order
to analyze flood risk, a model is proposed to identify the factors associated
with increased flooding at a local scale. The study area includes National
Harbor, MD, and the surrounding area of Fort Washington. The objective is to
assess flood risk due to an increase in sea level rise for the study area of
interest. The study demonstrated that coastal flood risk increased with sea
level rise even though the predicted level of impact is fairly insignificant
for the study area. The level of impact from increased flooding is highly
dependent on the location of the properties and other topographic information.",http://arxiv.org/abs/2411.11014v1
"ACE2: Accurately learning subseasonal to decadal atmospheric variability
  and forced responses",2024-11-18T03:57:07Z,"Oliver Watt-Meyer, Brian Henn, Jeremy McGibbon, Spencer K. Clark, Anna Kwa, W. Andre Perkins, Elynn Wu, Lucas Harris, Christopher S. Bretherton","Existing machine learning models of weather variability are not formulated to
enable assessment of their response to varying external boundary conditions
such as sea surface temperature and greenhouse gases. Here we present ACE2 (Ai2
Climate Emulator version 2) and its application to reproducing atmospheric
variability over the past 80 years on timescales from days to decades. ACE2 is
a 450M-parameter autoregressive machine learning emulator, operating with
6-hour temporal resolution, 1{\deg} horizontal resolution and eight vertical
layers. It exactly conserves global dry air mass and moisture and can be
stepped forward stably for arbitrarily many steps with a throughput of about
1500 simulated years per wall clock day. ACE2 generates emergent phenomena such
as tropical cyclones, the Madden Julian Oscillation, and sudden stratospheric
warmings. Furthermore, it accurately reproduces the atmospheric response to El
Ni\~no variability and global trends of temperature over the past 80 years.
However, its sensitivities to separately changing sea surface temperature and
carbon dioxide are not entirely realistic.",http://arxiv.org/abs/2411.11268v1
"Is Locational Marginal Price All You Need for Locational Marginal
  Emission?",2024-11-18T22:32:06Z,"Xuan He, Danny H. K. Tsang, Yize Chen","Growing concerns over climate change call for improved techniques for
estimating and quantifying the greenhouse gas emissions associated with
electricity generation and transmission. Among the emission metrics designated
for power grids, locational marginal emission (LME) can provide system
operators and electricity market participants with valuable information on the
emissions associated with electricity usage at various locations in the power
network. In this paper, by investigating the operating patterns and physical
interpretations of marginal emissions and costs in the security-constrained
economic dispatch (SCED) problem, we identify and draw the exact connection
between locational marginal price (LMP) and LME. Such interpretation helps
instantly derive LME given nodal demand vectors or LMP, and also reveals the
interplay between network congestion and nodal emission pattern. Our proposed
approach helps reduce the computation time of LME by an order of magnitude
compared to analytical approaches, while it can also serve as a plug-and-play
module accompanied by an off-the-shelf market clearing and LMP calculation
process.",http://arxiv.org/abs/2411.12104v1
"Learning Pore-scale Multi-phase Flow from Experimental Data with Graph
  Neural Network",2024-11-21T15:01:17Z,"Yuxuan Gu, Catherine Spurin, Gege Wen","Understanding the process of multiphase fluid flow through porous media is
crucial for many climate change mitigation technologies, including CO$_2$
geological storage, hydrogen storage, and fuel cells. However, current
numerical models are often incapable of accurately capturing the complex
pore-scale physics observed in experiments. In this study, we address this
challenge using a graph neural network-based approach and directly learn
pore-scale fluid flow using micro-CT experimental data. We propose a
Long-Short-Edge MeshGraphNet (LSE-MGN) that predicts the state of each node in
the pore space at each time step. During inference, given an initial state, the
model can autoregressively predict the evolution of the multiphase flow process
over time. This approach successfully captures the physics from the
high-resolution experimental data while maintaining computational efficiency,
providing a promising direction for accurate and efficient pore-scale modeling
of complex multiphase fluid flow dynamics.",http://arxiv.org/abs/2411.14192v1
"Windstorm Economic Impacts on the Spanish Resilience: A Machine Learning
  Real-Data Approach",2024-11-05T10:26:05Z,"Matheus Puime Pedra, Josune Hernantes, Leire Casals, Leire Labaka","Climate change-associated disasters have become a significant concern,
principally when affecting urban areas. Assessing these regions' resilience to
strengthen their disaster management is crucial, especially in the areas
vulnerable to windstorms, one of Spain's most critical disasters. Smart cities
and machine learning offer promising solutions to manage disasters, but
accurately estimating economic losses from windstorms can be difficult due to
the unique characteristics of each region and limited data. This study proposes
utilizing ML classification models to enhance disaster resilience by analyzing
publicly available data on windstorms in the Spanish areas. This approach can
help decision-makers make informed decisions regarding preparedness and
mitigation actions, ultimately creating a more resilient urban environment that
can better withstand windstorms in the future.",http://arxiv.org/abs/2411.14439v1
HVAC-DPT: A Decision Pretrained Transformer for HVAC Control,2024-11-29T14:46:37Z,Ana√Øs Berkes,"Building operations consume approximately 40% of global energy, with Heating,
Ventilation, and Air Conditioning (HVAC) systems responsible for up to 50% of
this consumption. As HVAC energy demands are expected to rise, optimising
system efficiency is crucial for reducing future energy use and mitigating
climate change. Existing control strategies lack generalisation and require
extensive training and data, limiting their rapid deployment across diverse
buildings. This paper introduces HVAC-DPT, a Decision-Pretrained Transformer
using in-context Reinforcement Learning (RL) for multi-zone HVAC control.
HVAC-DPT frames HVAC control as a sequential prediction task, training a causal
transformer on interaction histories generated by diverse RL agents. This
approach enables HVAC-DPT to refine its policy in-context, without modifying
network parameters, allowing for deployment across different buildings without
the need for additional training or data collection. HVAC-DPT reduces energy
consumption in unseen buildings by 45% compared to the baseline controller,
offering a scalable and effective approach to mitigating the increasing
environmental impact of HVAC systems.",http://arxiv.org/abs/2411.19746v1
"Frost/Defrost Models for Air-Source Heat Pumps with Retained Water
  Refreezing Considered",2024-11-15T16:42:22Z,"Jiacheng Ma, Matthis Thorade","Cyclic frosting and defrosting operations constitute a common characteristic
of air-source heat pumps in cold climates during winter. Simulation models that
can capture simultaneous heat and mass transfer phenomena associated with
frost/defrost behaviors and their impact on the overall heat pump system
performance are of critical importance to improved controls of heat delivery
and frost mitigation. This paper presents a novel frost formulation using an
enthalpy method to systematically capture all phase-change behaviors including
frost formation and melting, retained water refreezing and melting, and water
drainage during cyclic frosting and defrosting operations. A Fuzzy modeling
approach is proposed to smoothly switch source terms when evaluating the
dynamics of frost and water mediums for numerical robustness. The proposed
frost/defrost model is incorporated into a flat-tube outdoor heat exchanger
model of an automotive heat pump system model to investigate system responses
under cyclic operations of frosting and reverse-cycle defrosting.",http://arxiv.org/abs/2412.00017v1
"I Spy With My Little Eye: A Minimum Cost Multicut Investigation of
  Dataset Frames",2024-12-02T09:09:47Z,"Katharina Prasse, Isaac Bravo, Stefanie Walter, Margret Keuper","Visual framing analysis is a key method in social sciences for determining
common themes and concepts in a given discourse. To reduce manual effort, image
clustering can significantly speed up the annotation process. In this work, we
phrase the clustering task as a Minimum Cost Multicut Problem [MP]. Solutions
to the MP have been shown to provide clusterings that maximize the posterior
probability, solely from provided local, pairwise probabilities of two images
belonging to the same cluster. We discuss the efficacy of numerous embedding
spaces to detect visual frames and show its superiority over other clustering
methods. To this end, we employ the climate change dataset \textit{ClimateTV}
which contains images commonly used for visual frame analysis. For broad visual
frames, DINOv2 is a suitable embedding space, while ConvNeXt V2 returns a
larger number of clusters which contain fine-grain differences, i.e. speech and
protest. Our insights into embedding space differences in combination with the
optimal clustering - by definition - advances automated visual frame detection.
Our code can be found at https://github.com/KathPra/MP4VisualFrameDetection.",http://arxiv.org/abs/2412.01296v1
"DYffCast: Regional Precipitation Nowcasting Using IMERG Satellite Data.
  A case study over South America",2024-12-02T22:20:31Z,"Daniel Seal, Rossella Arcucci, Salva R√ºhling-Cachay, C√©sar Quilodr√°n-Casas","Climate change is increasing the frequency of extreme precipitation events,
making weather disasters such as flooding and landslides more likely. The
ability to accurately nowcast precipitation is therefore becoming more critical
for safeguarding society by providing immediate, accurate information to
decision makers. Motivated by the recent success of generative models at
precipitation nowcasting, this paper: extends the DYffusion framework to this
task and evaluates its performance at forecasting IMERG satellite precipitation
data up to a 4-hour horizon; modifies the DYffusion framework to improve its
ability to model rainfall data; and introduces a novel loss function that
combines MSE, MAE and the LPIPS perceptual score. In a quantitative evaluation
of forecasts up to a 4-hour horizon, the modified DYffusion framework trained
with the novel loss outperforms four competitor models. It has the highest CSI
scores for weak, moderate, and heavy rain thresholds and retains an LPIPS score
$<$ 0.2 for the entire roll-out, degrading the least as lead-time increases.
The proposed nowcasting model demonstrates visually stable and sharp forecasts
up to a 2-hour horizon on a heavy rain case study. Code is available at
https://github.com/Dseal95/DYffcast.",http://arxiv.org/abs/2412.02723v1
"Social Media Informatics for Sustainable Cities and Societies: An
  Overview of the Applications, associated Challenges, and Potential Solutions",2024-12-03T08:53:32Z,"Jebran Khan, Kashif Ahmad, Senthil Kumar Jagatheesaperumal, Nasir Ahmad, Kyung-Ah Sohn","In the modern world, our cities and societies face several technological and
societal challenges, such as rapid urbanization, global warming & climate
change, the digital divide, and social inequalities, increasing the need for
more sustainable cities and societies. Addressing these challenges requires a
multifaceted approach involving all the stakeholders, sustainable planning,
efficient resource management, innovative solutions, and modern technologies.
Like other modern technologies, social media informatics also plays its part in
developing more sustainable and resilient cities and societies. Despite its
limitations, social media informatics has proven very effective in various
sustainable cities and society applications. In this paper, we review and
analyze the role of social media informatics in sustainable cities and society
by providing a detailed overview of its applications, associated challenges,
and potential solutions. This work is expected to provide a baseline for future
research in the domain.",http://arxiv.org/abs/2412.03600v2
"Modeling wildfire dynamics through a physics-based approach
  incorporating fuel moisture and landscape heterogeneity",2024-12-05T07:44:57Z,"Adri√°n Navas-Montilla, Cordula Reisch, Pablo Diaz, Ilhan √ñzgen-Xian","Anthropogenic climate change has increased the probability, severity, and
duration of heat waves and droughts, subsequently escalating the risk of
wildfires. Mathematical and computational models can enhance our understanding
of wildfire propagation dynamics. In this work, we present a simplified
Advection-Diffusion-Reaction (ADR) model that accounts for the effect of fuel
moisture, and also considers wind, local radiation, natural convection and
topography. The model explicitly represents fuel moisture effects by means of
the apparent calorific capacity method, distinguishing between live and dead
fuel moisture content. Using this model, we conduct exploratory simulations and
present theoretical insights into various modeling decisions in the context of
ADR-based models. We aim to shed light on the interplay between the different
modeled mechanisms in wildfire propagation to identify key factors influencing
fire spread and to estimate the model's predictive capacity.",http://arxiv.org/abs/2412.04517v1
"Who Sets the Agenda on Social Media? Ideology and Polarization in Online
  Debates",2024-12-06T16:48:22Z,"Edoardo Loru, Alessandro Galeazzi, Anita Bonetti, Emanuele Sangiorgio, Niccol√≤ Di Marco, Matteo Cinelli, Andrea Baronchelli, Walter Quattrociocchi","The abundance of information on social media has reshaped public discussions,
shifting attention to the mechanisms that drive online discourse. This study
analyzes large-scale Twitter (now X) data from three global debates -- Climate
Change, COVID-19, and the Russo-Ukrainian War -- to investigate the structural
dynamics of engagement. Our findings reveal that discussions are not primarily
shaped by specific categories of actors, such as media or activists, but by
shared ideological alignment. Users consistently form polarized communities,
where their ideological stance in one debate predicts their positions in
others. This polarization transcends individual topics, reflecting a broader
pattern of ideological divides. Furthermore, the influence of individual actors
within these communities appears secondary to the reinforcing effects of
selective exposure and shared narratives. Overall, our results underscore that
ideological alignment, rather than actor prominence, plays a central role in
structuring online discourse and shaping the spread of information in polarized
environments.",http://arxiv.org/abs/2412.05176v1
"Accurate Water Level Monitoring in AWD Rice Cultivation Using
  Convolutional Neural Networks",2024-12-11T15:44:08Z,"Ahmed Rafi Hasan, Niloy Kumar Kundu, Saad Hasan, Mohammad Rashedul Hoque, Swakkhar Shatabda","The Alternate Wetting and Drying (AWD) method is a rice-growing water
management technique promoted as a sustainable alternative to Continuous
Flooding (CF). Climate change has placed the agricultural sector in a
challenging position, particularly as global water resources become
increasingly scarce, affecting rice production on irrigated lowlands. Rice, a
staple food for over half of the world's population, demands significantly more
water than other major crops. In Bangladesh, Boro rice, in particular, requires
considerable water inputs during its cultivation. Traditionally, farmers
manually measure water levels, a process that is both time-consuming and prone
to errors. While ultrasonic sensors offer improvements in water height
measurement, they still face limitations, such as susceptibility to weather
conditions and environmental factors. To address these issues, we propose a
novel approach that automates water height measurement using computer vision,
specifically through a convolutional neural network (CNN). Our attention-based
architecture achieved an $R^2$ score of 0.9885 and a Mean Squared Error (MSE)
of 0.2766, providing a more accurate and efficient solution for managing AWD
systems.",http://arxiv.org/abs/2412.08477v2
The Global Carbon Budget as a cointegrated system,2024-12-12T12:28:38Z,"Mikkel Bennedsen, Eric Hillebrand, Morten √òrregaard Nielsen","The Global Carbon Budget, maintained by the Global Carbon Project, summarizes
Earth's global carbon cycle through four annual time series beginning in 1959:
atmospheric CO$_2$ concentrations, anthropogenic CO$_2$ emissions, and CO$_2$
uptake by land and ocean. We analyze these four time series as a multivariate
(cointegrated) system. Statistical tests show that the four time series are
cointegrated with rank three and identify anthropogenic CO$_2$ emissions as the
single stochastic trend driving the nonstationary dynamics of the system. The
three cointegrated relations correspond to the physical relations that the
sinks are linearly related to atmospheric concentrations and that the change in
concentrations equals emissions minus the combined uptake by land and ocean.
Furthermore, likelihood ratio tests show that a parametrically restricted
error-correction model that embodies these physical relations and accounts for
the El Ni\~no/Southern Oscillation cannot be rejected on the data. The model
can be used for both in-sample and out-of-sample analysis. In an application of
the latter, we demonstrate that projections based on this model, using Shared
Socioeconomic Pathways scenarios, yield results consistent with established
climate science.",http://arxiv.org/abs/2412.09226v3
"Legal Challenges in Renewable Energy Development: A Comparative Study of
  China and Selected Countries",2024-12-13T15:22:00Z,"Aliasghar Baziar, Navid Parsa","This exhaustive investigation is dedicated to delving into the intricate
legal aspects that underlie the inefficiency in the advancement and utilization
of sustainable energies, with a primary focus on the dynamic landscape of China
and carefully selected representative nations. In an era where the global
community increasingly acknowledges the pressing need for
environmentally-friendly alternatives to traditional fossil fuels, renewable
energy sources have rightfully garnered substantial attention as encouraging
solutions. Nevertheless, notwithstanding their potential to revolutionize the
energy sector and counteract climate change, a multitude of legal and
regulatory barriers may present formidable hindrances that impede their
seamless integration into the energy landscape. With a resolute and
concentrated aim, the research sets forth on a painstaking exploration and
analysis of the intricate legal frameworks, policies, and institutional
arrangements in place within China and the chosen representative nations. The
ultimate objective is to discern and identify potential challenges and
inefficiencies that could hinder the progress of renewable energy projects and
initiatives.",http://arxiv.org/abs/2412.10203v1
"Towards Using Machine Learning to Generatively Simulate EV Charging in
  Urban Areas",2024-12-13T19:55:19Z,"Marek Miltner, Jakub Z√≠ka, Daniel Va≈°ata, Artem Bryksa, Magda Friedjungov√°, Ond≈ôej ≈†togl, Ram Rajagopal, Old≈ôich Star√Ω","This study addresses the challenge of predicting electric vehicle (EV)
charging profiles in urban locations with limited data. Utilizing a neural
network architecture, we aim to uncover latent charging profiles influenced by
spatio-temporal factors. Our model focuses on peak power demand and daily load
shapes, providing insights into charging behavior. Our results indicate
significant impacts from the type of Basic Administrative Units on predicted
load curves, which contributes to the understanding and optimization of EV
charging infrastructure in urban settings and allows Distribution System
Operators (DSO) to more efficiently plan EV charging infrastructure expansion.",http://arxiv.org/abs/2412.10531v2
Predicting the role of inequalities on human mobility patterns,2024-12-17T21:03:59Z,"Alain Boldini, Pietro De Lellis, Salvatore Imperatore, Rishita Das, Luis Ceferino, Manuel Heitor, Maurizio Porfiri","Whether in search of better trade opportunities or escaping wars, humans have
always been on the move. For almost a century, mathematical models of human
mobility have been instrumental in the quantification of commuting patterns and
migratory fluxes. Equity is a common premise of most of these mathematical
models, such that living conditions and job opportunities are assumed to be
equivalent across cities. Growing inequalities in modern urban economy and
pressing effects of climate change significantly strain this premise. Here, we
propose a mobility model that is aware of inequalities across cities in terms
of living conditions and job opportunities. Comparing results with real
datasets, we show that the proposed model outperforms the state-of-the-art in
predicting migration patterns in South Sudan and commuting fluxes in the United
States. This model paves the way to critical research on resilience and
sustainability of urban systems.",http://arxiv.org/abs/2412.13331v1
"A Cross-Domain Study of the Use of Persuasion Techniques in Online
  Disinformation",2024-12-19T17:46:13Z,"Jo√£o A. Leite, Olesya Razuvayevskaya, Carolina Scarton, Kalina Bontcheva","Disinformation, irrespective of domain or language, aims to deceive or
manipulate public opinion, typically through employing advanced persuasion
techniques. Qualitative and quantitative research on the weaponisation of
persuasion techniques in disinformation has been mostly topic-specific (e.g.,
COVID-19) with limited cross-domain studies, resulting in a lack of
comprehensive understanding of these strategies. This study employs a
state-of-the-art persuasion technique classifier to conduct a large-scale,
multi-domain analysis of the role of 16 persuasion techniques in disinformation
narratives. It shows how different persuasion techniques are employed
disproportionately in different disinformation domains. We also include a
detailed case study on climate change disinformation, highlighting how
linguistic, psychological, and cultural factors shape the adaptation of
persuasion strategies to fit unique thematic contexts.",http://arxiv.org/abs/2412.15098v1
The impact of behavioral diversity in multi-agent reinforcement learning,2024-12-19T21:13:32Z,"Matteo Bettini, Ryan Kortvelesy, Amanda Prorok","Many of the world's most pressing issues, such as climate change and global
peace, require complex collective problem-solving skills. Recent studies
indicate that diversity in individuals' behaviors is key to developing such
skills and increasing collective performance. Yet behavioral diversity in
collective artificial learning is understudied, with today's machine learning
paradigms commonly favoring homogeneous agent strategies over heterogeneous
ones, mainly due to computational considerations. In this work, we employ
diversity measurement and control paradigms to study the impact of behavioral
heterogeneity in several facets of multi-agent reinforcement learning. Through
experiments in team play and other cooperative tasks, we show the emergence of
unbiased behavioral roles that improve team outcomes; how behavioral diversity
synergizes with morphological diversity; how diverse agents are more effective
at finding cooperative solutions in sparse reward settings; and how
behaviorally heterogeneous teams learn and retain latent skills to overcome
repeated disruptions. Overall, our results indicate that, by controlling
diversity, we can obtain non-trivial benefits over homogeneous training
paradigms, demonstrating that diversity is a fundamental component of
collective artificial learning, an insight thus far overlooked.",http://arxiv.org/abs/2412.16244v2
"Iterative Encoding-Decoding VAEs Anomaly Detection in NOAA's DART Time
  Series: A Machine Learning Approach for Enhancing Data Integrity for NASA's
  GRACE-FO Verification and Validation",2024-12-20T22:19:11Z,Kevin Lee,"NOAA's Deep-ocean Assessment and Reporting of Tsunamis (DART) data are
critical for NASA-JPL's tsunami detection, real-time operations, and
oceanographic research. However, these time-series data often contain spikes,
steps, and drifts that degrade data quality and obscure essential oceanographic
features. To address these anomalies, the work introduces an Iterative
Encoding-Decoding Variational Autoencoders (Iterative Encoding-Decoding VAEs)
model to improve the quality of DART time series. Unlike traditional filtering
and thresholding methods that risk distorting inherent signal characteristics,
Iterative Encoding-Decoding VAEs progressively remove anomalies while
preserving the data's latent structure. A hybrid thresholding approach further
retains genuine oceanographic features near boundaries. Applied to complex DART
datasets, this approach yields reconstructions that better maintain key oceanic
properties compared to classical statistical techniques, offering improved
robustness against spike removal and subtle step changes. The resulting
high-quality data supports critical verification and validation efforts for the
GRACE-FO mission at NASA-JPL, where accurate surface measurements are essential
to modeling Earth's gravitational field and global water dynamics. Ultimately,
this data processing method enhances tsunami detection and underpins future
climate modeling with improved interpretability and reliability.",http://arxiv.org/abs/2412.16375v1
"Emergent poverty traps and inequality at multiple levels impedes social
  mobility",2024-12-09T12:29:27Z,"Charles Dupont, Debraj Roy","Eradicating extreme poverty and inequality are the key leverage points to
achieve the seventeen Sustainable Development goals. Yet, the reduction in
extreme poverty and inequality are vulnerable to shocks such as the pandemic
and climate change. We find that that these vulnerabilities emerge from the
interaction between individual and institutional mechanisms. Individual
characteristics like risk aversion, attention, and saving propensity can lead
to sub-optimal diversification and low capital accumulation. These individual
drivers are reinforced by institutional mechanisms such as lack of financial
inclusion, access to technology, and economic segregation, leading to
persistent inequality and poverty traps. Our experiments demonstrate that
addressing above factors yields 'double dividend' - reducing poverty and
inequality within-and-between communities and create positive feedback that can
withstand shocks.",http://arxiv.org/abs/2412.17822v1
Quantifying the Risk of Pastoral Conflict in 4 Central African Countries,2024-12-25T06:37:29Z,"Lirika Solaa, Youdinghuan Chen, Samantha K. Murphy, V. S. Subrahmanian","Climate change is becoming a widely recognized risk factor of farmer-herder
conflict in Africa. Using an 8 year dataset (Jan 2015 to Sep 2022) of detailed
weather and terrain data across four African nations, we apply statistical and
machine learning methods to analyze pastoral conflict. We test hypotheses
linking these variables with pastoral conflict within each country using
geospatial and statistical analysis. Complementing this analysis are risk maps
automatically updated for decision-makers. Our models estimate which cells have
a high likelihood of experiencing pastoral conflict with high predictive
accuracy and study the variation of this accuracy with the granularity of the
cells.",http://arxiv.org/abs/2412.18799v1
"Personalized and Safe Route Planning for Asthma Patients Using Real-Time
  Environmental Data",2024-12-13T14:33:19Z,"Nada Ayman, Shaimaa Alaa, Mohamed Hussein, Ali Hamdi","Asthmatic patients are very frequently affected by the quality of air,
climatic conditions, and traffic density during outdoor activities. Most of the
conventional routing algorithms, such as Dijkstra's algorithm, usually fail to
consider these health dimensions, hence resulting in suboptimal or risky
recommendations. Here, the health-aware heuristic framework is presented that
shall utilize real-time data provided by the Microsoft Weather API. The
advanced A* algorithm provides dynamic changes in routes depending on air
quality indices, temperature, traffic density, and other patient-related health
data. The power of the model is realized by running simulations in city
environments and outperforming the state-of-the-art methodology in terms of
recommendation accuracy at low computational overhead. It provides
health-sensitive route recommendations, keeping in mind the avoidance of
high-risk areas and ensuring safer and more suitable travel options for
asthmatic patients.",http://arxiv.org/abs/2501.10372v1
"Engineering Carbon Credits Towards A Responsible FinTech Era: The
  Practices, Implications, and Future",2024-12-22T05:48:02Z,"Qingwen Zeng, Hanlin Xu, Nanjun Xu, Flora Salim, Junbin Gao, Huaming Chen","Carbon emissions significantly contribute to climate change, and carbon
credits have emerged as a key tool for mitigating environmental damage and
helping organizations manage their carbon footprint. Despite their growing
importance across sectors, fully leveraging carbon credits remains challenging.
This study explores engineering practices and fintech solutions to enhance
carbon emission management. We first review the negative impacts of carbon
emission non-disclosure, revealing its adverse effects on financial stability
and market value. Organizations are encouraged to actively manage emissions and
disclose relevant data to mitigate risks. Next, we analyze factors influencing
carbon prices and review advanced prediction algorithms that optimize carbon
credit purchasing strategies, reducing costs and improving efficiency.
Additionally, we examine corporate carbon emission prediction models, which
offer accurate performance assessments and aid in planning future carbon credit
needs. By integrating carbon price and emission predictions, we propose
research directions, including corporate carbon management cost forecasting.
This study provides a foundation for future quantitative research on the
financial and market impacts of carbon management practices and is the first
systematic review focusing on computing solutions and engineering practices for
carbon credits.",http://arxiv.org/abs/2501.14750v1
"Assessing The Spatially Heterogeneous Impact of Recurrent Flooding On
  Accessibility: A Case Study of The Hampton Roads Region:Part 2 Transit
  Accessibility",2024-01-13T00:24:01Z,"Luwei Zeng, T. Donna Chen, John S. Miller, Jonathan L. Goodall, Faria Tuz Zahura","Due to accelerated sea level rise and climate change, the transportation
system is increasingly affected by recurrent flooding coastal regions, yet the
cumulative travel disruption effects are not well understood. In Part 1 of this
study, the accessibility impacts of recurrent flooding on the auto mode were
examined. In this paper (Part 2 of the study), the impact of recurrent flooding
on transit service accessibility was quantified with the aid of spatially and
temporally disaggregated crowdsourced flood incident data from WAZE. A fixed
route transit network is built for five time of day periods for 710 traffic
analysis zones (TAZs), to capture the spatial and temporal variation of transit
accessibility reduction due to recurrent flooding. Results show that the
greatest transit accessibility reduction occurs during the morning peak hour,
with individual TAZ transit accessibility reduction ranging from 0 to 88.2% for
work trips (with an average of 6.4%) and ranging from 0 to 99.9% for non-work
trips (with an average of 3.7%). Furthermore, social vulnerability analysis
indicates that TAZs with a greater share of people with higher vulnerability in
transportation and socioeconomic status are more likely to experience recurrent
flooding-induced transit accessibility reduction. Results from this study
reinforce the notion that transportation impacts under recurrent flooding are
not uniformly experienced throughout a region, and this spatial and temporal
variation translates to different impacts borne by various population groups.
Disaggregate impact analysis like this study can support transportation
engineers and planners to prioritize resources to ensure equitable transit
accessibility under increasing climate disruptions.",http://arxiv.org/abs/2402.03335v1
"Long-window hybrid variational data assimilation methods for chaotic
  climate models tested with the Lorenz 63 system",2024-03-05T17:58:32Z,"Philip David Kennedy, Abhirup Banerjee, Armin K√∂hl, Detlef Stammer","A hybrid 4D-variational data assimilation method for chaotic climate models
is introduced using the Lorenz '63 model. This approach aims to optimise an
Earth system model (ESM), for which no adjoint exists, by utilising an adjoint
model of a different, potentially simpler ESM. The technique relies on
synchronisation of the model to observed time series data employing the
dynamical state and parameter estimation (DSPE) method to stabilise the tangent
linear system by reducing all positive Lyapunov exponents to negative values.
Therefore, long windows can be used to improve parameter estimation. In this
new extension a second layer of synchronisation is added between the two
models, with and without an adjoint, to facilitate linearisation around the
trajectory of the model without an adjoint. The method is conceptually
demonstrated by synchronising two Lorenz '63 systems, representing two ESMs,
one with and the other without an adjoint model. Results are presented for an
idealised case of identical, perfect models and for a more realistic case in
which they differ from one another. If employed with a coarser ESM with an
adjoint, the method will save computational power as only one forward run with
the full ESM per iteration needs to be carried out. It is demonstrated that
there is negligible error and uncertainty change compared to the 'traditional'
optimisation of full ESM with an adjoint. In a variation of the method
outlined, synchronisation between two identical models can be used to filter
noisy data. This reduces optimised parametric model uncertainty by
approximately one third. Such a precision gain could prove valuable for
seasonal, annual, and decadal predictions.",http://arxiv.org/abs/2403.03166v3
"A probabilistic framework for learning non-intrusive corrections to
  long-time climate simulations from short-time training data",2024-08-02T18:34:30Z,"Benedikt Barthel Sorensen, Leonardo Zepeda-N√∫√±ez, Ignacio Lopez-Gomez, Zhong Yi Wan, Rob Carver, Fei Sha, Themistoklis Sapsis","Chaotic systems, such as turbulent flows, are ubiquitous in science and
engineering. However, their study remains a challenge due to the large range
scales, and the strong interaction with other, often not fully understood,
physics. As a consequence, the spatiotemporal resolution required for accurate
simulation of these systems is typically computationally infeasible,
particularly for applications of long-term risk assessment, such as the
quantification of extreme weather risk due to climate change. While data-driven
modeling offers some promise of alleviating these obstacles, the scarcity of
high-quality simulations results in limited available data to train such
models, which is often compounded by the lack of stability for long-horizon
simulations. As such, the computational, algorithmic, and data restrictions
generally imply that the probability of rare extreme events is not accurately
captured. In this work we present a general strategy for training neural
network models to non-intrusively correct under-resolved long-time simulations
of chaotic systems. The approach is based on training a post-processing
correction operator on under-resolved simulations nudged towards a
high-fidelity reference. This enables us to learn the dynamics of the
underlying system directly, which allows us to use very little training data,
even when the statistics thereof are far from converged. Additionally, through
the use of probabilistic network architectures we are able to leverage the
uncertainty due to the limited training data to further improve extrapolation
capabilities. We apply our framework to severely under-resolved simulations of
quasi-geostrophic flow and demonstrate its ability to accurately predict the
anisotropic statistics over time horizons more than 30 times longer than the
data seen in training.",http://arxiv.org/abs/2408.02688v2
"Microphysical Prescriptions for Parameterized Water Cloud Formation on
  Ultra-cool Substellar Objects",2024-08-16T18:04:25Z,"James Mang, Caroline V. Morley, Tyler D. Robinson, Peter Gao","Water must condense into ice clouds in the coldest brown dwarfs and
exoplanets. When they form, these icy clouds change the emergent spectra,
temperature structure, and albedo of the substellar atmosphere. The properties
of clouds are governed by complex microphysics but these complexities are often
not captured by the simpler parameterized cloud models used in climate models
or retrieval models. Here, we combine microphysical cloud modeling and 1D
climate modeling to incorporate insights from microphysical models into a
self-consistent, parameterized cloud model. Using the 1D Community Aerosol and
Radiation Model for Atmospheres (CARMA), we generate microphysical water clouds
and compare their properties with those from the widely-used EddySed cloud
model (Ackerman & Marley 2001) for a grid of Y dwarfs. We find that the mass of
water condensate in our CARMA water clouds is significantly limited by
available condensation nuclei; in models without additional seed particles for
clouds added, the atmosphere becomes supersaturated. We incorporate water
latent heat release in the convective and radiative parts of the atmosphere and
find no significant impact on water-ice cloud formation for typical gas giant
compositions. Our analysis reveals the CARMA cloud profiles have a gradual
decrease in opacity of approximately 4% per bar below the cloud base.
Incorporating this gradual cloud base falloff and a variable $f_{sed}$
parameter allows spectra generated from the parameterized Eddysed model to
better match those of the microphysical CARMA model. This work provides
recommendations for efficiently generating microphysically-informed water
clouds for future models of cold substellar objects with H/He atmospheres.",http://arxiv.org/abs/2408.08958v2
"Aerodynamic Performance and Impact Analysis of a MEMS-Based Non-Invasive
  Monitoring System for Wind Turbine Blades",2024-08-21T09:23:49Z,"Nicolas Sch√§rer, Denis Mikhaylov, C√©dric Sievi, Badoui Hanna, Caroline Braud, Julien Deparday, Sarah Barber, Tommaso Polonelli, Michele Magno","Wind power generation plays a crucial role in transitioning away from fossil
fuel-dependent energy sources, contributing significantly to the mitigation of
climate change. Monitoring and evaluating the aerodynamics of large wind
turbine rotors is crucial to enable more wind energy deployment. This is
necessary to achieve the European climate goal of a reduction in net greenhouse
gas emissions by at least 55% by 2030, compared to 1990 levels. This paper
presents a comparison between two measurement systems for evaluating the
aerodynamic performance of wind turbine rotor blades on a full-scale wind
tunnel test. One system uses an array of ten commercial compact ultra-low power
micro-electromechanical systems (MEMS) pressure sensors placed on the blade
surface, while the other employs high-accuracy lab-based pressure scanners
embedded in the airfoil. The tests are conducted at a Reynolds number of 3.5 x
10^6, which represents typical operating conditions for wind turbines. MEMS
sensors are of particular interest, as they can enable real-time monitoring
which would be impossible with the ground truth system. This work provides an
accurate quantification of the impact of the MEMS system on the blade
aerodynamics and its measurement accuracy. Our results indicate that MEMS
sensors, with a total sensing power below 1.6 mW, can measure key aerodynamic
parameters like Angle of Attack (AoA) and flow separation with a precision of
1{\deg}. Although there are minor differences in measurements due to sensor
encapsulation, the MEMS system does not significantly compromise blade
aerodynamics, with a maximum shift in the angle of attack for flow separation
of only 1{\deg}. These findings indicate that surface and low-power MEMS sensor
systems are a promising approach for efficient and sustainable wind turbine
monitoring using self-sustaining Internet of Things devices and wireless sensor
networks.",http://arxiv.org/abs/2408.11458v1
Impact of vegetation albedo on the habitability of Earth-like exoplanets,2024-09-03T09:45:24Z,"Erica Bisesi, Giuseppe Murante, Antonello Provenzale, Lorenzo Biasiotti, Jost von Hardenberg, Stavro Ivanovski, Michele Maris, Sergio Monai, Laura Silva, Paolo Simonetti, Giovanni Vladilo","Vegetation can modify the planetary surface albedo via the Charney mechanism,
as plants are usually darker than the bare surface of the continents. We
updated ESTM (Earth-like Surface Temperature Model) to incorporate the
presence, distribution and evolution of two dynamically competing vegetation
types that resemble grasslands and trees (the latter in the double stages of
life: adults and seedlings). The newly developed model was applied to estimate
how the climate-vegetation system reaches equilibrium across different rocky
planetary configurations, and to assess its impact on temperature and
habitability. With respect to a world with bare granite continents, the effect
of vegetation-albedo feedback is to increase the average surface temperature.
Since grasses and trees exhibit different albedos, they affect temperature to
different degrees. The ultimate impact on climate depends on the outcome of the
competition between these vegetation types. The change in albedo due to
vegetation extends the habitable zone and enhances the overall planetary
habitability beyond its traditional outer edge. This effect is especially
relevant for planets that have a larger extension of continents than Earth. For
Earth, the semi-major axis d = 1.04 UA represents the turning point where
vegetation enhances habitability from h = 0.0 to h = 0.485 (in the
grass-dominance case), to h = 0.584 (in the case of coexistence between grasses
and trees), and to h = 0.612 (in the tree-dominance case). This illustrates the
transition from a snowball state to a planet with intermediate habitability at
the outer edge of the circumstellar habitability zone.",http://arxiv.org/abs/2409.01746v1
"Satellite-Based Quantification of Contrail Radiative Forcing over
  Europe: A Two-Week Analysis of Aviation-Induced Climate Effects",2024-09-16T10:54:36Z,"Irene Ortiz, Ermioni Dimitropoulou, Pierre de Buyl, Nicolas Clerbaux, Javier Garc√≠a-Heras, Amin Jafarimoghaddam, Hugues Brenot, Jeroen van Gent, Klaus Sievers, Evelyn Otero, Parthiban Loganathan, Manuel Soler","Aviation's non-CO$_2$ effects, especially the impact of aviation-induced
contrails, drive atmospheric changes and can influence climate dynamics.
Although contrails are believed to contribute to global warming through their
net warming effect, uncertainties persist due to the challenges in accurately
measuring their radiative impacts. This study aims to address this knowledge
gap by investigating the relationship between aviation-induced contrails, as
observed in Meteosat Second Generation (MSG) satellite imagery, and their
impact on radiative forcing (RF) over a two-week study. Results show that while
daytime contrails generally have a cooling effect, the higher number of
nighttime contrails results in a net warming effect over the entire day. Net RF
values for detected contrails range approximately from -8 TW to 2.5 TW during
the day and from 0 to 6 TW at night. Our findings also show a 41.03% increase
in contrail coverage from January 24-30, 2023, to the same week in 2024,
accompanied by a 128.7% rise in contrail radiative forcing (CRF), indicating
greater warming from the added contrails. These findings highlight the
necessity of considering temporal factors, such as the timing and duration of
contrail formation, when assessing their overall warming impact. They also
indicate a potential increase in contrail-induced warming from 2023 to 2024,
attributable to the rise in contrail coverage. Further investigation into these
trends is crucial for the development of effective mitigation strategies.",http://arxiv.org/abs/2409.10166v2
"Decoupling of carbonate-organic carbon isotope during the Carnian
  Pluvial Episode",2024-12-18T07:26:19Z,"Enhao Jia, Kui Wu, Yong Du, Yuyang Wu, Fengyu Wang, Xu Dai, Huyue Song, Daoliang Chu, Lei Zhong, Zhiwei Yuan, Xiangmin Chen, Zhe Li, Haijun Song","The Carnian Pluvial Episode (CPE) was a major global climate change event in
the early Late Triassic that significantly affected marine ecosystems and
carbon cycles. One of the most prominent features of the CPE is the coupled
multiple negative carbonate-organic carbon isotope excursions. However, at
Erguan and Xiashulao from eastern Tethys, a decoupling between
carbonate-organic carbon isotope during CPE was observed. At the end of early
Carnian (Julian), the carbonate carbon isotope showed a negative excursion of
2-3 per-mille, while the organic carbon isotope exhibited a positive excursion
of about 3-4 per-mille. In addition, increased terrestrial inputs is indicated
by the rising C/N (3 to 10) and decreasing Y/Ho (42 to 27) that coexist with
this decoupling. The coupling of carbon isotope negative excursions is from the
shallow shelves and the deep slopes, whereas the decoupling occurs from the
deep shelf to the shallow slope. In the deep shelf to the shallow slope,
sedimentary organic matter is mainly sourced from pelagic before the CPE as
evidenced by low C/N (3) and high Y/Ho (36-42). During the CPE, the increased
fresh water flux (Sr/Ba <1) enhanced terrestrial input in organic matter, which
may cause positive excursions in the carbon isotope record with elevated TOC
content. As a result, the carbonate-organic carbon isotope decoupled. In
contrast, organic matter in sediments from the shallow shelf and deep slope are
mainly from terrestrial and pelagic sources, respectively. This study reveals
the significant impact of terrestrial inputs on marine carbon cycling during
the Carnian Pluvial Episode, highlighting the crucial role of climate events in
modifying the carbon isotope record.",http://arxiv.org/abs/2412.13562v2
"A Physics-informed machine learning model for time-dependent wave runup
  prediction",2024-01-12T18:58:37Z,"Saeed Saviz Naeini, Reda Snaiki","Wave runup is a critical factor affecting coastal flooding, shoreline
changes, and damage to coastal structures. Climate change is also expected to
amplify wave runup's impact on coastal areas. Therefore, fast and accurate wave
runup estimation is essential for effective coastal engineering design and
management. However, predicting the time-dependent wave runup is challenging
due to the intrinsic nonlinearities and non-stationarity of the process, even
with the use of the most advanced machine learning techniques. In this study, a
physics-informed machine learning-based approach is proposed to efficiently and
accurately simulate time-series wave runup. The methodology combines the
computational efficiency of the Surfbeat (XBSB) mode with the accuracy of the
nonhydrostatic (XBNH) mode of the XBeach model. Specifically, a conditional
generative adversarial network (cGAN) is used to map the image representation
of wave runup from XBSB to the corresponding image from XBNH. These images are
generated by first converting wave runup signals into time-frequency scalograms
and then transforming them into image representations. The cGAN model achieves
improved performance in image-to-image mapping tasks by incorporating
physics-based knowledge from XBSB. After training the model, the high-fidelity
XBNH-based scalograms can be predicted, which are then employed to reconstruct
the time-series wave runup using the inverse wavelet transform. The simulation
results underscore the efficiency and robustness of the proposed model in
predicting wave runup, suggesting its potential value for applications in risk
assessment and management.",http://arxiv.org/abs/2401.08684v1
"PixelDINO: Semi-Supervised Semantic Segmentation for Detecting
  Permafrost Disturbances",2024-01-17T15:20:10Z,"Konrad Heidler, Ingmar Nitze, Guido Grosse, Xiao Xiang Zhu","Arctic Permafrost is facing significant changes due to global climate change.
As these regions are largely inaccessible, remote sensing plays a crucial rule
in better understanding the underlying processes not just on a local scale, but
across the Arctic. In this study, we focus on the remote detection of
retrogressive thaw slumps (RTS), a permafrost disturbance comparable to
landslides induced by thawing. For such analyses from space, deep learning has
become an indispensable tool, but limited labelled training data remains a
challenge for training accurate models. To improve model generalization across
the Arctic without the need for additional labelled data, we present a
semi-supervised learning approach to train semantic segmentation models to
detect RTS. Our framework called PixelDINO is trained in parallel on labelled
data as well as unlabelled data. For the unlabelled data, the model segments
the imagery into self-taught pseudo-classes and the training procedure ensures
consistency of these pseudo-classes across strong augmentations of the input
data. Our experimental results demonstrate that PixelDINO can improve model
performance both over supervised baseline methods as well as existing
semi-supervised semantic segmentation approaches, highlighting its potential
for training robust models that generalize well to regions that were not
included in the training data. The project page containing code and other
materials for this study can be found at
\url{https://khdlr.github.io/PixelDINO/}.",http://arxiv.org/abs/2401.09271v1
"When Geoscience Meets Generative AI and Large Language Models:
  Foundations, Trends, and Future Challenges",2024-01-25T12:03:50Z,"Abdenour Hadid, Tanujit Chakraborty, Daniel Busby","Generative Artificial Intelligence (GAI) represents an emerging field that
promises the creation of synthetic data and outputs in different modalities.
GAI has recently shown impressive results across a large spectrum of
applications ranging from biology, medicine, education, legislation, computer
science, and finance. As one strives for enhanced safety, efficiency, and
sustainability, generative AI indeed emerges as a key differentiator and
promises a paradigm shift in the field. This paper explores the potential
applications of generative AI and large language models in geoscience. The
recent developments in the field of machine learning and deep learning have
enabled the generative model's utility for tackling diverse prediction
problems, simulation, and multi-criteria decision-making challenges related to
geoscience and Earth system dynamics. This survey discusses several GAI models
that have been used in geoscience comprising generative adversarial networks
(GANs), physics-informed neural networks (PINNs), and generative pre-trained
transformer (GPT)-based structures. These tools have helped the geoscience
community in several applications, including (but not limited to) data
generation/augmentation, super-resolution, panchromatic sharpening, haze
removal, restoration, and land surface changing. Some challenges still remain
such as ensuring physical interpretation, nefarious use cases, and
trustworthiness. Beyond that, GAI models show promises to the geoscience
community, especially with the support to climate change, urban science,
atmospheric science, marine science, and planetary science through their
extraordinary ability to data-driven modeling and uncertainty quantification.",http://arxiv.org/abs/2402.03349v1
"Downscaling GRACE-derived ocean bottom pressure anomalies using
  self-supervised data fusion",2024-04-08T18:50:13Z,"Junyang Gou, Lara B√∂rger, Michael Schindelegger, Benedikt Soja","The gravimetry measurements from the Gravity Recovery and Climate Experiment
(GRACE) and its follow-on (GRACE-FO) satellite mission provide an essential way
to monitor changes in ocean bottom pressure ($p_b$), which is a critical
variable in understanding ocean circulation. However, the coarse spatial
resolution of the GRACE(-FO) fields blurs important spatial details, such as
$p_b$ gradients. In this study, we employ a self-supervised deep learning
algorithm to downscale global monthly $p_b$ anomalies derived from GRACE(-FO)
observations to an equal-angle $0.25^\circ$ grid in the absence of
high-resolution ground truth. The optimization process is realized by
constraining the outputs to follow the large-scale mass conservation contained
in the gravity field estimates while learning the spatial details from two
ocean reanalysis products. The downscaled product agrees with GRACE(-FO)
solutions over large ocean basins at the millimeter level in terms of
equivalent water height and shows signs of outperforming them when evaluating
short spatial scale variability. In particular, the downscaled $p_b$ product
has more realistic signal content near the coast and exhibits better agreement
with tide gauge measurements at around 80% of 465 globally distributed
stations. Our method presents a novel way of combining the advantages of
satellite measurements and ocean models at the product level, with potential
downstream applications for studies of the large-scale ocean circulation,
coastal sea level variability, and changes in global geodetic parameters.",http://arxiv.org/abs/2404.05818v1
"Analysing the interaction of expansion decisions by end customers and
  grid development in the context of a municipal energy system",2024-04-22T17:21:39Z,"Paul Maximilian R√∂hrig, Nancy Radermacher, Luis B√∂ttcher, Andreas Ulbig","In order to achieve greenhouse gas neutrality by 2045, the Climate Protection
Act sets emission reduction targets for the years 2030 and 2040, as well as
decreasing annual emission volumes for some sectors, including the building
sector. Measures to decarbonize the building sector include energy retrofits
and the expansion of renewable, decentralized power generators and low-CO2 heat
generators. These measures thus change both the load and the generation of the
future energy supply concept. Considering the interactions of the changed
installed technologies on the building level and their influence on the
electrical grid infrastructure is necessary. The grid operator will remedy the
future congested grid states by grid expansion measures and pass on the costs
to the connected grid users, which in turn could influence their behaviour and
decisions. The aim of this work is a holistic analysis of the staggered
interactions of generation expansion and grid expansion for a future
decentralized energy supply concept conditioned by the expansion in the field
of self-generation. To enable the analysis of the interactions, a
multi-criteria optimization procedure for expansion and operation decisions at
the building level is combined with an approach to determine grid expansion. As
part of this work, the effect of an expansion of hosting capacity on the grid
charges and thus the decision-making behaviour was investigated.",http://arxiv.org/abs/2404.14371v1
Multi-layer diffusion model of photovoltaic installations,2024-08-19T11:20:52Z,Tomasz Weron,"Nowadays, harmful effects of climate change are becoming increasingly
apparent. A vital issue that must be addressed is the generation of energy from
non-renewable and often polluting sources. For this reason, the development of
renewable energy sources is of great importance. Unfortunately, too rapid
spread of renewables can disrupt stability of the power system and lead to
energy blackouts. One should not simply support it, without ensuring
sustainability and understanding of the diffusion process. In this research, we
propose a new agent-based model of diffusion of photovoltaic panels. It is an
extension of the q-voter model that utilizes a multi-layer network structure.
The novelty is that both opinion dynamics and diffusion of innovation are
studied simultaneously on a multidimensional structure. The model is analyzed
using Monte Carlo simulations and the mean-field approximation. The impact of
parameters and specifications on the basic properties of the model is
discussed. Firstly, we show that for a certain range of parameters, innovation
always succeeds, regardless of the initial conditions. Secondly, that the
mean-field approximation gives qualitatively the same results as computer
simulations, even though it does not utilize knowledge of the network
structure.",http://arxiv.org/abs/2408.09904v3
L√©vy walk of pions in heavy-ion collisions,2024-09-16T15:15:59Z,"D√°niel Kincses, M√°rton Nagy, M√°t√© Csan√°d","The process of L\'evy walk, i.e., movement patterns described by heavy-tailed
random walks, plays a role in various phenomena, from chemical and
microbiological systems through marine predators to climate change. Recent
experiments have suggested that this phenomenon also appears in heavy-ion
collisions. However, the theoretical interpretation supporting such findings is
still debated. In high-energy collisions of heavy nuclei, the strongly
interacting Quark Gluon Plasma is created, which, similarly to the early
Universe, undergoes a rapid expansion and transition back to hadronic matter.
In the subsequent expanding hadron gas, particles interact until kinetic
freeze-out, when their momenta stop changing, and they freely transition toward
the detectors. Measuring spatial freeze-out distributions is a crucial tool in
understanding the dynamics of the created matter and the interactions among its
constituents. In this paper, we introduce a three-dimensional analysis of the
spatial freeze-out distribution of pions (the most abundant particles in such
collisions). Utilising Monte-Carlo simulations of high-energy collisions, we
show that the chain of processes ending in a final state pion has a step length
distribution leading to L\'evy-stable distributions. Subsequently, we show that
simulated pion freeze-out distributions indeed exhibit heavy tails and can be
described by a three-dimensional elliptically contoured symmetric L\'evy-stable
distribution.",http://arxiv.org/abs/2409.10373v2
"The Response of Planetary Atmospheres to the Impact of Icy Comets I:
  Tidally-Locked exo-Earths",2024-09-17T13:03:42Z,"Felix Sainsbury-Martinez, Catherine Walsh, Greg Cooke","Impacts by rocky and icy bodies are thought to have played a key role in
shaping the composition of solar system objects, including the Earth's
habitability. Hence, it is likely that they play a similar role in exoplanetary
systems. We investigate how an icy cometary impact affects the atmospheric
chemistry, climate, and composition of an Earth-like, tidally-locked,
terrestrial exoplanet, a prime target in the search for a habitable exoplanet
beyond our solar system. We couple a cometary impact model which includes
thermal ablation and pressure driven breakup with the 3D Earth System Model
WACCM6/CESM2, and use this model to investigate the effects of the water and
thermal energy delivery associated with an $R=2.5$ km pure water ice cometary
impact on an Earth-like atmosphere. We find that water is the primary driver of
longer timescale changes to the atmospheric chemistry and composition by acting
as a source of opacity, cloud ice, and atmospheric hydrogen/oxygen. The water
opacity drives heating at $\sim5\times10^{-4}$ bar, and cooling below, due to a
decreased flux reaching the surface. The increase in atmospheric hydrogen and
oxygen also drives an increase in the abundance of hydrogen/oxygen rich
molecules, with the exception of ozone, whose column density decreases by
$\sim10\%$. These atmospheric changes are potentially observable for $\sim$ 1-2
years post-impact, particularly those associated with cloud ice scattering.
They also persist, albeit at a much reduced level, to our quasi-steady-state,
suggesting that sustained bombardment or multiple large impacts have the
potential to shape the composition and habitability of terrestrial exoplanets.",http://arxiv.org/abs/2409.11151v2
"New Insights into Global Warming: End-to-End Visual Analysis and
  Prediction of Temperature Variations",2024-09-12T12:04:54Z,"Meihua Zhou, Nan Wan, Tianlong Zheng, Hanwen Xu, Li Yang, Tingting Wang","Global warming presents an unprecedented challenge to our planet however
comprehensive understanding remains hindered by geographical biases temporal
limitations and lack of standardization in existing research. An end to end
visual analysis of global warming using three distinct temperature datasets is
presented. A baseline adjusted from the Paris Agreements one point five degrees
Celsius benchmark based on data analysis is employed. A closed loop design from
visualization to prediction and clustering is created using classic models
tailored to the characteristics of the data. This approach reduces complexity
and eliminates the need for advanced feature engineering. A lightweight
convolutional neural network and long short term memory model specifically
designed for global temperature change is proposed achieving exceptional
accuracy in long term forecasting with a mean squared error of three times ten
to the power of negative six and an R squared value of zero point nine nine
nine nine. Dynamic time warping and KMeans clustering elucidate national level
temperature anomalies and carbon emission patterns. This comprehensive method
reveals intricate spatiotemporal characteristics of global temperature
variations and provides warming trend attribution. The findings offer new
insights into climate change dynamics demonstrating that simplicity and
precision can coexist in environmental analysis.",http://arxiv.org/abs/2409.16311v1
Soil organic carbon sequestration potential and policy optimization,2024-09-26T18:27:34Z,"Jacob Spertus, Philip Stark, Whendee Silver, Eric Slessarev","Land management could help mitigate climate change by sequestering
atmospheric carbon dioxide as soil organic carbon (SOC). The impact of a given
management change on the SOC content of a given volume of soil is generally
unknown, but is likely moderated by features of the land that collectively
determine its sequestration potential. To maximize sequestration, management
interventions should be preferentially applied to fields with the highest
sequestration potential and the lowest cost of application. We present a
design-based statistical framework for estimating sequestration potential,
average treatment effects, and optimal management policies from a randomized
experiment with baseline covariate information. We review the myriad and nested
sources of uncertainty that arise in this context and formalize the problem
using potential outcomes. We show that a particular regression estimator --
regressing field-level SOC on management indicators and their interactions with
covariates -- can help identify effective policies. The regression estimator
also gives asymptotically valid inference on average treatment effects under
the randomized design -- without modeling assumptions -- and can increase
precision and power compared to the difference-in-means $T$-test. We conclude
by discussing the saturation hypothesis in relation to sequestration potential,
other study designs including observational studies of SOC, models for policy
costs, nonparametric inference, and broader policy uncertainties.",http://arxiv.org/abs/2409.18198v1
"On the predictability of springtime ozone depletion events using the
  ECCC Global Deterministic Prediction System",2024-10-15T13:28:01Z,"J. de Grandpre, I. Ivanova, Y. J. Rochon, C. Jouan, P. A. Vaillancourt","Ozone depletion events are recurring phenomena in both polar regions,
characterized by significant interannual variability. In this study, the
Environment and Climate Change Canada (ECCC) Global Deterministic Prediction
System is used to investigate the medium-range predictability of ozone and
weather throughout the anomalous polar ozone depletion events of 2020. The
system includes ozone assimilation and makes use of a prognostic ozone field
for the computation of heating rates. The ozone scheme uses simplified
photochemical modules to represent the impact of both gas-phase and
heterogeneous reactions throughout polar ozone depletion events. The study
shows that during the Boreal and Austral spring seasons, the predictability of
the total ozone column exceeds 10 days and is comparable to the predictability
of large-scale weather variables. It also demonstrates that over both polar
regions, the inclusion of ozone radiative coupling has a significant impact on
the temperature and wind distributions throughout the stratosphere. Over
Antarctica, the ozone coupled forecasts are systematically colder at all lead
times, which helps eliminate a temperature bias present in the model using
climatological ozone. The strength of the polar vortex also increases
significantly throughout the lower stratosphere, in better agreement with zonal
wind analyses. Over the Arctic the use of an ozone-interactive model also
produces significant changes in the temperature and wind forecasts, but the
impact on the quality of the weather forecasts is generally neutral. The study
shows the overall benefits of using ozone coupled models in the highly
perturbed springtime conditions of the polar regions.",http://arxiv.org/abs/2410.11592v1
"TRIZ Method for Urban Building Energy Optimization: GWO-SARIMA-LSTM
  Forecasting model",2024-10-20T04:46:42Z,"Shirong Zheng, Shaobo Liu, Zhenhong Zhang, Dian Gu, Chunqiu Xia, Huadong Pang, Enock Mintah Ampaw","With the advancement of global climate change and sustainable development
goals, urban building energy consumption optimization and carbon emission
reduction have become the focus of research. Traditional energy consumption
prediction methods often lack accuracy and adaptability due to their inability
to fully consider complex energy consumption patterns, especially in dealing
with seasonal fluctuations and dynamic changes. This study proposes a hybrid
deep learning model that combines TRIZ innovation theory with GWO, SARIMA and
LSTM to improve the accuracy of building energy consumption prediction. TRIZ
plays a key role in model design, providing innovative solutions to achieve an
effective balance between energy efficiency, cost and comfort by systematically
analyzing the contradictions in energy consumption optimization. GWO is used to
optimize the parameters of the model to ensure that the model maintains high
accuracy under different conditions. The SARIMA model focuses on capturing
seasonal trends in the data, while the LSTM model handles short-term and
long-term dependencies in the data, further improving the accuracy of the
prediction. The main contribution of this research is the development of a
robust model that leverages the strengths of TRIZ and advanced deep learning
techniques, improving the accuracy of energy consumption predictions. Our
experiments demonstrate a significant 15% reduction in prediction error
compared to existing models. This innovative approach not only enhances urban
energy management but also provides a new framework for optimizing energy use
and reducing carbon emissions, contributing to sustainable development.",http://arxiv.org/abs/2410.15283v1
DDD-GenDT: Dynamic Data-driven Generative Digital Twin Framework,2024-12-28T01:13:30Z,"Yu-Zheng Lin, Qinxuan Shi, Zhanglong Yang, Banafsheh Saber Latibari, Sicong Shao, Soheil Salehi, Pratik Satam","Digital twin (DT) technology has emerged as a transformative approach to
simulate, predict, and optimize the behavior of physical systems, with
applications that span manufacturing, healthcare, climate science, and more.
However, the development of DT models often faces challenges such as high data
requirements, integration complexity, and limited adaptability to dynamic
changes in physical systems. This paper presents a new method inspired by
dynamic data-driven applications systems (DDDAS), called the dynamic
data-driven generative of digital twins framework (DDD-GenDT), which combines
the physical system with LLM, allowing LLM to act as DT to interact with the
physical system operating status and generate the corresponding physical
behaviors. We apply DDD-GenDT to the computer numerical control (CNC) machining
process, and we use the spindle current measurement data in the NASA milling
wear data set as an example to enable LLMs to forecast the physical behavior
from historical data and interact with current observations. Experimental
results show that in the zero-shot prediction setting, the LLM-based DT can
adapt to the change in the system, and the average RMSE of the GPT-4 prediction
is 0.479A, which is 4.79% of the maximum spindle motor current measurement of
10A, with little training data and instructions required. Furthermore, we
analyze the performance of DDD-GenDT in this specific application and their
potential to construct digital twins. We also discuss the limitations and
challenges that may arise in practical implementations.",http://arxiv.org/abs/2501.00051v1
Cross Domain Early Crop Mapping using CropSTGAN,2024-01-15T00:27:41Z,"Yiqun Wang, Hui Huang, Radu State","Driven by abundant satellite imagery, machine learning-based approaches have
recently been promoted to generate high-resolution crop cultivation maps to
support many agricultural applications. One of the major challenges faced by
these approaches is the limited availability of ground truth labels. In the
absence of ground truth, existing work usually adopts the ""direct transfer
strategy"" that trains a classifier using historical labels collected from other
regions and then applies the trained model to the target region. Unfortunately,
the spectral features of crops exhibit inter-region and inter-annual
variability due to changes in soil composition, climate conditions, and crop
progress, the resultant models perform poorly on new and unseen regions or
years. Despite recent efforts, such as the application of the deep adaptation
neural network (DANN) model structure in the deep adaptation crop
classification network (DACCN), to tackle the above cross-domain challenges,
their effectiveness diminishes significantly when there is a large
dissimilarity between the source and target regions. This paper introduces the
Crop Mapping Spectral-temporal Generative Adversarial Neural Network
(CropSTGAN), a novel solution for cross-domain challenges, that doesn't require
target domain labels. CropSTGAN learns to transform the target domain's
spectral features to those of the source domain, effectively bridging large
dissimilarities. Additionally, it employs an identity loss to maintain the
intrinsic local structure of the data. Comprehensive experiments across various
regions and years demonstrate the benefits and effectiveness of the proposed
approach. In experiments, CropSTGAN is benchmarked against various
state-of-the-art (SOTA) methods. Notably, CropSTGAN significantly outperforms
these methods in scenarios with large data distribution dissimilarities between
the target and source domains.",http://arxiv.org/abs/2401.07398v2
Eco-driving Intelligent Systems and Algorithms: A Patent Review,2024-01-15T09:51:25Z,"Zhipeng Ma, Bo N√∏rregaard J√∏rgensen, Zheng Grace Ma","The transportation industry remains a significant contributor to greenhouse
gas emissions, highlighting the requirement for intelligent systems to enhance
vehicle energy efficiency. The intellectual property rights of developed
systems should be protected by patents. However, there is no patent overview of
eco-driving intelligent systems. Unlike a scientific article, a patent
documentation indicates both novelty and commercialization potential of an
inventor. To address this research gap, this paper provides a patent overview
of eco-driving intelligent systems and algorithms. 424 patents in the Google
Patent database are analyzed. The patent analysis results show that the top
three Cooperative Patent Classifications are: Y02T - climate change mitigation
technologies related to transportation (50.7%), B60W - Conjoint control of
vehicle subunits of different types or different functions (34.4%) and B60L -
Propulsion of electrically-propelled vehicles (20.2%). 219 patents were filed
after 2016 when deep learning became popular and can be categorized into five
groups: vehicle energy management, smart driving, ecological and sustainable
driving, fuel consumption reduction, and driving behavior optimization.
Furthermore, all 219 patents involve the physical components of the intelligent
system and/or novel machine learning/deep learning algorithms. Moreover, over
70% of them are granted by the China National Intellectual Property
Administration.",http://arxiv.org/abs/2401.07559v1
"Graph Dual-stream Convolutional Attention Fusion for Precipitation
  Nowcasting",2024-01-15T20:54:20Z,"Lorand Vatamany, Siamak Mehrkanoon","Accurate precipitation nowcasting is crucial for applications such as flood
prediction, disaster management, agriculture optimization, and transportation
management. While many studies have approached this task using
sequence-to-sequence models, most focus on single regions, ignoring
correlations between disjoint areas. We reformulate precipitation nowcasting as
a spatiotemporal graph sequence problem. Specifically, we propose Graph
Dual-stream Convolutional Attention Fusion, a novel extension of the graph
attention network. Our model's dual-stream design employs distinct attention
mechanisms for spatial and temporal interactions, capturing their unique
dynamics. A gated fusion module integrates both streams, leveraging spatial and
temporal information for improved predictive accuracy. Additionally, our
framework enhances graph attention by directly processing three-dimensional
tensors within graph nodes, removing the need for reshaping. This capability
enables handling complex, high-dimensional data and exploiting higher-order
correlations between data dimensions. Depthwise-separable convolutions are also
incorporated to refine local feature extraction and efficiently manage
high-dimensional inputs. We evaluate our model using seven years of
precipitation data from Copernicus Climate Change Services, covering Europe and
neighboring regions. Experimental results demonstrate superior performance of
our approach compared to other models. Moreover, visualizations of seasonal
spatial and temporal attention scores provide insights into the most
significant connections between regions and time steps.",http://arxiv.org/abs/2401.07958v3
Narratives of Collective Action in YouTube's Discourse on Veganism,2024-01-17T13:44:36Z,"Arianna Pera, Luca Maria Aiello","Narratives can be powerful tools for inspiring action on pressing societal
issues such as climate change. While social science theories offer frameworks
for understanding the narratives that arise within collective movements, these
are rarely applied to the vast data available from social media platforms,
which play a significant role in shaping public opinion and mobilizing
collective action. This gap in the empirical evaluation of online narratives
limits our understanding of their relationship with public response. In this
study, we focus on plant-based diets as a form of pro-environmental action and
employ natural language processing to operationalize a theoretical framework of
moral narratives specific to the vegan movement. We apply this framework to
narratives found in YouTube videos promoting environmental initiatives such as
Veganuary, Meatless March, and No Meat May. Our analysis reveals that several
narrative types, as defined by the theory, are empirically present in the data.
To identify narratives with the potential to elicit positive public engagement,
we used text processing to estimate the proportion of comments supporting
collective action across narrative types. Video narratives advocating social
fight, whether through protest or through efforts to convert others to the
cause, are associated with a stronger sense of collective action in the
respective comments. These narrative types also demonstrate increased semantic
coherence and alignment between the message and public response, markers
typically associated with successful collective action. Our work offers new
insights into the complex factors that influence the emergence of collective
action, thereby informing the development of effective communication strategies
within social movements.",http://arxiv.org/abs/2401.09210v2
"A Survey on Energy Consumption and Environmental Impact of Video
  Streaming",2024-01-18T10:10:25Z,"Samira Afzal, Narges Mehran, Zoha Azimi Ourimi, Farzad Tashtarian, Hadi Amirpour, Radu Prodan, Christian Timmerer","Climate change challenges require a notable decrease in worldwide greenhouse
gas (GHG) emissions across technology sectors. Digital technologies, especially
video streaming, accounting for most Internet traffic, make no exception. Video
streaming demand increases with remote working, multimedia communication
services (e.g., WhatsApp, Skype), video streaming content (e.g., YouTube,
Netflix), video resolution (4K/8K, 50 fps/60 fps), and multi-view video, making
energy consumption and environmental footprint critical. This survey
contributes to a better understanding of sustainable and efficient video
streaming technologies by providing insights into the state-of-the-art and
potential future directions for researchers, developers, and engineers, service
providers, hosting platforms, and consumers. We widen this survey's focus on
content provisioning and content consumption based on the observation that
continuously active network equipment underneath video streaming consumes
substantial energy independent of the transmitted data type. We propose a
taxonomy of factors that affect the energy consumption in video streaming, such
as encoding schemes, resource requirements, storage, content retrieval,
decoding, and display. We identify notable weaknesses in video streaming that
require further research for improved energy efficiency: (1) fixed bitrate
ladders in HTTP live streaming; (2) inefficient hardware utilization of
existing video players; (3) lack of comprehensive open energy measurement
dataset covering various device types and coding parameters for reproducible
research.",http://arxiv.org/abs/2401.09854v1
"How norms shape the evolution of prosocial behavior. Compassion,
  Universalizability, Reciprocity, Equity: A C.U.R.E for social dilemmas",2024-01-23T16:42:13Z,"Brian Mintz, Feng Fu","How cooperation evolves and particularly maintains at a large scale remains
an open problem for improving humanity across domains ranging from climate
change to pandemic response. To shed light on how behavioral norms can resolve
the social dilemma of cooperation, here we present a formal mathematical model
of individuals' decision making under general social norms, encompassing a
variety of concerns and motivations an individual may have beyond simply
maximizing their own payoffs. Using the canonical game of the Prisoner's
Dilemma, we compare four different norms: compassion, universalizability,
reciprocity, and equity, to determine which social forces can facilitate the
evolution of cooperation, if any. We analyze our model through a variety of
limiting cases, including weak selection, low mutation, and large population
sizes. This is complemented by computer simulations of population dynamics via
a Fisher process, which confirm our theoretical results. We find that the first
two norms lead to the emergence of cooperation in a wide range of games, but
the latter two do not on their own. Due to its generality, our framework can be
used to investigate many more norms, as well as how norms themselves emerge and
evolve. Our work complements recent work on fair-minded learning dynamics and
provides a useful bottom-up perspective into understanding the impact of
top-down social norms on collective cooperative intelligence.",http://arxiv.org/abs/2401.13015v1
"Image based Crop Monitoring Technologies in Protected Horticulture: A
  Review",2024-01-25T03:58:27Z,"Namal Jayasuriya, Yi Guo, Wen Hu, Oula Ghannoum","Future food security is a major concern of the 21st century with the growing
global population and climate changes. In addressing these challenges,
protected cropping ensures food production year-round and increases crop
production per land area by controlling environment conditions. Maintaining the
growth and health of crops in these facilities is essential to ensure optimum
food production. However, this is a laborious work and is currently done
manually. Image-based non-destructive plant phenotyping is an emerging research
area that reduces the skilled labour cost while enhancing the monitoring of
crop growth, health, and identifying phenotype-genotype relations for plant
breeding. With the proliferations of protected infrastructures and targeted
plants, different technologies and sensor setups are needed for image-based
crop monitoring. Conveyor-type plant-to-sensor systems, bench-top or
gantry-based systems are commonly found in research facilities focussing on
phenotyping of small, relatively short, or movable model plants. This review
examines the literature on crop monitoring and phenotyping platforms in both
field and protected facilities and explains different camera technologies and
their ability to extract different plant traits. The review highlights the
future research directions of image-based monitoring of commercial scale
protected crops where crops can be relatively tall or vertically supported
under semi controlled environments, which presents new challenges and is rarely
covered in the literature.",http://arxiv.org/abs/2401.13928v1
"Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar
  Power Generation",2024-01-24T02:08:48Z,"Md Shazid Islam, A S M Jahid Hasan, Md Saydur Rahman, Jubair Yusuf, Md Saiful Islam Sajol, Farhana Akter Tumpa","The prediction of solar power generation is a challenging task due to its
dependence on climatic characteristics that exhibit spatial and temporal
variability. The performance of a prediction model may vary across different
places due to changes in data distribution, resulting in a model that works
well in one region but not in others. Furthermore, as a consequence of global
warming, there is a notable acceleration in the alteration of weather patterns
on an annual basis. This phenomenon introduces the potential for diminished
efficacy of existing models, even within the same geographical region, as time
progresses. In this paper, a domain adaptive deep learning-based framework is
proposed to estimate solar power generation using weather features that can
solve the aforementioned challenges. A feed-forward deep convolutional network
model is trained for a known location dataset in a supervised manner and
utilized to predict the solar power of an unknown location later. This adaptive
data-driven approach exhibits notable advantages in terms of computing speed,
storage efficiency, and its ability to improve outcomes in scenarios where
state-of-the-art non-adaptive methods fail. Our method has shown an improvement
of $10.47 \%$, $7.44 \%$, $5.11\%$ in solar power prediction accuracy compared
to best performing non-adaptive method for California (CA), Florida (FL) and
New York (NY), respectively.",http://arxiv.org/abs/2401.14422v2
"The Role of Intelligent Transportation Systems and Artificial
  Intelligence in Energy Efficiency and Emission Reduction",2024-01-25T23:07:32Z,"Omar Rinchi, Ahmad Alsharoa, Ibrahem Shatnawi, Anvita Arora","Despite the technological advancements in the transportation sector, the
industry continues to grapple with increasing energy consumption and vehicular
emissions, which intensify environmental degradation and climate change. The
inefficient management of traffic flow, the underutilization of transport
network interconnectivity, and the limited implementation of artificial
intelligence (AI)-driven predictive models pose significant challenges to
achieving energy efficiency and emission reduction. Thus, there is a timely and
critical need for an integrated, sophisticated approach that leverages
intelligent transportation systems (ITSs) and AI for energy conservation and
emission reduction. In this paper, we explore the role of ITSs and AI in future
enhanced energy and emission reduction (EER). More specifically, we discuss the
impact of sensors at different levels of ITS on improving EER. We also
investigate the potential networking connections in ITSs and provide an
illustration of how they improve EER. Finally, we discuss potential AI services
for improved EER in the future. The findings discussed in this paper will
contribute to the ongoing discussion about the vital role of ITSs and AI
applications in addressing the challenges associated with achieving energy
savings and emission reductions in the transportation sector. Additionally, it
will provide insights for policymakers and industry professionals to enable
them to develop policies and implementation plans for the integration of ITSs
and AI technologies in the transportation sector.",http://arxiv.org/abs/2401.14560v1
"Globally Scalable Glacier Mapping by Deep Learning Matches Expert
  Delineation Accuracy",2024-01-25T20:41:17Z,"Konstantin A. Maslov, Claudio Persello, Thomas Schellenberger, Alfred Stein","Accurate global glacier mapping is critical for understanding climate change
impacts. Despite its importance, automated glacier mapping at a global scale
remains largely unexplored. Here we address this gap and propose
Glacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deep
learning model, and five strategies for multitemporal global-scale glacier
mapping using open satellite imagery. Assessing the spatial, temporal and
cross-sensor generalisation shows that our best strategy achieves intersection
over union >0.85 on previously unobserved images in most cases, which drops to
>0.75 for debris-rich areas such as High-Mountain Asia and increases to >0.90
for regions dominated by clean ice. A comparative validation against human
expert uncertainties in terms of area and distance deviations underscores
GlaViTU performance, approaching or matching expert-level delineation. Adding
synthetic aperture radar data, namely, backscatter and interferometric
coherence, increases the accuracy in all regions where available. The
calibrated confidence for glacier extents is reported making the predictions
more reliable and interpretable. We also release a benchmark dataset that
covers 9% of glaciers worldwide. Our results support efforts towards automated
multitemporal and global glacier mapping.",http://arxiv.org/abs/2401.15113v4
"Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach
  Without Reanalysis Data",2024-01-28T18:28:33Z,"Young-Jae Park, Minseok Seo, Doyi Kim, Hyeri Kim, Sanghoon Choi, Beomkyu Choi, Jeongwon Ryu, Sohee Son, Hae-Gon Jeon, Yeji Choi","In the face of escalating climate changes, typhoon intensities and their
ensuing damage have surged. Accurate trajectory prediction is crucial for
effective damage control. Traditional physics-based models, while
comprehensive, are computationally intensive and rely heavily on the expertise
of forecasters. Contemporary data-driven methods often rely on reanalysis data,
which can be considered to be the closest to the true representation of weather
conditions. However, reanalysis data is not produced in real-time and requires
time for adjustment because prediction models are calibrated with observational
data. This reanalysis data, such as ERA5, falls short in challenging real-world
situations. Optimal preparedness necessitates predictions at least 72 hours in
advance, beyond the capabilities of standard physics models. In response to
these constraints, we present an approach that harnesses real-time Unified
Model (UM) data, sidestepping the limitations of reanalysis data. Our model
provides predictions at 6-hour intervals for up to 72 hours in advance and
outperforms both state-of-the-art data-driven methods and numerical weather
prediction models. In line with our efforts to mitigate adversities inflicted
by \rthree{typhoons}, we release our preprocessed \textit{PHYSICS TRACK}
dataset, which includes ERA5 reanalysis data, typhoon best-track, and UM
forecast data.",http://arxiv.org/abs/2401.15726v1
"GHG emissions in the EU-28. A multilevel club convergence study of the
  Emission Trading System and Effort Sharing Decision mechanisms",2024-02-01T21:29:36Z,"Mar√≠a Jos√© Presno, Manuel Landajo, Paula Fern√°ndez Gonz√°lez","The European Union is engaged in the fight against climate change. A crucial
issue to enforce common environmental guidelines is environmental convergence.
States converging in environmental variables are expected to be able to jointly
develop and implement environmental policies. Convergence in environmental
indicators may also help determine the efficiency and speed of those policies.
This paper employs a multilevel club convergence approach to analyze
convergence in the evolution of GHG emissions among the EU-28 members, on a
search for countries transitioning from disequilibrium to specific steady-state
positions. Overall convergence is rejected, with club composition depending on
the specific period (1990-2017, 2005-2017) and emissions categories (global,
ETS, ESD) analyzed. Some countries (e.g. the United Kingdom and Denmark) are
consistently located in clubs outperforming the EU's average in terms of speed
of emissions reductions, for both the whole and the most recent periods, and
for both ETS and ESD emissions. At the other end, Germany (with a large
industrial and export basis), Ireland (with the strongest GDP growth in the EU
in recent years) and most Eastern EU members underperform after 2005, almost
reversing their previous positions when the study begins in 1990.",http://arxiv.org/abs/2402.01784v2
"EBV: Electronic Bee-Veterinarian for Principled Mining and Forecasting
  of Honeybee Time Series",2024-02-02T21:05:56Z,"Mst. Shamima Hossain, Christos Faloutsos, Boris Baer, Hyoseung Kim, Vassilis J. Tsotras","Honeybees are vital for pollination and food production. Among many factors,
extreme temperature (e.g., due to climate change) is particularly dangerous for
bee health. Anticipating such extremities would allow beekeepers to take early
preventive action. Thus, given sensor (temperature) time series data from
beehives, how can we find patterns and do forecasting? Forecasting is crucial
as it helps spot unexpected behavior and thus issue warnings to the beekeepers.
In that case, what are the right models for forecasting? ARIMA, RNNs, or
something else?
  We propose the EBV (Electronic Bee-Veterinarian) method, which has the
following desirable properties: (i) principled: it is based on a) diffusion
equations from physics and b) control theory for feedback-loop controllers;
(ii) effective: it works well on multiple, real-world time sequences, (iii)
explainable: it needs only a handful of parameters (e.g., bee strength) that
beekeepers can easily understand and trust, and (iv) scalable: it performs
linearly in time. We applied our method to multiple real-world time sequences,
and found that it yields accurate forecasting (up to 49% improvement in RMSE
compared to baselines), and segmentation. Specifically, discontinuities
detected by EBV mostly coincide with domain expert's opinions, showcasing our
approach's potential and practical feasibility. Moreover, EBV is scalable and
fast, taking about 20 minutes on a stock laptop for reconstructing two months
of sensor data.",http://arxiv.org/abs/2402.01902v1
"Intelligent Agricultural Greenhouse Control System Based on Internet of
  Things and Machine Learning",2024-02-14T09:07:00Z,Cangqing Wang,"This study endeavors to conceptualize and execute a sophisticated
agricultural greenhouse control system grounded in the amalgamation of the
Internet of Things (IoT) and machine learning. Through meticulous monitoring of
intrinsic environmental parameters within the greenhouse and the integration of
machine learning algorithms, the conditions within the greenhouse are aptly
modulated. The envisaged outcome is an enhancement in crop growth efficiency
and yield, accompanied by a reduction in resource wastage. In the backdrop of
escalating global population figures and the escalating exigencies of climate
change, agriculture confronts unprecedented challenges. Conventional
agricultural paradigms have proven inadequate in addressing the imperatives of
food safety and production efficiency. Against this backdrop, greenhouse
agriculture emerges as a viable solution, proffering a controlled milieu for
crop cultivation to augment yields, refine quality, and diminish reliance on
natural resources [b1]. Nevertheless, greenhouse agriculture contends with a
gamut of challenges. Traditional greenhouse management strategies, often
grounded in experiential knowledge and predefined rules, lack targeted
personalized regulation, thereby resulting in resource inefficiencies. The
exigencies of real-time monitoring and precise control of the greenhouse's
internal environment gain paramount importance with the burgeoning scale of
agriculture. To redress this challenge, the study introduces IoT technology and
machine learning algorithms into greenhouse agriculture, aspiring to institute
an intelligent agricultural greenhouse control system conducive to augmenting
the efficiency and sustainability of agricultural production.",http://arxiv.org/abs/2402.09488v1
Explaining the emergence of land-use frontiers,2024-02-19T19:50:29Z,"Patrick Meyfroidt, Dilini Abeygunawardane, Matthias Baumann, Adia Bey, Ana Buchadas, Cristina Chiarella, Victoria Junquera, Angela Kronenburg Garc√≠a, Tobias Kuemmerle, Yann le Polain de Waroux, Eduardo Oliveira, Michelle Picoli, Siyu Qin, Virginia Rodriguez Garc√≠a, Philippe Rufin","Land use expansion is linked to major sustainability concerns including
climate change, food security and biodiversity loss. This expansion is largely
concentrated in so-called frontiers, defined here as places experiencing marked
transformations due to rapid resource exploitation. Understanding the
mechanisms shaping these frontiers is crucial for sustainability. Previous work
focused mainly on explaining how active frontiers advance, in particular into
tropical forests. Comparatively, our understanding of how frontiers emerge in
territories considered marginal in terms of agricultural productivity and
global market integration remains weak. We synthesize conceptual tools
explaining resource and land-use frontiers, including theories of land rent and
agglomeration economies, of frontiers as successive waves, spaces of
territorialization, friction, and opportunities, anticipation and expectation.
We then propose a new theory of frontier emergence, which identifies exogenous
pushes, legacies of past waves, and actors anticipations as key mechanisms by
which frontiers emerge. Processes of abnormal rent creation and capture and the
built-up of agglomeration economies then constitute key mechanisms sustaining
active frontiers. Finally, we discuss five implications for the governance of
frontiers for sustainability. Our theory focuses on agriculture and
deforestation frontiers in the tropics, but can be inspirational for other
frontier processes including for extractive resources, such as minerals.",http://arxiv.org/abs/2402.12487v1
"AI Language Models Could Both Help and Harm Equity in Marine
  Policymaking: The Case Study of the BBNJ Question-Answering Bot",2024-03-04T06:21:02Z,"Matt Ziegler, Sarah Lothian, Brian O'Neill, Richard Anderson, Yoshitaka Ota","AI Large Language Models (LLMs) like ChatGPT are set to reshape some aspects
of policymaking processes. Policy practitioners are already using ChatGPT for
help with a variety of tasks: from drafting statements, submissions, and
presentations, to conducting background research. We are cautiously hopeful
that LLMs could be used to promote a marginally more balanced footing among
decision makers in policy negotiations by assisting with certain tedious work,
particularly benefiting developing countries who face capacity constraints that
put them at a disadvantage in negotiations. However, the risks are particularly
concerning for environmental and marine policy uses, due to the urgency of
crises like climate change, high uncertainty, and trans-boundary impact.
  To explore the realistic potentials, limitations, and equity risks for LLMs
in marine policymaking, we present a case study of an AI chatbot for the
recently adopted Biodiversity Beyond National Jurisdiction Agreement (BBNJ),
and critique its answers to key policy questions. Our case study demonstrates
the dangers of LLMs in marine policymaking via their potential bias towards
generating text that favors the perspectives of mainly Western economic centers
of power, while neglecting developing countries' viewpoints. We describe
several ways these biases can enter the system, including: (1) biases in the
underlying foundational language models; (2) biases arising from the chatbot's
connection to UN negotiation documents, and (3) biases arising from the
application design. We urge caution in the use of generative AI in ocean policy
processes and call for more research on its equity and fairness implications.
Our work also underscores the need for developing countries' policymakers to
develop the technical capacity to engage with AI on their own terms.",http://arxiv.org/abs/2403.01755v1
"Deep learning for multi-label classification of coral conditions in the
  Indo-Pacific via underwater photogrammetry",2024-03-09T14:42:16Z,"Xinlei Shao, Hongruixuan Chen, Kirsty Magson, Jiaqi Wang, Jian Song, Jundong Chen, Jun Sasaki","Since coral reef ecosystems face threats from human activities and climate
change, coral conservation programs are implemented worldwide. Monitoring coral
health provides references for guiding conservation activities. However,
current labor-intensive methods result in a backlog of unsorted images,
highlighting the need for automated classification. Few studies have
simultaneously utilized accurate annotations along with updated algorithms and
datasets. This study aimed to create a dataset representing common coral
conditions and associated stressors in the Indo-Pacific. Concurrently, it
assessed existing classification algorithms and proposed a new multi-label
method for automatically detecting coral conditions and extracting ecological
information. A dataset containing over 20,000 high-resolution coral images of
different health conditions and stressors was constructed based on the field
survey. Seven representative deep learning architectures were tested on this
dataset, and their performance was quantitatively evaluated using the F1 metric
and the match ratio. Based on this evaluation, a new method utilizing the
ensemble learning approach was proposed. The proposed method accurately
classified coral conditions as healthy, compromised, dead, and rubble; it also
identified corresponding stressors, including competition, disease, predation,
and physical issues. This method can help develop the coral image archive,
guide conservation activities, and provide references for decision-making for
reef managers and conservationists. The proposed ensemble learning approach
outperforms others on the dataset, showing State-Of-The-Art (SOTA) performance.
Future research should improve its generalizability and accuracy to support
global coral conservation efforts.",http://arxiv.org/abs/2403.05930v2
"CarbonNet: How Computer Vision Plays a Role in Climate Change?
  Application: Learning Geomechanics from Subsurface Geometry of CCS to
  Mitigate Global Warming",2024-03-09T22:25:14Z,"Wei Chen, Yunan Li, Yuan Tian","We introduce a new approach using computer vision to predict the land surface
displacement from subsurface geometry images for Carbon Capture and
Sequestration (CCS). CCS has been proved to be a key component for a carbon
neutral society. However, scientists see there are challenges along the way
including the high computational cost due to the large model scale and
limitations to generalize a pre-trained model with complex physics. We tackle
those challenges by training models directly from the subsurface geometry
images. The goal is to understand the respons of land surface displacement due
to carbon injection and utilize our trained models to inform decision making in
CCS projects.
  We implement multiple models (CNN, ResNet, and ResNetUNet) for static
mechanics problem, which is a image prediction problem. Next, we use the LSTM
and transformer for transient mechanics scenario, which is a video prediction
problem. It shows ResNetUNet outperforms the others thanks to its architecture
in static mechanics problem, and LSTM shows comparable performance to
transformer in transient problem. This report proceeds by outlining our dataset
in detail followed by model descriptions in method section. Result and
discussion state the key learning, observations, and conclusion with future
work rounds out the paper.",http://arxiv.org/abs/2403.06025v3
"Disentangling Resilience from Robustness: Contextual Dualism,
  Interactionism, and Game-Theoretic Paradigms",2024-03-10T20:09:23Z,"Quanyan Zhu, Tamer Basar","This article explains the distinctions between robustness and resilience in
control systems. Resilience confronts a distinct set of challenges, posing new
ones for designing controllers for feedback systems, networks, and machines
that prioritize resilience over robustness. The concept of resilience is
explored through a three-stage model, emphasizing the need for a proactive
preparation and automated response to elastic events. A toy model is first used
to illustrate the tradeoffs between resilience and robustness. Then, it delves
into contextual dualism and interactionism, and introduces game-theoretic
paradigms as a unifying framework to consolidate resilience and robustness. The
article concludes by discussing the interplay between robustness and
resilience, suggesting that a comprehensive theory of resilience and
quantification metrics, and formalization through game-theoretic frameworks are
necessary. The exploration extends to system-of-systems resilience and various
mechanisms, including the integration of AI techniques and non-technical
solutions, like cyber insurance, to achieve comprehensive resilience in control
systems. As we approach 2030, the systems and control community is at the
opportune moment to lay scientific foundations of resilience by bridging
feedback control theory, game theory, and learning theory. Resilient control
systems will enhance overall quality of life, enable the development of a
resilient society, and create a societal-scale impact amid global challenges
such as climate change, conflicts, and cyber insecurity.",http://arxiv.org/abs/2403.06299v1
"A Synergistic Approach to Wildfire Prevention and Management Using AI,
  ML, and 5G Technology in the United States",2024-02-27T04:09:30Z,"Stanley Chinedu Okoro, Alexander Lopez, Austine Unuriode","Over the past few years, wildfires have become a worldwide environmental
emergency, resulting in substantial harm to natural habitats and playing a part
in the acceleration of climate change. Wildfire management methods involve
prevention, response, and recovery efforts. Despite improvements in detection
techniques, the rising occurrence of wildfires demands creative solutions for
prompt identification and effective control. This research investigates
proactive methods for detecting and handling wildfires in the United States,
utilizing Artificial Intelligence (AI), Machine Learning (ML), and 5G
technology. The specific objective of this research covers proactive detection
and prevention of wildfires using advanced technology; Active monitoring and
mapping with remote sensing and signaling leveraging on 5G technology; and
Advanced response mechanisms to wildfire using drones and IOT devices. This
study was based on secondary data collected from government databases and
analyzed using descriptive statistics. In addition, past publications were
reviewed through content analysis, and narrative synthesis was used to present
the observations from various studies. The results showed that developing new
technology presents an opportunity to detect and manage wildfires proactively.
Utilizing advanced technology could save lives and prevent significant economic
losses caused by wildfires. Various methods, such as AI-enabled remote sensing
and 5G-based active monitoring, can enhance proactive wildfire detection and
management. In addition, super intelligent drones and IOT devices can be used
for safer responses to wildfires. This forms the core of the recommendation to
the fire Management Agencies and the government.",http://arxiv.org/abs/2403.14657v1
Data Analytics for Improving Energy Efficiency in Short Sea Shipping,2024-04-01T04:08:08Z,"Mohamed Abuella, Hadi Fanaee, M. Amine Atou, Slawomir Nowaczyk, Simon Johansson, Ethan Faghani","To meet the urgent requirements for the climate change mitigation, several
proactive measures of energy efficiency have been implemented in maritime
industry. Many of these practices depend highly on the onboard data of vessel's
operation and environmental conditions. In this paper, a high resolution
onboard data from passenger vessels in short-sea shipping (SSS) have been
collected and preprocessed. We first investigated the available data to deploy
it effectively to model the physics of the vessel, and hence the vessel
performance. Since in SSS, the weather measurements and forecasts might have
not been in temporal and spatial resolutions that accurately representing the
actual environmental conditions. Then, We proposed a data-driven modeling
approach for vessel energy efficiency. This approach addresses the challenges
of data representation and energy modeling by combining and aggregating data
from multiple sources and seamlessly integrates explainable artificial
intelligence (XAI) to attain clear insights about the energy efficiency for a
vessel in SSS. After that, the developed model of energy efficiency has been
utilized in developing a framework for optimizing the vessel voyage to minimize
the fuel consumption and meeting the constraint of arrival time. Moreover, we
developed a spatial clustering approach for labeling the vessel paths to detect
the paths for vessels with operating routes of repeatable and semi-repeatable
paths.",http://arxiv.org/abs/2404.00902v1
"Sustainable and Precision Agriculture with the Internet of Everything
  (IoE)",2024-04-09T14:25:52Z,"Adil Z. Babar, Ozgur B. Akan","Agriculture faces critical challenges from population growth, resource
scarcity, and climate change, driving a shift toward advanced,
technology-integrated farming. Mechanization has transformed agriculture,
enhancing sustainability and crop productivity. Now, technologies like
artificial intelligence (AI), robotics, biotechnology, blockchain, and the
Internet of Things (IoT) are advancing precision agriculture. The concept of
the Internet of Everything (IoE) has gained traction due to its holistic
approach to integrating various IoT specializations, called IoXs with X
referring to a specific domain. This paper explores the transformative role of
IoE in agriculture, expanding beyond traditional IoT applications to integrate
niche subdomains like molecular communication (MC), the Internet of Nano Things
(IoNT), the Internet of Bio-Nano Things (IoBNT), designer phages, and the
Internet of Fungus (IoF). Our study provides a detailed review of how these IoE
subdomains, in conjunction with 6G, blockchain, and machine learning (ML), can
enhance precision farming in areas like crop monitoring, resource management,
and disease control. Unlike prior IoT centric reviews, this work uniquely
focuses on IoEs potential to advance agriculture at molecular and biological
scales, achieving more precise resource utilization and resilience. Key
contributions include an exploration of these technologies applicability,
associated challenges, and recommendations for future research directions
within precision agriculture.",http://arxiv.org/abs/2404.06341v3
More extreme Indian monsoon daily rainfall in El Ni√±o summers,2024-04-18T17:55:56Z,"Spencer A Hill, Destiny Zamir Meyers, Adam H Sobel, Michela Biasutti, Mark A Cane, Michael K Tippett, Fiaz Ahmed","Extreme rainfall in the Indian summer monsoon can be destructive and deadly.
Although El Ni\~no/ events in the equatorial Pacific make dry days and whole
summers more likely throughout India, their influence on daily extremes is not
well established. Despite this summer-mean drying effect, we show using
observational data spanning 1901-2020 that El Ni\~no increases extreme rainfall
likelihoods within monsoonal India, especially in the the summer's core rainy
areas of central-eastern India and the narrow southwestern coastal band.
Conversely, extremes are broadly suppressed in the drier southeast and far
northwest, and more moderate accumulations are inhibited throughout the domain.
These rainfall signals appear driven by corresponding ones in convective
buoyancy, provided both the undilute instability of near-surface air and its
dilution by mixing with drier air above are accounted for. When the summer ENSO
state is predicted from a seasonal forecast ensemble initialized in May, the
extreme rainfall patterns broadly persist, suggesting the potential for
skillful seasonal forecasts. The framework of analyzing the full distributions
of rainfall and convective buoyancy could be usefully applied to hourly
extremes, other tropical regions under ENSO, other variability modes, and to
trends in extreme rainfall under climate change.",http://arxiv.org/abs/2404.12419v1
Urban topology and dynamics can assess green areas importance,2024-04-19T14:18:54Z,"Jacopo Moi, Gherardo Chirici, Leonardo Chiesi, Saverio Francini, Costanza Borghi, Paolo Costa, Bianca Gramellini, Guido Caldarelli","Green areas are a crucial element in evolution of a city, contributing to
improve citizens' life, to reduce effects of climate change, and to make
possible the survival of other species in urban areas. Unfortunately, the above
effects are difficult to assess quantitatively for regulators, stakeholders and
experts, making troublesome the planning of city development. Here we present a
method to estimate the impact of these areas in the city life based on the
network topology of the city itself and on a simple model of dynamics on this
structure. Movements between various areas of the city are simulated by means
of an agent-based biased-diffusion process where citizens try to reach the
nearest Public Green Area (PGA) from their position and the model is fed with
real data about the density of populations in the cases of study. Firstly, we
define a centrality measure of PGA's based on average farness measured on the
city network; this approach outperforms information based on the simple
topology. We then improve this quantity by taking into account the occupation
of PGA's, thereby providing a quantitative measure of PGA usage for regulators.",http://arxiv.org/abs/2404.12902v2
"Spatio-temporal Joint Analysis of PM2.5 and Ozone in California with
  INLA",2024-04-20T09:42:37Z,"Jianan Pan, Kunyang He, Kai Wang, Qing Mu, Chengxiu Ling","The substantial threat of concurrent air pollutants to public health is
increasingly severe under climate change. To identify the common drivers and
extent of spatio-temporal similarity of PM2.5 and ozone, this paper proposed a
log Gaussian-Gumbel Bayesian hierarchical model allowing for sharing a
SPDE-AR(1) spatio-temporal interaction structure. The proposed model
outperforms in terms of estimation accuracy and prediction capacity for its
increased parsimony and reduced uncertainty, especially for the shared ozone
sub-model. Besides the consistently significant influence of temperature
(positive), extreme drought (positive), fire burnt area (positive), and wind
speed (negative) on both PM2.5 and ozone, surface pressure and GDP per capita
(precipitation) demonstrate only positive associations with PM2.5 (ozone),
while population density relates to neither. In addition, our results show the
distinct spatio-temporal interactions and different seasonal patterns of PM2.5
and ozone, with peaks of PM2.5 and ozone in cold and hot seasons, respectively.
Finally, with the aid of the excursion function, we see that the areas around
the intersection of San Luis Obispo and Santa Barbara counties are likely to
exceed the unhealthy ozone level for sensitive groups throughout the year. Our
findings provide new insights for regional and seasonal strategies in the
co-control of PM2.5 and ozone. Our methodology is expected to be utilized when
interest lies in multiple interrelated processes in the fields of environment
and epidemiology.",http://arxiv.org/abs/2404.14446v1
"Coupling in situ and remote sensing data to assess $Œ±$- and
  $Œ≤$-diversity over biogeographic gradients",2024-04-29T07:53:50Z,"Maxime Lenormand, Jean-Baptiste F√©ret, Guillaume Papuga, Samuel Alleaume, Sandra Luque","The challenges presented by climate change are escalating and pose
significant threats to global biodiversity, which in turn increases the risk of
species extinctions. Therefore, meticulous monitoring efforts are necessary to
mitigate the consequential impacts on both human well-being and environmental
equilibrium. Biodiversity mapping is pivotal for establishing conservation
priorities, often accomplished by assessing alpha, beta, and gamma diversity
levels. Two main data sources, in situ and remote sensing (RS) data, are key
for this task. In situ methods entail direct data collection from specific
study areas, offering detailed insights into ecological patterns, albeit
limited by resource constraints. Conversely, RS provides a broader
observational platform, albeit at lower spatial resolution than in situ
approaches. RS-derived diversity metrics have potential, particularly in
linking spectral and biological diversity through high-resolution imagery for
precise differentiation at fine scales. Coupling in situ and RS data
underscores their complementary nature, contingent upon various factors
including study scale and logistical considerations. In situ methods excel in
precision, while RS offers efficiency and broader coverage. Despite prior
investigations predominantly relying on limited datasets, our study endeavors
to employ both in situ and RS data to assess plant and spectral species
diversity across France at a high spatial resolution, integrating diverse
metrics to unravel different biogeographical structures while gaining in
understanding the relationship between plant and spectral diversity within and
across bioregions.",http://arxiv.org/abs/2404.18485v1
"S3Former: Self-supervised High-resolution Transformer for Solar PV
  Profiling",2024-05-07T16:56:21Z,"Minh Tran, Adrian De Luis, Haitao Liao, Ying Huang, Roy McCann, Alan Mantooth, Jack Cothren, Ngan Le","As the impact of climate change escalates, the global necessity to transition
to sustainable energy sources becomes increasingly evident. Renewable energies
have emerged as a viable solution for users, with Photovoltaic energy being a
favored choice for small installations due to its reliability and efficiency.
Accurate mapping of PV installations is crucial for understanding the extension
of its adoption and informing energy policy. To meet this need, we introduce
S3Former, designed to segment solar panels from aerial imagery and provide size
and location information critical for analyzing the impact of such
installations on the grid. Solar panel identification is challenging due to
factors such as varying weather conditions, roof characteristics, Ground
Sampling Distance variations and lack of appropriate initialization weights for
optimized training. To tackle these complexities, S3Former features a Masked
Attention Mask Transformer incorporating a self-supervised learning pretrained
backbone. Specifically, our model leverages low-level and high-level features
extracted from the backbone and incorporates an instance query mechanism
incorporated on the Transformer architecture to enhance the localization of
solar PV installations. We introduce a self-supervised learning phase (pretext
task) to improve the initialization weights on the backbone of S3Former. We
evaluated S3Former using diverse datasets, demonstrate improvement
state-of-the-art models.",http://arxiv.org/abs/2405.04489v1
"Knowledge Gaps and Research Needs for Modeling CO2 Mineralization in the
  Basalt-CO2-Water System: A Review of Laboratory Experiments",2024-05-08T15:19:50Z,"Peng Lu, John Apps, Guanru Zhang, Alexander Gysi Chen Zhu","Carbon capture and storage in basalt is being actively investigated as a
scalable climate change mitigation option. Accurate geochemical modeling
prediction of the extent and rate of CO2 mineralization is a critical component
in assessing the local and global feasibility and efficacy of this strategy. In
this study, we review basalt-CO2-water interaction experimental studies
conducted during the last two decades to determine whether they provide useable
information for geochemical modeling. Most of the cited experiments generate
data on the temporal evolution of water composition, and a few provide
identification of secondary precipitates and their compositions, offering
empirical and semi-quantitative information about the reactivity of basalts and
the likelihood of secondary carbonate mineralization at various temperatures,
pHs, and pCO2 conditions. However, most experiments provide insufficient
information on the properties and quantity of secondary minerals formed,
prohibiting accurate mass balance calculations and hence more quantitative
geochemical modeling studies. Primary Ca, Mg, and Fe-bearing minerals in basalt
control the availability of major ions released into aqueous solution for
carbonate precipitation, and many secondary minerals, i.e., smectites, Ca-Mg-Fe
carbonates, and zeolites, provide sinks for the same major ions, some of which
are difficult to quantify experimentally. Thus, we have a multi-source and
multi-sink inverse mass balance problem with insufficient constraints on the
bulk system in which the temporal evolution of major ions does not provide
sufficient information on which mineral(s) dissolve or the sequence of
dissolution and precipitation reactions. Going forward, we propose that future
experimental work should focus on trace elements and multiple isotopic tracers
and better characterize the solid reaction products with modern analytical
instruments.",http://arxiv.org/abs/2405.05122v1
"Quantifying risk of a noise-induced AMOC collapse from northern and
  tropical Atlantic Ocean variability",2024-05-17T17:27:50Z,"R. Chapman, P. Ashwin, J. Baker, R. A. Wood","The Atlantic Meridional Overturning Circulation (AMOC) exerts a major
influence on global climate. There is much debate about whether the current
strong AMOC may collapse as a result of anthropogenic forcing and/or internal
variability. Increasing the noise in simple salt-advection models can change
the apparent AMOC tipping threshold. However, it's not clear if 'present-day'
variability is strong enough to induce a collapse. Here, we investigate how
internal variability affects the likelihood of AMOC collapse. We examine
internal variability of basin-scale salinities and temperatures in four CMIP6
pre-industrial simulations. We fit this to an empirical, process-based AMOC box
model, and find that noise-induced AMOC collapse (defined as a decade in which
the mean AMOC strength falls below 5 Sv) is unlikely for pre-industrial CMIP6
variability unless external forcing shifts the AMOC closer to a threshold.
However, CMIP6 models seem to underestimate present-day Atlantic Ocean
variability, and stronger variability substantially increases the likelihood of
noise-induced collapse, especially if forcing brings the AMOC close to a
stability threshold. Surprisingly, we find a case where forcing temporarily
overshoots a stability threshold but noise decreases the probability of
collapse. Accurately modelling internal decadal variability is essential for
understanding the increased uncertainty in AMOC projections.",http://arxiv.org/abs/2405.10929v2
"TinyM$^2$Net-V3: Memory-Aware Compressed Multimodal Deep Neural Networks
  for Sustainable Edge Deployment",2024-05-20T20:03:51Z,"Hasib-Al Rashid, Tinoosh Mohsenin","The advancement of sophisticated artificial intelligence (AI) algorithms has
led to a notable increase in energy usage and carbon dioxide emissions,
intensifying concerns about climate change. This growing problem has brought
the environmental sustainability of AI technologies to the forefront,
especially as they expand across various sectors. In response to these
challenges, there is an urgent need for the development of sustainable AI
solutions. These solutions must focus on energy-efficient embedded systems that
are capable of handling diverse data types even in environments with limited
resources, thereby ensuring both technological progress and environmental
responsibility. Integrating complementary multimodal data into tiny machine
learning models for edge devices is challenging due to increased complexity,
latency, and power consumption. This work introduces TinyM$^2$Net-V3, a system
that processes different modalities of complementary data, designs deep neural
network (DNN) models, and employs model compression techniques including
knowledge distillation and low bit-width quantization with memory-aware
considerations to fit models within lower memory hierarchy levels, reducing
latency and enhancing energy efficiency on resource-constrained devices. We
evaluated TinyM$^2$Net-V3 in two multimodal case studies: COVID-19 detection
using cough, speech, and breathing audios, and pose classification from depth
and thermal images. With tiny inference models (6 KB and 58 KB), we achieved
92.95% and 90.7% accuracies, respectively. Our tiny machine learning models,
deployed on resource limited hardware, demonstrated low latencies within
milliseconds and very high power efficiency.",http://arxiv.org/abs/2405.12353v1
"Lessons to learn for better safeguarding of genetic resources during
  tree pandemics: the case of ash dieback in Europe",2024-05-22T02:47:23Z,"Jan-Peter George, Mari Rusanen, Egbert Beuker, Leena Yrj√§n√§, Volkmar Timmermann, Nenad Potocic, Sakari V√§lim√§ki, Heino Konrad","Ash dieback (ADB) is threatening populations of European ash (Fraxinus
excelsior & F. angustifolia) for more than three decades. Although much
knowledge has been gathered in the recent past, practical conservation measures
have been mostly implemented at local scale. Since range contraction in both
ash species will be exacerbated in the near future by westward expansion of the
emerald ash borer and climate change, systematic conservation frameworks need
to be developed to avoid long-term population-genetic consequences and
depletion of genomic diversity. In this article, we address the advantages and
obstacles of conservation approaches aiming to conserve genetic diversity
in-situ or ex-situ during tree pandemics. We are reviewing 47 studies which
were published on ash dieback to unravel three important dimensions of ongoing
conservation approaches or perceived conservation problems: i) conservation
philosophy (i.e. natural selection, resistance breeding or genetic
conservation), ii) the spatial scale (ecosystem, country, continent), and iii)
the integration of genetic safety margins in conservation planning. Although
nearly equal proportions of the reviewed studies mention breeding or active
conservation as possible long-term solutions, only 17% consider that additional
threats exist which may further reduce genetic diversity in both ash species.
We also identify and discuss several knowledge gaps and limitations which may
have limited the initiation of conservation projects at national and
international level so far. Finally, we demonstrate that there is not much time
left for filling these gaps, because European-wide forest health monitoring
data indicates a significant decline of ash populations in the last 5 years.",http://arxiv.org/abs/2405.13305v1
"A Transformer variant for multi-step forecasting of water level and
  hydrometeorological sensitivity analysis based on explainable artificial
  intelligence technology",2024-05-22T13:50:42Z,"Mingyu Liu, Nana Bao, Xingting Yan, Chenyang Li, Kai Peng","Understanding the combined influences of meteorological and hydrological
factors on water level and flood events is essential, particularly in today's
changing climate environments. Transformer, as one kind of the cutting-edge
deep learning methods, offers an effective approach to model intricate
nonlinear processes, enables the extraction of key features and water level
predictions. EXplainable Artificial Intelligence (XAI) methods play important
roles in enhancing the understandings of how different factors impact water
level. In this study, we propose a Transformer variant by integrating sparse
attention mechanism and introducing nonlinear output layer for the decoder
module. The variant model is utilized for multi-step forecasting of water
level, by considering meteorological and hydrological factors simultaneously.
It is shown that the variant model outperforms traditional Transformer across
different lead times with respect to various evaluation metrics. The
sensitivity analyses based on XAI technology demonstrate the significant
influence of meteorological factors on water level evolution, in which
temperature is shown to be the most dominant meteorological factor. Therefore,
incorporating both meteorological and hydrological factors is necessary for
reliable hydrological prediction and flood prevention. In the meantime, XAI
technology provides insights into certain predictions, which is beneficial for
understanding the prediction results and evaluating the reasonability.",http://arxiv.org/abs/2405.13646v1
"Automatic Coral Detection with YOLO: A Deep Learning Approach for
  Efficient and Accurate Coral Reef Monitoring",2024-04-03T08:00:46Z,"Ouassine Younes, Zahir Jihad, Conruyt No√´l, Kayal Mohsen, A. Martin Philippe, Chenin Eric, Bigot Lionel, Vignes Lebbe Regine","Coral reefs are vital ecosystems that are under increasing threat due to
local human impacts and climate change. Efficient and accurate monitoring of
coral reefs is crucial for their conservation and management. In this paper, we
present an automatic coral detection system utilizing the You Only Look Once
(YOLO) deep learning model, which is specifically tailored for underwater
imagery analysis. To train and evaluate our system, we employ a dataset
consisting of 400 original underwater images. We increased the number of
annotated images to 580 through image manipulation using data augmentation
techniques, which can improve the model's performance by providing more diverse
examples for training. The dataset is carefully collected from underwater
videos that capture various coral reef environments, species, and lighting
conditions. Our system leverages the YOLOv5 algorithm's real-time object
detection capabilities, enabling efficient and accurate coral detection. We
used YOLOv5 to extract discriminating features from the annotated dataset,
enabling the system to generalize, including previously unseen underwater
images. The successful implementation of the automatic coral detection system
with YOLOv5 on our original image dataset highlights the potential of advanced
computer vision techniques for coral reef research and conservation. Further
research will focus on refining the algorithm to handle challenging underwater
image conditions, and expanding the dataset to incorporate a wider range of
coral species and spatio-temporal variations.",http://arxiv.org/abs/2405.14879v1
"Enhancing Pollinator Conservation towards Agriculture 4.0: Monitoring of
  Bees through Object Recognition",2024-05-24T10:45:24Z,"Ajay John Alex, Chloe M. Barnes, Pedro Machado, Isibor Ihianle, G√°bor Mark√≥, Martin Bencsik, Jordan J. Bird","In an era of rapid climate change and its adverse effects on food production,
technological intervention to monitor pollinator conservation is of paramount
importance for environmental monitoring and conservation for global food
security. The survival of the human species depends on the conservation of
pollinators. This article explores the use of Computer Vision and Object
Recognition to autonomously track and report bee behaviour from images. A novel
dataset of 9664 images containing bees is extracted from video streams and
annotated with bounding boxes. With training, validation and testing sets
(6722, 1915, and 997 images, respectively), the results of the COCO-based YOLO
model fine-tuning approaches show that YOLOv5m is the most effective approach
in terms of recognition accuracy. However, YOLOv5s was shown to be the most
optimal for real-time bee detection with an average processing and inference
time of 5.1ms per video frame at the cost of slightly lower ability. The
trained model is then packaged within an explainable AI interface, which
converts detection events into timestamped reports and charts, with the aim of
facilitating use by non-technical users such as expert stakeholders from the
apiculture industry towards informing responsible consumption and production.",http://arxiv.org/abs/2405.15428v1
Responsible AI for Earth Observation,2024-05-31T14:47:27Z,"Pedram Ghamisi, Weikang Yu, Andrea Marinoni, Caroline M. Gevaert, Claudio Persello, Sivasakthy Selvakumaran, Manuela Girotto, Benjamin P. Horton, Philippe Rufin, Patrick Hostert, Fabio Pacifici, Peter M. Atkinson","The convergence of artificial intelligence (AI) and Earth observation (EO)
technologies has brought geoscience and remote sensing into an era of
unparalleled capabilities. AI's transformative impact on data analysis,
particularly derived from EO platforms, holds great promise in addressing
global challenges such as environmental monitoring, disaster response and
climate change analysis. However, the rapid integration of AI necessitates a
careful examination of the responsible dimensions inherent in its application
within these domains. In this paper, we represent a pioneering effort to
systematically define the intersection of AI and EO, with a central focus on
responsible AI practices. Specifically, we identify several critical components
guiding this exploration from both academia and industry perspectives within
the EO field: AI and EO for social good, mitigating unfair biases, AI security
in EO, geo-privacy and privacy-preserving measures, as well as maintaining
scientific excellence, open data, and guiding AI usage based on ethical
principles. Furthermore, the paper explores potential opportunities and
emerging trends, providing valuable insights for future research endeavors.",http://arxiv.org/abs/2405.20868v1
"Multifidelity digital twin for real-time monitoring of structural
  dynamics in aquaculture net cages",2024-06-06T21:26:30Z,"Eirini Katsidoniotaki, Biao Su, Eleni Kelasidi, Themistoklis P. Sapsis","As the global population grows and climate change intensifies, sustainable
food production is critical. Marine aquaculture offers a viable solution,
providing a sustainable protein source. However, the industry's expansion
requires novel technologies for remote management and autonomous operations.
Digital twin technology can advance the aquaculture industry, but its adoption
has been limited. Fish net cages, which are flexible floating structures, are
critical yet vulnerable components of aquaculture farms. Exposed to harsh and
dynamic marine environments, the cages experience significant loads and risk
damage, leading to fish escapes, environmental impacts, and financial losses.
We propose a multifidelity surrogate modeling framework for integration into a
digital twin for real-time monitoring of aquaculture net cage structural
dynamics under stochastic marine conditions. Central to this framework is the
nonlinear autoregressive Gaussian process method, which learns complex,
nonlinear cross-correlations between models of varying fidelity. It combines
low-fidelity simulation data with a small set of high-fidelity field sensor
measurements, which offer the real dynamics but are costly and spatially
sparse. Validated at the SINTEF ACE fish farm in Norway, our digital twin
receives online metocean data and accurately predicts net cage displacements
and mooring line loads, aligning closely with field measurements. The proposed
framework is beneficial where application-specific data are scarce, offering
rapid predictions and real-time system representation. The developed digital
twin prevents potential damages by assessing structural integrity and
facilitates remote operations with unmanned underwater vehicles. Our work also
compares GP and GCNs for predicting net cage deformation, highlighting the
latter's effectiveness in complex structural applications.",http://arxiv.org/abs/2406.04519v2
AGBD: A Global-scale Biomass Dataset,2024-06-07T13:34:17Z,"Ghjulia Sialelli, Torben Peters, Jan D. Wegner, Konrad Schindler","Accurate estimates of Above Ground Biomass (AGB) are essential in addressing
two of humanity's biggest challenges, climate change and biodiversity loss.
Existing datasets for AGB estimation from satellite imagery are limited. Either
they focus on specific, local regions at high resolution, or they offer global
coverage at low resolution. There is a need for a machine learning-ready,
globally representative, high-resolution benchmark. Our findings indicate
significant variability in biomass estimates across different vegetation types,
emphasizing the necessity for a dataset that accurately captures global
diversity. To address these gaps, we introduce a comprehensive new dataset that
is globally distributed, covers a range of vegetation types, and spans several
years. This dataset combines AGB reference data from the GEDI mission with data
from Sentinel-2 and PALSAR-2 imagery. Additionally, it includes pre-processed
high-level features such as a dense canopy height map, an elevation map, and a
land-cover classification map. We also produce a dense, high-resolution (10m)
map of AGB predictions for the entire area covered by the dataset. Rigorously
tested, our dataset is accompanied by several benchmark models and is publicly
available. It can be easily accessed using a single line of code, offering a
solid basis for efforts towards global AGB estimation. The GitHub repository
github.com/ghjuliasialelli/AGBD serves as a one-stop shop for all code and
data.",http://arxiv.org/abs/2406.04928v2
"Petrophysical Characterization of Fractured Limestone from Beauce
  Aquifer Vadose Zone (O-ZNS Observatory, France)",2024-06-24T08:29:09Z,"Abdoul Nasser Yacouba, C. Mallet, J. Deparis, S. Gaboreau, P. Leroy, D. Jougnot, M. Azaroual","In recent years, water needs increased, driven by climate change and world
population growth. In this context, we study groundwater flow in the vadose
zone of Beauce aquifer (O-ZNS site, France). This lacustrine limestone vadose
zone is characterized by multi-scale heterogeneities. They are defined by
strongly various pore structures. This leads to uncertainties for reservoir
properties prediction using geophysical methods which impacts reservoir models
for flow simulations.In this study, we combined microstructure description and
petrophysical analysis in order to model and predict reservoir properties based
on different limestones facies and to infer the influence of
weathering/fracturing on both acoustics and electrical properties.A total of 16
samples from these facies were cored and characterized by their porosity,
permeability, acoustic velocities, complex electrical properties and
microstructure analysis.Based on our multi-method approach, we demonstrated the
influence of rock structure on reservoir properties prediction and modelling.
Petrophysical and microstructure characterization have highlighted two main
facies (microporous and homogenous facies and macroporous and heterogenous
facies) which can be used to improve reservoir and flow models. However,
further development is needed in order to quantify macropores and their link
with weathering and to assess permeability models using electrical properties.",http://arxiv.org/abs/2406.16436v1
"Assessment of the environmental impacts of the Cherenkov Telescope Array
  Mid-Sized Telescope",2024-06-25T14:31:47Z,"Gabrielle dos Santos Ilha, Marianne Boix, J√ºrgen Kn√∂dlseder, Philippe Garnier, Ludovic Montastruc, Pierre Jean, Giovanni Pareschi, Alexander Steiner, Fran√ßois Toussenel","Astronomical observatories have been identified as substantial contributors
to the carbon footprint of astrophysical research. Being part of the
collaboration that currently develops the Medium-Sized Telescope (MST) of the
Cherenkov Telescope Array, a ground-based observatory for very-high-energy
gamma rays that will comprise 64 telescopes deployed on two sites, we assessed
the environmental impacts of one MST on the Northern site by means of a Life
Cycle Assessment. We identified resource use and climate change as the most
significant impacts, being driven by telescope manufacturing and energy
consumption during operations. We estimate life cycle greenhouse gas emissions
of 2,660 +/- 274 tCO2 equivalent for the telescope, 44% of which arise from
construction, 1% from on-site assembly and commissioning, and 55% from
operations over 30 years. Environmental impacts can be reduced by using
renewable energies during construction and operations, use of less electronic
components and metal casting, and use of recycled materials. We propose
complementing project requirements with environmental budgets as an effective
measure for impact management and reductions.",http://arxiv.org/abs/2406.17589v1
"French wine: Combination of multiple open data sources to mapping the
  expected harvest value",2024-06-28T08:21:55Z,Martial Ph√©lipp√©-Guinvarc'h,"The purpose of this paper is to estimate a representative and detailed map of
the harvest value in wine using structured and unstructured open data sources.
With climate change and new environmental and ecological policies, wine
producers are facing new challenges. The ability to model the evolution of
these risks is strategic for wine producers and research in order to adapt.
Many research projects require the values exposed to risk. For example, to
assess the economic impact of risks or the premium of crop insurance, or to
choose between different agroecological solutions in a cost-benefit approach.
The high spatial heterogeneity and complexity of wine characteristics add to
the challenge of these production values and the need to improve our spatial
assessment of these harvest-expected values.Structured, exhaustive and detailed
historical data are collected by the customs services, but they are not open.
To achieve this, we combine the aggregate of the vineyard register and the data
of the Public Body for Products of Official Quality and Origin. There are
several techniques available to merge, combine or complete missing data. We
have chosen to use optimization methods to re-estimate the area by appellation
and by county, which can then be converted into expected harvest values using
olympic average yields by appellation and crop insurance prices. This approach
allows us to capture the heterogeneity in production values faced by different
vineyards, thereby facilitating further research on risk assessment in the wine
industry.",http://arxiv.org/abs/2406.19732v1
Climate change analysis from LRD manifold functional regression,2024-06-29T09:29:47Z,"Diana P. Ovalle-Mu√±oz, M. Dolores Ruiz-Medina","This work is motivated by the problem of predicting downward solar radiation
flux spherical maps from the observation of atmospheric pressure at high cloud
bottom. To this aim nonlinear functional regression is implemented under
strong-correlated functional data. The link operator reflects the heat transfer
in the atmosphere. A latent parametric linear functional regression model
reduces uncertainty in the support of this operator. An additive long-memory
manifold-supported functional time series error models persistence in time of
random fluctuations observed in the response. Time is incorporated via the
scalar covariates in the latent linear functional regression model. The
functional regression parameters in this model are supported on a connected and
compact two point homogeneous space. Its Generalized Least--Squares (GLS)
parameter estimation is achieved. When the second-order structure of the
functional error term is unknown, its minimum contrast estimation is obtained
in the spectral domain. The performance of the theoretical and plug-in
nonlinear functional regression predictors is illustrated in the simulation
study undertaken in the sphere. The Supplementary Material provides a detailed
empirical analysis in the one way ANOVA context. The real-data application
extends the purely spatial statistical analysis of atmospheric pressure at high
cloud bottom, and downward solar radiation flux in Alegria et al. (2021) to the
spatiotemporal context.",http://arxiv.org/abs/2407.00381v2
"Exploring the Role of Randomization on Belief Rigidity in Online Social
  Networks",2024-07-01T21:40:44Z,"Adiba Mahbub Proma, Neeley Pate, Raiyan Abdul Baten, Sifeng Chen, James Druckman, Gourab Ghoshal, Ehsan Hoque","People often stick to their existing beliefs, ignoring contradicting evidence
or only interacting with those who reinforce their views. Social media
platforms often facilitate such tendencies of homophily and echo-chambers as
they promote highly personalized content to maximize user engagement. However,
increased belief rigidity can negatively affect real-world policy decisions
such as leading to climate change inaction and increased vaccine hesitancy. To
understand and effectively tackle belief rigidity on online social networks,
designing and evaluating various intervention strategies is crucial, and
increasing randomization in the network can be considered one such
intervention. In this paper, we empirically quantify the effects of a
randomized social network structure on belief rigidity, specifically examining
the potential benefits of introducing randomness into the network. We show that
individuals' beliefs are positively influenced by peer opinions, regardless of
whether those opinions are similar to or differ from their own by passively
sensing belief rigidity through our experimental framework. Moreover, people
incorporate a slightly higher variety of different peers (based on their
opinions) into their networks when the recommendation algorithm provides them
with diverse content, compared to when it provides them with similar content.
Our results indicate that in some cases, there might be benefits to
randomization, providing empirical evidence that a more randomized network
could be a feasible way of helping people get out of their echo-chambers. Our
findings have broader implications in computing and platform design of social
media, and can help combat overly rigid beliefs in online social networks.",http://arxiv.org/abs/2407.01820v1
"Mass-Balance MRV for Carbon Dioxide Removal by Enhanced Rock Weathering:
  Methods, Simulation, and Inference",2024-07-02T04:43:56Z,"Mark Baum, Henry Liu, Lily Schacht, Jake Schneider, Mary Yap","Carbon dioxide will likely need to be removed from the atmosphere to avoid
significant future warming and climate change. Technologies are being developed
to remove large quantities of carbon from the atmosphere. Enhanced rock
weathering (ERW), where fine-grained silicate minerals are spread on soil, is a
promising carbon removal method that can also support crop yields and maintain
overall soil health. Quantifying the amount of carbon removed by ERW is crucial
for understanding the potential of ERW globally and for building trust in
commercial operations. However, reliable and scalable quantification in complex
media like soil is challenging and there is not yet a consensus on the best
method of doing so. Here we discuss mass-balance methods, where stocks of base
cations in soil are monitored over time to infer the amount of inorganic carbon
brought into solution by weathering reactions. First, we review the fundamental
concepts of mass-balance methods and explain different ways of approaching the
mass-balance problem. Then we discuss experimental planning and data
collection, suggesting some best practices. Next, we present a software package
designed to facilitate a range of tasks in ERW like uncertainty analysis,
planning field trials, and validating statistical methods. Finally, we briefly
review ways of estimating carbon removal using mass balance before discussing
some advantages of Bayesian inference in this context and presenting an example
Bayesian model. The model is fit to simulated data and recovers the correct
answer with a clear representation of uncertainty.",http://arxiv.org/abs/2407.01949v1
Dynamics of Heatwave Intensification over the Indian Region,2024-07-05T04:58:04Z,"Lekshmi S, Rajib Chattopadhyay, D. S. Pai","In a warming world, heatwaves over India have become intense and are causing
severe health impacts. Studies have identified the presence of amplified Rossby
waves and their association with the intensification of heatwaves. Earlier
studies have identified two dominant modes of temperature variability in India
and their possible role in the development of dry (mode 1) and moist (mode 2)
heatwaves. These modes are associated with midlatitude Rossby waves intruding
over the Indian region. However the role of regional forcing and the
teleconnection behind the intensification of the heatwaves over India is
missing. The present study has analyzed the dynamical mechanisms for the
regional intensification of the circulation features associated with the
dominant moist heatwave mode (mode 2). Considering the predominant barotropic
nature of the observed circulation features of the mode, a simple barotropic
vorticity equation model forced with extratropical and regional vorticity
sources is used to understand the intensification of the heat waves. It was
found that a wave response initiated by a cyclonic vorticity over the Bay of
Bengal superimposes with the mid-latitude anticyclonic vorticity generated
Rossby waves intruding over India. This superimposition results in the
amplification and persistence of the anticyclonic vorticity phase over the
Northwest Indian region, leading to the intensification of circulation. It was
also found that the barotropically forced intensified circulation leads to the
intensification of the heat stress. Under a climate change scenario, different
circulation regimes, characterized by zonal stationary wave number and jet
speed, which can favor the intensification are also identified.",http://arxiv.org/abs/2407.04256v1
"Equilibrium Strategies of Carbon Emission Reduction in Agricultural
  Product Supply Chain under Carbon Sink Trading",2024-07-06T15:02:27Z,"Tingting Meng, Yukun Cheng, Xujin Pu, Rui Li","As global climate change and environmental issues escalate, carbon reduction
has emerged as a paramount global concern. Agriculture accounts for
approximately 30% of global greenhouse gas emissions, making carbon reduction
in this sector crucial for attaining global emission targets. Carbon sink
trading serves as a supplementary mechanism to achieve carbon peaking and
neutrality, helping to lower the rate ofcarbon emissions. However, practical
projects and research in the field of carbon sink trading are not enough
currently. This work aims to thoroughly explore the cooperative models between
farmers and retailers within the context of agricultural carbon sink trading,
as well as the optimal decisions on the efforts to reduce carbon emission for
both parties under different cooperative models. To this end, we delve into
three distinct cooperative frameworks: the decentralized, the Stackelberg, and
the centralized models, each accompanied by a corresponding differentialgame
model. The Hamilton-Jacobi-Bellman equation is utilized to investigate the
equilibrium strategies of each participant under these three cooperative
models, respectively. Furthermore, we conducte numerical simulations to analyze
the carbon emission reduction efforts of farmers and retailers, the carbon
emission reduction level of the agricultural supply chain, and the overall
profits of the supply chain. We also compare scenarios with and without carbon
sink trading to provide a comprehensive assessment. The numerical results
indicate that the centralized modelexcels in all aspects, followed by the
Stackelberg model, with the decentralized model showing the weakest
performance. Additionally, carbon sink trading can significantly increase the
profits of the participants under each cooperative model.",http://arxiv.org/abs/2407.05099v1
"Multi-scale assessment of high-resolution reanalysis precipitation
  fields over Italy",2024-07-16T08:53:33Z,"Francesco Cavalleri, Cristian Lussana, Francesca Viterbo, Michele Brunetti, Riccardo Bonanno, Veronica Manara, Matteo Lacavalla, Simone Sperati, Mario Raffa, Valerio Capecchi, Davide Cesari, Antonio Giordani, Ines Maria Luisa Cerenzia, Maurizio Maugeri","This study focuses on the validation of high-resolution regional reanalyses
to understand their effectiveness in reproducing precipitation patterns over
Italy, a climate change hotspot characterized by coastal sea-land interaction
and complex orography. Nine reanalysis products were evaluated, with the ECMWF
global reanalysis ERA5 serving as a benchmark. These included both European
(COSMO-REA6, CERRA) and Italy-specific (BOLAM, MERIDA, MERIDA-HRES, MOLOCH,
SPHERA, VHR-REA\_IT) datasets, using different models and parametrizations. The
inter-comparison involved determining the effective resolution of daily
precipitation fields using wavelet techniques and assessing intense
precipitation statistics through frequency distributions. In-situ observations
and observational gridded datasets were used to independently validate
reanalysis precipitation fields. The capability of reanalyses to depict daily
precipitation patterns was assessed, highlighting a maximum radius of
precipitation misplacement of about 15 km, with notably lower skills during
summer. An overall overestimation of precipitation was identified in the
reanalysis climatological fields over the Po Valley and the Alps, whereas
multiple products showed an underestimation of precipitations across the
North-West coast, the Apennines, and Southern Italy. Finally, a comparison with
a time-consistent observational dataset (UniMi/ISAC-CNR) revealed a non-stable
deviation from observations in the annual precipitation cumulate of the
reanalysis products analyzed. This should be taken into account when
interpreting precipitation trends over Italy.",http://arxiv.org/abs/2407.11517v1
"A data-flow oriented software architecture for heterogeneous marine data
  streams",2024-07-18T07:40:19Z,"Keila Lima, Ngoc-Thanh Nguyen, Rogardt Heldal, Lars Michael Kristensen, Tosin Daniel Oyetoyan, Patrizio Pelliccione, Eric Knauss","Marine in-situ data is collected by sensors mounted on fixed or mobile
systems deployed into the ocean. This type of data is crucial both for the
ocean industries and public authorities, e.g., for monitoring and forecasting
the state of marine ecosystems and/or climate changes. Various public
organizations have collected, managed, and openly shared in-situ marine data in
the past decade. Recently, initiatives like the Ocean Decade Corporate Data
Group have incentivized the sharing of marine data of public interest from
private companies aiding in ocean management. However, there is no clear
understanding of the impact of data quality in the engineering of systems, as
well as on how to manage and exploit the collected data.
  In this paper, we propose main architectural decisions and a data
flow-oriented component and connector view for marine in-situ data streams. Our
results are based on a longitudinal empirical software engineering process, and
driven by knowledge extracted from the experts in the marine domain from public
and private organizations, and challenges identified in the literature. The
proposed software architecture is instantiated and exemplified in a prototype
implementation.",http://arxiv.org/abs/2407.13231v1
Offline Digital Euro: a Minimum Viable CBDC using Groth-Sahai proofs,2024-07-01T09:55:14Z,"Leon Kempen, Johan Pouwelse","Current digital payment solutions are fragile and offer less privacy than
traditional cash.
  Their critical dependency on an online service used to perform and validate
transactions makes them void if this service is unreachable.
  Moreover, no transaction can be executed during server malfunctions or power
outages.
  Due to climate change, the likelihood of extreme weather increases. As
extreme weather is a major cause of power outages, the frequency of power
outages is expected to increase.
  The lack of privacy is an inherent result of their account-based design or
the use of a public ledger.
  The critical dependency and lack of privacy can be resolved with a Central
Bank Digital Currency that can be used offline.
  This thesis proposes a design and a first implementation for an offline-first
digital euro.
  The protocol offers complete privacy during transactions using zero-knowledge
proofs.
  Furthermore, transactions can be executed offline without third parties and
retroactive double-spending detection is facilitated.
  To protect the users' privacy, but also guard against money laundering, we
have added the following privacy-guarding mechanism.
  The bank and trusted third parties for law enforcement must collaborate to
decrypt transactions, revealing the digital pseudonym used in the transaction.
  Importantly, the transaction can be decrypted without decrypting prior
transactions attached to the digital euro.
  The protocol has a working initial implementation showcasing its usability
and demonstrating functionality.",http://arxiv.org/abs/2407.13776v1
"Quanv4EO: Empowering Earth Observation by means of Quanvolutional Neural
  Networks",2024-07-24T09:11:34Z,"Alessandro Sebastianelli, Francesco Mauro, Giulia Ciabatti, Dario Spiller, Bertrand Le Saux, Paolo Gamba, Silvia Ullo","A significant amount of remotely sensed data is generated daily by many Earth
observation (EO) spaceborne and airborne sensors over different countries of
our planet. Different applications use those data, such as natural hazard
monitoring, global climate change, urban planning, and more. Many challenges
are brought by the use of these big data in the context of remote sensing
applications. In recent years, employment of machine learning (ML) and deep
learning (DL)-based algorithms have allowed a more efficient use of these data
but the issues in managing, processing, and efficiently exploiting them have
even increased since classical computers have reached their limits. This
article highlights a significant shift towards leveraging quantum computing
techniques in processing large volumes of remote sensing data. The proposed
Quanv4EO model introduces a quanvolution method for preprocessing
multi-dimensional EO data. First its effectiveness is demonstrated through
image classification tasks on MNIST and Fashion MNIST datasets, and later on,
its capabilities on remote sensing image classification and filtering are
shown. Key findings suggest that the proposed model not only maintains high
precision in image classification but also shows improvements of around 5\% in
EO use cases compared to classical approaches. Moreover, the proposed framework
stands out for its reduced parameter size and the absence of training quantum
kernels, enabling better scalability for processing massive datasets. These
advancements underscore the promising potential of quantum computing in
addressing the limitations of classical algorithms in remote sensing
applications, offering a more efficient and effective alternative for image
data classification and analysis.",http://arxiv.org/abs/2407.17108v1
"Climate Change in Hell: Long-Term Variation in Transits of the
  Evaporating Planet K2-22b",2024-07-24T15:50:38Z,"E. Gaidos, H. Parviainen, E. Esparza-Borges, A. Fukui, K. Isogai, K. Kawauchi, J. de Leon, M. Mori, F. Murgas, N. Narita, E. Palle, N. Watanabe","Context: Rocky planets on ultra-short period orbits can have surface magma
oceans and rock-vapour atmospheres in which dust can condense. Observations of
that dust can inform about the composition surface conditions on these objects.
Aims: We constrain the properties and long-term (decade) behaviour of the
transiting dust cloud from the ""evaporating"" planet K2-22b. Methods: We
observed K2-22b around 40 predicted transits with MuSCAT ground-based
multi-optical channel imagers, and complemented these data with long-term
monitoring by the ground-based ATLAS (2018-2024) and space-based TESS
(2021-2023) surveys. Results: We detected signals during 7 transits, none of
which showed significant wavelength dependence. The expected number of
MuSCAT-detected transits is >=22, indicating a decline in mean transit depth
since the K2 discovery observations in 2014. Conclusions: Lack of significant
wavelength dependence indicates that dust grains are large or the cloud is
optically thick. Long-term trends of depth could be due to a magnetic cycle on
the host star or overturn of the planet's dayside surface magma ocean. The
possibility that K2-22b is disappearing altogether is ruled out by the
stability of the transit ephemeris against non-gravitational forces, which
constrains the mass to be at least comparable to Ceres.",http://arxiv.org/abs/2407.17372v1
"Feasibility study of upper atmosphere density measurement on the ISS by
  observations of the CXB transmitted through the Earth rim",2024-07-26T04:31:29Z,"Takumi Kishimoto, Kumiko K. Nobukawa, Ayaki Takeda, Takeshi G. Tsuru, Satoru Katsuda, Nakazawa Kazuhiro, Koji Mori, Masayoshi Nobukawa, Hiroyuki Uchida, Yoshihisa Kawabe, Satoru Kuwano, Eisuke Kurogi, Yamato Ito, Yuma Aoki","Measurements of the upper atmosphere at ~100 km are important to investigate
climate change, space weather forecasting, and the interaction between the Sun
and the Earth. Atmospheric occultations of cosmic X-ray sources are an
effective technique to measure the neutral density in the upper atmosphere. We
are developing the instrument SUIM dedicated to continuous observations of
atmospheric occultations. SUIM will be mounted on a platform on the exterior of
the International Space Station for six months and pointed at the Earth's rim
to observe atmospheric absorption of the cosmic X-ray background (CXB). In this
paper, we conducted a feasibility study of SUIM by estimating the CXB
statistics and the fraction of the non-X-ray background (NXB) in the observed
data. The estimated CXB statistics are enough to evaluate the atmospheric
absorption of CXB for every 15 km of altitude. On the other hand, the NXB will
be dominant in the X-ray spectra of SUIM. Assuming that the NXB per detection
area of SUIM is comparable to that of the soft X-ray Imager onboard Hitomi, the
NXB level will be much higher than the CXB one and account for ~80% of the
total SUIM spectra.",http://arxiv.org/abs/2407.18507v1
A Stochastic Precipitating Quasi-Geostrophic Model,2024-07-30T15:01:25Z,"Nan Chen, Changhong Mou, Leslie M. Smith, Yeyu Zhang","Efficient and effective modeling of complex systems, incorporating cloud
physics and precipitation, is essential for accurate climate modeling and
forecasting. However, simulating these systems is computationally demanding
since microphysics has crucial contributions to the dynamics of moisture and
precipitation. In this paper, appropriate stochastic models are developed for
the phase-transition dynamics of water, focusing on the precipitating
quasi-geostrophic (PQG) model as a prototype. By treating the moisture, phase
transitions, and latent heat release as integral components of the system, the
PQG model constitutes a set of partial differential equations (PDEs) that
involve Heaviside nonlinearities due to phase changes of water. Despite
systematically characterizing the precipitation physics, expensive iterative
algorithms are needed to find a PDE inversion at each numerical integration
time step. As a crucial step toward building an effective stochastic model, a
computationally efficient Markov jump process is designed to randomly simulate
transitions between saturated and unsaturated states that avoids using the
expensive iterative solver. The transition rates, which are deterministic, are
derived from the physical fields, guaranteeing physical and statistical
consistency with nature. Furthermore, to maintain the consistent spatial
pattern of precipitation, the stochastic model incorporates an adaptive
parameterization that automatically adjusts the transitions based on spatial
information. Numerical tests show the stochastic model retains critical
properties of the original PQG system while significantly reducing
computational demands. It accurately captures observed precipitation patterns,
including the spatial distribution and temporal variability of rainfall,
alongside reproducing essential dynamic features such as potential vorticity
fields and zonal mean flows.",http://arxiv.org/abs/2407.20886v1
"Monitoring of Hermit Crabs Using drone-captured imagery and Deep
  Learning based Super-Resolution Reconstruction and Improved YOLOv8",2024-08-07T05:47:15Z,"Fan Zhao, Yijia Chen, Dianhan Xi, Yongying Liu, Jiaqi Wang, Shigeru Tabeta, Katsunori Mizuno","Hermit crabs play a crucial role in coastal ecosystems by dispersing seeds,
cleaning up debris, and disturbing soil. They serve as vital indicators of
marine environmental health, responding to climate change and pollution.
Traditional survey methods, like quadrat sampling, are labor-intensive,
time-consuming, and environmentally dependent. This study presents an
innovative approach combining UAV-based remote sensing with Super-Resolution
Reconstruction (SRR) and the CRAB-YOLO detection network, a modification of
YOLOv8s, to monitor hermit crabs. SRR enhances image quality by addressing
issues such as motion blur and insufficient resolution, significantly improving
detection accuracy over conventional low-resolution fuzzy images. The CRAB-YOLO
network integrates three improvements for detection accuracy, hermit crab
characteristics, and computational efficiency, achieving state-of-the-art
(SOTA) performance compared to other mainstream detection models. The RDN
networks demonstrated the best image reconstruction performance, and CRAB-YOLO
achieved a mean average precision (mAP) of 69.5% on the SRR test set, a 40%
improvement over the conventional Bicubic method with a magnification factor of
4. These results indicate that the proposed method is effective in detecting
hermit crabs, offering a cost-effective and automated solution for extensive
hermit crab monitoring, thereby aiding coastal benthos conservation.",http://arxiv.org/abs/2408.03559v1
"Numerical Study of CO2 Conversion to SAF in a Fixed Bed Catalytic
  Reactor",2024-08-10T13:19:45Z,"Shan Ruiqin, MA Shengwei, Nguyen Van Bo, Kang Chang Wei, Lim Teck-Bin Arthur","CO2 hydrogenation to hydrocarbon refers to an indirect pathway of CO2
utilization. Among them, the conversion of CO2 with green H2 to sustainable
aviation fuel (SAF) with high energy density has gained much attention. It
offers a promising way to reduce greenhouse gas emissions, address the fossil
fuel crises, and transform a climate killer into valuable products. However,
this low-carbon technology is intrinsically complicated. It involves the
development of a catalyst, the design of a reaction system, and its operation
and product refining. Hence, it is important to understand the chemical process
of CO2 hydrogenation in the reactor. In this study, numerical simulations of a
fixed bed catalytic reactor for CO2-to-SAF conversion are conducted by coupling
CFD with heterogeneous catalytic reactions at the catalytic surface. The heat
and mass transfer between the catalyst surface and surrounding fluid flow are
resolved in the simulation. A detailed understanding of the reacting flow and
catalytic processes is obtained from this study. The impact of operating
parameters, i.e., temperature, pressure, mass flow rate, and the ratio between
CO2 and H2, is also explored, which provides important insights into the
catalytic reactor design and operation.",http://arxiv.org/abs/2408.05551v1
"Visualization Atlases: Explaining and Exploring Complex Topics through
  Data, Visualization, and Narration",2024-08-14T11:55:50Z,"Jinrui Wang, Xinhuan Shu, Benjamin Bach, Uta Hinrichs","This paper defines, analyzes, and discusses the emerging genre of
visualization atlases. We currently witness an increase in web-based,
data-driven initiatives that call themselves ""atlases"" while explaining
complex, contemporary issues through data and visualizations: climate change,
sustainability, AI, or cultural discoveries. To understand this emerging genre
and inform their design, study, and authoring support, we conducted a
systematic analysis of 33 visualization atlases and semi-structured interviews
with eight visualization atlas creators. Based on our results, we contribute
(1) a definition of a visualization atlas as a compendium of (web) pages aimed
at explaining and supporting exploration of data about a dedicated topic
through data, visualizations and narration. (2) a set of design patterns of 8
design dimensions, (3) insights into the atlas creation from interviews and (4)
the definition of 5 visualization atlas genres. We found that visualization
atlases are unique in the way they combine i) exploratory visualization, ii)
narrative elements from data-driven storytelling and iii) structured navigation
mechanisms. They target a wide range of audiences with different levels of
domain knowledge, acting as tools for study, communication, and discovery. We
conclude with a discussion of current design practices and emerging questions
around the ethics and potential real-world impact of visualization atlases,
aimed to inform the design and study of visualization atlases.",http://arxiv.org/abs/2408.07483v1
SustainDC: Benchmarking for Sustainable Data Center Control,2024-08-14T22:43:52Z,"Avisek Naug, Antonio Guillen, Ricardo Luna, Vineet Gundecha, Desik Rengarajan, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Dejan Markovikj, Lekhapriya D Kashyap, Soumyendu Sarkar","Machine learning has driven an exponential increase in computational demand,
leading to massive data centers that consume significant amounts of energy and
contribute to climate change. This makes sustainable data center control a
priority. In this paper, we introduce SustainDC, a set of Python environments
for benchmarking multi-agent reinforcement learning (MARL) algorithms for data
centers (DC). SustainDC supports custom DC configurations and tasks such as
workload scheduling, cooling optimization, and auxiliary battery management,
with multiple agents managing these operations while accounting for the effects
of each other. We evaluate various MARL algorithms on SustainDC, showing their
performance across diverse DC designs, locations, weather conditions, grid
carbon intensity, and workload requirements. Our results highlight significant
opportunities for improvement of data center operations using MARL algorithms.
Given the increasing use of DC due to AI, SustainDC provides a crucial platform
for the development and benchmarking of advanced algorithms essential for
achieving sustainable computing and addressing other heterogeneous real-world
challenges.",http://arxiv.org/abs/2408.07841v4
"The Oasis Project: UHI mitigation strategies applied to Parisian
  schoolyards",2024-08-05T13:33:16Z,"Ghid Karam, Martin Hendel, Bob√©e C√©cilia, Berthe Alexandre, Bordin Patricia, Royon Laurent","Paris is experimenting and implementing strategies to increase the capital's
resilience and promote climate change adaptation. Major heatwaves have been
hitting the French Capital lately (Bador et al., 2017) and have thus focused
attention on UHI countermeasures, such as pavement-watering solutions and urban
greening. The Oasis Project is one such strategy aiming to transform
schoolyards into urban cool islands that would benefit surrounding
neighborhoods and their inhabitants during heatwaves.The work presented here
focuses on identifying high-priority schoolyards among the 670 city-owned
schools. This is conducted using a GIS tool used to identify areas with high
cooling potential, which would benefit most from UHI countermeasures. After
extracting bare schoolyard from the facilities, we built a cooling indicator
based on the ratio of high solar irradiance surface to the whole schoolyard
area. We were thus able to identify 38 schoolyards with high cooling potential,
157 with medium cooling potential and 286 facilities with moderate cooling
potential, out of the 670 facilities.The methodology can be applied to other
cities, and therefore helps set up GIS tools that can provide municipalities
with meaningful insight into their urban cooling strategy.",http://arxiv.org/abs/2408.08886v1
Penalized Likelihood Approach for the Four-parameter Kappa Distribution,2024-08-19T01:28:28Z,"Nipada Papukdee, Jeong-Soo Park, Piyapatr Busababodhin","The four-parameter kappa distribution (K4D) is a generalized form of some
commonly used distributions such as generalized logistic, generalized Pareto,
generalized Gumbel, and generalized extreme value (GEV) distributions. Owing to
its flexibility, the K4D is widely applied in modeling in several fields such
as hydrology and climatic change. For the estimation of the four parameters,
the maximum likelihood approach and the method of L-moments are usually
employed. The L-moment estimator (LME) method works well for some parameter
spaces, with up to a moderate sample size, but it is sometimes not feasible in
terms of computing the appropriate estimates. Meanwhile, the maximum likelihood
estimator (MLE) is optimal for large samples and applicable to a very wide
range of situations, including non-stationary data. However, using the MLE of
K4D with small sample sizes shows substantially poor performance in terms of a
large variance of the estimator. We therefore propose a maximum penalized
likelihood estimation (MPLE) of K4D by adjusting the existing penalty functions
that restrict the parameter space. Eighteen combinations of penalties for two
shape parameters are considered and compared. The MPLE retains modeling
flexibility and large sample optimality while also improving on small sample
properties. The properties of the proposed estimator are verified through a
Monte Carlo simulation, and an application case is demonstrated taking
Thailand's annual maximum temperature data. Based on this study, we suggest
using combinations of penalty functions in general.",http://arxiv.org/abs/2408.09631v1
"Data-driven Modeling of Combined Sewer Systems for Urban Sustainability:
  An Empirical Evaluation",2024-08-21T13:46:58Z,"Vipin Singh, Tianheng Ling, Teodor Chiaburu, Felix Biessmann","Climate change poses complex challenges, with extreme weather events becoming
increasingly frequent and difficult to model. Examples include the dynamics of
Combined Sewer Systems (CSS). Overburdened CSS during heavy rainfall will
overflow untreated wastewater into surface water bodies. Classical approaches
to modeling the impact of extreme rainfall events rely on physical simulations,
which are particularly challenging to create for large urban infrastructures.
Deep Learning (DL) models offer a cost-effective alternative for modeling the
complex dynamics of sewer systems. In this study, we present a comprehensive
empirical evaluation of several state-of-the-art DL time series models for
predicting sewer system dynamics in a large urban infrastructure, utilizing
three years of measurement data. We especially investigate the potential of DL
models to maintain predictive precision during network outages by comparing
global models, which have access to all variables within the sewer system, and
local models, which are limited to data from a restricted set of local sensors.
Our findings demonstrate that DL models can accurately predict the dynamics of
sewer system load, even under network outage conditions. These results suggest
that DL models can effectively aid in balancing the load redistribution in CSS,
thereby enhancing the sustainability and resilience of urban infrastructures.",http://arxiv.org/abs/2408.11619v3
Water flow model on vegetated hillslopes with erosion,2024-08-22T15:06:18Z,"Stelian Ion, Dorin Marinescu, Stefan-Gicu Cruceanu","The water circulation in the Soil-Plant-Atmosphere continuum and particularly
the soil erosion induced by water are problems of main concern in the new era
of climate change. The present paper aims to provide a mathematical tool to
investigate the water-soil and water-plant interactions involved in the complex
process of water flow on plant-covered soil surfaces. Basically, the
mathematical model consists of an extended Saint-Venant system of equations for
water flow coupled with Hairsine-Rose equations for soil erosion. The classical
Saint-Venant model is thus modified in order to take into account the presence
of plants on the soil surface. On the premise that the model adequately
reflects the essence of the reality, a series of numerical experiments are
performed to analyze the plant induced effects on water dynamics and soil
erosion intensity.
  Given that both, the mathematical model and the accompanying software are
flexible enough to reflect the variability of the environmental variables such
as soil structure, soil surface roughness, or plant cover structure, each
numerical experiment is constructed as an image of a target hydrological
context. The dam break problem, flash floods, water-induced soil erosion in a
catchment basin are all subjects of numerical analysis. It is shown that the
presence of the plant cover drastically modifies the water dynamics and the
distribution of the soil eroded particles and one can quantitatively evaluate
such effects. The methods described in the paper can also help one to manage
the environmental resources in order to avoid the water induced disasters.",http://arxiv.org/abs/2408.12465v1
"Digital Ecosystem for FAIR Time Series Data Management in Environmental
  System Science",2024-09-05T08:53:23Z,"J. Bumberger, M. Abbrent, N. Brinckmann, J. Hemmen, R. Kunkel, C. Lorenz, P. L√ºnenschlo√ü, B. Palm, T. Schnicke, C. Schulz, H. van der Schaaf, D. Sch√§fer","Addressing the challenges posed by climate change, biodiversity loss, and
environmental pollution requires comprehensive monitoring and effective data
management strategies that are applicable across various scales in
environmental system science. This paper introduces a versatile and
transferable digital ecosystem for managing time series data, designed to
adhere to the FAIR principles (Findable, Accessible, Interoperable, and
Reusable). The system is highly adaptable, cloud-ready, and suitable for
deployment in a wide range of settings, from small-scale projects to
large-scale monitoring initiatives. The ecosystem comprises three core
components: the Sensor Management System (SMS) for detailed metadata
registration and management; time$.$IO, a platform for efficient time series
data storage, transfer, and real-time visualization; and the System for
Automated Quality Control (SaQC), which ensures data integrity through
real-time analysis and quality assurance. The modular architecture, combined
with standardized protocols and interfaces, ensures that the ecosystem can be
easily transferred and deployed across different environments and institutions.
This approach enhances data accessibility for a broad spectrum of stakeholders,
including researchers, policymakers, and the public, while fostering
collaboration and advancing scientific research in environmental monitoring.",http://arxiv.org/abs/2409.03351v3
"SINDyG: Sparse Identification of Nonlinear Dynamical Systems from
  Graph-Structured Data",2024-09-02T17:51:37Z,"Mohammad Amin Basiri, Sina Khanmohammadi","The combination of machine learning (ML) and sparsity-promoting techniques is
enabling direct extraction of governing equations from data, revolutionizing
computational modeling in diverse fields of science and engineering. The
discovered dynamical models could be used to address challenges in climate
science, neuroscience, ecology, finance, epidemiology, and beyond. However,
most existing sparse identification methods for discovering dynamical systems
treat the whole system as one without considering the interactions between
subsystems. As a result, such models are not able to capture small changes in
the emergent system behavior. To address this issue, we developed a new method
called Sparse Identification of Nonlinear Dynamical Systems from
Graph-structured data (SINDyG), which incorporates the network structure into
sparse regression to identify model parameters that explain the underlying
network dynamics. We showcase the application of our proposed method using
several case studies of neuronal dynamics, where we model the macroscopic
oscillation of a population of neurons using the extended Stuart-Landau (SL)
equation and utilize the SINDyG method to identify the underlying nonlinear
dynamics. Our extensive computational experiments validate the improved
accuracy and simplicity of discovered network dynamics when compared to the
original SINDy approach.",http://arxiv.org/abs/2409.04463v2
"Estimating Atmospheric Variables from Digital Typhoon Satellite Images
  via Conditional Denoising Diffusion Models",2024-09-12T11:42:40Z,"Zhangyue Ling, Pritthijit Nath, C√©sar Quilodr√°n-Casas","This study explores the application of diffusion models in the field of
typhoons, predicting multiple ERA5 meteorological variables simultaneously from
Digital Typhoon satellite images. The focus of this study is taken to be
Taiwan, an area very vulnerable to typhoons. By comparing the performance of
Conditional Denoising Diffusion Probability Model (CDDPM) with Convolutional
Neural Networks (CNN) and Squeeze-and-Excitation Networks (SENet), results
suggest that the CDDPM performs best in generating accurate and realistic
meteorological data. Specifically, CDDPM achieved a PSNR of 32.807, which is
approximately 7.9% higher than CNN and 5.5% higher than SENet. Furthermore,
CDDPM recorded an RMSE of 0.032, showing a 11.1% improvement over CNN and 8.6%
improvement over SENet. A key application of this research can be for
imputation purposes in missing meteorological datasets and generate additional
high-quality meteorological data using satellite images. It is hoped that the
results of this analysis will enable more robust and detailed forecasting,
reducing the impact of severe weather events on vulnerable regions. Code
accessible at https://github.com/TammyLing/Typhoon-forecasting.",http://arxiv.org/abs/2409.07961v3
"A Review on Flood Risk Conceptual Frameworks and Development of
  Hierarchical Structures for Assessment Criteria",2024-09-13T13:10:34Z,"Nazgol Tabasi, Mohammad Fereshtehpour, Bardia Roghani","Climate change and rapid urbanization have led to more frequent and severe
flooding, causing significant damage. The existing literature on flood risk
encompasses a variety of dimensions, such as physical, economic, social,
political, environmental, infrastructural, and managerial aspects. This paper
aims to provide an extensive review of proposed conceptual frameworks and their
components used in flood risk assessment. For this purpose, Initially,
conceptual frameworks were extracted to configure the components of flood risk
including hazard, vulnerability, exposure, resilience, and susceptibility.
Subsequently, a comprehensive set of criteria from the literature were
identified, addressing risk components. In this paper, the risk conceptual
framework is defined by the intersection of vulnerability and hazard.
Vulnerability, shaped by exposure and susceptibility, can be reduced by
enhancing resiliency, which includes coping and adaptive capacities. In total,
102 criteria/subcriteria were identified and classified into three hierarchical
structures of hazard, susceptibility, and resilience. Finally, flood risk
assessment methods were reviewed, with an emphasis on their applicability and
characteristics. The review highlighted the strengths and limitations of
various methods, providing a comprehensive overview of their suitability for
different scenarios. The outcomes of this review could serve as a valuable
reference for professionals involved in flood risk assessment, aiding in the
identification of the most appropriate risk concepts, assessment criteria, and
suitable methods for quantification based on the specific study area and data
availability.",http://arxiv.org/abs/2409.08803v1
"Weather Prediction Using CNN-LSTM for Time Series Analysis: A Case Study
  on Delhi Temperature Data",2024-09-14T11:06:07Z,"Bangyu Li, Yang Qian","As global climate change intensifies, accurate weather forecasting is
increasingly crucial for sectors such as agriculture, energy management, and
environmental protection. Traditional methods, which rely on physical and
statistical models, often struggle with complex, nonlinear, and time-varying
data, underscoring the need for more advanced techniques. This study explores a
hybrid CNN-LSTM model to enhance temperature forecasting accuracy for the Delhi
region, using historical meteorological data from 1996 to 2017. We employed
both direct and indirect methods, including comprehensive data preprocessing
and exploratory analysis, to construct and train our model. The CNN component
effectively extracts spatial features, while the LSTM captures temporal
dependencies, leading to improved prediction accuracy. Experimental results
indicate that the CNN-LSTM model significantly outperforms traditional
forecasting methods in terms of both accuracy and stability, with a mean square
error (MSE) of 3.26217 and a root mean square error (RMSE) of 1.80615. The
hybrid model demonstrates its potential as a robust tool for temperature
prediction, offering valuable insights for meteorological forecasting and
related fields. Future research should focus on optimizing model architecture,
exploring additional feature extraction techniques, and addressing challenges
such as overfitting and computational complexity. This approach not only
advances temperature forecasting but also provides a foundation for applying
deep learning to other time series forecasting tasks.",http://arxiv.org/abs/2409.09414v1
"My Views Do Not Reflect Those of My Employer: Differences in Behavior of
  Organizations' Official and Personal Social Media Accounts",2024-09-18T07:31:47Z,"Esa Palosaari, Ted Hsuan Yun Chen, Arttu Malkam√§ki, Mikko Kivel√§","On social media, the boundaries between people's private and public lives
often blur. The need to navigate both roles, which are governed by distinct
norms, impacts how individuals conduct themselves online, and presents
methodological challenges for researchers. We conduct a systematic exploration
on how an organization's official Twitter accounts and its members' personal
accounts differ. Using a climate change Twitter data set as our case, we find
substantial differences in activity and connectivity across the organizational
levels we examined. The levels differed considerably in their overall retweet
network structures, and accounts within each level were more likely to have
similar connections than accounts at different levels. We illustrate the
implications of these differences for applied research by showing that the
levels closer to the core of the organization display more sectoral homophily
but less triadic closure, and how each level consists of very different group
structures. Our results show that the common practice of solely analyzing
accounts from a single organizational level, grouping together all levels, or
excluding certain levels can lead to a skewed understanding of how
organizations are represented on social media.",http://arxiv.org/abs/2409.11759v1
Learning to Simulate Aerosol Dynamics with Graph Neural Networks,2024-09-20T19:21:43Z,"Fabiana Ferracina, Payton Beeler, Mahantesh Halappanavar, Bala Krishnamoorthy, Marco Minutoli, Laura Fierce","Aerosol effects on climate, weather, and air quality depend on
characteristics of individual particles, which are tremendously diverse and
change in time. Particle-resolved models are the only models able to capture
this diversity in particle physiochemical properties, and these models are
computationally expensive. As a strategy for accelerating particle-resolved
microphysics models, we introduce Graph-based Learning of Aerosol Dynamics
(GLAD) and use this model to train a surrogate of the particle-resolved model
PartMC-MOSAIC. GLAD implements a Graph Network-based Simulator (GNS), a machine
learning framework that has been used to simulate particle-based fluid dynamics
models. In GLAD, each particle is represented as a node in a graph, and the
evolution of the particle population over time is simulated through learned
message passing. We demonstrate our GNS approach on a simple aerosol system
that includes condensation of sulfuric acid onto particles composed of sulfate,
black carbon, organic carbon, and water. A graph with particles as nodes is
constructed, and a graph neural network (GNN) is then trained using the model
output from PartMC-MOSAIC. The trained GNN can then be used for simulating and
predicting aerosol dynamics over time. Results demonstrate the framework's
ability to accurately learn chemical dynamics and generalize across different
scenarios, achieving efficient training and prediction times. We evaluate the
performance across three scenarios, highlighting the framework's robustness and
adaptability in modeling aerosol microphysics and chemistry.",http://arxiv.org/abs/2409.13861v1
"Identification of extreme weather events and impacts of the disasters in
  Brazil",2024-09-11T20:20:41Z,"Davi Lazzari, Am√°lia Garcez, Nicole Poltozi, Gianluca Pozzi, Carolina Brito","An important consequence of human-induced climate change emerges through
extreme weather events. The impact of extreme weather events is quantified in
some parts of the globe, but it remains underestimated in several countries. In
this work we first quantify the extreme temperature and precipitation events in
Brazil using data from the Brazilian Institute of Meteorology, which includes
634 meteorological stations that have worked intermittently since 1961. We show
that the anomaly in temperature has increased by more than 1{\deg}C in the last
60 years and that extreme events are heterogeneously distributed in the
country. In terms of precipitation, our analyses show that it is getting drier
in the Northwest region of Brazil while excessive precipitation events are
increasing in the South, in agreement with previous works. We then use data
from S2iD, an official database that registers disasters in Brazil to estimate
their impact in terms of human damage and financial costs in the last ten
years. The analysis shows that the drought extreme events are the most
expensive, several of them reaching a cost of over a billion USD. Although we
are not able to attribute the natural disasters registered in one database to
the extreme weather events identified using the meteorological data, we discuss
the possible correlations between them. Finally, we present a proposal of using
extreme value theory to estimate the probability of having severe extreme
events of precipitation in locations where there are already some natural
disasters.",http://arxiv.org/abs/2409.16309v1
"A Robotic System for Precision Pollination in Apples: Design,
  Development and Field Evaluation",2024-09-30T03:42:48Z,"Uddhav Bhattarai, Ranjan Sapkota, Safal Kshetri, Changki Mo, Matthew D. Whiting, Qin Zhang, Manoj Karkee","Global food production depends upon successful pollination, a process that
relies on natural and managed pollinators. However, natural pollinators are
declining due to different factors, including climate change, habitat loss, and
pesticide use. Thus, developing alternative pollination methods is essential
for sustainable crop production. This paper introduces a robotic system for
precision pollination in apples, which are not self-pollinating and require
precise delivery of pollen to the stigmatic surfaces of the flowers. The
proposed robotic system consists of a machine vision system to identify target
flowers and a mechatronic system with a 6-DOF UR5e robotic manipulator and an
electrostatic sprayer. Field trials of this system in 'Honeycrisp' and 'Fuji'
apple orchards have shown promising results, with the ability to pollinate
flower clusters at an average spray cycle time of 6.5 seconds. The robotic
pollination system has achieved encouraging fruit set and quality, comparable
to naturally pollinated fruits in terms of color, weight, diameter, firmness,
soluble solids, and starch content. However, the results for fruit set and
quality varied between different apple cultivars and pollen concentrations.
This study demonstrates the potential for a robotic artificial pollination
system to be an efficient and sustainable method for commercial apple
production. Further research is needed to refine the system and assess its
suitability across diverse orchard environments and apple cultivars.",http://arxiv.org/abs/2409.19918v1
"On Energization and Loss of the Ionized Heavy Atom and Molecule in Mars'
  Atmosphere",2024-10-01T16:13:37Z,"J. -T. Zhao, Q. -G. Zong, Z. -Y. Liu, X. -Z. Zhou, S. Wang, W. -H. Ip, C. Yue, J. -H. Li, Y. -X. Hao, R. Rankin, A. Degeling, S. -Y. Fu, H. Zou, Y. -F. Wang","The absence of global magnetic fields is often cited to explain why Mars
lacks a dense atmosphere. This line of thought is based on a prevailing theory
that magnetic fields can shield the atmosphere from solar wind erosion.
However, we present observations here to demonstrate a counterintuitive
understanding: unlike the global intrinsic magnetic field, the remnant crustal
magnetic fields can enhance atmosphere loss when considering loss induced by
plasma wave-particle interactions. An analysis of MAVEN data, combined with
observation-based simulations, reveals that the bulk of O+ ions would be in
resonance with ultra-low frequency (ULF) waves when the latter were present.
This interaction then results in significant particle energization, thus
enhancing ion escaping. A more detailed analysis attributes the occurrence of
the resonance to the presence of Mars' crustal magnetic fields, which cause the
majority of nearby ions to gyrate at a frequency matching the resonant
condition ({\omega}-k_{\parallel} v_{\parallel}={\Omega}_i) of the waves. The
ULF waves, fundamental drivers of this entire process, are excited and
propelled by the upstream solar wind. Consequently, our findings offer a
plausible explanation for the mysterious changes in Mars' climate, suggesting
that the ancient solar wind imparted substantially more energy.",http://arxiv.org/abs/2410.00832v1
"Data Optimisation of Machine Learning Models for Smart Irrigation in
  Urban Parks",2024-10-03T09:42:16Z,"Nasser Ghadiri, Bahman Javadi, Oliver Obst, Sebastian Pfautsch","Urban environments face significant challenges due to climate change,
including extreme heat, drought, and water scarcity, which impact public
health, community well-being, and local economies. Effective management of
these issues is crucial, particularly in areas like Sydney Olympic Park, which
relies on one of Australia's largest irrigation systems. The Smart Irrigation
Management for Parks and Cool Towns (SIMPaCT) project, initiated in 2021,
leverages advanced technologies and machine learning models to optimize
irrigation and induce physical cooling. This paper introduces two novel methods
to enhance the efficiency of the SIMPaCT system's extensive sensor network and
applied machine learning models. The first method employs clustering of sensor
time series data using K-shape and K-means algorithms to estimate readings from
missing sensors, ensuring continuous and reliable data. This approach can
detect anomalies, correct data sources, and identify and remove redundant
sensors to reduce maintenance costs. The second method involves sequential data
collection from different sensor locations using robotic systems, significantly
reducing the need for high numbers of stationary sensors. Together, these
methods aim to maintain accurate soil moisture predictions while optimizing
sensor deployment and reducing maintenance costs, thereby enhancing the
efficiency and effectiveness of the smart irrigation system. Our evaluations
demonstrate significant improvements in the efficiency and cost-effectiveness
of soil moisture monitoring networks. The cluster-based replacement of missing
sensors provides up to 5.4% decrease in average error. The sequential sensor
data collection as a robotic emulation shows 17.2% and 2.1% decrease in average
error for circular and linear paths respectively.",http://arxiv.org/abs/2410.02335v1
"UFLUX v2.0: A Process-Informed Machine Learning Framework for Efficient
  and Explainable Modelling of Terrestrial Carbon Uptake",2024-10-04T22:28:30Z,"Wenquan Dong, Songyan Zhu, Jian Xu, Casey M. Ryan, Man Chen, Jingya Zeng, Hao Yu, Congfeng Cao, Jiancheng Shi","Gross Primary Productivity (GPP), the amount of carbon plants fixed by
photosynthesis, is pivotal for understanding the global carbon cycle and
ecosystem functioning. Process-based models built on the knowledge of
ecological processes are susceptible to biases stemming from their assumptions
and approximations. These limitations potentially result in considerable
uncertainties in global GPP estimation, which may pose significant challenges
to our Net Zero goals. This study presents UFLUX v2.0, a process-informed model
that integrates state-of-art ecological knowledge and advanced machine learning
techniques to reduce uncertainties in GPP estimation by learning the biases
between process-based models and eddy covariance (EC) measurements. In our
findings, UFLUX v2.0 demonstrated a substantial improvement in model accuracy,
achieving an R^2 of 0.79 with a reduced RMSE of 1.60 g C m^-2 d^-1, compared to
the process-based model's R^2 of 0.51 and RMSE of 3.09 g C m^-2 d^-1. Our
global GPP distribution analysis indicates that while UFLUX v2.0 and the
process-based model achieved similar global total GPP (137.47 Pg C and 132.23
Pg C, respectively), they exhibited large differences in spatial distribution,
particularly in latitudinal gradients. These differences are very likely due to
systematic biases in the process-based model and differing sensitivities to
climate and environmental conditions. This study offers improved adaptability
for GPP modelling across diverse ecosystems, and further enhances our
understanding of global carbon cycles and its responses to environmental
changes.",http://arxiv.org/abs/2410.03951v1
"Energy-Cautious Designation of Kinematic Parameters for a Sustainable
  Parallel-Serial Heavy-Duty Manipulator Driven by Electromechanical Linear
  Actuator",2024-10-11T07:55:24Z,"Alvaro Paz, Mohammad Bahari, Jouni Mattila","Electrification, a key strategy in combating climate change, is transforming
industries, and off-highway machines (OHM) will be next to transition from
combustion engines and hydraulic actuation to sustainable fully electrified
machines. Electromechanical linear actuators (EMLAs) offer superior efficiency,
safety, and reduced maintenance, and they unlock vast potential for
high-performance autonomous operations. However, a key challenge lies in
optimizing the kinematic parameters of OHMs' on-board manipulators for EMLA
integration to exploit the full capabilities of actuation systems and maximize
their performance. This work addresses this challenge by delving into the
structural optimization of a prevalent closed kinematic chain configuration
commonly employed in OHM manipulators. Our approach aims to retain the
manipulator's existing capabilities while reducing its energy expenditure,
paving the way for a greener future in industrial automation, one in which
sustainable and high-performing robotized OHMs can evolve. The feasibility of
our methodology is validated through simulation results obtained on a
commercially available parallel-serial heavy-duty manipulator mounted on a
battery electric vehicle. The results demonstrate the efficacy of our approach
in modifying kinematic parameters to facilitate the replacement of conventional
hydraulic actuators with EMLAs, all while minimizing the overall energy
consumption of the system.",http://arxiv.org/abs/2410.08600v1
"IceDiff: High Resolution and High-Quality Sea Ice Forecasting with
  Generative Diffusion Prior",2024-10-10T08:53:41Z,"Jingyi Xu, Siwei Tu, Weidong Yang, Shuhao Li, Keyi Liu, Yeqi Luo, Lipeng Ma, Ben Fei, Lei Bai","Variation of Arctic sea ice has significant impacts on polar ecosystems,
transporting routes, coastal communities, and global climate. Tracing the
change of sea ice at a finer scale is paramount for both operational
applications and scientific studies. Recent pan-Arctic sea ice forecasting
methods that leverage advances in artificial intelligence has made promising
progress over numerical models. However, forecasting sea ice at higher
resolutions is still under-explored. To bridge the gap, we propose a two-staged
deep learning framework, IceDiff, to forecast sea ice concentration at finer
scales. IceDiff first leverages an independently trained vision transformer to
generate coarse yet superior forecasting over previous methods at a regular
25km x 25km grid. This high-quality sea ice forecasting can be utilized as
reliable guidance for the next stage. Subsequently, an unconditional diffusion
model pre-trained on sea ice concentration maps is utilized for sampling
down-scaled sea ice forecasting via a zero-shot guided sampling strategy and a
patch-based method. For the first time, IceDiff demonstrates sea ice
forecasting with the 6.25km x 6.25km resolution. IceDiff extends the boundary
of existing sea ice forecasting models and more importantly, its capability to
generate high-resolution sea ice concentration data is vital for pragmatic
usages and research.",http://arxiv.org/abs/2410.09111v1
"Integrating Reinforcement Learning and Large Language Models for Crop
  Production Process Management Optimization and Control through A New
  Knowledge-Based Deep Learning Paradigm",2024-10-13T00:31:16Z,"Dong Chen, Yanbo Huang","Efficient and sustainable crop production process management is crucial to
meet the growing global demand for food, fuel, and feed while minimizing
environmental impacts. Traditional crop management practices, often developed
through empirical experience, face significant challenges in adapting to the
dynamic nature of modern agriculture, which is influenced by factors such as
climate change, soil variability, and market conditions. Recently,
reinforcement learning (RL) and large language models (LLMs) bring
transformative potential, with RL providing adaptive methodologies to learn
optimal strategies and LLMs offering vast, superhuman knowledge across
agricultural domains, enabling informed, context-specific decision-making. This
paper systematically examines how the integration of RL and LLMs into crop
management decision support systems (DSSs) can drive advancements in
agricultural practice. We explore recent advancements in RL and LLM algorithms,
their application within crop management, and the use of crop management
simulators to develop these technologies. The convergence of RL and LLMs with
crop management DSSs presents new opportunities to optimize agricultural
practices through data-driven, adaptive solutions that can address the
uncertainties and complexities of crop production. However, this integration
also brings challenges, particularly in real-world deployment. We discuss these
challenges and propose potential solutions, including the use of offline RL and
enhanced LLM integration, to maximize the effectiveness and sustainability of
crop management. Our findings emphasize the need for continued research and
innovation to unlock the full potential of these advanced tools in transforming
agricultural systems into optimal and controllable ones.",http://arxiv.org/abs/2410.09680v1
"On the Reliability of Large Language Models to Misinformed and
  Demographically-Informed Prompts",2024-10-06T07:40:11Z,"Toluwani Aremu, Oluwakemi Akinwehinmi, Chukwuemeka Nwagu, Syed Ishtiaque Ahmed, Rita Orji, Pedro Arnau Del Amo, Abdulmotaleb El Saddik","We investigate and observe the behaviour and performance of Large Language
Model (LLM)-backed chatbots in addressing misinformed prompts and questions
with demographic information within the domains of Climate Change and Mental
Health. Through a combination of quantitative and qualitative methods, we
assess the chatbots' ability to discern the veracity of statements, their
adherence to facts, and the presence of bias or misinformation in their
responses. Our quantitative analysis using True/False questions reveals that
these chatbots can be relied on to give the right answers to these close-ended
questions. However, the qualitative insights, gathered from domain experts,
shows that there are still concerns regarding privacy, ethical implications,
and the necessity for chatbots to direct users to professional services. We
conclude that while these chatbots hold significant promise, their deployment
in sensitive areas necessitates careful consideration, ethical oversight, and
rigorous refinement to ensure they serve as a beneficial augmentation to human
expertise rather than an autonomous solution.",http://arxiv.org/abs/2410.10850v2
"Thermal Comfort in Sight: Thermal Affordance and its Visual Assessment
  for Sustainable Streetscape Design",2024-10-12T04:03:36Z,"Sijie Yang, Adrian Chong, Pengyuan Liu, Filip Biljecki","In response to climate change and urban heat island effects, enhancing human
thermal comfort in cities is crucial for sustainable urban development.
Traditional methods for investigating the urban thermal environment and
corresponding human thermal comfort level are often resource intensive,
inefficient, and limited in scope. To address these challenges, we (1)
introduce a new concept named thermal affordance, which formalizes the
integrated inherent capacity of a streetscape to influence human thermal
comfort based on its visual and physical features; and (2) an efficient method
to evaluate it (visual assessment of thermal affordance -- VATA), which
combines street view imagery (SVI), online and in-field surveys, and
statistical learning algorithms. VATA extracts five categories of image
features from SVI data and establishes 19 visual-perceptual indicators for
streetscape visual assessment. Using a multi-task neural network and elastic
net regression, we model their chained relationship to predict and comprehend
thermal affordance for Singapore. VATA predictions are validated with
field-investigated OTC data, providing a cost-effective, scalable, and
transferable method to assess the thermal comfort potential of urban
streetscape. Moreover, we demonstrate its utility by generating a geospatially
explicit mapping of thermal affordance, outlining a model update workflow for
long-term urban-scale analysis, and implementing a two-stage prediction and
inference approach (IF-VPI-VATA) to guide future streetscape improvements. This
framework can inform streetscape design to support sustainable, liveable, and
resilient urban environments.",http://arxiv.org/abs/2410.11887v4
Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models,2024-10-16T17:48:34Z,"Luis Barroso-Luque, Muhammed Shuaibi, Xiang Fu, Brandon M. Wood, Misko Dzamba, Meng Gao, Ammar Rizvi, C. Lawrence Zitnick, Zachary W. Ulissi","The ability to discover new materials with desirable properties is critical
for numerous applications from helping mitigate climate change to advances in
next generation computing hardware. AI has the potential to accelerate
materials discovery and design by more effectively exploring the chemical space
compared to other computational methods or by trial-and-error. While
substantial progress has been made on AI for materials data, benchmarks, and
models, a barrier that has emerged is the lack of publicly available training
data and open pre-trained models. To address this, we present a Meta FAIR
release of the Open Materials 2024 (OMat24) large-scale open dataset and an
accompanying set of pre-trained models. OMat24 contains over 110 million
density functional theory (DFT) calculations focused on structural and
compositional diversity. Our EquiformerV2 models achieve state-of-the-art
performance on the Matbench Discovery leaderboard and are capable of predicting
ground-state stability and formation energies to an F1 score above 0.9 and an
accuracy of 20 meV/atom, respectively. We explore the impact of model size,
auxiliary denoising objectives, and fine-tuning on performance across a range
of datasets including OMat24, MPtraj, and Alexandria. The open release of the
OMat24 dataset and models enables the research community to build upon our
efforts and drive further advancements in AI-assisted materials science.",http://arxiv.org/abs/2410.12771v1
"Hiformer: Hybrid Frequency Feature Enhancement Inverted Transformer for
  Long-Term Wind Power Prediction",2024-10-17T08:00:36Z,"Chongyang Wan, Shunbo Lei, Yuan Luo","The increasing severity of climate change necessitates an urgent transition
to renewable energy sources, making the large-scale adoption of wind energy
crucial for mitigating environmental impact. However, the inherent uncertainty
of wind power poses challenges for grid stability, underscoring the need for
accurate wind energy prediction models to enable effective power system
planning and operation. While many existing studies on wind power prediction
focus on short-term forecasting, they often overlook the importance of
long-term predictions. Long-term wind power forecasting is essential for
effective power grid dispatch and market transactions, as it requires careful
consideration of weather features such as wind speed and direction, which
directly influence power output. Consequently, methods designed for short-term
predictions may lead to inaccurate results and high computational costs in
long-term settings. To adress these limitations, we propose a novel approach
called Hybrid Frequency Feature Enhancement Inverted Transformer (Hiformer).
Hiformer introduces a unique structure that integrates signal decomposition
technology with weather feature extraction technique to enhance the modeling of
correlations between meteorological conditions and wind power generation.
Additionally, Hiformer employs an encoder-only architecture, which reduces the
computational complexity associated with long-term wind power forecasting.
Compared to the state-of-the-art methods, Hiformer: (i) can improve the
prediction accuracy by up to 52.5\%; and (ii) can reduce computational time by
up to 68.5\%.",http://arxiv.org/abs/2410.13303v1
"Data-driven rainfall prediction at a regional scale: a case study with
  Ghana",2024-10-17T22:07:53Z,"Indrajit Kalita, Lucia Vilallonga, Yves Atchade","With a warming planet, tropical regions are expected to experience the brunt
of climate change, with more intense and more volatile rainfall events.
Currently, state-of-the-art numerical weather prediction (NWP) models are known
to struggle to produce skillful rainfall forecasts in tropical regions of
Africa. There is thus a pressing need for improved rainfall forecasting in
these regions. Over the last decade or so, the increased availability of
large-scale meteorological datasets and the development of powerful machine
learning models have opened up new opportunities for data-driven weather
forecasting. Focusing on Ghana in this study, we use these tools to develop two
U-Net convolutional neural network (CNN) models, to predict 24h rainfall at 12h
and 30h lead-time. The models were trained using data from the ERA5 reanalysis
dataset, and the GPM-IMERG dataset. A special attention was paid to
interpretability. We developed a novel statistical methodology that allowed us
to probe the relative importance of the meteorological variables input in our
model, offering useful insights into the factors that drive precipitation in
the Ghana region. Empirically, we found that our 12h lead-time model has
performances that match, and in some accounts are better than the 18h lead-time
forecasts produced by the ECMWF (as available in the TIGGE dataset). We also
found that combining our data-driven model with classical NWP further improves
forecast accuracy.",http://arxiv.org/abs/2410.14062v2
"A Scientific Machine Learning Approach for Predicting and Forecasting
  Battery Degradation in Electric Vehicles",2024-10-18T09:57:59Z,"Sharv Murgai, Hrishikesh Bhagwat, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat","Carbon emissions are rising at an alarming rate, posing a significant threat
to global efforts to mitigate climate change. Electric vehicles have emerged as
a promising solution, but their reliance on lithium-ion batteries introduces
the critical challenge of battery degradation. Accurate prediction and
forecasting of battery degradation over both short and long time spans are
essential for optimizing performance, extending battery life, and ensuring
effective long-term energy management. This directly influences the
reliability, safety, and sustainability of EVs, supporting their widespread
adoption and aligning with key UN SDGs. In this paper, we present a novel
approach to the prediction and long-term forecasting of battery degradation
using Scientific Machine Learning framework which integrates domain knowledge
with neural networks, offering more interpretable and scientifically grounded
solutions for both predicting short-term battery health and forecasting
degradation over extended periods. This hybrid approach captures both known and
unknown degradation dynamics, improving predictive accuracy while reducing data
requirements. We incorporate ground-truth data to inform our models, ensuring
that both the predictions and forecasts reflect practical conditions. The model
achieved MSE of 9.90 with the UDE and 11.55 with the NeuralODE, in experimental
data, a loss of 1.6986 with the UDE, and a MSE of 2.49 in the NeuralODE,
demonstrating the enhanced precision of our approach. This integration of
data-driven insights with SciML's strengths in interpretability and scalability
allows for robust battery management. By enhancing battery longevity and
minimizing waste, our approach contributes to the sustainability of energy
systems and accelerates the global transition toward cleaner, more responsible
energy solutions, aligning with the UN's SDG agenda.",http://arxiv.org/abs/2410.14347v1
The role of edge states for early-warning of tipping points,2024-10-04T06:21:19Z,"Johannes Lohmann, Alfred B. Hansen, Alessandro Lovo, Ruth Chapman, Freddy Bouchet, Valerio Lucarini","Tipping points (TP) are often described as low-dimensional bifurcations, and
are associated with early-warning signals (EWS) due to critical slowing down
(CSD). CSD is an increase in amplitude and correlation of noise-induced
fluctuations away from a reference attractor as the TP is approached. But for
high-dimensional systems it is not obvious which variables or observables would
display the critical dynamics and carry CSD. Many variables may display no CSD,
or show changes in variability not related to a TP. It is thus helpful to
identify beforehand which observables are relevant for a given TP. Here we
propose this may be achieved by knowledge of an unstable edge state that
separates the reference from an alternative attractor that remains after the
TP. This is because stochastic fluctuations away from the reference attractor
are preferentially directed towards the edge state along a most likely path
(the instanton). As the TP is approached the edge state and reference attractor
typically become closer, and the fluctuations can evolve further along the
instanton. This can be exploited to find observables with substantial CSD,
which we demonstrate using conceptual dynamical systems models and climate
model simulations of a collapse of the Atlantic Meridional Overturning
Circulation (AMOC).",http://arxiv.org/abs/2410.16277v2
"Local and Remote Forcing Factors of Heatwave in India -A Reanalysis and
  Adjoint model based study",2024-10-22T15:08:03Z,"Abhirup Banerjee, Armin Koehl, Frank Lunkeit, Detlef Stammer","Continental heatwaves can dramatically impact ecosystems and societies, e.g.,
by leading to excess mortality, wildfires, and harvest failures. With a warming
climate, their impacts potentially intensify globally, but the Indian
subcontinent appears to be particularly vulnerable to such extreme events. In
this study, we use reanalysis and the adjoint of the atmospheric model, PlaSim,
to identify drivers of heatwaves occurring April and May over north-central
India. Reanalysis results suggest that the existence of high temperatures in
the study region is highly sensitive to the low local soil moisture which is
observed weeks before a heatwave commences. Soil moisture variability in
northern India is influenced by moisture transport from the west during
winter--spring. Preceding dry soil moisture conditions can be associated with a
`persistent jet' conditions linked to atmospheric dynamical changes in the
North Atlantic region. An associated northward shift in the upper tropospheric
zonal wind occurs approximately a month prior to the heatwaves, influencing the
area and intensity of western disturbances embedded in the jet stream. This
weakens the moisture flow from the north of the Arabian Sea, further reducing
soil moisture levels and creating conditions conducive to heatwaves. An adjoint
sensitivity analysis and forward model perturbation experiments confirm the
causal relationships for the proposed heatwave development mechanism over
north-central India, identifying the remote influence of North Atlantic sea
surface temperature variability on extreme temperatures in India. Our findings
highlight the complex interplay of local and remote factors in heatwave
development over India.",http://arxiv.org/abs/2410.17083v1
"A Deep Learning Approach to Estimate Canopy Height and Uncertainty by
  Integrating Seasonal Optical, SAR and Limited GEDI LiDAR Data over Northern
  Forests",2024-10-08T20:27:11Z,"Jose B. Castro, Cheryl Rogers, Camile Sothe, Dominic Cyr, Alemu Gonsamo","Accurate forest canopy height estimation is essential for evaluating
aboveground biomass and carbon stock dynamics, supporting ecosystem monitoring
services like timber provisioning, climate change mitigation, and biodiversity
conservation. However, despite advancements in spaceborne LiDAR technology,
data for northern high latitudes remain limited due to orbital and sampling
constraints. This study introduces a methodology for generating spatially
continuous, high-resolution canopy height and uncertainty estimates using Deep
Learning Regression models. We integrate multi-source, multi-seasonal satellite
data from Sentinel-1, Landsat, and ALOS-PALSAR-2, with spaceborne GEDI LiDAR as
reference data. Our approach was tested in Ontario, Canada, and validated with
airborne LiDAR, demonstrating strong performance. The best results were
achieved by incorporating seasonal Sentinel-1 and Landsat features alongside
PALSAR data, yielding an R-square of 0.72, RMSE of 3.43 m, and bias of 2.44 m.
Using seasonal data instead of summer-only data improved variability by 10%,
reduced error by 0.45 m, and decreased bias by 1 m. The deep learning model's
weighting strategy notably reduced errors in tall canopy height estimates
compared to a recent global model, though it overestimated lower canopy
heights. Uncertainty maps highlighted greater uncertainty near forest edges,
where GEDI measurements are prone to errors and SAR data may encounter
backscatter issues like foreshortening, layover, and shadow. This study
enhances canopy height estimation techniques in areas lacking spaceborne LiDAR
coverage, providing essential tools for forestry, environmental monitoring, and
carbon stock estimation.",http://arxiv.org/abs/2410.18108v1
"Establishing Nationwide Power System Vulnerability Index across US
  Counties Using Interpretable Machine Learning",2024-10-11T06:37:05Z,"Junwei Ma, Bo Li, Olufemi A. Omitaomu, Ali Mostafavi","Power outages have become increasingly frequent, intense, and prolonged in
the US due to climate change, aging electrical grids, and rising energy demand.
However, largely due to the absence of granular spatiotemporal outage data, we
lack data-driven evidence and analytics-based metrics to quantify power system
vulnerability. This limitation has hindered the ability to effectively evaluate
and address vulnerability to power outages in US communities. Here, we
collected ~179 million power outage records at 15-minute intervals across 3022
US contiguous counties (96.15% of the area) from 2014 to 2023. We developed a
power system vulnerability assessment framework based on three dimensions
(intensity, frequency, and duration) and applied interpretable machine learning
models (XGBoost and SHAP) to compute Power System Vulnerability Index (PSVI) at
the county level. Our analysis reveals a consistent increase in power system
vulnerability over the past decade. We identified 318 counties across 45 states
as hotspots for high power system vulnerability, particularly in the West Coast
(California and Washington), the East Coast (Florida and the Northeast area),
the Great Lakes megalopolis (Chicago-Detroit metropolitan areas), and the Gulf
of Mexico (Texas). Heterogeneity analysis indicates that urban counties,
counties with interconnected grids, and states with high solar generation
exhibit significantly higher vulnerability. Our results highlight the
significance of the proposed PSVI for evaluating the vulnerability of
communities to power outages. The findings underscore the widespread and
pervasive impact of power outages across the country and offer crucial insights
to support infrastructure operators, policymakers, and emergency managers in
formulating policies and programs aimed at enhancing the resilience of the US
power infrastructure.",http://arxiv.org/abs/2410.19754v2
"GeoFUSE: A High-Efficiency Surrogate Model for Seawater Intrusion
  Prediction and Uncertainty Reduction",2024-10-26T08:10:32Z,"Su Jiang, Chuyang Liu, Dipankar Dwivedi","Seawater intrusion into coastal aquifers poses a significant threat to
groundwater resources, especially with rising sea levels due to climate change.
Accurate modeling and uncertainty quantification of this process are crucial
but are often hindered by the high computational costs of traditional numerical
simulations. In this work, we develop GeoFUSE, a novel deep-learning-based
surrogate framework that integrates the U-Net Fourier Neural Operator (U-FNO)
with Principal Component Analysis (PCA) and Ensemble Smoother with Multiple
Data Assimilation (ESMDA). GeoFUSE enables fast and efficient simulation of
seawater intrusion while significantly reducing uncertainty in model
predictions. We apply GeoFUSE to a 2D cross-section of the Beaver Creek tidal
stream-floodplain system in Washington State. Using 1,500 geological
realizations, we train the U-FNO surrogate model to approximate salinity
distribution and accumulation. The U-FNO model successfully reduces the
computational time from hours (using PFLOTRAN simulations) to seconds,
achieving a speedup of approximately 360,000 times while maintaining high
accuracy. By integrating measurement data from monitoring wells, the framework
significantly reduces geological uncertainty and improves the predictive
accuracy of the salinity distribution over a 20-year period. Our results
demonstrate that GeoFUSE improves computational efficiency and provides a
robust tool for real-time uncertainty quantification and decision making in
groundwater management. Future work will extend GeoFUSE to 3D models and
incorporate additional factors such as sea-level rise and extreme weather
events, making it applicable to a broader range of coastal and subsurface flow
systems.",http://arxiv.org/abs/2410.20118v1
"Analysis of Diurnal Air Temperature Trends and Pattern Similarities in
  Highland and Lowland Stations of Italy and UK",2024-10-28T04:44:47Z,"Chalachew Muluken Liyew. Rosa Meo, Stefano Ferraris, Elvira Di Nardo","In this paper, an analysis of hourly air temperatures in four groups of 32
stations of the UK highland (five stations), UK lowland (four stations),
Italian highland (eleven stations), and Italian lowland (twelve stations) at
various altitudes was conducted over the period from 2002 to 2021. The study
aimed to examine the trends of each hour of the day in that period, over
different averaging time windows (-10 day, -30 day, and -60 day). The trends
were computed using the Mann-Kendall trend test and Sen's slope estimator. The
similarity of trends within and across the groups of stations was assessed
using the hierarchical clustering with dynamic time warping technique. An
additional analysis was conducted to show the correlation of trends among the
group of stations using the correlation distance matrix. Hierarchical
clustering and distance correlation analysis show trend similarities and
correlations, also indicating dissimilarities among different groups. Using 30
day averages, significant warming trends in specific months at the Italian
stations are evident, especially in February, July, August, and December. The
UK highland stations did not show statistically significant trends, but clear
pattern similarities were found within the groups, especially in certain
months. The ultimate goal of this paper is to provide insights into temperature
dynamics and climate change characteristics on regional and diurnal scales.",http://arxiv.org/abs/2410.20726v1
"Privacy-Preserving for Images in Satellite Communications: A
  Comprehensive Review of Chaos-Based Encryption",2024-10-28T16:17:07Z,"Farrukh Bin Rashid, Windhya Rankothge, Somayeh Sadeghi, Hesamodin Mohammadian, Ali Ghorbani","In an era where global connectivity has become critical, satellite
communication is essential for businesses, governments, and individuals. Widely
used services with satellite communication such as climate change monitoring,
military surveillance and real-time event broadcasting, involve data in the
form of images rather text. Therefore, securing image transmission in satellite
communication using efficient and effective encryption approaches, has gained a
significant attention from academia as well as the industry. In this paper, we
specifically focus on chaos based image encryption as one of the key
privacy-preserving techniques for satellite communication. While there are
several privacy enhancing techniques for protecting image data but chaos based
encryption has distinct advantages such as high flexibility, high security,
less computational overheads, less computing power and ease of implementation.
First, we present a solid background about satellite communication and image
encryption in satellite communication, covering theoretical aspects of chaotic
systems and their practical usage for image encryption. Next we present a
comprehensive literature review on all state-of-the-art studies specifically
for chaos based satellite image encryption, with a detailed analysis of the
evaluation process, including evaluation parameters and conditions. Finally, we
discuss about existing challenges and open research problems for chaos based
satellite image encryption.",http://arxiv.org/abs/2410.21177v1
"When Circular Economy Meets the Smart City Ecosystem: Defining the Smart
  and Circular City",2024-10-29T13:00:50Z,"Georgios Mylonas, Athanasios Kalogeras, Sobah Abbas Petersen, Luis Mu√±oz, Ioannis Chatzigiannakis","Smart cities have been a very active research area in the past 20 years,
while continuously adapting to new technological advancements and keeping up
with the times regarding sustainability and climate change. In this context,
there have been numerous proposals to expand the scope of smart cities,
focusing on resilience and sustainability, among other aspects, resulting in
terms like smart sustainable cities. At the same time, there is an ongoing
discussion regarding the degree in which smart cities put people at their
centre. In this work, we argue toward expanding the current smart city
definition by integrating the circular economy as one of its central pillars
and adopting the term smart (and) circular city. We discuss the ways a smart
and circular city encompasses both sustainability and smartness in an integral
manner, while also being well-positioned to foster novel business activity and
models and helping to place citizens at the heart of the smart city. In this
sense, we also argue that previous research in smart cities and technologies,
such as those related to Industry 4.0, can serve as a cornerstone to implement
circular economy activities within cities, at a scale that exceeds current
activities that are based on more conventional approaches. We also outline
current open challenges in this domain and research questions that still need
to be addressed.",http://arxiv.org/abs/2410.22012v1
"The Influence of Ridership Weighting on Targeting and Recovery
  Strategies for Urban Rail Rapid Transit Systems",2024-10-31T07:24:29Z,"Aran Chakraborty, Yushi Tsukimoto, August Posch, Jack Watson, Auroop Ganguly","The resilience of urban rapid transit systems (URTs) to a rapidly evolving
threat space is of much concern. Extreme rainfall events are both intensifying
and growing more frequent under continuing climate change, exposing transit
systems to flooding, while cyber threats and emerging technologies such as
unmanned aerial vehicles are exposing such systems to targeted disruptions. An
imperative has emerged to model how networked infrastructure systems fail and
devise strategies to efficiently recover from disruptions. Passenger flow
approaches can quantify more dimensions of resilience than network
science-based approaches, but the former typically requires granular data from
automatic fare collection and suffers from large runtime complexities. Some
attempts have been made to include accessible low-resolution ridership data in
topological frameworks. However, there is yet to be a systematic investigation
of the effects of incorporating low-dimensional, coarsely-averaged ridership
volume into topological network science methodologies. We simulate targeted
attack and recovery sequences using station-level ridership, four centrality
measures, and weighted combinations thereof. Resilience is quantified using two
topological measures of performance: the node count of a network's giant
connected component (GCC), and a new measure termed the ""highest ridership
connected component"" (HRCC). Three transit systems are used as case studies:
the subways of Boston, New York, and Osaka. Results show that centrality-based
strategies are most effective when measuring performance via GCC, while
centrality-ridership hybrid strategies perform strongest by HRCC. We show that
the most effective strategies vary by network characteristics and the mission
goals of emergency managers, highlighting the need to plan for strategic
adversaries and rapid recovery according to each city's unique needs.",http://arxiv.org/abs/2410.23688v1
"Consumer Segmentation and Participation Drivers in Community-Supported
  Agriculture: A Choice Experiment and PLS-SEM Approach",2024-10-17T15:37:28Z,"Sota Takagi, Miki Saijo, Takumi Ohashi","As the global food system faces increasing challenges from sustainability,
climate change, and food security issues, alternative food networks like
Community-Supported Agriculture (CSA) play an essential role in fostering
stronger connections between consumers and producers. However, understanding
consumer engagement with CSA is fragmented, particularly in Japan where CSA
participation is still emerging. This study aims to identify potential CSA
participants in Japan and validate existing theories on CSA participation
through a quantitative analysis of 2,484 Japanese consumers. Using choice
experiments, Latent Class Analysis, and Partial Least Squares Structural
Equation Modeling, we identified five distinct consumer segments. The
""Sustainable Food Seekers"" group showed the highest positive utility for CSA,
driven primarily by ""Food Education and Learning Opportunities"" and
""Contribution to Environmental and Social Issues."" These factors were
consistently significant across all segments, suggesting that many Japanese
consumers value CSA for its educational and environmental benefits. In
contrast, factors related to ""Variety of Ingredients"" were less influential in
determining participation intentions. The findings suggest that promoting CSA
in Japan may be most effective by emphasizing its role in environmental and
social impact, rather than focusing solely on product attributes like organic
certification, which is readily available in supermarkets. This reflects a key
distinction between CSA adoption in Japan and in other cultural contexts, where
access to organic produce is a primary driver. For ""Sustainable Food Seekers,""
CSA offers a way to contribute to broader societal goals rather than just
securing organic products.",http://arxiv.org/abs/2411.00010v2
"Theoretical performance limitations and filter selection based on Fisher
  information of a computational photonic crystal spectrometer for trace-gas
  retrieval",2024-11-04T12:52:14Z,"Marijn Siemons, Ralf Kohlhaas","As global climate change severely impacts our world, there is an increasing
demand to monitor trace gases with a high spatial resolution and accuracy. At
the same time, these instruments need to be compact in order have
constellations for short revisit times. Here we present a new spectrometer
instrument concept for trace gas detection, where photonic crystals filters
replace traditional diffraction based optical elements. In this concept, 2D
photonic crystal slabs with unique transmission profiles are bonded on a
detector inside a regular telescope. As the instrument flies over the earth,
different integrated intensities for each filter are measured for a single
ground resolution element with a regular telescope. From this detector data,
trace gas concentrations are retrieved. As an initial test case we focused on
methane and carbon dioxide retrieval and estimated the performance of such an
instrument. We derive the Cram\'er-Rao lower bound for trace-gas retrieval for
such a spectrometer using Fisher information and compare this with the achieved
performance. We furthermore set up a framework how to select photonic crystal
filters based on maximizing the Fisher information carried by the filters and
how to use the Cram\'er-Rao lower bound to find good filter sets. The retrieval
performance of such an instrument is found to be between 0.4% to 0.9% for
methane and 0.2% to 0.5% for carbon dioxide detection for a 300x300 m2 ground
resolution element and realistic instrument parameters.",http://arxiv.org/abs/2411.02048v1
Advancing Sustainability via Recommender Systems: A Survey,2024-11-12T09:19:32Z,"Xin Zhou, Lei Zhang, Honglei Zhang, Yixin Zhang, Xiaoxiong Zhang, Jie Zhang, Zhiqi Shen","Human behavioral patterns and consumption paradigms have emerged as pivotal
determinants in environmental degradation and climate change, with quotidian
decisions pertaining to transportation, energy utilization, and resource
consumption collectively precipitating substantial ecological impacts.
Recommender systems, which generate personalized suggestions based on user
preferences and historical interaction data, exert considerable influence on
individual behavioral trajectories. However, conventional recommender systems
predominantly optimize for user engagement and economic metrics, inadvertently
neglecting the environmental and societal ramifications of their
recommendations, potentially catalyzing over-consumption and reinforcing
unsustainable behavioral patterns. Given their instrumental role in shaping
user decisions, there exists an imperative need for sustainable recommender
systems that incorporate sustainability principles to foster eco-conscious and
socially responsible choices. This comprehensive survey addresses this critical
research gap by presenting a systematic analysis of sustainable recommender
systems. As these systems can simultaneously advance multiple sustainability
objectives--including resource conservation, sustainable consumer behavior, and
social impact enhancement--examining their implementations across distinct
application domains provides a more rigorous analytical framework. Through a
methodological analysis of domain-specific implementations encompassing
transportation, food, buildings, and auxiliary sectors, we can better elucidate
how these systems holistically advance sustainability objectives while
addressing sector-specific constraints and opportunities. Moreover, we
delineate future research directions for evolving recommender systems beyond
sustainability advocacy toward fostering environmental resilience and social
consciousness in society.",http://arxiv.org/abs/2411.07658v1
"Everything You Wanted to Know About Consumer Light Management in Smart
  Energy",2024-11-13T05:57:44Z,"Prajnyajit Mohanty, Umesh C. Pati, Kamalakanta Mahapatra, Saraju P. Mohanty","Consumer lighting plays a significant role in the development of smart cities
and smart villages. With the advancement of (IoT) technology, smart lighting
solutions have become more prevalent in residential areas as well. These
solutions provide consumers with increased energy efficiency, added
convenience, and improved security. On the other hand, the growing number of
IoT devices has become a global concern due to the carbon footprint and carbon
emissions associated with these devices. The overuse of batteries increases
maintenance and cost to IoT devices and simultaneously possesses adverse
environmental effects, ultimately exacerbating the pace of climate change.
Therefore, in tandom with the principles of Industry 4.0, it has become crucial
for manufacturing and research industries to prioritize sustainable measures
adhering to smart energy as a prevention to the negative impacts. Consequently,
it has undoubtedly garnered global interest from scientists, researchers, and
industrialists to integrate state-of-the-art technologies in order to solve the
current issues in consumer light management systems making it a complete
sustainable, and smart solution for consumer lighting application. This
manuscript provides a thorough investigation of various methods as well as
techniques to design a state-of-the-art IoT-enabled consumer light management
system. It critically reviews the existing works done in consumer light
management systems, emphasizing the significant limitations and the need for
sustainability. The top-down approach of developing sustainable computing
frameworks for IoT-enabled consumer light management has been reviewed based on
the multidisciplinary technologies involved and state-of-the-art works in the
respective domains. Lastly, this article concludes by highlighting possible
avenues for future research.",http://arxiv.org/abs/2411.08353v1
"DaYu: Data-Driven Model for Geostationary Satellite Observed Cloud
  Images Forecasting",2024-11-15T12:36:01Z,"Xujun Wei, Feng Zhang, Renhe Zhang, Wenwen Li, Cuiping Liu, Bin Guo, Jingwei Li, Haoyang Fu, Xu Tang","In the past few years, Artificial Intelligence (AI)-based weather forecasting
methods have widely demonstrated strong competitiveness among the weather
forecasting systems. However, these methods are insufficient for
high-spatial-resolution short-term nowcasting within 6 hours, which is crucial
for warning short-duration, mesoscale and small-scale weather events.
Geostationary satellite remote sensing provides detailed, high spatio-temporal
and all-day observations, which can address the above limitations of existing
methods. Therefore, this paper proposed an advanced data-driven thermal
infrared cloud images forecasting model, ""DaYu."" Unlike existing data-driven
weather forecasting models, DaYu is specifically designed for geostationary
satellite observations, with a temporal resolution of 0.5 hours and a spatial
resolution of ${0.05}^\circ$ $\times$ ${0.05}^\circ$. DaYu is based on a
large-scale transformer architecture, which enables it to capture fine-grained
cloud structures and learn fast-changing spatio-temporal evolution features
effectively. Moreover, its attention mechanism design achieves a balance in
computational complexity, making it practical for applications. DaYu not only
achieves accurate forecasts up to 3 hours with a correlation coefficient higher
than 0.9, 6 hours higher than 0.8, and 12 hours higher than 0.7, but also
detects short-duration, mesoscale, and small-scale weather events with enhanced
detail, effectively addressing the shortcomings of existing methods in
providing detailed short-term nowcasting within 6 hours. Furthermore, DaYu has
significant potential in short-term climate disaster prevention and mitigation.",http://arxiv.org/abs/2411.10144v1
"The role of Solar Activity in shaping Precipitation Extremes: A Regional
  Exploration in Kerala, India",2024-11-02T17:02:22Z,"Elizabeth Thomas, S. Vineeth, Noble P. Abraham","There has been global attention focused on extreme climatic changes. The
purpose of this paper is to explore the response of extreme precipitation
events to solar activity, over Kerala, India. The three solar indices - sunspot
number, F10.7 index, and cosmic ray intensity - are examined, and their
relationship to rainfall is examined during a 57-year period (1965 - 2021),
starting with Solar Cycle 20. Both solar and rainfall data are considered on an
annual scale as well as on a seasonal scale by dividing them into winter,
pre-monsoon, monsoon, and post-monsoon seasons. The solar indices are used to
calculate correlation coefficients with seasonal rainfall. Through correlation
analysis, it is found that the precipitation in Kerala is correlated with the
sunspot activity, but with different significance. When solar activity is high,
the winter and monsoon seasons exhibit strong correlations with high
significance. The solar influence at the regional level is also studied. The
central and southern parts of Kerala appear to be influenced by the Sun during
periods of high activity. The years with excess and deficiency of rainfall are
calculated and compared with the solar indices. It was observed that the years
with excessive and insufficient rainfall coincide with the years when the solar
activity is at its highest or minimum. It is suggested that there is a physical
link and a way to predict extreme rainfall events in Kerala based on the
association between solar activity and those events.",http://arxiv.org/abs/2411.10460v1
Efficient Denoising Method to Improve The Resolution of Satellite Images,2024-11-11T03:33:53Z,Jhanavi Hegde,"Satellites are widely used to estimate and monitor ground cover, providing
critical information to address the challenges posed by climate change.
High-resolution satellite images help to identify smaller features on the
ground and classification of ground cover types. Small satellites have become
very popular recently due to their cost-effectiveness. However, smaller
satellites have weaker spatial resolution, and preprocessing using recent
generative models made it possible to enhance the resolution of these satellite
images. The objective of this paper is to propose computationally efficient
guided or image-conditioned denoising diffusion models (DDMs) to perform
super-resolution on low-quality images. Denoising based on stochastic ordinary
differential equations (ODEs) typically takes hundreds of iterations and it can
be reduced using deterministic ODEs. I propose Consistency Models (CM) that
utilize deterministic ODEs for efficient denoising and perform super resolution
on satellite images. The DOTA v2.0 image dataset that is used to develop object
detectors needed for urban planning and ground cover estimation, is used in
this project. The Stable Diffusion model is used as the base model, and the DDM
in Stable Diffusion is converted into a Consistency Model (CM) using
Teacher-Student Distillation to apply deterministic denoising. Stable diffusion
with modified CM has successfully improved the resolution of satellite images
by a factor of 16, and the computational time was reduced by a factor of 20
compared to stochastic denoising methods. The FID score of low-resolution
images improved from 10.0 to 1.9 after increasing the image resolution using my
algorithm for consistency models.",http://arxiv.org/abs/2411.10476v1
"MpoxVLM: A Vision-Language Model for Diagnosing Skin Lesions from Mpox
  Virus Infection",2024-11-16T21:09:04Z,"Xu Cao, Wenqian Ye, Kenny Moise, Megan Coffee","In the aftermath of the COVID-19 pandemic and amid accelerating climate
change, emerging infectious diseases, particularly those arising from zoonotic
spillover, remain a global threat. Mpox (caused by the monkeypox virus) is a
notable example of a zoonotic infection that often goes undiagnosed, especially
as its rash progresses through stages, complicating detection across diverse
populations with different presentations. In August 2024, the WHO
Director-General declared the mpox outbreak a public health emergency of
international concern for a second time. Despite the deployment of deep
learning techniques for detecting diseases from skin lesion images, a robust
and publicly accessible foundation model for mpox diagnosis is still lacking
due to the unavailability of open-source mpox skin lesion images, multimodal
clinical data, and specialized training pipelines. To address this gap, we
propose MpoxVLM, a vision-language model (VLM) designed to detect mpox by
analyzing both skin lesion images and patient clinical information. MpoxVLM
integrates the CLIP visual encoder, an enhanced Vision Transformer (ViT)
classifier for skin lesions, and LLaMA-2-7B models, pre-trained and fine-tuned
on visual instruction-following question-answer pairs from our newly released
mpox skin lesion dataset. Our work achieves 90.38% accuracy for mpox detection,
offering a promising pathway to improve early diagnostic accuracy in combating
mpox.",http://arxiv.org/abs/2411.10888v1
"Tree Species Classification using Machine Learning and 3D Tomographic
  SAR -- a case study in Northern Europe",2024-11-19T22:25:26Z,"Colverd Grace, Schade Laura, Takami Jumpei, Bot Karol, Gallego Joseph","Tree species classification plays an important role in nature conservation,
forest inventories, forest management, and the protection of endangered
species. Over the past four decades, remote sensing technologies have been
extensively utilized for tree species classification, with Synthetic Aperture
Radar (SAR) emerging as a key technique. In this study, we employed TomoSense,
a 3D tomographic dataset, which utilizes a stack of single-look complex (SLC)
images, a byproduct of SAR, captured at different incidence angles to generate
a three-dimensional representation of the terrain. Our research focuses on
evaluating multiple tabular machine-learning models using the height
information derived from the tomographic image intensities to classify eight
distinct tree species. The SLC data and tomographic imagery were analyzed
across different polarimetric configurations and geosplit configurations. We
investigated the impact of these variations on classification accuracy,
comparing the performance of various tabular machine-learning models and
optimizing them using Bayesian optimization. Additionally, we incorporated a
proxy for actual tree height using point cloud data from Light Detection and
Ranging (LiDAR) to provide height statistics associated with the model's
predictions. This comparison offers insights into the reliability of
tomographic data in predicting tree species classification based on height.",http://arxiv.org/abs/2411.12897v1
"Adaptive Process-Guided Learning: An Application in Predicting Lake DO
  Concentrations",2024-11-20T01:58:20Z,"Runlong Yu, Chonghao Qiu, Robert Ladwig, Paul C. Hanson, Yiqun Xie, Yanhua Li, Xiaowei Jia","This paper introduces a \textit{Process-Guided Learning (Pril)} framework
that integrates physical models with recurrent neural networks (RNNs) to
enhance the prediction of dissolved oxygen (DO) concentrations in lakes, which
is crucial for sustaining water quality and ecosystem health. Unlike
traditional RNNs, which may deliver high accuracy but often lack physical
consistency and broad applicability, the \textit{Pril} method incorporates
differential DO equations for each lake layer, modeling it as a first-order
linear solution using a forward Euler scheme with a daily timestep. However,
this method is sensitive to numerical instabilities. When drastic fluctuations
occur, the numerical integration is neither mass-conservative nor stable.
Especially during stratified conditions, exogenous fluxes into each layer cause
significant within-day changes in DO concentrations. To address this challenge,
we further propose an \textit{Adaptive Process-Guided Learning (April)} model,
which dynamically adjusts timesteps from daily to sub-daily intervals with the
aim of mitigating the discrepancies caused by variations in entrainment fluxes.
\textit{April} uses a generator-discriminator architecture to identify days
with significant DO fluctuations and employs a multi-step Euler scheme with
sub-daily timesteps to effectively manage these variations. We have tested our
methods on a wide range of lakes in the Midwestern USA, and demonstrated robust
capability in predicting DO concentrations even with limited training data.
While primarily focused on aquatic ecosystems, this approach is broadly
applicable to diverse scientific and engineering disciplines that utilize
process-based models, such as power engineering, climate science, and
biomedicine.",http://arxiv.org/abs/2411.12973v1
Neural machine translation of seismic waves for petrophysical inversion,2024-11-20T17:39:29Z,"Jos√© Cunha Teixeira, Ludovic Bodet, Agn√®s Rivi√®re, Santiago G. Solazzi, Am√©lie Hallier, Alexandrine Gesret, Sanae El Janyani, Marine Dangeard, Amine Dhemaied, Jos√©phine Boisson Gaboriau","Effective structural assessment of urban infrastructure is essential for
sustainable land use and resilience to climate change and natural hazards.
Seismic wave methods are widely applied in these areas for subsurface
characterization and monitoring, yet they often rely on time-consuming
inversion techniques that fall short in delivering comprehensive geological,
hydrogeological, and geomechanical descriptions. Here, we explore the
effectiveness of a passive seismic approach coupled with artificial
intelligence (AI) for monitoring geological structures and hydrogeological
conditions in the context of sinkhole hazard assessment. We introduce a
deterministic petrophysical inversion technique based on a language model that
decodes seismic wave velocity measurements to infer soil petrophysical and
mechanical parameters as textual descriptions. Results successfully delineate
3D subsurface structures with their respective soil nature and mechanical
characteristics, while accurately predicting daily water table levels.
Validation demonstrates high accuracy, with a normalized root mean square error
of 8%, closely rivaling with conventional stochastic seismic inversion methods,
while delivering broader insights into subsurface conditions 2,000 times
faster. These findings underscore the potential of advanced AI techniques to
significantly enhance subsurface characterization across diverse scales,
supporting decision-making for natural hazard mitigation.",http://arxiv.org/abs/2411.13491v1
"Developing Global Aerosol Models based on the Analysis of 30-Year Ground
  Measurements by AERONET (AEROEX models) and Implication on Satellite based
  Aerosol Retrievals",2024-11-23T10:44:53Z,"Manoj K Mishra, Shameela S F, Pradyuman Singh Rathore","The AErosol RObotic NETwork (AERONET), established in 1993 with limited
global sites, has grown to over 900 locations, providing three decades of
continuous aerosol data. While earlier studies based on shorter time periods
(10-12 years) and fewer sites (approximately 250) made significant
contributions to aerosol research, the vast AERONET dataset (1993-2023) calls
for a comprehensive reevaluation to refine global aerosol models and improve
satellite retrievals. This is particularly important in light of major
environmental changes such as industrialization, land use shifts, and natural
events like wildfires and dust storms. In this study, a set of fine and coarse
aerosol models called AERONET-Extended (AEROEX) models are developed based on
cluster analysis of 30-years AERONET data, analyzing over 202,000 samples using
Gaussian Mixture Models to classify aerosol types by season and region.
Aerosols are categorized into spherical, spheroidal, and mixed types using
particle linear depolarization ratio and fine mode fraction. Four fine-mode
aerosol models were derived based on differences in scattering and absorption
properties, revealing regional/seasonal variations, particularly in North
America, Europe and Asia. Additionally, two coarse-mode aerosol models were
identified, separated by their absorbing properties in dust-prone and polluted
regions. We performed simulation analysis showing that the new models
significantly improve satellite-based aerosol optical depth retrievals compared
to widely used dark target aerosol models. A global aerosol model map,
generated at 1x1 degree resolution for each season using Random Forest and
expert refinement, provides valuable insights for climate and atmospheric
studies, improving satellite-based aerosol retrievals at global scale.",http://arxiv.org/abs/2411.15518v1
"Advancing Electrochemical CO$_2$ Capture with Redox-Active Metal-Organic
  Frameworks",2024-11-25T14:46:28Z,"Iuliia Vetik, Nikita ≈Ωoglo, Akmal Kosimov, Ritums Cepitis, Veera Krasnenko, Huilin Qing, Priyanshu Chandra, Katherine Mirica, Ruben Rizo, Enrique Herrero, Jose Solla-Gull√≥n, Teedhat Trisukhon, Jamie W. Gittins, Alexander C. Forse, Vitali Grozovski, Nadezda Kongi, Vladislav Ivani≈°t≈°ev","Addressing climate change calls for action to control CO$_2$ pollution.
Direct air and ocean capture offer a solution to this challenge. Making carbon
capture competitive with alternatives, such as forestation and mineralisation,
requires fundamentally novel approaches and ideas. One such approach is
electrosorption, which is currently limited by the availability of suitable
electrosorbents. In this work, we introduce a metal-organic
copper-2,3,6,7,10,11-hexahydroxytriphenylene (Cu$_3$(HHTP)$_2$) metal-organic
framework (MOF) that can act as electrosorbent for CO$_2$ capture, thereby
expanding the palette of materials that can be used for this process.
Cu$_3$(HHTP)$_2$ is the first MOF to switch its ability to capture and release
CO$_2$ in aqueous electrolytes. By using cyclic voltammetry (CV) and
differential electrochemical mass spectrometry (DEMS), we demonstrate
reversible CO$_2$ electrosorption. Based on density functional theory (DFT)
calculations, we provide atomistic insights into the mechanism of
electrosorption and conclude that efficient CO$_2$ capture is facilitated by a
combination of redox-active copper and aromatic HHTP ligand within Cu3(HHTP)2.
By showcasing the applicability of Cu$_3$(HHTP)$_2$ -- with a CO$_2$ capacity
of 2 mmol g$^{-1}$ and an adsorption enthalpy of -20 kJ mol$^{-1}$ - this study
encourages further exploration of conductive redox-active MOFs in the search
for superior CO$_2$ electrosorbents.",http://arxiv.org/abs/2411.16444v1
"Improved implicit diffusion model with knowledge distillation to
  estimate the spatial distribution density of carbon stock in remote sensing
  imagery",2024-11-27T01:06:05Z,Zhenyu Yu,"The forest serves as the most significant terrestrial carbon stock mechanism,
effectively reducing atmospheric CO$_2$ concentrations and mitigating climate
change. Remote sensing provides high data accuracy and enables large-scale
observations. Optical images facilitate long-term monitoring, which is crucial
for future carbon stock estimation studies. This study focuses on Huize County,
Qujing City, Yunnan Province, China, utilizing GF-1 WFV satellite imagery. The
KD-VGG and KD-UNet modules were introduced for initial feature extraction, and
the improved implicit diffusion model (IIDM) was proposed. The results showed:
(1) The VGG module improved initial feature extraction, improving accuracy, and
reducing inference time with optimized model parameters. (2) The
Cross-attention + MLPs module enabled effective feature fusion, establishing
critical relationships between global and local features, achieving
high-accuracy estimation. (3) The IIDM model, a novel contribution,
demonstrated the highest estimation accuracy with an RMSE of 12.17\%,
significantly improving by 41.69\% to 42.33\% compared to the regression model.
In carbon stock estimation, the generative model excelled in extracting deeper
features, significantly outperforming other models, demonstrating the
feasibility of AI-generated content in quantitative remote sensing. The
16-meter resolution estimates provide a robust basis for tailoring forest
carbon sink regulations, enhancing regional carbon stock management.",http://arxiv.org/abs/2411.17973v1
Advancing global aerosol forecasting with artificial intelligence,2024-12-03T15:21:26Z,"Ke Gui, Xutao Zhang, Huizheng Che, Lei Li, Yu Zheng, Linchang An, Yucong Miao, Hujia Zhao, Oleg Dubovik, Brent Holben, Jun Wang, Pawan Gupta, Elena S. Lind, Carlos Toledano, Hong Wang, Zhili Wang, Yaqiang Wang, Xiaomeng Huang, Kan Dai, Xiangao Xia, Xiaofeng Xu, Xiaoye Zhang","Aerosol forecasting is essential for air quality warnings, health risk
assessment, and climate change mitigation. However, it is more complex than
weather forecasting due to the intricate interactions between aerosol
physicochemical processes and atmospheric dynamics, resulting in significant
uncertainty and high computational costs. Here, we develop an artificial
intelligence-driven global aerosol-meteorology forecasting system (AI-GAMFS),
which provides reliable 5-day, 3-hourly forecasts of aerosol optical components
and surface concentrations at a 0.5{\deg} x 0.625{\deg} resolution. AI-GAMFS
combines Vision Transformer and U-Net in a backbone network, robustly capturing
the complex aerosol-meteorology interactions via global attention and
spatiotemporal encoding. Trained on 42 years of advanced aerosol reanalysis
data and initialized with GEOS Forward Processing (GEOS-FP) analyses, AI-GAMFS
delivers operational 5-day forecasts in one minute. It outperforms the
Copernicus Atmosphere Monitoring Service (CAMS) global forecasting system,
GEOS-FP forecasts, and several regional dust forecasting systems in forecasting
most aerosol variables including aerosol optical depth and dust components. Our
results mark a significant step forward in leveraging AI to refine
physics-based aerosol forecasting, facilitating more accurate global warnings
for aerosol pollution events, such as dust storms and wildfires.",http://arxiv.org/abs/2412.02498v1
"PCTreeS: 3D Point Cloud Tree Species Classification Using Airborne LiDAR
  Images",2024-12-06T02:09:52Z,"Hongjin Lin, Matthew Nazari, Derek Zheng","Reliable large-scale data on the state of forests is crucial for monitoring
ecosystem health, carbon stock, and the impact of climate change. Current
knowledge of tree species distribution relies heavily on manual data collection
in the field, which often takes years to complete, resulting in limited
datasets that cover only a small subset of the world's forests. Recent works
show that state-of-the-art deep learning models using Light Detection and
Ranging (LiDAR) images enable accurate and scalable classification of tree
species in various ecosystems. While LiDAR images contain rich 3D information,
most previous works flatten the 3D images into 2D projections to use
Convolutional Neural Networks (CNNs). This paper offers three significant
contributions: (1) we apply the deep learning framework for tree classification
in tropical savannas; (2) we use Airborne LiDAR images, which have a lower
resolution but greater scalability than Terrestrial LiDAR images used in most
previous works; (3) we introduce the approach of directly feeding 3D point
cloud images into a vision transformer model (PCTreeS). Our results show that
the PCTreeS approach outperforms current CNN baselines with 2D projections in
AUC (0.81), overall accuracy (0.72), and training time (~45 mins). This paper
also motivates further LiDAR image collection and validation for accurate
large-scale automatic classification of tree species.",http://arxiv.org/abs/2412.04714v1
Applying Machine Learning Tools for Urban Resilience Against Floods,2024-12-09T04:56:33Z,"Mahla Ardebili Pour, Mohammad B. Ghiasi, Ali Karkehabadi","Floods are among the most prevalent and destructive natural disasters, often
leading to severe social and economic impacts in urban areas due to the high
concentration of assets and population density. In Iran, particularly in
Tehran, recurring flood events underscore the urgent need for robust urban
resilience strategies. This paper explores flood resilience models to identify
the most effective approach for District 6 in Tehran. Through an extensive
literature review, various resilience models were analyzed, with the Climate
Disaster Resilience Index (CDRI) emerging as the most suitable model for this
district due to its comprehensive resilience dimensions: Physical, Social,
Economic, Organizational, and Natural Health resilience. Although the CDRI
model provides a structured approach to resilience measurement, it remains a
static model focused on spatial characteristics and lacks temporal
adaptability. An extensive literature review enhances the CDRI model by
integrating data from 2013 to 2022 in three-year intervals and applying machine
learning techniques to predict resilience dimensions for 2025. This integration
enables a dynamic resilience model that can accommodate temporal changes,
providing a more adaptable and data driven foundation for urban flood
resilience planning. By employing artificial intelligence to reflect evolving
urban conditions, this model offers valuable insights for policymakers and
urban planners to enhance flood resilience in Tehrans critical District 6.",http://arxiv.org/abs/2412.06205v1
"The role of Enhanced Geothermal Systems in the energy transition at
  Cornell -- Report of a workshop held at Cornell University, Ithaca, October
  23-24, 2024",2024-12-16T03:43:13Z,"Chlo√© Arson, Dominic Balog-Way, Koenraad Beckers, Wayne Bezner-Kerr, Sarah Carson, Stacey Edwards, Patrick Fulton, Michael Gillenwater, Trystan Goetze, Olaf Gustafson, Tony Ingraffea, Terry Jordan, Katherine McComas, Sheila Olmstead, Seth Saltiel, Jeff Tester, Cole Tucker, Marguerite Wells","To review the lessons learnt from recent deep geothermal case studies and
plan strategically the research, development, regulation, and communication
work required for the implementation of an Enhanced Geothermal System (EGS) at
Cornell University, a group of engineers and scholars convened a two-day
workshop on the Ithaca campus, on October 23-24, 2024. The event was funded by
Cornell Atkinson Center for Sustainability. This report is a summary of the
content of the presentations and discussions that took place during the
workshop. The first section focuses on philosophical, sociological, economic,
and regulatory questions posed by EGS deployment as a means to mitigate climate
change. The second section tackles the scientific and technological research
areas associated with EGS. The third section aims to assess the feasibility of
developing EGS for heat direct use at Cornell University, based on results and
information available to date. The report concludes with a summary of the most
salient technological and scientific breakthroughs, and a plan for future
technological and academic engagement in EGS projects at Cornell.",http://arxiv.org/abs/2412.11421v1
"Machine learning in wastewater treatment: insights from modelling a
  pilot denitrification reactor",2024-12-18T16:49:23Z,"Eivind B√∏hn, S√∏lve Eidnes, Kjell Rune Jonassen","Wastewater treatment plants are increasingly recognized as promising
candidates for machine learning applications, due to their societal importance
and high availability of data. However, their varied designs, operational
conditions, and influent characteristics hinder straightforward automation. In
this study, we use data from a pilot reactor at the Veas treatment facility in
Norway to explore how machine learning can be used to optimize biological
nitrate ($\mathrm{NO_3^-}$) reduction to molecular nitrogen ($\mathrm{N_2}$) in
the biogeochemical process known as \textit{denitrification}. Rather than
focusing solely on predictive accuracy, our approach prioritizes understanding
the foundational requirements for effective data-driven modelling of wastewater
treatment. Specifically, we aim to identify which process parameters are most
critical, the necessary data quantity and quality, how to structure data
effectively, and what properties are required by the models. We find that
nonlinear models perform best on the training and validation data sets,
indicating nonlinear relationships to be learned, but linear models transfer
better to the unseen test data, which comes later in time. The variable
measuring the water temperature has a particularly detrimental effect on the
models, owing to a significant change in distributions between training and
test data. We therefore conclude that multiple years of data is necessary to
learn robust machine learning models. By addressing foundational elements,
particularly in the context of the climatic variability faced by northern
regions, this work lays the groundwork for a more structured and tailored
approach to machine learning for wastewater treatment. We share publicly both
the data and code used to produce the results in the paper.",http://arxiv.org/abs/2412.14030v1
"A District-level Ensemble Model to Enhance Dengue Prediction and Control
  for the Mekong Delta Region of Vietnam",2024-12-20T08:00:50Z,"Wala Draidi Areed, Thi Thanh Thao Nguyen, Kien Quoc Do, Thinh Nguyen, Vinh Bui, Elisabeth Nelson, Joshua L. Warren, Quang-Van Doan, Nam Vu Sinh, Nicholas Osborne, Russell Richards, Nu Quy Linh Tran, Hong Le, Tuan Pham, Trinh Manh Hung, Son Nghiem, Hai Phung, Cordia Chu, Robert Dubrow, Daniel M. Weinberger, Dung Phung","The Mekong Delta Region of Vietnam faces increasing dengue risks driven by
urbanization, globalization, and climate change. This study introduces a
probabilistic forecasting model for predicting dengue incidence and outbreaks
with one to three month lead times, integrating meteorological,
sociodemographic, preventive, and epidemiological data. Seventy-two models were
evaluated, and an ensemble combining top-performing spatiotemporal, supervised
PCA, and semi-mechanistic hhh4 frameworks was developed. Using data from
2004-2022 for training, validation, and evaluation, the ensemble model
demonstrated 69% accuracy at a 3-month horizon, outperforming a baseline model.
While effective, its performance declined in years with atypical seasonality,
such as 2019 and 2022. The model provides critical lead time for targeted
dengue prevention and control measures, addressing a growing public health need
in the region.",http://arxiv.org/abs/2412.15645v1
"A Novel Task-Driven Method with Evolvable Interactive Agents Using Event
  Trees for Enhanced Emergency Decision Support",2024-12-24T04:53:46Z,"Xingyu Xiao, Peng Chen, Ben Qi, Jingang Liang, Jiejuan Tong, Haitao Wang","As climate change and other global challenges increase the likelihood of
unforeseen emergencies, the limitations of human-driven strategies in critical
situations become more pronounced. Inadequate pre-established emergency plans
can lead operators to become overwhelmed during complex systems malfunctions.
This study addresses the urgent need for agile decision-making in response to
various unforeseen incidents through a novel approach, EvoTaskTree (a
task-driven method with evolvable interactive agents using event trees for
emergency decision support). This advanced approach integrates two types of
agents powered by large language models (LLMs): task executors, responsible for
executing critical procedures, and task validators, ensuring the efficacy of
those actions. By leveraging insights from event tree analysis, our framework
encompasses three crucial tasks: initiating event subevent analysis, event tree
header event analysis, and decision recommendations. The agents learn from both
successful and unsuccessful responses from these tasks. Finally, we use nuclear
power plants as a demonstration of a safety-critical system. Our findings
indicate that the designed agents are not only effective but also outperform
existing approaches, achieving an impressive accuracy rate of up to 100 % in
processing previously unencoun32 tered incident scenarios. This paper
demonstrates that EvoTaskTree significantly enhances the rapid formulation of
emergency decision-making.",http://arxiv.org/abs/2501.06193v1
"Enhancing Green Economy with Artificial Intelligence: Role of Energy Use
  and FDI in the United States",2024-12-20T03:03:21Z,"Abdullah Al Abrar Chowdhury, Azizul Hakim Rafi, Adita Sultana, Abdulla All Noman","The escalating challenge of climate change necessitates an urgent exploration
of factors influencing carbon emissions. This study contributes to the
discourse by examining the interplay of technological, economic, and
demographic factors on environmental sustainability. This study investigates
the impact of artificial intelligence (AI) innovation, economic growth, foreign
direct investment (FDI), energy consumption, and urbanization on CO2 emissions
in the United States from 1990 to 2022. Employing the ARDL framework integrated
with the STIRPAT model, the findings reveal a dual narrative: while AI
innovation mitigates environmental stress, economic growth, energy use, FDI,
and urbanization exacerbate environmental degradation. Unit root tests (ADF,
PP, and DF-GLS) confirm mixed integration levels among variables, and the ARDL
bounds test establishes long-term co-integration. The analysis highlights that
AI innovation positively correlates with CO2 reduction when environmental
safeguards are in place, whereas GDP growth, energy consumption, FDI, and
urbanization intensify CO2 emissions. Robustness checks using FMOLS, DOLS, and
CCR validate the ARDL findings. Additionally, Pairwise Granger causality tests
reveal significant one-way causal links between CO2 emissions and economic
growth, AI innovation, energy use, FDI, and urbanization. These relationships
emphasize the critical role of AI-driven technological advancements,
sustainable investments, and green energy in fostering ecological
sustainability. The study suggests policy measures such as encouraging green
FDI, advancing AI technologies, adopting sustainable energy practices, and
implementing eco-friendly urban development to promote sustainable growth in
the USA.",http://arxiv.org/abs/2501.14747v1
"Segment Anything Model Can Not Segment Anything: Assessing AI Foundation
  Model's Generalizability in Permafrost Mapping",2024-01-16T19:10:09Z,"Wenwen Li, Chia-Yu Hsu, Sizhe Wang, Yezhou Yang, Hyunho Lee, Anna Liljedahl, Chandi Witharana, Yili Yang, Brendan M. Rogers, Samantha T. Arundel, Matthew B. Jones, Kenton McHenry, Patricia Solis","This paper assesses trending AI foundation models, especially emerging
computer vision foundation models and their performance in natural landscape
feature segmentation. While the term foundation model has quickly garnered
interest from the geospatial domain, its definition remains vague. Hence, this
paper will first introduce AI foundation models and their defining
characteristics. Built upon the tremendous success achieved by Large Language
Models (LLMs) as the foundation models for language tasks, this paper discusses
the challenges of building foundation models for geospatial artificial
intelligence (GeoAI) vision tasks. To evaluate the performance of large AI
vision models, especially Meta's Segment Anything Model (SAM), we implemented
different instance segmentation pipelines that minimize the changes to SAM to
leverage its power as a foundation model. A series of prompt strategies was
developed to test SAM's performance regarding its theoretical upper bound of
predictive accuracy, zero-shot performance, and domain adaptability through
fine-tuning. The analysis used two permafrost feature datasets, ice-wedge
polygons and retrogressive thaw slumps because (1) these landform features are
more challenging to segment than manmade features due to their complicated
formation mechanisms, diverse forms, and vague boundaries; (2) their presence
and changes are important indicators for Arctic warming and climate change. The
results show that although promising, SAM still has room for improvement to
support AI-augmented terrain mapping. The spatial and domain generalizability
of this finding is further validated using a more general dataset EuroCrop for
agricultural field mapping. Finally, we discuss future research directions that
strengthen SAM's applicability in challenging geospatial domains.",http://arxiv.org/abs/2401.08787v1
"Spatio-temporal analysis for extreme temperature indices over the Levant
  region",2024-01-17T07:34:48Z,"Ala A. M. Salameh, Sonia R. G√°miz-Fortis, Yolanda Castro-D√≠ez, Ahmad Abu Hammad, Mar√≠a Jes√∫s Esteban-Parra","The temporal and spatial trends of 16 climate extreme indices based on daily
maximum and minimum temperatures during the period 1987-2016 at 28 stations
distributed across Israel and Palestine in the Levant region were annually and
seasonally analysed. The Man-Kendall test and the Sen's slope estimator were
employed for the trend analysis. Results showed that the region has
significantly experienced a dominant warming trend for the last three decades,
with more intense changes for minimum temperatures than for maximum. At annual
scale, maximum values of minimum temperatures exhibited significant increasing
trends up to 0.68{\deg}C/decade. Changes detected were more pronounced than
those for the absolute extreme temperature indices, with 93 and 89% of stations
significantly showed increasing trends in TX90p and TN90p, respectively. The
duration and fixed threshold extreme indices confirmed the trend toward a
warming, with the 86% of the stations exhibited significant increasing trends
in the annual SU25 and TR20. Moreover, 57% of stations showed significant
increasing trends in their SU30 index. At seasonal scale, the analysis of
trends for extreme temperature indices showed intense and broad significant
increasing trends in all absolute extreme temperature indices. In summer, more
than 75% of total stations exhibited significant increasing trends for warm
days and warm nights. In winter and spring, 71% of the total stations also
showed significant increasing trends in SU25 index, whereas the percentage of
stations reached 82% in summer and 64% in autumn for significant increasing
trends in TR20 index. Finally, the influence of large-scale circulation
patterns on temperature extremes was examined. The results highlighted the
presence of significant correlations between most of the selected extreme
temperature indices and the North Sea-Caspian pattern at annual and seasonal
scales.",http://arxiv.org/abs/2401.09022v1
Leveraging LSTM and GAN for Modern Malware Detection,2024-05-07T14:57:24Z,"Ishita Gupta, Sneha Kumari, Priya Jha, Mohona Ghosh","The malware booming is a cyberspace equal to the effect of climate change to
ecosystems in terms of danger. In the case of significant investments in
cybersecurity technologies and staff training, the global community has become
locked up in the eternal war with cyber security threats. The multi-form and
changing faces of malware are continuously pushing the boundaries of the
cybersecurity practitioners employ various approaches like detection and
mitigate in coping with this issue. Some old mannerisms like signature-based
detection and behavioral analysis are slow to adapt to the speedy evolution of
malware types. Consequently, this paper proposes the utilization of the Deep
Learning Model, LSTM networks, and GANs to amplify malware detection accuracy
and speed. A fast-growing, state-of-the-art technology that leverages raw
bytestream-based data and deep learning architectures, the AI technology
provides better accuracy and performance than the traditional methods.
Integration of LSTM and GAN model is the technique that is used for the
synthetic generation of data, leading to the expansion of the training
datasets, and as a result, the detection accuracy is improved. The paper uses
the VirusShare dataset which has more than one million unique samples of the
malware as the training and evaluation set for the presented models. Through
thorough data preparation including tokenization, augmentation, as well as
model training, the LSTM and GAN models convey the better performance in the
tasks compared to straight classifiers. The research outcomes come out with 98%
accuracy that shows the efficiency of deep learning plays a decisive role in
proactive cybersecurity defense. Aside from that, the paper studies the output
of ensemble learning and model fusion methods as a way to reduce biases and
lift model complexity.",http://arxiv.org/abs/2405.04373v1
"Comparing remote sensing-based forest biomass mapping approaches using
  new forest inventory plots in contrasting forests in northeastern and
  southwestern China",2024-05-24T11:10:58Z,"Wenquan Dong, Edward T. A. Mitchard, Yuwei Chen, Man Chen, Congfeng Cao, Peilun Hu, Cong Xu, Steven Hancock","Large-scale high spatial resolution aboveground biomass (AGB) maps play a
crucial role in determining forest carbon stocks and how they are changing,
which is instrumental in understanding the global carbon cycle, and
implementing policy to mitigate climate change. The advent of the new
space-borne LiDAR sensor, NASA's GEDI instrument, provides unparalleled
possibilities for the accurate and unbiased estimation of forest AGB at high
resolution, particularly in dense and tall forests, where Synthetic Aperture
Radar (SAR) and passive optical data exhibit saturation. However, GEDI is a
sampling instrument, collecting dispersed footprints, and its data must be
combined with that from other continuous cover satellites to create
high-resolution maps, using local machine learning methods. In this study, we
developed local models to estimate forest AGB from GEDI L2A data, as the models
used to create GEDI L4 AGB data incorporated minimal field data from China. We
then applied LightGBM and random forest regression to generate wall-to-wall AGB
maps at 25 m resolution, using extensive GEDI footprints as well as Sentinel-1
data, ALOS-2 PALSAR-2 and Sentinel-2 optical data. Through a 5-fold
cross-validation, LightGBM demonstrated a slightly better performance than
Random Forest across two contrasting regions. However, in both regions, the
computation speed of LightGBM is substantially faster than that of the random
forest model, requiring roughly one-third of the time to compute on the same
hardware. Through the validation against field data, the 25 m resolution AGB
maps generated using the local models developed in this study exhibited higher
accuracy compared to the GEDI L4B AGB data. We found in both regions an
increase in error as slope increased. The trained models were tested on nearby
but different regions and exhibited good performance.",http://arxiv.org/abs/2405.15438v1
Foundation Models for the Electric Power Grid,2024-07-12T17:09:47Z,"Hendrik F. Hamann, Thomas Brunschwiler, Blazhe Gjorgiev, Leonardo S. A. Martins, Alban Puech, Anna Varbella, Jonas Weiss, Juan Bernabe-Moreno, Alexandre Blondin Mass√©, Seong Choi, Ian Foster, Bri-Mathias Hodge, Rishabh Jain, Kibaek Kim, Vincent Mai, Fran√ßois Mirall√®s, Martin De Montigny, Octavio Ramos-Lea√±os, Hussein Supr√™me, Le Xie, El-Nasser S. Youssef, Arnaud Zinflou, Alexander J. Belyi, Ricardo J. Bessa, Bishnu Prasad Bhattarai, Johannes Schmude, Stanislav Sobolevsky","Foundation models (FMs) currently dominate news headlines. They employ
advanced deep learning architectures to extract structural information
autonomously from vast datasets through self-supervision. The resulting rich
representations of complex systems and dynamics can be applied to many
downstream applications. Therefore, FMs can find uses in electric power grids,
challenged by the energy transition and climate change. In this paper, we call
for the development of, and state why we believe in, the potential of FMs for
electric grids. We highlight their strengths and weaknesses amidst the
challenges of a changing grid. We argue that an FM learning from diverse grid
data and topologies could unlock transformative capabilities, pioneering a new
approach in leveraging AI to redefine how we manage complexity and uncertainty
in the electric grid. Finally, we discuss a power grid FM concept, namely
GridFM, based on graph neural networks and show how different downstream tasks
benefit.",http://arxiv.org/abs/2407.09434v2
"HAMSTER: Hyperspectral Albedo Maps dataset with high Spatial and
  TEmporal Resolution",2024-07-25T13:26:50Z,"Giulia Roccetti, Luca Bugliaro, Felix G√∂dde, Claudia Emde, Ulrich Hamann, Mihail Manev, Michael Sterzik, Cedric Wehrum","Surface albedo is an important parameter in radiative transfer simulations of
the Earth's system, as it is fundamental to correctly calculate the energy
budget of the planet. The Moderate Resolution Imaging Spectroradiometer (MODIS)
instruments on NASA's Terra and Aqua satellites continuously monitor daily and
yearly changes in reflection at the planetary surface. The MODIS Surface
Reflectance black-sky albedo dataset (MCD43D, version 6.1) gives detailed
albedo maps in seven spectral bands in the visible and near-infrared range.
These albedo maps allow us to classify different Lambertian surface types and
their seasonal and yearly variability and change, albeit only in seven spectral
bands. However, a complete set of albedo maps covering the entire wavelength
range is required to simulate radiance spectra, and to correctly retrieve
atmospheric and cloud properties from Earth's remote sensing. We use a
Principal Component Analysis (PCA) regression algorithm to generate
hyperspectral albedo maps of Earth. Combining different datasets of
hyperspectral reflectance laboratory measurements for various dry soils,
vegetation surfaces, and mixtures of both, we reconstruct the albedo maps in
the entire wavelength range from 400 to 2500~nm. The PCA method is trained with
a 10-years average of MODIS data for each day of the year. We obtain
hyperspectral albedo maps with a spatial resolution of 0.05{\deg} in latitude
and longitude, a spectral resolution of 10~nm, and a temporal resolution of
1~day. Using the hyperspectral albedo maps, we estimate the spectral profiles
of different land surfaces, such as forests, deserts, cities and icy surfaces,
and study their seasonal variability. These albedo maps shall enable to refine
calculations of Earth's energy budget, its seasonal variability, and improve
climate simulations.",http://arxiv.org/abs/2407.18030v1
"Direct observation of thermal hysteresis in the molecular dynamics of
  barocaloric neopentyl glycol",2024-10-23T13:41:49Z,"Frederic Rendell-Bhatti, Markus Appel, Connor S. Inglis, Melony Dilshad, Neha Mehta, Jonathan Radcliffe, Xavier Moya, Donald A. MacLaren, David Boldrin","Barocalorics (BCs) are emerging as promising alternatives to vapour-phase
refrigerants, which are problematic as they exacerbate climate change when they
inevitably leak into the atmosphere. However, the commercialisation of BC
refrigerants is significantly hindered by hysteresis in the solid-solid phase
transition that would be exploited in a refrigeration cycle. Here, we provide
new insight into the hysteresis that is a critical step towards the rational
design of viable BCs. By studying the benchmark BC plastic crystal, neopentyl
glycol (NPG), we observe directly the liberation of the hydroxyl rotational
modes that unlock the hydrogen bond network, distinguishing for the first time
the molecular reorientation and hydroxymethyl rotational modes. We showcase the
use high-resolution inelastic fixed-window scans in combination with
quasielastic neutron scattering (QENS) measurements to build a comprehensive
microscopic understanding of the NPG phase transition, directly tracking the
molecular dynamics of the phase transition. Hysteresis previously observed in
calorimetric studies of NPG is now observed directly as hysteresis in molecular
rotational modes, and hence in the formation and disruption of hydrogen
bonding. Furthermore, by tracking the thermal activation of three main
reorientation modes, we suggest that their fractional excitations may resolve
an outstanding discrepancy between measured and calculated entropy change.
These results allow for direct study of the molecular dynamics that govern the
thermal hysteresis of small molecule energy materials. They will be broadly
applicable, as many promising BC material families possess first-order
transitions involving molecular reorientations.",http://arxiv.org/abs/2410.17869v3
"Short- and long-term relationships between the Yucatan Channel transport
  and the Loop Current System",2024-11-04T15:55:45Z,"Efra√≠n Moreles, Benjam√≠n Mart√≠nez-L√≥pez, Susana Higuera-Parra, Erick R. Olvera-Prado, Jorge Zavala-Hidalgo","This work uses twin 22-year free-running simulations of the Gulf of Mexico
hydrodynamics performed with the HYCOM, one considering only ocean dynamics and
the other incorporating atmospheric forcing, to study the behavior of the
Yucatan Channel transport (YCT), the Loop Current (LC), the Loop Current Eddies
(LCEs), their relationships, and the atmospheric forcing effect on them in
short (daily) and long (monthly) time scales. A more comprehensive description
of the LC intrusion and LCE separations was obtained by considering the upper
eastern or western YCT (whose magnitudes are determined by the longitudinal
displacements of the Yucatan Current's core), a perspective not evident when
considering the upper total YCT; specifically, the eastern YCT provides the
most meaningful description of the studied processes. Atmospheric forcing
mainly affects the extended stage of the LC by creating a higher dispersion in
the YCT and LC circulation values in comparison when considering only ocean
dynamics. For the long-term analysis, standardized indexes that integrate the
daily values of the eastern YCT and LC circulation in time were used; their
temporal propagation and persistence (the changes of their characteristics from
short to long time scales) were studied. Intrinsic ocean dynamics produces a
persistent YCT and LC intrusion behavior and consistent LCE separation patterns
from daily to 5-month scales. The atmospheric forcing effects are more
emphasized on the LC intrusion and LCE separations than on the YCT: the YCT
persistence is maintained but not that of the LC intrusion. An increased
occurrence of LCE separations with low or moderate LC intrusion is expected due
to climate change. Using the standardized indexes of the LC metrics to
construct a predictive model of the LC intrusion and LCE separations using only
current and past LC information is proposed for future research.",http://arxiv.org/abs/2411.02202v1
"Sustainability concepts for digital research infrastructures developed
  through ground-level stakeholder empowerment",2024-11-21T16:54:04Z,"Florian Ahrens, Dawn Geatches, Niall McCarroll, Justin Buck, Alvaro Lorenzo-Lopez, Hossein Keshtkar, Nadine Fayyad, Hamidreza Hassanloo, Danae Manika","The UK Research and Innovation Digital Research Infrastructure (DRI) needs to
operate sustainably in the future, encompassing its use of energy and
resources, and embedded computer hardware carbon emissions. Transition concepts
towards less unsustainable operations will inform the future design and
operations of DRI. A problem remains that, while the skills and knowledge for
solving net zero challenges already exist within the UK's DRI community, the
mechanisms for sharing them and enabling behavior change are missing. Without
adopting community-driven approaches, individual stakeholders may feel isolated
and uncertain about how to play their role in the transition. A research
programme was funded to give voice to the ground-level stakeholders of the DRI
ecosystem for the co-creation of carbon downshift concepts. This article
presents the results of the programme, with the goal to inform a fair and just
transition from the ground-level, complementing the top-down interventions of
energy efficiency policies and renewable energies integration. A workshop-based
innovation method was developed for researching stakeholder recommendations and
perspectives on the sustainable transition of the UK's DRI. We find that giving
a purposeful voice to the stakeholders for shaping their own future sustainable
DRI environment can be achieved by a guided, expert-integrated, interactive and
problem-focused workshop series. The chosen workshop design is impactful on
creating bottom-up agency for climate action by first defining the high-level
problems of unsustainability in energy and fossil-fuel consumption, and then
connecting them to the ground-level circumstances of DRI stakeholders. This
approach to stakeholder management should initiate a sustainable transition
that promises to kick-start impactful changes from within communities, adding
to high-level efforts from economics, policy, and governance.",http://arxiv.org/abs/2411.14301v1
"Taec: a Manually annotated text dataset for trait and phenotype
  extraction and entity linking in wheat breeding literature",2024-01-15T03:23:24Z,"Claire N√©dellec, Clara Sauvion, Robert Bossy, Mariya Borovikova, Louise Del√©ger","Wheat varieties show a large diversity of traits and phenotypes. Linking them
to genetic variability is essential for shorter and more efficient wheat
breeding programs. Newly desirable wheat variety traits include disease
resistance to reduce pesticide use, adaptation to climate change, resistance to
heat and drought stresses, or low gluten content of grains. Wheat breeding
experiments are documented by a large body of scientific literature and
observational data obtained in-field and under controlled conditions. The
cross-referencing of complementary information from the literature and
observational data is essential to the study of the genotype-phenotype
relationship and to the improvement of wheat selection. The scientific
literature on genetic marker-assisted selection describes much information
about the genotype-phenotype relationship. However, the variety of expressions
used to refer to traits and phenotype values in scientific articles is a hinder
to finding information and cross-referencing it. When trained adequately by
annotated examples, recent text mining methods perform highly in named entity
recognition and linking in the scientific domain. While several corpora contain
annotations of human and animal phenotypes, currently, no corpus is available
for training and evaluating named entity recognition and entity-linking methods
in plant phenotype literature. The Triticum aestivum trait Corpus is a new gold
standard for traits and phenotypes of wheat. It consists of 540 PubMed
references fully annotated for trait, phenotype, and species named entities
using the Wheat Trait and Phenotype Ontology and the species taxonomy of the
National Center for Biotechnology Information. A study of the performance of
tools trained on the Triticum aestivum trait Corpus shows that the corpus is
suitable for the training and evaluation of named entity recognition and
linking.",http://arxiv.org/abs/2401.07447v1
"DengueNet: Dengue Prediction using Spatiotemporal Satellite Imagery for
  Resource-Limited Countries",2024-01-20T04:55:29Z,"Kuan-Ting Kuo, Dana Moukheiber, Sebastian Cajas Ordonez, David Restrepo, Atika Rahman Paddo, Tsung-Yu Chen, Lama Moukheiber, Mira Moukheiber, Sulaiman Moukheiber, Saptarshi Purkayastha, Po-Chih Kuo, Leo Anthony Celi","Dengue fever presents a substantial challenge in developing countries where
sanitation infrastructure is inadequate. The absence of comprehensive
healthcare systems exacerbates the severity of dengue infections, potentially
leading to life-threatening circumstances. Rapid response to dengue outbreaks
is also challenging due to limited information exchange and integration. While
timely dengue outbreak forecasts have the potential to prevent such outbreaks,
the majority of dengue prediction studies have predominantly relied on data
that impose significant burdens on individual countries for collection. In this
study, our aim is to improve health equity in resource-constrained countries by
exploring the effectiveness of high-resolution satellite imagery as a
nontraditional and readily accessible data source. By leveraging the wealth of
publicly available and easily obtainable satellite imagery, we present a
scalable satellite extraction framework based on Sentinel Hub, a cloud-based
computing platform. Furthermore, we introduce DengueNet, an innovative
architecture that combines Vision Transformer, Radiomics, and Long Short-term
Memory to extract and integrate spatiotemporal features from satellite images.
This enables dengue predictions on an epi-week basis. To evaluate the
effectiveness of our proposed method, we conducted experiments on five
municipalities in Colombia. We utilized a dataset comprising 780
high-resolution Sentinel-2 satellite images for training and evaluation. The
performance of DengueNet was assessed using the mean absolute error (MAE)
metric. Across the five municipalities, DengueNet achieved an average MAE of
43.92. Our findings strongly support the efficacy of satellite imagery as a
valuable resource for dengue prediction, particularly in informing public
health policies within countries where manually collected data is scarce and
dengue virus prevalence is severe.",http://arxiv.org/abs/2401.11114v2
"An Online Hierarchical Energy Management System for Energy Communities,
  Complying with the Current Technical Legislation Framework",2024-01-22T15:29:54Z,"Antonino Capillo, Enrico De Santis, Fabio Massimo Frattale Mascioli, Antonello Rizzi","Efforts in the fight against Climate Change are increasingly oriented towards
new energy efficiency strategies in Smart Grids (SGs). In 2018, with proper
legislation, the European Union (EU) defined the Renewable Energy Community
(REC) as a local electrical grid whose participants share their self-produced
renewable energy, aiming at reducing bill costs by taking advantage of proper
incentives. That action aspires to accelerate the spread of local renewable
energy exploitation, whose costs could not be within everyone's reach. Since a
REC is technically an SG, the strategies above can be applied, and
specifically, practical Energy Management Systems (EMSs) are required.
Therefore, in this work, an online Hierarchical EMS (HEMS) is synthesized for
REC cost minimization to evaluate its superiority over a local self-consumption
approach. EU technical indications (as inherited from Italy) are diligently
followed, aiming for results that are as realistic as possible. Power flows
between REC nodes, or Microgrids (MGs) are optimized by taking Energy Storage
Systems (ESSs) and PV plant costs, energy purchase costs, and REC incentives. A
hybrid Fuzzy Inference System - Genetic Algorithm (FIS-GA) model is implemented
with the GA encoding the FIS parameters. Power generation and consumption,
which are the overall system input, are predicted by a LSTM trained on
historical data. The proposed hierarchical model achieves good precision in
short computation times and outperforms the self-consumption approach, leading
to about 20% savings compared to the latter. In addition, the Explainable AI
(XAI), which characterizes the model through the FIS, makes results more
reliable thanks to an excellent human interpretation level. To finish, the HEMS
is parametrized so that it is straightforward to switch to another Country's
technical legislation framework.",http://arxiv.org/abs/2402.01688v1
"Generative Nowcasting of Marine Fog Visibility in the Grand Banks area
  and Sable Island in Canada",2024-02-09T21:57:57Z,"Eren Gultepe, Sen Wang, Byron Blomquist, Harindra J. S. Fernando, O. Patrick Kreidl, David J. Delene, Ismail Gultepe","This study presents the application of generative deep learning techniques to
evaluate marine fog visibility nowcasting using the FATIMA (Fog and turbulence
interactions in the marine atmosphere) campaign observations collected during
July 2022 in the North Atlantic in the Grand Banks area and vicinity of Sable
Island (SI), northeast of Canada. The measurements were collected using the
Vaisala Forward Scatter Sensor model FD70 and Weather Transmitter model WXT50,
and Gill R3A ultrasonic anemometer mounted on the Research Vessel Atlantic
Condor. To perform nowcasting, the time series of fog visibility (Vis), wind
speed, dew point depression, and relative humidity with respect to water were
preprocessed to have lagged time step features. Generative nowcasting of Vis
time series for lead times of 30 and 60 minutes were performed using
conditional generative adversarial networks (cGAN) regression at visibility
thresholds of Vis < 1 km and < 10 km. Extreme gradient boosting (XGBoost) was
used as a baseline method for comparison against cGAN. At the 30 min lead time,
Vis was best predicted with cGAN at Vis < 1 km (RMSE = 0.151 km) and with
XGBoost at Vis < 10 km (RMSE = 2.821 km). At the 60 min lead time, Vis was best
predicted with XGBoost at Vis < 1 km (RMSE = 0.167 km) and Vis < 10 km (RMSE =
3.508 km), but the cGAN RMSE was similar to XGBoost. Despite nowcasting Vis at
30 min being quite difficult, the ability of the cGAN model to track the
variation in Vis at 1 km suggests that there is potential for generative
analysis of marine fog visibility using observational meteorological
parameters.",http://arxiv.org/abs/2402.06800v1
Examining the detectability of ringing on highly eccentric exoplanets,2024-03-01T23:23:44Z,"Mathijs Vanrespaille, Robin Baeyens, Aaron David Schneider, Ludmila Carone, Leen Decin","Eccentric exoplanets offer an opportunity to study the response of an
atmosphere to changing thermal forcing and the robustness of the super-rotating
equatorial jet seen on tidally locked hot Jupiters. However, the atmospheric
dynamics on eccentric planets strongly depend on the planetary rotation period,
which is difficult to constrain observationally. The ringing phenomenon,
whereby the observed emission increases and decreases after the periastron
passage as the flash-heated hemisphere rotates into and out of view, can
provide a tight constraint on rotation. We studied five highly eccentric
transiting exoplanets HAT-P-2 b, HD 80606 b, TOI-3362 b, TOI-4127 b and HD
17156 b to find which displays strong ringing signals that are sufficiently
strong for the James Webb Space Telescope (JWST) to detect. We implemented the
treatment of eccentricity and non-synchronous rotation in the non-grey climate
model expeRT/MITgcm and generated synthetic light curves. We find four
detectable ringing peaks on HD 80606 b and some undetectable ringing on
TOI-4127 b and HD 17156 b. The lack of clouds, photo-chemistry and obliquity in
our models may have led us to overestimate the amplitude of the ringing
however. The strength of the ringing signal is mostly determined by the
eccentricity, planetary rotation period, planet-to-star radius ratio and
apparent magnitude of the system. We searched for more exoplanets that could
show ringing but found no candidates as promising as HD 80606 b. We recommend
prioritising HD 80606 b as a target for ringing with JWST. A baseline of five
days after the periastron passage would capture three ringing peaks, which is
sufficient to tightly constrain the planetary rotation period. An extension to
seven days would add a fourth peak, which would allow us to verify the rotation
period.",http://arxiv.org/abs/2403.01026v2
"CO2 capture using boron, nitrogen, and phosphorus-doped C20 in the
  present electric field: A DFT study",2024-03-21T16:00:36Z,"Parham Rezaee, Shervin Alikhah Asl, Mohammad Hasan Javadi, Shahab Rezaee, Razieh Morad, Mahmood Akbari, Seyed Shahriar Arab, Malik Maaza","Burning fossil fuels emits a significant amount of CO2, causing climate
change concerns. CO2 Capture and Storage (CCS) aims to reduce emissions, with
fullerenes showing promise as CO2 adsorbents. Recent research focuses on
modifying fullerenes using an electric field. In light of this, we carried out
DFT studies on some B, N, and P doped C20 (C20-nXn (n = 0, 1, 2, and 3; X = B,
N, and P)) in the absence and presence of an electric field in the range of
0-0.02 a.u.. The cohesive energy was calculated to ensure their thermodynamic
stability showing, that despite having lesser cohesive energies than C20, they
appear in a favorable range. Moreover, the charge distribution for all
structures was depicted using the ESP map. Most importantly, we evaluated the
adsorption energy, height, and CO2 angle, demonstrating the B and N-doped
fullerenes had the stronger interaction with CO2, which by far exceeded C20's,
improving its physisorption to physicochemical adsorption. Although the
adsorption energy of P-doped fullerenes was not as satisfactory, in most cases,
increasing the electric field led to enhancing CO2 adsorption and incorporating
chemical attributes to CO2-fullerene interaction. The HOMO--LUMO plots were
obtained by which we discovered that unlike the P-doped C20, the surprising
activity of B and N-doped C20s against CO2 originates from a high concentration
of the HOMO-LUMO orbitals on B and N atoms. Additionally, the charge
distribution for all structures was depicted using the ESP map. In the present
article, we attempt to introduce more effective fullerene-based materials for
CO2 capture as well as strategies to enhance their efficiency and revealing
adsorption nature over B, N, and P-doped fullerenes.",http://arxiv.org/abs/2403.14507v1
"The Directionality of Gravitational and Thermal Diffusive Transport in
  Geologic Fluid Storage",2024-03-25T11:53:00Z,"Anna Herring, Ruotong Huang, Adrian Sheppard","Diffusive transport has implications for the long-term status of underground
storage of hydrogen (H$_2$) fuel and carbon dioxide (CO$_2$), technologies
which are being pursued to mitigate climate change and advance the energy
transition. Once injected underground, CO$_2$ and H$_2$ will exist in
multiphase fluid-water-rock systems: being partially-soluble, injected fluids
can flow through the porous rack in a connected plume, become disconnected and
trapped as ganglia surrounded by groundwater within the storage rock pore
space, and also dissolve and migrate through the aqueous phase. Recent analyses
have focused on the concentration gradients induced by differing capillary
pressure between fluid ganglia which can drive diffusive transport (""Ostwald
ripening""). However, studies have neglected or excessively simplified important
factors; namely: the non-ideality of gases under geologic conditions, the
opposing equilibrium state of dissolved CO$_2$ and H$_2$ driven by the partial
molar density of dissolved solutes, and entropic and thermodiffusive effects
resulting from geothermal gradients. We conduct an analysis from thermodynamic
first principles and use this to provide numerical estimates at conditions
relevant to underground storage reservoirs. We show that entropic contributions
to the free energy are so significant as to cause a reversal in the direction
of diffusive transport in systems with geothermal gradients. For CO$_2$, even
geothermal gradients less than 10 C/km induce downwards diffusion at depths
relevant to storage. Diffusive transport of H$_2$ is less affected, but still
reverses direction under typical gradients. Contrary to previous studies, we
find that in diffusion and convection will likely work in concert - both
driving CO$_2$ downwards, and both driving H$_2$ upwards - for conditions
representative of their respective storage reservoirs.",http://arxiv.org/abs/2403.16659v3
"Cost-benefit analysis of ecosystem modelling to support fisheries
  management",2024-03-26T07:24:28Z,"Matthew H. Holden, Eva E. Plag√°nyi, Elizabeth A. Fulton, Alexander B. Campbell, Rachel Janes, Robyn A. Lovett, Montana Wickens, Matthew P. Adams, Larissa Lubiana Botelho, Catherine M. Dichmont, Philip Erm, Kate J Helmstedt, Ryan F. Heneghan, Manuela Mendiolar, Anthony J. Richardson, Jacob G. D. Rogers, Kate Saunders, Liam Timms","Mathematical and statistical models underlie many of the world's most
important fisheries management decisions. Since the 19th century, difficulty
calibrating and fitting such models has been used to justify the selection of
simple, stationary, single-species models to aid tactical fisheries management
decisions. Whereas these justifications are reasonable, it is imperative that
we quantify the value of different levels of model complexity for supporting
fisheries management, especially given a changing climate, where old
methodologies may no longer perform as well as in the past. Here we argue that
cost-benefit analysis is an ideal lens to assess the value of model complexity
in fisheries management. While some studies have reported the benefits of model
complexity in fisheries, modeling costs are rarely considered. In the absence
of cost data in the literature, we report, as a starting point, relative costs
of single-species stock assessment and marine ecosystem models from two
Australian organizations. We found that costs varied by two orders of
magnitude, and that ecosystem model costs increased with model complexity.
Using these costs, we walk through a hypothetical example of cost-benefit
analysis. The demonstration is intended to catalyze the reporting of modeling
costs and benefits.",http://arxiv.org/abs/2403.17446v1
A Comprehensive Review of Coastal Compound Flooding Literature,2024-03-28T13:22:14Z,"Joshua Green, Ivan D. Haigh, Niall Quinn, Jeff Neal, Thomas Wahl, Melissa Wood, Dirk Eilander, Marleen de Ruiter, Philip Ward, Paula Camus","Compound flooding, where the combination or successive occurrence of two or
more flood drivers leads to an extreme impact, can greatly exacerbate the
adverse consequences associated with flooding in coastal regions. This paper
reviews the practices and trends in coastal compound flood research
methodologies and applications, as well as synthesizes key findings at regional
and global scales. Systematic review is employed to construct a literature
database of 271 studies relevant to compound flood hazards in a coastal
context. This review explores the types of compound flood events, their
mechanistic processes, and synthesizes the definitions and terms exhibited
throughout the literature. Considered in the review are six flood drivers
(fluvial, pluvial, coastal, groundwater, damming/dam failure, and tsunami) and
five precursor events and environmental conditions (soil moisture, snow,
temp/heat, fire, and drought). Furthermore, this review summarizes the trends
in research methodology, examines the wide range of study applications, and
considers the influences of climate change and urban environments. Finally,
this review highlights the knowledge gaps in compound flood research and
discusses the implications of review findings on future practices. Our five
recommendations for future compound flood research are to: 1) adopt consistent
definitions, terminology, and approaches; 2) expand the geographic coverage of
research; 3) pursue more inter-comparison projects; 4) develop modelling
frameworks that better couple dynamic earth systems; and 5) design urban and
coastal infrastructure with compound flooding in mind. We hope this review will
help to enhance understanding of compound flooding, guide areas for future
research focus, and close knowledge gaps.",http://arxiv.org/abs/2404.01321v1
"Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to
  Deep Learning Profiling Attacks",2024-04-05T08:27:36Z,"Ana-Maria Cretu, Miruna Rusu, Yves-Alexandre de Montjoye","Smart meters, devices measuring the electricity and gas consumption of a
household, are currently being deployed at a fast rate throughout the world.
The data they collect are extremely useful, including in the fight against
climate change. However, these data and the information that can be inferred
from them are highly sensitive. Re-pseudonymization, i.e., the frequent
replacement of random identifiers over time, is widely used to share smart
meter data while mitigating the risk of re-identification. We here show how, in
spite of re-pseudonymization, households' consumption records can be pieced
together with high accuracy in large-scale datasets. We propose the first deep
learning-based profiling attack against re-pseudonymized smart meter data. Our
attack combines neural network embeddings, which are used to extract features
from weekly consumption records and are tailored to the smart meter
identification task, with a nearest neighbor classifier. We evaluate six neural
networks architectures as the embedding model. Our results suggest that the
Transformer and CNN-LSTM architectures vastly outperform previous methods as
well as other architectures, successfully identifying the correct household
73.4% of the time among 5139 households based on electricity and gas
consumption records (54.5% for electricity only). We further show that the
features extracted by the embedding model maintain their effectiveness when
transferred to a set of users disjoint from the one used to train the model.
Finally, we extensively evaluate the robustness of our results. Taken together,
our results strongly suggest that even frequent re-pseudonymization strategies
can be reversed, strongly limiting their ability to prevent re-identification
in practice.",http://arxiv.org/abs/2404.03948v1
"Advancing Forest Fire Prevention: Deep Reinforcement Learning for
  Effective Firebreak Placement",2024-04-12T15:10:57Z,"Lucas Murray, Tatiana Castillo, Jaime Carrasco, Andr√©s Weintraub, Richard Weber, Isaac Mart√≠n de Diego, Jos√© Ram√≥n Gonz√°lez, Jordi Garc√≠a-Gonzalo","Over the past decades, the increase in both frequency and intensity of
large-scale wildfires due to climate change has emerged as a significant
natural threat. The pressing need to design resilient landscapes capable of
withstanding such disasters has become paramount, requiring the development of
advanced decision-support tools. Existing methodologies, including Mixed
Integer Programming, Stochastic Optimization, and Network Theory, have proven
effective but are hindered by computational demands, limiting their
applicability.
  In response to this challenge, we propose using artificial intelligence
techniques, specifically Deep Reinforcement Learning, to address the complex
problem of firebreak placement in the landscape. We employ value-function based
approaches like Deep Q-Learning, Double Deep Q-Learning, and Dueling Double
Deep Q-Learning. Utilizing the Cell2Fire fire spread simulator combined with
Convolutional Neural Networks, we have successfully implemented a computational
agent capable of learning firebreak locations within a forest environment,
achieving good results.
  Furthermore, we incorporate a pre-training loop, initially teaching our agent
to mimic a heuristic-based algorithm and observe that it consistently exceeds
the performance of these solutions. Our findings underscore the immense
potential of Deep Reinforcement Learning for operational research challenges,
especially in fire prevention. Our approach demonstrates convergence with
highly favorable results in problem instances as large as 40 x 40 cells,
marking a significant milestone in applying Reinforcement Learning to this
critical issue.
  To the best of our knowledge, this study represents a pioneering effort in
using Reinforcement Learning to address the aforementioned problem, offering
promising perspectives in fire prevention and landscape management",http://arxiv.org/abs/2404.08523v1
"INDUS: Effective and Efficient Language Models for Scientific
  Applications",2024-05-17T12:15:07Z,"Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, Iksha Gurung, Nishan Pantha, Rong Zhang, Bharath Dandala, Rahul Ramachandran, Manil Maskey, Kaylin Bugbee, Mike Little, Elizabeth Fancher, Irina Gerasimov, Armin Mehrabian, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grezes, Megan Ansdell, Alberto Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panagiotis Vagenas, S. Karthik Mukkavilli, Peter Staar, Sanaz Vahidinia, Ryan McGranaghan, Tsendgar Lee","Large language models (LLMs) trained on general domain corpora showed
remarkable results on natural language processing (NLP) tasks. However,
previous research demonstrated LLMs trained using domain-focused corpora
perform better on specialized tasks. Inspired by this insight, we developed
INDUS, a comprehensive suite of LLMs tailored for the closely-related domains
of Earth science, biology, physics, heliophysics, planetary sciences and
astrophysics, and trained using curated scientific corpora drawn from diverse
data sources. The suite of models include: (1) an encoder model trained using
domain-specific vocabulary and corpora to address NLP tasks, (2) a
contrastive-learning based text embedding model trained using a diverse set of
datasets to address information retrieval tasks and (3) smaller versions of
these models created using knowledge distillation for applications which have
latency or resource constraints. We also created three new scientific benchmark
datasets, CLIMATE-CHANGE NER (entity-recognition), NASA-QA (extractive QA) and
NASA-IR (IR) to accelerate research in these multi-disciplinary fields. We show
that our models outperform both general-purpose (RoBERTa) and domain-specific
(SCIBERT) encoders on these new tasks as well as existing tasks in the domains
of interest. Furthermore, we demonstrate the use of these models in two
industrial settings -- as a retrieval model for large-scale vector search
applications and in automatic content tagging systems.",http://arxiv.org/abs/2405.10725v3
Artificial Greenhouse Gases as Exoplanet Technosignatures,2024-05-18T02:37:23Z,"Edward W. Schwieterman, Thomas J. Fauchez, Jacob Haqq-Misra, Ravi K. Kopparapu, Daniel Angerhausen, Daria Pidhorodetska, Michaela Leung, Evan L. Sneed, Elsa Ducrot","Atmospheric pollutants such as CFCs and NO$_{2}$ have been proposed as
potential remotely detectable atmospheric technosignature gases. Here we
investigate the potential for artificial greenhouse gases including CF$_{4}$,
C$_{2}$F$_{6}$, C$_{3}$F$_{8}$, SF$_{6}$, and NF$_{3}$ to generate detectable
atmospheric signatures. In contrast to passive incidental byproducts of
industrial processes, artificial greenhouse gases would represent an
intentional effort to change the climate of a planet with long-lived, low
toxicity gases and would possess low false positive potential. An
extraterrestrial civilization may be motivated to undertake such an effort to
arrest a predicted snowball state on their home world or to terraform an
otherwise uninhabitable terrestrial planet within their system. Because
artificial greenhouse gases strongly absorb in the thermal mid-infrared window
of temperate atmospheres, a terraformed planet will logically possess strong
absorption features from these gases at mid-IR wavelengths ($\sim$8-12 $\mu$m),
possibly accompanied by diagnostic features in the near-IR. As a proof of
concept, we calculate the needed observation time to detect 1 [10](100) ppm of
C$_{2}$F$_{6}$/C$_{3}$F$_{8}$/SF$_{6}$ on TRAPPIST-1f with JWST MIRI/LRS and
NIRSpec. We find that a combination of 1[10](100) ppm each of C$_{2}$F$_{6}$,
C$_{3}$F$_{8}$, and SF$_{6}$ can be detected with an S/N $\geq$ 5 in as few as
25[10](5) transits with MIRI/LRS. We further explore mid-infrared
direct-imaging scenarios with the LIFE mission concept and find these gases are
more detectable than standard biosignatures at these concentrations.
Consequently, artificial greenhouse gases can be readily detected (or excluded)
during normal planetary characterization observations with no additional
overhead.",http://arxiv.org/abs/2405.11149v2
"Learning to connect in action: Measuring and understanding the emergence
  of boundary spanners in volatile times",2024-05-20T13:09:52Z,"Vittorio Nespeca, Tina Comes, Frances Brazier","Collective intelligence of diverse groups is key for tackling many of today's
grand challenges such as fostering resilience and climate adaptation.
Information exchange across such diverse groups is crucial for collective
intelligence, especially in volatile environments. To facilitate inter-group
information exchange, Informational Boundary Spanners (IBSs) as pivotal
information exchange 'hubs' are promising. However, the mechanisms that drive
the emergence of IBSs remain poorly understood. To address this gap there is
first a need for a method to identify and measure the emergence of IBSs.
Second, an Agent-Based Modelling (ABM) framework is not available to
systematically study mechanisms for the emergence of IBSs in volatile
environments. Third, even though the ability to learn who provides high-quality
information is thought to be essential to explain the emergence of IBSs, a
rigorous test of this mechanism is missing. The learning mechanism is
formalized using an ABM framework, with the model's outputs analyzed using the
proposed IBS emergence measurement method. To illustrate both the method and
the learning mechanism, we present a case study focused on information sharing
in the volatile environment of a disaster. The study shows that learning
constitutes a mechanism for the emergence of effective IBSs in (a)
low-volatility environments characterised by low uncertainty and (b) in
high-volatility environments characterised by rapid change if the number of
inter-group connections is sufficient. With the method and model, this paper
aims to lay the foundations for exploring mechanisms for the emergence of IBSs
that facilitate inter-group information exchange. This article advances
collective intelligence by providing the essential elements for measuring and
understanding the emergence of IBSs and exploring the effect of learning on
their emergence in volatile environments.",http://arxiv.org/abs/2405.11998v1
Automatic Counting and Classification of Mosquito Eggs in Field Traps,2024-05-31T07:48:48Z,"Javier Naranjo-Alcazar, Jordi Grau-Haro, Pedro Zuccarello, David Almenar, Jesus Lopez-Ballester","Insect pest control poses a global challenge, affecting public health, food
safety, and the environment. Diseases transmitted by mosquitoes are expanding
beyond tropical regions due to climate change. Agricultural pests further
exacerbate economic losses by damaging crops. The Sterile Insect Technique
(SIT) emerges as an eco-friendly alternative to chemical pesticides, involving
the sterilization and release of male insects to curb population growth. This
work focuses on the automation of the analysis of field ovitraps used to
follow-up a SIT program for the Aedes albopictus mosquito in the Valencian
Community, Spain, funded by the Conselleria de Agricultura, Agua, Ganaderia y
Pesca. Previous research has leveraged deep learning algorithms to automate egg
counting in ovitraps, yet faced challenges such as manual handling and limited
analysis capacity. Innovations in our study include classifying eggs as hatched
or unhatched and reconstructing ovitraps from partial images, mitigating issues
of duplicity and cut eggs. Also, our device can analyze multiple ovitraps
simultaneously without the need of manual replacement. This approach
significantly enhances the accuracy of egg counting and classification,
providing a valuable tool for large-scale field studies.
  This document describes part of the work of the project Application of
Industry 4.0 techniques to the production of tiger mosquitoes for the Sterile
Insect Technique (MoTIA2,IMDEEA/2022/70), financed by the Valencian Institute
for Business Competitiveness (IVACE) and the FEDER funds. The participation of
J.Naranjo-Alcazar, J.Grau-Haro and P.Zuccarello has been possible thanks to
funding from IVACE and FEDER funds. The participation of D.Almenar has been
financed by the Conselleria de Agricultura, Agua, Ganaderia y Pesca of the
Generalitat Valenciana and the Subdireccion de Innovacion y Desarrollo de
Servicios (TRAGSA group).",http://arxiv.org/abs/2405.20656v5
"Hybrid metapopulation agent-based epidemiological models for efficient
  insight on the individual scale: a contribution to green computing",2024-06-06T09:09:42Z,"Julia Bicker, Ren√© Schmieding, Michael Meyer-Hermann, Martin J. K√ºhn","Emerging infectious diseases and climate change are two of the major
challenges in 21st century. Although over the past decades, highly-resolved
mathematical models have contributed in understanding dynamics of infectious
diseases and are of great aid when it comes to finding suitable intervention
measures, they may need substantial computational effort and produce
significant CO2 emissions. Two popular modeling approaches for mitigating
infectious disease dynamics are agent-based and population-based models.
Agent-based models (ABMs) offer a microscopic view and are thus able to capture
heterogeneous human contact behavior and mobility patterns. However, insights
on individual-level dynamics come with high computational effort that scales
with the number of agents. On the other hand, population-based models using
e.g. ordinary differential equations (ODEs) are computationally efficient even
for large populations due to their complexity being independent of the
population size. Yet, population-based models are restricted in their
granularity as they assume a (to some extent) homogeneous and well-mixed
population. To manage the trade-off between computational complexity and level
of detail, we propose spatial- and temporal-hybrid models that use ABMs only in
an area or time frame of interest. To account for relevant influences to
disease dynamics, e.g., from outside, due to commuting activities, we use
population-based models, only adding moderate computational costs. Our
hybridization approach demonstrates significant reduction in computational
effort by up to 98% -- without losing the required depth in information in the
focus frame. Concluding, hybrid epidemiological models can provide insights on
the individual scale where necessary, using aggregated models where possible,
thereby making a contribution to green computing.",http://arxiv.org/abs/2406.04386v2
"Nonlinear Interactions of Planetary-Scale Waves in Mesospheric Winds
  Observed at 52¬∞N Latitude and Two Longitudes",2024-06-09T16:43:43Z,"Maosheng He, Jeffrey M. Forbes, Gunter Stober, Christoph Jacobi, Guozhu Li, Libo Liu, Jiyao Xu","Nine years of mesospheric wind data from two meteor radars at 52{\deg}N
latitude were analyzed to investigate planetary waves (PWs) and tides by
estimating their zonal wavenumber through longitudinal phase differences. Our
results reveal that PW normal modes (NMs) primarily drive multi-day
oscillations, showing seasonal variability and statistical associations with
Sudden Stratospheric Warming (SSW) events. Specifically, a significant 6-day NM
emerges in April, followed by predominant 4- and 2-day NMs until June, with
peaks of 2-, 4-, and 6-day NMs spanning July to October. Furthermore, our study
provides the first observational verification of frequency and zonal wavenumber
of over ten secondary waves from nonlinear interactions among planetary-scale
waves. One notable finding is the prevalence of non-migrating components in
winter 24-hour and summer 8-hour tides, attributed to these nonlinear
interactions. Our findings underscore the diverse nonlinear dynamics of
planetary-scale waves, triggering a variety of periodic oscillations.",http://arxiv.org/abs/2406.05848v2
"A Sociotechnical Lens for Evaluating Computer Vision Models: A Case
  Study on Detecting and Reasoning about Gender and Emotion",2024-06-12T13:52:30Z,"Sha Luo, Sang Jung Kim, Zening Duan, Kaiping Chen","In the evolving landscape of computer vision (CV) technologies, the automatic
detection and interpretation of gender and emotion in images is a critical area
of study. This paper investigates social biases in CV models, emphasizing the
limitations of traditional evaluation metrics such as precision, recall, and
accuracy. These metrics often fall short in capturing the complexities of
gender and emotion, which are fluid and culturally nuanced constructs. Our
study proposes a sociotechnical framework for evaluating CV models,
incorporating both technical performance measures and considerations of social
fairness. Using a dataset of 5,570 images related to vaccination and climate
change, we empirically compared the performance of various CV models, including
traditional models like DeepFace and FER, and generative models like GPT-4
Vision. Our analysis involved manually validating the gender and emotional
expressions in a subset of images to serve as benchmarks. Our findings reveal
that while GPT-4 Vision outperforms other models in technical accuracy for
gender classification, it exhibits discriminatory biases, particularly in
response to transgender and non-binary personas. Furthermore, the model's
emotion detection skew heavily towards positive emotions, with a notable bias
towards associating female images with happiness, especially when prompted by
male personas. These findings underscore the necessity of developing more
comprehensive evaluation criteria that address both validity and discriminatory
biases in CV models. Our proposed framework provides guidelines for researchers
to critically assess CV tools, ensuring their application in communication
research is both ethical and effective. The significant contribution of this
study lies in its emphasis on a sociotechnical approach, advocating for CV
technologies that support social good and mitigate biases rather than
perpetuate them.",http://arxiv.org/abs/2406.08222v2
"Evaluation of Deep Learning Semantic Segmentation for Land Cover Mapping
  on Multispectral, Hyperspectral and High Spatial Aerial Imagery",2024-06-20T11:40:12Z,"Ilham Adi Panuntun, Ying-Nong Chen, Ilham Jamaluddin, Thi Linh Chi Tran","In the rise of climate change, land cover mapping has become such an urgent
need in environmental monitoring. The accuracy of land cover classification has
gotten increasingly based on the improvement of remote sensing data. Land cover
classification using satellite imageries has been explored and become more
prevalent in recent years, but the methodologies remain some drawbacks of
subjective and time-consuming. Some deep learning techniques have been utilized
to overcome these limitations. However, most studies implemented just one image
type to evaluate algorithms for land cover mapping. Therefore, our study
conducted deep learning semantic segmentation in multispectral, hyperspectral,
and high spatial aerial image datasets for landcover mapping. This research
implemented a semantic segmentation method such as Unet, Linknet, FPN, and
PSPnet for categorizing vegetation, water, and others (i.e., soil and
impervious surface). The LinkNet model obtained high accuracy in IoU
(Intersection Over Union) at 0.92 in all datasets, which is comparable with
other mentioned techniques. In evaluation with different image types, the
multispectral images showed higher performance with the IoU, and F1-score are
0.993 and 0.997, respectively. Our outcome highlighted the efficiency and broad
applicability of LinkNet and multispectral image on land cover classification.
This research contributes to establishing an approach on landcover segmentation
via open source for long-term future application.",http://arxiv.org/abs/2406.14220v2
BioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity,2024-06-25T17:09:54Z,"Chih-Hsuan Yang, Benjamin Feuer, Zaki Jubery, Zi K. Deng, Andre Nakkab, Md Zahid Hasan, Shivani Chiranjeevi, Kelly Marshall, Nirmal Baishnab, Asheesh K Singh, Arti Singh, Soumik Sarkar, Nirav Merchant, Chinmay Hegde, Baskar Ganapathysubramanian","We introduce BioTrove, the largest publicly accessible dataset designed to
advance AI applications in biodiversity. Curated from the iNaturalist platform
and vetted to include only research-grade data, BioTrove contains 161.9 million
images, offering unprecedented scale and diversity from three primary kingdoms:
Animalia (""animals""), Fungi (""fungi""), and Plantae (""plants""), spanning
approximately 366.6K species. Each image is annotated with scientific names,
taxonomic hierarchies, and common names, providing rich metadata to support
accurate AI model development across diverse species and ecosystems.
  We demonstrate the value of BioTrove by releasing a suite of CLIP models
trained using a subset of 40 million captioned images, known as BioTrove-Train.
This subset focuses on seven categories within the dataset that are
underrepresented in standard image recognition models, selected for their
critical role in biodiversity and agriculture: Aves (""birds""), Arachnida
(""spiders/ticks/mites""), Insecta (""insects""), Plantae (""plants""), Fungi
(""fungi""), Mollusca (""snails""), and Reptilia (""snakes/lizards""). To support
rigorous assessment, we introduce several new benchmarks and report model
accuracy for zero-shot learning across life stages, rare species, confounding
species, and multiple taxonomic levels.
  We anticipate that BioTrove will spur the development of AI models capable of
supporting digital tools for pest control, crop monitoring, biodiversity
assessment, and environmental conservation. These advancements are crucial for
ensuring food security, preserving ecosystems, and mitigating the impacts of
climate change. BioTrove is publicly available, easily accessible, and ready
for immediate use.",http://arxiv.org/abs/2406.17720v2
"Fine-tuning of Geospatial Foundation Models for Aboveground Biomass
  Estimation",2024-06-28T12:54:10Z,"Michal Muszynski, Levente Klein, Ademir Ferreira da Silva, Anjani Prasad Atluri, Carlos Gomes, Daniela Szwarcman, Gurkanwar Singh, Kewen Gu, Maciel Zortea, Naomi Simumba, Paolo Fraccaro, Shraddha Singh, Steve Meliksetian, Campbell Watson, Daiki Kimura, Harini Srinivasan","Global vegetation structure mapping is critical for understanding the global
carbon cycle and maximizing the efficacy of nature-based carbon sequestration
initiatives. Moreover, vegetation structure mapping can help reduce the impacts
of climate change by, for example, guiding actions to improve water security,
increase biodiversity and reduce flood risk. Global satellite measurements
provide an important set of observations for monitoring and managing
deforestation and degradation of existing forests, natural forest regeneration,
reforestation, biodiversity restoration, and the implementation of sustainable
agricultural practices. In this paper, we explore the effectiveness of
fine-tuning of a geospatial foundation model to estimate above-ground biomass
(AGB) using space-borne data collected across different eco-regions in Brazil.
The fine-tuned model architecture consisted of a Swin-B transformer as the
encoder (i.e., backbone) and a single convolutional layer for the decoder head.
All results were compared to a U-Net which was trained as the baseline model
Experimental results of this sparse-label prediction task demonstrate that the
fine-tuned geospatial foundation model with a frozen encoder has comparable
performance to a U-Net trained from scratch. This is despite the fine-tuned
model having 13 times less parameters requiring optimization, which saves both
time and compute resources. Further, we explore the transfer-learning
capabilities of the geospatial foundation models by fine-tuning on satellite
imagery with sparse labels from different eco-regions in Brazil.",http://arxiv.org/abs/2406.19888v1
"Asymmetries in the simulated ozone distribution on TRAPPIST-1e due to
  orography",2024-07-02T17:21:00Z,"Anand Bhongade, Daniel R Marsh, Felix Sainsbury-Martinez, Gregory J Cooke","TRAPPIST-1e is a tidally locked rocky exoplanet orbiting the habitable zone
of an M dwarf star. Upcoming observations are expected to reveal new rocky
exoplanets and their atmospheres around M dwarf stars. To interpret these
future observations we need to model the atmospheres of such exoplanets. We
configured CESM2-WACCM6, a chemistry climate model, for the orbit and stellar
irradiance of TRAPPIST-1e assuming an initial Earth-like atmospheric
composition. Our aim is to characterize the possible ozone (O$_3$) distribution
and explore how this is influenced by the atmospheric circulation shaped by
orography, using the Helmholtz wind decomposition and meridional mass
streamfunction. The model included Earth-like orography and the substellar
point was located over the Pacific Ocean. For such a scenario, our analysis
reveals a North-South asymmetry in the simulated O$_3$ distribution. The O$_3$
concentration is highest at pressures $>$ 10 hPa (below $\sim$30 km) near the
South Pole. This asymmetry arises from the higher landmass fraction in the
Northern Hemisphere, which causes drag in near-surface flows and leads to an
asymmetric meridional overturning circulation. Catalytic species were roughly
symmetrically distributed and were not found to be primary driver for the O$_3$
asymmetry. The total ozone column (TOC) density was higher for TRAPPIST-1e
compared to Earth, with 8000 Dobson Units (DU) near the South Pole and 2000 DU
near the North Pole. The results emphasise the sensitivity of O$_3$ to model
parameters, illustrating how incorporating Earth-like orography can affect
atmospheric dynamics and O$_3$ distribution. This link between surface features
and atmospheric dynamics underlines the importance of how changing model
parameters used to study exoplanet atmospheres can influence the interpretation
of observations.",http://arxiv.org/abs/2407.02444v2
A Comprehensive Study of Disaster Support Mobile Apps,2024-07-11T02:58:12Z,"Muhamad Syukron, Anuradha Madugalla, Mojtaba Shahin, John Grundy","Context: Disasters are a common global occurrence with climate change leading
to increase both their frequency and intensity. To reduce the impact of these
disasters on lives and livelihoods it is important to provide accurate warnings
and information about recovery and mitigation. Today most emergency management
agencies deliver this information via mobile apps.
  Objective: There is a large collection of disaster mobile apps available
across the globe. But a detailed study is not yet conducted on these apps and
their reviews to understand their key features and user feedback. In this paper
we present a comprehensive analysis to address this research gap.
  Method: We conducted a detailed analysis of 45 disaster apps and 28,161
reviews on these apps. We manually analysed the features of these 45 apps and
for review analysis employed topic modelling and sentiment analysis techniques.
  Results: We identified 13 key features in these apps and categorised them in
to the 4 stages of disaster life cycle. Our analysis revealed 22 topics with
highest discussions being on apps alert functionality, app satisfaction and use
of maps. Sentiment analysis of reviews showed that while 22\% of users provided
positive feedback, 9.5\% were negative and 6.8\% were neutral. It also showed
that signup/signin issues, network issues and app configuration issues were the
most frustrating to users. These impacted user safety as these prevented them
from accessing the app when it mattered most.
  Conclusions: We provide a set of practical recommendations for future
disaster app developers. Our findings will help emergency agencies develop
better disaster apps by ensuring key features are supported in their apps, by
understanding commonly discussed user issues. This will help to improve the
disaster app eco-system and lead to more user friendly and supportive disaster
support apps in the future.",http://arxiv.org/abs/2407.08145v1
"Low latency carbon budget analysis reveals a large decline of the land
  carbon sink in 2023",2024-07-17T09:54:53Z,"Piyu Ke, Philippe Ciais, Stephen Sitch, Wei Li, Ana Bastos, Zhu Liu, Yidi Xu, Xiaofan Gui, Jiang Bian, Daniel S Goll, Yi Xi, Wanjing Li, Michael O'Sullivan, Jeffeson Goncalves de Souza, Pierre Friedlingstein, Frederic Chevallier","In 2023, the CO2 growth rate was 3.37 +/- 0.11 ppm at Mauna Loa, 86% above
the previous year, and hitting a record high since observations began in 1958,
while global fossil fuel CO2 emissions only increased by 0.6 +/- 0.5%. This
implies an unprecedented weakening of land and ocean sinks, and raises the
question of where and why this reduction happened. Here we show a global net
land CO2 sink of 0.44 +/- 0.21 GtC yr-1, the weakest since 2003. We used
dynamic global vegetation models, satellites fire emissions, an atmospheric
inversion based on OCO-2 measurements, and emulators of ocean biogeochemical
and data driven models to deliver a fast-track carbon budget in 2023. Those
models ensured consistency with previous carbon budgets. Regional flux
anomalies from 2015-2022 are consistent between top-down and bottom-up
approaches, with the largest abnormal carbon loss in the Amazon during the
drought in the second half of 2023 (0.31 +/- 0.19 GtC yr-1), extreme fire
emissions of 0.58 +/- 0.10 GtC yr-1 in Canada and a loss in South-East Asia
(0.13 +/- 0.12 GtC yr-1). Since 2015, land CO2 uptake north of 20 degree N
declined by half to 1.13 +/- 0.24 GtC yr-1 in 2023. Meanwhile, the tropics
recovered from the 2015-16 El Nino carbon loss, gained carbon during the La
Nina years (2020-2023), then switched to a carbon loss during the 2023 El Nino
(0.56 +/- 0.23 GtC yr-1). The ocean sink was stronger than normal in the
equatorial eastern Pacific due to reduced upwelling from La Nina's retreat in
early 2023 and the development of El Nino later. Land regions exposed to
extreme heat in 2023 contributed a gross carbon loss of 1.73 GtC yr-1,
indicating that record warming in 2023 had a strong negative impact on the
capacity of terrestrial ecosystems to mitigate climate change.",http://arxiv.org/abs/2407.12447v1
"Enhancing Worldwide Image Geolocation by Ensembling Satellite-Based
  Ground-Level Attribute Predictors",2024-07-18T19:15:52Z,"Michael J. Bianco, David Eigen, Michael Gormish","We examine the challenge of estimating the location of a single ground-level
image in the absence of GPS or other location metadata. Currently, geolocation
systems are evaluated by measuring the Great Circle Distance between the
predicted location and ground truth. Because this measurement only uses a
single point, it cannot assess the distribution of predictions by geolocation
systems. Evaluation of a distribution of potential locations (areas) is
required when there are follow-on procedures to further narrow down or verify
the location. This is especially important in poorly-sampled regions e.g. rural
and wilderness areas.
  In this paper, we introduce a novel metric, Recall vs Area (RvA), which
measures the accuracy of estimated distributions of locations. RvA treats image
geolocation results similarly to document retrieval, measuring recall as a
function of area: For a ranked list of (possibly discontiguous) predicted
regions, we measure the area required for accumulated regions to contain the
ground truth coordinate. This produces a curve similar to a precision-recall
curve, where ""precision"" is replaced by square kilometers area, enabling
evaluation for different downstream search area budgets.
  Following from this view of the problem, we then examine an ensembling
approach to global-scale image geolocation, which incorporates information from
multiple sources, and can readily incorporate multiple models, attribute
predictors, and data sources. We study its effectiveness by combining the
geolocation models GeoEstimation and the current state-of-the-art, GeoCLIP,
with attribute predictors based on Oak Ridge National Laboratory LandScan and
European Space Agency Climate Change Initiative Land Cover. We find significant
improvements in image geolocation for areas that are under-represented in the
training set, particularly non-urban areas, on both Im2GPS3k and Street View
images.",http://arxiv.org/abs/2407.13862v2
"Reporting and Analysing the Environmental Impact of Language Models on
  the Example of Commonsense Question Answering with External Knowledge",2024-07-24T16:16:16Z,"Aida Usmanova, Junbo Huang, Debayan Banerjee, Ricardo Usbeck","Human-produced emissions are growing at an alarming rate, causing already
observable changes in the climate and environment in general. Each year global
carbon dioxide emissions hit a new record, and it is reported that 0.5% of
total US greenhouse gas emissions are attributed to data centres as of 2021.
The release of ChatGPT in late 2022 sparked social interest in Large Language
Models (LLMs), the new generation of Language Models with a large number of
parameters and trained on massive amounts of data. Currently, numerous
companies are releasing products featuring various LLMs, with many more models
in development and awaiting release. Deep Learning research is a competitive
field, with only models that reach top performance attracting attention and
being utilized. Hence, achieving better accuracy and results is often the first
priority, while the model's efficiency and the environmental impact of the
study are neglected. However, LLMs demand substantial computational resources
and are very costly to train, both financially and environmentally. It becomes
essential to raise awareness and promote conscious decisions about algorithmic
and hardware choices. Providing information on training time, the approximate
carbon dioxide emissions and power consumption would assist future studies in
making necessary adjustments and determining the compatibility of available
computational resources with model requirements. In this study, we infused T5
LLM with external knowledge and fine-tuned the model for Question-Answering
task. Furthermore, we calculated and reported the approximate environmental
impact for both steps. The findings demonstrate that the smaller models may not
always be sustainable options, and increased training does not always imply
better performance. The most optimal outcome is achieved by carefully
considering both performance and efficiency factors.",http://arxiv.org/abs/2408.01453v1
"A Framework for Assessing Cumulative Exposure to Extreme Temperatures
  During Transit Trip",2024-08-07T20:47:39Z,"Huiying Fan, Hongyu Lu, Geyu Lyu, Angshuman Guin, Randall Guensler","The combined influence of urban heat islands, climate change, and extreme
temperature events are increasingly impacting transit travelers, especially
vulnerable populations such as older adults, people with disabilities, and
those with chronic diseases. Previous studies have generally attempted to
address this issue at either the micro- or macro-level, but each approach
presents different limitations in modeling the impacts on transit trips. Other
research proposes a meso-level approach to address some of these gaps, but the
use of additive exposure calculation and spatial shortest path routing poses
constraints meso-modeling accuracy. This study introduces HeatPath Analyzer, a
framework to assess the exposure of transit riders to extreme temperatures,
using TransitSim 4.0 to generate second-by-second spatio-temporal trip
trajectories, the traveler activity profiles, and thermal comfort levels along
the entire journey. The approach uses heat stress combines the standards
proposed by the NWS and CDC to estimate cumulative exposure for transit riders,
with specific parameters tailored to the elderly and people with disabilities.
The framework assesses the influence of extreme heat and winter chill. A case
study in Atlanta, GA, reveals that 10.2% of trips on an average summer weekday
in 2019 were at risk of extreme heat. The results uncover exposure disparities
across different transit trip mode segments, and across mitigation-based and
adaptation-based strategies. While the mitigation-based strategy highlights
high-exposure segments such as long ingress and egress, adaptation should be
prioritized toward the middle or second half of the trip when a traveler is
waiting for transit or transferring between routes. A comparison between the
traditional additive approach and the dynamic approach presented also shows
significant disparities, which, if overlooked, can mislead policy decisions.",http://arxiv.org/abs/2408.04081v1
"Experimental Investigation of Tidally-Forced Internal Wave Turbulence at
  High Reynolds Number",2024-08-10T16:42:56Z,"Zachary Taebel, Alberto Scotti, Pierre-Yves Passaggia, Dylan Bruney","Through basin-scale circulations, the ocean regulates global distributions of
heat, nutrients, and greenhouse gases. To properly predict the future of the
ocean under climate change, we need to develop a thorough understanding of the
underlying mechanisms that drive global circulations. An estimated 2 TW of
power is required to support interior mixing. Roughly half of this power is
believed to come from tidal flow over topography, producing internal gravity
waves (IGW's), which can radiate energy throughout the ocean interior. But it
is difficult to track the subsequent journey from tidal injection to
dissipation, as the energy cascade spans an enormous range of spatio-temporal
scales and multiple different nonlinear transfer mechanisms. To investigate the
full energy pathway from topographic forcing to irreversible mixing, we built a
model ocean in a large-scale laboratory wavetank (9 m x 2.9 m x 0.75 m)
allowing Reynolds numbers up to O(10$^5$). We replicate the tidal forcing by
oscillating an idealized ocean ridge. We track energy transfer across the first
cascade, driven by wave turbulence, using Background Oriented Schlieren (BOS)
over the full tank. Through the BOS we observe the formation of various sets of
subharmonics, driven by Triadic Resonant Instabilities (TRI). At later times,
the subharmonics born from TRI engage in different interactions, which
ultimately develop a continuum of waves at frequencies up to $N$. We validate
the three-wave resonant conditions through a Fourier decomposition and confirm
a backward cascade in frequency but a forward cascade in vertical wavenumber.
Through our spatial analysis, we identify relevant three-wave interactions and
show the significance of elastic scattering, a nonlocal interaction, in our
fully evolved system. We note however that the majority of our triads are
local, which have been historically overlooked.",http://arxiv.org/abs/2408.05593v1
"Mitigating Metropolitan Carbon Emissions with Dynamic Eco-driving at
  Scale",2024-08-10T18:23:59Z,"Vindula Jayawardana, Baptiste Freydt, Ao Qu, Cameron Hickert, Edgar Sanchez, Catherine Tang, Mark Taylor, Blaine Leonard, Cathy Wu","The sheer scale and diversity of transportation make it a formidable sector
to decarbonize. Here, we consider an emerging opportunity to reduce carbon
emissions: the growing adoption of semi-autonomous vehicles, which can be
programmed to mitigate stop-and-go traffic through intelligent speed commands
and, thus, reduce emissions. But would such dynamic eco-driving move the needle
on climate change? A comprehensive impact analysis has been out of reach due to
the vast array of traffic scenarios and the complexity of vehicle emissions. We
address this challenge with large-scale scenario modeling efforts and by using
multi-task deep reinforcement learning with a carefully designed network
decomposition strategy. We perform an in-depth prospective impact assessment of
dynamic eco-driving at 6,011 signalized intersections across three major US
metropolitan cities, simulating a million traffic scenarios. Overall, we find
that vehicle trajectories optimized for emissions can cut city-wide
intersection carbon emissions by 11-22%, without harming throughput or safety,
and with reasonable assumptions, equivalent to the national emissions of Israel
and Nigeria, respectively. We find that 10% eco-driving adoption yields 25%-50%
of the total reduction, and nearly 70% of the benefits come from 20% of
intersections, suggesting near-term implementation pathways. However, the
composition of this high-impact subset of intersections varies considerably
across different adoption levels, with minimal overlap, calling for careful
strategic planning for eco-driving deployments. Moreover, the impact of
eco-driving, when considered jointly with projections of vehicle
electrification and hybrid vehicle adoption remains significant. More broadly,
this work paves the way for large-scale analysis of traffic externalities, such
as time, safety, and air quality, and the potential impact of solution
strategies.",http://arxiv.org/abs/2408.05609v1
"Stochastic diffusion using mean-field limits to approximate master
  equations",2024-08-14T18:07:21Z,"Laurent H√©bert-Dufresne, Matthew M. Kling, Samuel F. Rosenblatt, Stephanie N. Miller, P. Alexander Burnham, Nicholas W. Landry, Nicholas J. Gotelli, Brian J. McGill","Stochastic diffusion is the noisy and uncertain process through which
dynamics like epidemics, or agents like animal species, disperse over a larger
area. Understanding these processes is becoming increasingly important as we
attempt to better prepare for potential pandemics and as species ranges shift
in response to climate change. Unfortunately, modeling of stochastic diffusion
is mostly done through inaccurate deterministic tools that fail to capture the
random nature of dispersal or else through expensive computational simulations.
In particular, standard tools fail to fully capture the heterogeneity of the
area over which this diffusion occurs. Rural areas with low population density
require different epidemic models than urban areas; likewise, the edges of a
species range require us to explicitly track low integer numbers of individuals
rather than vague averages. In this work, we introduce a series of new tools
called ""mean-FLAME"" models that track stochastic dispersion using approximate
master equations that explicitly follow the probability distribution of an area
of interest over all of its possible states, up to states that are active
enough to be approximated using a mean-field model. In one limit, this approach
is locally exact if we explicitly track enough states, and in the other limit
collapses back to traditional deterministic models if we track no state
explicitly. Applying this approach, we show how deterministic tools fail to
capture the uncertainty around the speed of nonlinear dynamical processes. This
is especially true for marginal areas that are close to unsuitable for
diffusion, like the edge of a species range or epidemics in small populations.
Capturing the uncertainty in such areas is key to producing accurate forecasts
and guiding potential interventions.",http://arxiv.org/abs/2408.07755v1
"Transdisciplinary collaborations for advancing sustainable and resilient
  agricultural systems",2024-09-18T22:09:06Z,"Vesna Bacheva, Imani Madison, Mathew Baldwin, Mark Beilstein, Douglas F. Call, Jessica A. Deaver, Kirill Efimenko, Jan Genzer, Khara Grieger, April Z. Gu, Mehmet Mert Ilman, Jen Liu, Sijin Li, Brooke K. Mayer, Anand Kumar Mishra, Juan Claudio Nino, Gloire Rubambiza, Phoebe Sengers, Robert Shepherd, Jesse Woodson, Hakim Weatherspoon, Margaret Frank, Jacob Jones, Rosangela Sozzani, Abraham Stroock","Feeding the growing human population sustainably amidst climate change is one
of the most important challenges in the 21st century. Current practices often
lead to the overuse of agronomic inputs, such as synthetic fertilizers and
water, resulting in environmental contamination and diminishing returns on crop
productivity. The complexity of agricultural systems, involving
plant-environment interactions and human management, presents significant
scientific and technical challenges for developing sustainable practices.
Addressing these challenges necessitates transdisciplinary research, involving
intense collaboration among fields such as plant science, engineering, computer
science, and social sciences. Here, we present five case studies from two
research centers demonstrating successful transdisciplinary approaches toward
more sustainable water and fertilizer use. These case studies span multiple
scales. Starting from whole-plant signaling, we explore how reporter plants can
transform our understanding of plant communication and enable efficient
application of water and fertilizers. We then show how new fertilizer
technologies could increase the availability of phosphorus in the soil. To
accelerate advancements in breeding new cultivars, we discuss robotic
technologies for high-throughput plant screening in different environments at a
population scale. At the ecosystem scale, we investigate phosphorus recovery
from aquatic systems and methods to minimize phosphorus leaching. Finally, as
agricultural outputs affect all people, we show how to integrate stakeholder
perspectives and needs into the research. With these case studies, we hope to
encourage the scientific community to adopt transdisciplinary research and
promote cross-training among biologists, engineers, and social scientists to
drive discovery and innovation in advancing sustainable agricultural systems.",http://arxiv.org/abs/2409.12337v1
A random planting model,2024-09-24T19:53:15Z,"Julian Talbot, Pascal Viot, David Colliaux","The adoption of agroecological practices will be crucial to address the
challenges of climate change and biodiversity loss. Such practices favor the
cultivation of plants in complex mixtures with layouts differing from the
monoculture approach of conventional agriculture. Inspired by random sequential
adsorption processes, we propose a one-dimensional model in which the plants
are represented as line segments that start as points and grow at a constant
rate until they reach length $\sigma$ after a time interval $\tau$. The
planting positions and times are randomly chosen with the constraint that plant
overlap is forbidden. We apply an exact, event-driven simulation to investigate
the resulting spatiotemporal patterns and yields in both mono- and duocultures.
After a transient period, with oscillations in the density and coverage, the
field reaches a steady state in which the mean age of plants is one half of the
time to maturity. The structure of the active plants is characterized by
correlation functions between the fluctuation of the age of a plant and its
$k$th neighbour. Nearest neighbours are negatively correlated, while next
nearest neighbours tend to have similar ages. The steady state yield increases
with the planting rate and approaches a maximum value of 4/3 plants per unit
length per unit time. For two species with the same size at maturity but
different growth rates, the more slowly growing species is enriched in the
harvest compared to the seed mix composition. If two species have the same time
to maturity but different sizes, the smaller one is enriched in the harvest
and, at a sufficiently high planting rate, the larger species may be completely
absent. For two species with the same ratio of $\sigma/\tau$ the selectivity is
insensitive to the planting rate. The model may be extended to higher
dimensions, more species and other planting strategies.",http://arxiv.org/abs/2409.16432v1
"Tracing the nonlinear formation of an interfacial wave spectral cascade
  from one to few to many",2024-10-11T14:17:23Z,"Sean M. D. Gregory, Silvia Schiattarella, Vitor S. Barroso, David I. Kaiser, Anastasios Avgoustidis, Silke Weinfurtner","Far-from-equilibrium phenomena unveil the intricate principles of complex
systems, including snowflake growth and fluid turbulence, with broad
applications ranging from foreign exchange trading to climate modeling. A
recurring feature across these systems is the emergence of a spectral cascade,
where energy is transferred across the system's length scales, following a
simple power law. The statistical theory of weak wave turbulence, in which only
leading order interactions are considered, successfully predicts scaling laws
for stationary states in idealised scenarios. Realistic conditions, such as
finite size and amplitude effects, and strong dissipation, remain beyond our
current understanding. Lacking comprehensive theoretical insight, we
experimentally trace the formation of a spectral cascade under these
conditions. Using an externally driven fluid-fluid interface, we successfully
resolve individual wave modes and track their real-time evolution from one to
few to many. This process culminates in a steady state whose power spectral
density is fully characterised by a power-law scaling. We further quantify
specific interactions through statistical correlations to reveal a hierarchy in
the wave-mixing order, thus confirming a key assumption of weak-wave
turbulence. We present a comprehensive time-evolution analysis that is crucial
in identifying critical points where the interface undergoes significant
changes. Our findings validate that the interfacial dynamics can be effectively
modelled using a weakly nonlinear Lagrangian theory, enabling us to explore its
applicability to other out-of-equilibrium systems. Notably, we uncover
intriguing connections to reheating scenarios following cosmic inflation in the
early universe.",http://arxiv.org/abs/2410.08842v1
"Spectroscopically resolved partial phase curve of the rapid heating and
  cooling of the highly-eccentric Hot Jupiter HAT-P-2b with WFC3",2024-10-14T16:00:16Z,"Bob Jacobs, Jean-Michel D√©sert, Nikole Lewis, Ryan C. Challener, L. C. Mayorga, Zo√´ de Beurs, Vivien Parmentier, Kevin B. Stevenson, Julien de Wit, Saugata Barat, Jonathan Fortney, Tiffany Kataria, Michael Line","The extreme environments of transiting close-in exoplanets in
highly-eccentric orbits are ideal for testing exo-climate physics.
Spectroscopically resolved phase curves not only allow for the characterization
of their thermal response to irradiation changes but also unveil
phase-dependent atmospheric chemistry and dynamics. We observed a partial phase
curve of the highly-eccentric close-in giant planet HAT-P-2b
($e=0.51,M=9M_{\rm{Jup}}$) with the Wide Field Camera 3 aboard the Hubble Space
Telescope. Using these data, we updated the planet's orbital parameters and
radius, and retrieved high-frequency pulsations consistent with the
planet-induced pulsations reported in Spitzer data. We found that the peak in
planetary flux occurred at $6.7\pm0.6$ hr after periastron, with a heating and
cooling timescales of $9.0^{+3.5}_{-2.1}$ hr, and $3.6^{+0.7}_{-0.6}$ hr,
respectively. We compare the light-curve to various 1-dimensional and
3-dimensional forward models, varying the planet's chemical composition. The
strong contrast in flux increase and decrease timescales before and after
periapse indicates an opacity term that emerges during the planet's heating
phase, potentially due to more H$^{-}$ than expected from chemical equilibrium
models. The phase-resolved spectra are largely featureless, that we interpret
as indicative an inhomogeneous dayside. However, we identified an anomalously
high flux in the spectroscopic bin coinciding with the hydrogen Paschen $\beta$
line and that is likely connected to the planet's orbit. We interpret this as
due to shock heating of the upper atmosphere given the short timescale
involved, or evidence for other star-planet interactions.",http://arxiv.org/abs/2410.11643v1
"Wavelet analysis of possible association between sunspot number and
  rainfall over Kerala, India: A case study",2024-11-14T07:00:38Z,"Elizabeth Thomas, S. Vineeth, Noble P. Abraham","Global attention has been focused on extreme climatic changes. This paper
investigates the relationship between different phases of solar activity and
extreme precipitation events in Kerala, India. Sunspot number and rainfall data
were analysed over 122 years (1901-2022) on an annual scale. A negative
correlation was observed in the winter and post-monsoon seasons, while positive
correlations were seen in the pre-monsoon and monsoon seasons, all of which
were statistically significant. Using cross-wavelet transform, the temporal
relationship between sunspot number and rainfall values was investigated,
revealing significant cross-power at an 8-12 year scale across all seasons.
Wavelet coherence between the two data sets demonstrated significant
correlation at the 2-4 and 4-8 year scales throughout the four seasons. The
results show that the seasonal rainfall over Kerala is related to solar
activity. The solar phases of Solar Cycles 14-24 were determined for all
seasons, and the years with excessive and insufficient rainfall were
identified. It was observed that the descending phase had an impact on excess
rainfall events during the winter and pre-monsoon seasons, while the ascending
phase notably affected the monsoon and post-monsoon seasons. The study
specifically examined the different magnetic polarities of sunspots in
alternating solar cycles, focusing on even and odd cycles. It was found that
extreme rainfall events were more frequent during the winter and pre-monsoon
seasons in the even cycles, whereas in the odd cycles, they were more prevalent
during the monsoon and post-monsoon seasons. These findings are presented for
the first time and may offer new perspectives on how different phases affect
rainfall. This study suggests a physical link between solar activity and
extreme precipitation in Kerala, which could increase predictability.",http://arxiv.org/abs/2411.09234v1
"Open Catalyst Experiments 2024 (OCx24): Bridging Experiments and
  Computational Models",2024-11-18T17:57:59Z,"Jehad Abed, Jiheon Kim, Muhammed Shuaibi, Brook Wander, Boris Duijf, Suhas Mahesh, Hyeonseok Lee, Vahe Gharakhanyan, Sjoerd Hoogland, Erdem Irtem, Janice Lan, Niels Schouten, Anagha Usha Vijayakumar, Jason Hattrick-Simpers, John R. Kitchin, Zachary W. Ulissi, Aaike van Vugt, Edward H. Sargent, David Sinton, C. Lawrence Zitnick","The search for low-cost, durable, and effective catalysts is essential for
green hydrogen production and carbon dioxide upcycling to help in the
mitigation of climate change. Discovery of new catalysts is currently limited
by the gap between what AI-accelerated computational models predict and what
experimental studies produce. To make progress, large and diverse experimental
datasets are needed that are reproducible and tested at industrially-relevant
conditions. We address these needs by utilizing a comprehensive high-throughput
characterization and experimental pipeline to create the Open Catalyst
Experiments 2024 (OCX24) dataset. The dataset contains 572 samples synthesized
using both wet and dry methods with X-ray fluorescence and X-ray diffraction
characterization. We prepared 441 gas diffusion electrodes, including
replicates, and evaluated them using zero-gap electrolysis for carbon dioxide
reduction (CO$_2$RR) and hydrogen evolution reactions (HER) at current
densities up to $300$ mA/cm$^2$. To find correlations with experimental
outcomes and to perform computational screens, DFT-verified adsorption energies
for six adsorbates were calculated on $\sim$20,000 inorganic materials
requiring 685 million AI-accelerated relaxations. Remarkably from this large
set of materials, a data driven Sabatier volcano independently identified Pt as
being a top candidate for HER without having any experimental measurements on
Pt or Pt-alloy samples. We anticipate the availability of experimental data
generated specifically for AI training, such as OCX24, will significantly
improve the utility of computational models in selecting materials for
experimental screening.",http://arxiv.org/abs/2411.11783v1
Quickest Change Detection with Confusing Change,2024-05-01T20:10:06Z,"Yu-Zhen Janice Chen, Jinhang Zuo, Venugopal V. Veeravalli, Don Towsley","In the problem of quickest change detection (QCD), a change occurs at some
unknown time in the distribution of a sequence of independent observations.
This work studies a QCD problem where the change is either a bad change, which
we aim to detect, or a confusing change, which is not of our interest. Our
objective is to detect a bad change as quickly as possible while avoiding
raising a false alarm for pre-change or a confusing change. We identify a
specific set of pre-change, bad change, and confusing change distributions that
pose challenges beyond the capabilities of standard Cumulative Sum (CuSum)
procedures. Proposing novel CuSum-based detection procedures, S-CuSum and
J-CuSum, leveraging two CuSum statistics, we offer solutions applicable across
all kinds of pre-change, bad change, and confusing change distributions. For
both S-CuSum and J-CuSum, we provide analytical performance guarantees and
validate them by numerical results. Furthermore, both procedures are
computationally efficient as they only require simple recursive updates.",http://arxiv.org/abs/2405.00842v1
"Change-Agent: Towards Interactive Comprehensive Remote Sensing Change
  Interpretation and Analysis",2024-03-28T17:55:42Z,"Chenyang Liu, Keyan Chen, Haotian Zhang, Zipeng Qi, Zhengxia Zou, Zhenwei Shi","Monitoring changes in the Earth's surface is crucial for understanding
natural processes and human impacts, necessitating precise and comprehensive
interpretation methodologies. Remote sensing satellite imagery offers a unique
perspective for monitoring these changes, leading to the emergence of remote
sensing image change interpretation (RSICI) as a significant research focus.
Current RSICI technology encompasses change detection and change captioning,
each with its limitations in providing comprehensive interpretation. To address
this, we propose an interactive Change-Agent, which can follow user
instructions to achieve comprehensive change interpretation and insightful
analysis, such as change detection and change captioning, change object
counting, change cause analysis, etc. The Change-Agent integrates a multi-level
change interpretation (MCI) model as the eyes and a large language model (LLM)
as the brain. The MCI model contains two branches of pixel-level change
detection and semantic-level change captioning, in which the BI-temporal
Iterative Interaction (BI3) layer is proposed to enhance the model's
discriminative feature representation capabilities. To support the training of
the MCI model, we build the LEVIR-MCI dataset with a large number of change
masks and captions of changes. Experiments demonstrate the SOTA performance of
the MCI model in achieving both change detection and change description
simultaneously, and highlight the promising application value of our
Change-Agent in facilitating comprehensive interpretation of surface changes,
which opens up a new avenue for intelligent remote sensing applications. To
facilitate future research, we will make our dataset and codebase of the MCI
model and Change-Agent publicly available at
https://github.com/Chen-Yang-Liu/Change-Agent",http://arxiv.org/abs/2403.19646v3
Towards Better Comprehension of Breaking Changes in the NPM Ecosystem,2024-08-26T17:18:38Z,"Dezhen Kong, Jiakun Liu, Lingfeng Bao, David Lo","Breaking changes cause a lot of effort to both downstream and upstream
developers: downstream developers need to adapt to breaking changes and
upstream developers are responsible for identifying and documenting them. In
the NPM ecosystem, characterized by frequent code changes and a high tolerance
for making breaking changes, the effort is larger.
  For better comprehension of breaking changes in the NPM ecosystem and to
enhance breaking change detection tools, we conduct a large-scale empirical
study to investigate breaking changes in the NPM ecosystem. We construct a
dataset of explicitly documented breaking changes from 381 popular NPM
projects. We find that 95.4% of the detected breaking changes can be covered by
developers' documentation, and about 19% of the breaking changes cannot be
detected by regression testing. Then in the process of investigating source
code of our collected breaking changes, we yield a taxonomy of JavaScript and
TypeScript-specific syntactic breaking changes and a taxonomy of major types of
behavioral breaking changes. Additionally, we investigate the reasons why
developers make breaking changes in NPM and find three major reasons, i.e., to
reduce code redundancy, to improve identifier name, and to improve API design,
and each category contains several sub-items.
  We provide actionable implications for future research, e.g., automatic
naming and renaming techniques should be applied in JavaScript projects to
improve identifier names, future research can try to detect more types of
behavioral breaking changes. By presenting the implications, we also discuss
the weakness of automatic renaming and breaking change detection approaches.",http://arxiv.org/abs/2408.14431v2
"Single-Temporal Supervised Learning for Universal Remote Sensing Change
  Detection",2024-06-22T00:03:21Z,"Zhuo Zheng, Yanfei Zhong, Ailong Ma, Liangpei Zhang","Bitemporal supervised learning paradigm always dominates remote sensing
change detection using numerous labeled bitemporal image pairs, especially for
high spatial resolution (HSR) remote sensing imagery. However, it is very
expensive and labor-intensive to label change regions in large-scale bitemporal
HSR remote sensing image pairs. In this paper, we propose single-temporal
supervised learning (STAR) for universal remote sensing change detection from a
new perspective of exploiting changes between unpaired images as supervisory
signals. STAR enables us to train a high-accuracy change detector only using
unpaired labeled images and can generalize to real-world bitemporal image
pairs. To demonstrate the flexibility and scalability of STAR, we design a
simple yet unified change detector, termed ChangeStar2, capable of addressing
binary change detection, object change detection, and semantic change detection
in one architecture. ChangeStar2 achieves state-of-the-art performances on
eight public remote sensing change detection datasets, covering above two
supervised settings, multiple change types, multiple scenarios. The code is
available at https://github.com/Z-Zheng/pytorch-change-models.",http://arxiv.org/abs/2406.15694v1
"Enhancing Perception of Key Changes in Remote Sensing Image Change
  Captioning",2024-09-19T09:33:33Z,"Cong Yang, Zuchao Li, Hongzan Jiao, Zhi Gao, Lefei Zhang","Recently, while significant progress has been made in remote sensing image
change captioning, existing methods fail to filter out areas unrelated to
actual changes, making models susceptible to irrelevant features. In this
article, we propose a novel multimodal framework for remote sensing image
change captioning, guided by Key Change Features and Instruction-tuned (KCFI).
This framework aims to fully leverage the intrinsic knowledge of large language
models through visual instructions and enhance the effectiveness and accuracy
of change features using pixel-level change detection tasks. Specifically, KCFI
includes a ViTs encoder for extracting bi-temporal remote sensing image
features, a key feature perceiver for identifying critical change areas, a
pixel-level change detection decoder to constrain key change features, and an
instruction-tuned decoder based on a large language model. Moreover, to ensure
that change description and change detection tasks are jointly optimized, we
employ a dynamic weight-averaging strategy to balance the losses between the
two tasks. We also explore various feature combinations for visual fine-tuning
instructions and demonstrate that using only key change features to guide the
large language model is the optimal choice. To validate the effectiveness of
our approach, we compare it against several state-of-the-art change captioning
methods on the LEVIR-CC dataset, achieving the best performance. Our code will
be available at https://github.com/yangcong356/KCFI.git.",http://arxiv.org/abs/2409.12612v1
Segment Any Change,2024-02-02T07:17:39Z,"Zhuo Zheng, Yanfei Zhong, Liangpei Zhang, Stefano Ermon","Visual foundation models have achieved remarkable results in zero-shot image
classification and segmentation, but zero-shot change detection remains an open
problem. In this paper, we propose the segment any change models (AnyChange), a
new type of change detection model that supports zero-shot prediction and
generalization on unseen change types and data distributions. AnyChange is
built on the segment anything model (SAM) via our training-free adaptation
method, bitemporal latent matching. By revealing and exploiting intra-image and
inter-image semantic similarities in SAM's latent space, bitemporal latent
matching endows SAM with zero-shot change detection capabilities in a
training-free way. We also propose a point query mechanism to enable
AnyChange's zero-shot object-centric change detection capability. We perform
extensive experiments to confirm the effectiveness of AnyChange for zero-shot
change detection. AnyChange sets a new record on the SECOND benchmark for
unsupervised change detection, exceeding the previous SOTA by up to 4.4% F$_1$
score, and achieving comparable accuracy with negligible manual annotations (1
pixel per image) for supervised change detection. Code is available at
https://github.com/Z-Zheng/pytorch-change-models.",http://arxiv.org/abs/2402.01188v4
"Exploring Sound Change Over Time: A Review of Computational and Human
  Perception",2024-07-06T14:44:59Z,"Siqi He, Wei Zhao","Computational and human perception are often considered separate approaches
for studying sound changes over time; few works have touched on the
intersection of both. To fill this research gap, we provide a pioneering review
contrasting computational with human perception from the perspectives of
methods and tasks. Overall, computational approaches rely on computer-driven
models to perceive historical sound changes on etymological datasets, while
human approaches use listener-driven models to perceive ongoing sound changes
on recording corpora. Despite their differences, both approaches complement
each other on phonetic and acoustic levels, showing the potential to achieve a
more comprehensive perception of sound change. Moreover, we call for a
comparative study on the datasets used by both approaches to investigate the
influence of historical sound changes on ongoing changes. Lastly, we discuss
the applications of sound change in computational linguistics, and point out
that perceiving sound change alone is insufficient, as many processes of
language change are complex, with entangled changes at syntactic, semantic, and
phonetic levels.",http://arxiv.org/abs/2407.05092v1
Specialized Change Detection using Segment Anything,2024-08-13T05:27:23Z,"Tahir Ahmad, Sudipan Saha","Change detection (CD) is a fundamental task in Earth observation. While most
change detection methods detect all changes, there is a growing need for
specialized methods targeting specific changes relevant to particular
applications while discarding the other changes. For instance, urban management
might prioritize detecting the disappearance of buildings due to natural
disasters or other reasons. Furthermore, while most supervised change detection
methods require large-scale training datasets, in many applications only one or
two training examples might be available instead of large datasets. Addressing
such needs, we propose a focused CD approach using the Segment Anything Model
(SAM), a versatile vision foundation model. Our method leverages a binary mask
of the object of interest in pre-change images to detect their disappearance in
post-change images. By using SAM's robust segmentation capabilities, we create
prompts from the pre-change mask, use those prompts to segment the post-change
image, and identify missing objects. This unsupervised approach demonstrated
for building disappearance detection, is adaptable to various domains requiring
specialized CD. Our contributions include defining a novel CD problem,
proposing a method using SAM, and demonstrating its effectiveness. The proposed
method also has benefits related to privacy preservation.",http://arxiv.org/abs/2408.06644v1
"Change-point detection in functional time series: Applications to
  age-specific mortality and fertility",2024-11-01T12:08:15Z,Han Lin Shang,"We consider determining change points in a time series of age-specific
mortality and fertility curves observed over time. We propose two detection
methods for identifying these change points. The first method uses a functional
cumulative sum statistic to pinpoint the change point. The second method
computes a univariate time series of integrated squared forecast errors after
fitting a functional time-series model before applying a change-point detection
method to the errors to determine the change point. Using Australian
age-specific fertility and mortality data, we apply these methods to locate the
change points and identify the optimal training period to achieve improved
forecast accuracy.",http://arxiv.org/abs/2411.00534v1
"ChangeGuard: Validating Code Changes via Pairwise Learning-Guided
  Execution",2024-10-21T15:13:32Z,"Lars Gr√∂ninger, Beatriz Souza, Michael Pradel","Code changes are an integral part of the software development process. Many
code changes are meant to improve the code without changing its functional
behavior, e.g., refactorings and performance improvements. Unfortunately,
validating whether a code change preserves the behavior is non-trivial,
particularly when the code change is performed deep inside a complex project.
This paper presents ChangeGuard, an approach that uses learning-guided
execution to compare the runtime behavior of a modified function. The approach
is enabled by the novel concept of pairwise learning-guided execution and by a
set of techniques that improve the robustness and coverage of the
state-of-the-art learning-guided execution technique. Our evaluation applies
ChangeGuard to a dataset of 224 manually annotated code changes from popular
Python open-source projects and to three datasets of code changes obtained by
applying automated code transformations. Our results show that the approach
identifies semantics-changing code changes with a precision of 77.1% and a
recall of 69.5%, and that it detects unexpected behavioral changes introduced
by automatic code refactoring tools. In contrast, the existing regression tests
of the analyzed projects miss the vast majority of semantics-changing code
changes, with a recall of only 7.6%. We envision our approach being useful for
detecting unintended behavioral changes early in the development process and
for improving the quality of automated code transformations.",http://arxiv.org/abs/2410.16092v1
An Empirical Study of Token-based Micro Commits,2024-05-15T07:52:13Z,"Masanari Kondo, Daniel M. German, Yasutaka Kamei, Naoyasu Ubayashi, Osamu Mizuno","In software development, developers frequently apply maintenance activities
to the source code that change a few lines by a single commit. A good
understanding of the characteristics of such small changes can support quality
assurance approaches (e.g., automated program repair), as it is likely that
small changes are addressing deficiencies in other changes; thus, understanding
the reasons for creating small changes can help understand the types of errors
introduced. Eventually, these reasons and the types of errors can be used to
enhance quality assurance approaches for improving code quality. While prior
studies used code churns to characterize and investigate the small changes,
such a definition has a critical limitation. Specifically, it loses the
information of changed tokens in a line. For example, this definition fails to
distinguish the following two one-line changes: (1) changing a string literal
to fix a displayed message and (2) changing a function call and adding a new
parameter. These are definitely maintenance activities, but we deduce that
researchers and practitioners are interested in supporting the latter change.
To address this limitation, in this paper, we define micro commits, a type of
small change based on changed tokens. Our goal is to quantify small changes
using changed tokens. Changed tokens allow us to identify small changes more
precisely. In fact, this token-level definition can distinguish the above
example. We investigate defined micro commits in four OSS projects and
understand their characteristics as the first empirical study on token-based
micro commits. We find that micro commits mainly replace a single name or
literal token, and micro commits are more likely used to fix bugs.
Additionally, we propose the use of token-based information to support software
engineering approaches in which very small changes significantly affect their
effectiveness.",http://arxiv.org/abs/2405.09165v1
"ChangeChat: An Interactive Model for Remote Sensing Change Analysis via
  Multimodal Instruction Tuning",2024-09-13T07:00:44Z,"Pei Deng, Wenqian Zhou, Hanlin Wu","Remote sensing (RS) change analysis is vital for monitoring Earth's dynamic
processes by detecting alterations in images over time. Traditional change
detection excels at identifying pixel-level changes but lacks the ability to
contextualize these alterations. While recent advancements in change captioning
offer natural language descriptions of changes, they do not support
interactive, user-specific queries. To address these limitations, we introduce
ChangeChat, the first bitemporal vision-language model (VLM) designed
specifically for RS change analysis. ChangeChat utilizes multimodal instruction
tuning, allowing it to handle complex queries such as change captioning,
category-specific quantification, and change localization. To enhance the
model's performance, we developed the ChangeChat-87k dataset, which was
generated using a combination of rule-based methods and GPT-assisted
techniques. Experiments show that ChangeChat offers a comprehensive,
interactive solution for RS change analysis, achieving performance comparable
to or even better than state-of-the-art (SOTA) methods on specific tasks, and
significantly surpassing the latest general-domain model, GPT-4. Code and
pre-trained weights are available at https://github.com/hanlinwu/ChangeChat.",http://arxiv.org/abs/2409.08582v1
Multi-View Pose-Agnostic Change Localization with Zero Labels,2024-12-05T06:28:54Z,"Chamuditha Jayanga Galappaththige, Jason Lai, Lloyd Windrim, Donald Dansereau, Niko Suenderhauf, Dimity Miller","Autonomous agents often require accurate methods for detecting and localizing
changes in their environment, particularly when observations are captured from
unconstrained and inconsistent viewpoints. We propose a novel label-free,
pose-agnostic change detection method that integrates information from multiple
viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS)
representation of the scene. With as few as 5 images of the post-change scene,
our approach can learn additional change channels in a 3DGS and produce change
masks that outperform single-view techniques. Our change-aware 3D scene
representation additionally enables the generation of accurate change masks for
unseen viewpoints. Experimental results demonstrate state-of-the-art
performance in complex multi-object scenes, achieving a 1.7$\times$ and
1.6$\times$ improvement in Mean Intersection Over Union and F1 score
respectively over other baselines. We also contribute a new real-world dataset
to benchmark change detection in diverse challenging scenes in the presence of
lighting variations.",http://arxiv.org/abs/2412.03911v1
"LRNet: Change detection of high-resolution remote sensing imagery via
  strategy of localization-then-refinement",2024-04-07T09:05:04Z,"Huan Zhong, Chen Wu, Ziqi Xiao","Change detection, as a research hotspot in the field of remote sensing, has
witnessed continuous development and progress. However, the discrimination of
boundary details remains a significant bottleneck due to the complexity of
surrounding elements between change areas and backgrounds. Discriminating the
boundaries of large change areas results in misalignment, while connecting
boundaries occurs for small change targets. To address the above issues, a
novel network based on the localization-then-refinement strategy is proposed in
this paper, namely LRNet. LRNet consists of two stages: localization and
refinement. In the localization stage, a three-branch encoder simultaneously
extracts original image features and their differential features for
interactive localization of the position of each change area. To minimize
information loss during feature extraction, learnable optimal pooling (LOP) is
proposed to replace the widely used max-pooling. Additionally, this process is
trainable and contributes to the overall optimization of the network. To
effectively interact features from different branches and accurately locate
change areas of various sizes, change alignment attention (C2A) and
hierarchical change alignment module (HCA) are proposed. In the refinement
stage, the localization results from the localization stage are corrected by
constraining the change areas and change edges through the edge-area alignment
module (E2A). Subsequently, the decoder, combined with the difference features
strengthened by C2A in the localization phase, refines change areas of
different sizes, ultimately achieving accurate boundary discrimination of
change areas. The proposed LRNet outperforms 13 other state-of-the-art methods
in terms of comprehensive evaluation metrics and provides the most precise
boundary discrimination results on the LEVIR-CD and WHU-CD datasets.",http://arxiv.org/abs/2404.04884v1
Understanding Code Change with Micro-Changes,2024-09-16T01:47:25Z,"Lei Chen, Michele Lanza, Shinpei Hayashi","A crucial activity in software maintenance and evolution is the comprehension
of the changes performed by developers, when they submit a pull request and/or
perform a commit on the repository. Typically, code changes are represented in
the form of code diffs, textual representations highlighting the differences
between two file versions, depicting the added, removed, and changed lines.
This simplistic representation must be interpreted by developers, and mentally
lifted to a higher abstraction level, that more closely resembles natural
language descriptions, and eases the creation of a mental model of the changes.
However, the textual diff-based representation is cumbersome, and the lifting
requires considerable domain knowledge and programming skills. We present an
approach, based on the concept of micro-change, to overcome these difficulties,
translating code diffs into a series of pre-defined change operations, which
can be described in natural language. We present a catalog of micro-changes,
together with an automated micro-change detector. To evaluate our approach, we
performed an empirical study on a large set of open-source repositories,
focusing on a subset of our micro-change catalog, namely those related to
changes affecting the conditional logic. We found that our detector is capable
of explaining more than 67% of the changes taking place in the systems under
study.",http://arxiv.org/abs/2409.09923v1
Sudden change of interferometric power for X shape states,2024-05-09T05:44:44Z,"D. Zhu, F. L. Zhang, J. L. Chen","Quantum interferometric power (IP) is a discordlike measure. We study the
dynamics of IP for two-qubit X shape states under different noisy environments.
Our study shows that IP exhibits sudden change, and one side quantum channel is
enough for the occurrence of a sudden change of IP. In particular, we show that
the initial state having no sudden change of quantum discord exhibits a sudden
change of IP under the dynamics of amplitude noise, but the converse is not
true. Besides, we also investigate the dynamics of IP under two different kinds
of composite noises. Our results also confirm that sudden change of IP occurs
under such composite noises.",http://arxiv.org/abs/2405.05560v1
"MineNetCD: A Benchmark for Global Mining Change Detection on Remote
  Sensing Imagery",2024-07-04T14:45:44Z,"Weikang Yu, Xiaokang Zhang, Xiao Xiang Zhu, Richard Gloaguen, Pedram Ghamisi","Monitoring changes triggered by mining activities is crucial for industrial
controlling, environmental management and regulatory compliance, yet it poses
significant challenges due to the vast and often remote locations of mining
sites. Remote sensing technologies have increasingly become indispensable to
detect and analyze these changes over time. We thus introduce MineNetCD, a
comprehensive benchmark designed for global mining change detection using
remote sensing imagery. The benchmark comprises three key contributions. First,
we establish a global mining change detection dataset featuring more than 70k
paired patches of bi-temporal high-resolution remote sensing images and
pixel-level annotations from 100 mining sites worldwide. Second, we develop a
novel baseline model based on a change-aware Fast Fourier Transform (ChangeFFT)
module, which enhances various backbones by leveraging essential spectrum
components within features in the frequency domain and capturing the
channel-wise correlation of bi-temporal feature differences to learn
change-aware representations. Third, we construct a unified change detection
(UCD) framework that integrates over 13 advanced change detection models. This
framework is designed for streamlined and efficient processing, utilizing the
cloud platform hosted by HuggingFace. Extensive experiments have been conducted
to demonstrate the superiority of the proposed baseline model compared with 12
state-of-the-art change detection approaches. Empirical studies on modularized
backbones comprehensively confirm the efficacy of different representation
learners on change detection. This contribution represents significant
advancements in the field of remote sensing and change detection, providing a
robust resource for future research and applications in global mining
monitoring. Dataset and Codes are available via the link.",http://arxiv.org/abs/2407.03971v1
"Adaptive Matrix Change Point Detection: Leveraging Structured Mean
  Shifts",2024-01-30T22:17:23Z,"Xinyu Zhang, Kung-Sik Chan","In high-dimensional time series, the component processes are often assembled
into a matrix to display their interrelationship. We focus on detecting mean
shifts with unknown change point locations in these matrix time series. Series
that are activated by a change may cluster along certain rows (columns), which
forms mode-specific change point alignment. Leveraging mode-specific change
point alignments may substantially enhance the power for change point
detection. Yet, there may be no mode-specific alignments in the change point
structure. We propose a powerful test to detect mode-specific change points,
yet robust to non-mode-specific changes. We show the validity of using the
multiplier bootstrap to compute the p-value of the proposed methods, and derive
non-asymptotic bounds on the size and power of the tests. We also propose a
parallel bootstrap, a computationally efficient approach for computing the
p-value of the proposed adaptive test. In particular, we show the consistency
of the proposed test, under mild regularity conditions. To obtain the
theoretical results, we derive new, sharp bounds on Gaussian approximation and
multiplier bootstrap approximation, which are of independent interest for high
dimensional problems with diverging sparsity.",http://arxiv.org/abs/2401.17473v2
"Discretionary Lane-Change Decision and Control via Parameterized Soft
  Actor-Critic for Hybrid Action Space",2024-02-24T11:18:12Z,"Yuan Lin, Xiao Liu, Zishun Zheng","This study focuses on a crucial task in the field of autonomous driving,
autonomous lane change. Autonomous lane change plays a pivotal role in
improving traffic flow, alleviating driver burden, and reducing the risk of
traffic accidents. However, due to the complexity and uncertainty of
lane-change scenarios, the functionality of autonomous lane change still faces
challenges. In this research, we conducted autonomous lane-change simulations
using both deep reinforcement learning (DRL) and model predictive control
(MPC). Specifically, we used the parameterized soft actor--critic (PASAC)
algorithm to train a DRL-based lane-change strategy to output both discrete
lane-change decisions and continuous longitudinal vehicle acceleration. We also
used MPC for lane selection based on the smallest predictive car-following
costs for the different lanes. For the first time, we compared the performance
of DRL and MPC in the context of lane-change decisions. The simulation results
indicated that, under the same reward/cost function and traffic flow, both MPC
and PASAC achieved a collision rate of 0%. PASAC demonstrated a comparable
performance to MPC in terms of average rewards/costs and vehicle speeds.",http://arxiv.org/abs/2402.15790v2
"Highway Discretionary Lane-change Decision and Control Using Model
  Predictive Control",2024-02-27T14:05:46Z,"Zishun Zheng, Yihan Wang, Yuan Lin","To enable autonomous vehicles to perform discretionary lane change amidst the
random traffic flow on highways, this paper introduces a decision-making and
control method for vehicle lane change based on Model Predictive Control (MPC).
This approach divides the driving control of vehicles on highways into two
parts: lane-change decision and lane-change control, both of which are solved
using the MPC method. In the lanechange decision module, the minimum driving
costs for each lane are computed and compared by solving the MPC problem to
make lane-change decisions. In the lane-change control module, a dynamic
bicycle model is incorporated, and a multi-objective cost function is designed
to obtain the optimal control inputs for the lane-change process. Additionally,
A long-short term memory (LSTM) model is used to predict the trajectories of
surrounding vehicles for both the MPC decision and control modules. The
proposed lane-change decision and control method is simulated and validated in
a driving simulator under random highway traffic conditions.",http://arxiv.org/abs/2402.17524v2
Causal Change Point Detection and Localization,2024-03-19T12:23:48Z,"Shimeng Huang, Jonas Peters, Niklas Pfister","Detecting and localizing change points in sequential data is of interest in
many areas of application. Various notions of change points have been proposed,
such as changes in mean, variance, or the linear regression coefficient. In
this work, we consider settings in which a response variable $Y$ and a set of
covariates $X=(X^1,\ldots,X^{d+1})$ are observed over time and aim to find
changes in the causal mechanism generating $Y$ from $X$. More specifically, we
assume $Y$ depends linearly on a subset of the covariates and aim to determine
at what time points either the dependency on the subset or the subset itself
changes. We call these time points causal change points (CCPs) and show that
they form a subset of the commonly studied regression change points. We propose
general methodology to both detect and localize CCPs. Although motivated by
causality, we define CCPs without referencing an underlying causal model. The
proposed definition of CCPs exploits a notion of invariance, which is a purely
observational quantity but -- under additional assumptions -- has a causal
meaning. For CCP localization, we propose a loss function that can be combined
with existing multiple change point algorithms to localize multiple CCPs
efficiently. We evaluate and illustrate our methods on simulated datasets.",http://arxiv.org/abs/2403.12677v1
ChangeBind: A Hybrid Change Encoder for Remote Sensing Change Detection,2024-04-26T17:47:14Z,"Mubashir Noman, Mustansar Fiaz, Hisham Cholakkal","Change detection (CD) is a fundamental task in remote sensing (RS) which aims
to detect the semantic changes between the same geographical regions at
different time stamps. Existing convolutional neural networks (CNNs) based
approaches often struggle to capture long-range dependencies. Whereas recent
transformer-based methods are prone to the dominant global representation and
may limit their capabilities to capture the subtle change regions due to the
complexity of the objects in the scene. To address these limitations, we
propose an effective Siamese-based framework to encode the semantic changes
occurring in the bi-temporal RS images. The main focus of our design is to
introduce a change encoder that leverages local and global feature
representations to capture both subtle and large change feature information
from multi-scale features to precisely estimate the change regions. Our
experimental study on two challenging CD datasets reveals the merits of our
approach and obtains state-of-the-art performance.",http://arxiv.org/abs/2404.17565v1
Robust Score-Based Quickest Change Detection,2024-07-15T01:53:04Z,"Sean Moushegian, Suya Wu, Enmao Diao, Jie Ding, Taposh Banerjee, Vahid Tarokh","Methods in the field of quickest change detection rapidly detect in real-time
a change in the data-generating distribution of an online data stream. Existing
methods have been able to detect this change point when the densities of the
pre- and post-change distributions are known. Recent work has extended these
results to the case where the pre- and post-change distributions are known only
by their score functions. This work considers the case where the pre- and
post-change score functions are known only to correspond to distributions in
two disjoint sets. This work employs a pair of ""least-favorable"" distributions
to robustify the existing score-based quickest change detection algorithm, the
properties of which are studied. This paper calculates the least-favorable
distributions for specific model classes and provides methods of estimating the
least-favorable distributions for common constructions. Simulation results are
provided demonstrating the performance of our robust change detection
algorithm.",http://arxiv.org/abs/2407.11094v2
"Novel Change Detection Framework in Remote Sensing Imagery Using
  Diffusion Models and Structural Similarity Index (SSIM)",2024-08-20T07:54:08Z,"Andrew Kiruluta, Eric Lundy, Andreas Lemos","Change detection is a crucial task in remote sensing, enabling the monitoring
of environmental changes, urban growth, and disaster impact. Conventional
change detection techniques, such as image differencing and ratioing, often
struggle with noise and fail to capture complex variations in imagery. Recent
advancements in machine learning, particularly generative models like diffusion
models, offer new opportunities for enhancing change detection accuracy. In
this paper, we propose a novel change detection framework that combines the
strengths of Stable Diffusion models with the Structural Similarity Index
(SSIM) to create robust and interpretable change maps. Our approach, named
Diffusion Based Change Detector, is evaluated on both synthetic and real-world
remote sensing datasets and compared with state-of-the-art methods. The results
demonstrate that our method significantly outperforms traditional differencing
techniques and recent deep learning-based methods, particularly in scenarios
with complex changes and noise.",http://arxiv.org/abs/2408.10619v1
ZeroSCD: Zero-Shot Street Scene Change Detection,2024-09-23T17:53:44Z,"Shyam Sundar Kannan, Byung-Cheol Min","Scene Change Detection is a challenging task in computer vision and robotics
that aims to identify differences between two images of the same scene captured
at different times. Traditional change detection methods rely on training
models that take these image pairs as input and estimate the changes, which
requires large amounts of annotated data, a costly and time-consuming process.
To overcome this, we propose ZeroSCD, a zero-shot scene change detection
framework that eliminates the need for training. ZeroSCD leverages pre-existing
models for place recognition and semantic segmentation, utilizing their
features and outputs to perform change detection. In this framework, features
extracted from the place recognition model are used to estimate correspondences
and detect changes between the two images. These are then combined with
segmentation results from the semantic segmentation model to precisely
delineate the boundaries of the detected changes. Extensive experiments on
benchmark datasets demonstrate that ZeroSCD outperforms several
state-of-the-art methods in change detection accuracy, despite not being
trained on any of the benchmark datasets, proving its effectiveness and
adaptability across different scenarios.",http://arxiv.org/abs/2409.15255v1
"Change-point detection in anomalous-diffusion trajectories utilising
  machine-learning-based uncertainty estimates",2024-10-18T06:39:35Z,"Henrik Seckler, Ralf Metzler","When recording the movement of individual animals, cells or molecules one
will often observe changes in their diffusive behaviour at certain points in
time along their trajectory. In order to capture the different diffusive modes
assembled in such heterogeneous trajectories it becomes necessary to segment
them by determining these change-points. Such a change-point detection can be
challenging for conventional statistical methods, especially when the changes
are subtle. We here apply Bayesian Deep Learning to obtain point-wise estimates
of not only the anomalous diffusion exponent but also the uncertainties in
these predictions from a single anomalous diffusion trajectory generated
according to four theoretical models of anomalous diffusion. We show that we
are able to achieve an accuracy similar to single-mode (without change-points)
predictions as well as a well calibrated uncertainty predictions of this
accuracy. Additionally, we find that the predicted uncertainties feature
interesting behaviour at the change-points leading us to examine the
capabilities of these predictions for change-point detection. While the series
of predicted uncertainties on their own are not sufficient to improve
change-point detection, they do lead to a performance boost when applied in
combination with the predicted anomalous diffusion exponents.",http://arxiv.org/abs/2410.14206v1
Survey in Characterization of Semantic Change,2024-02-29T12:13:50Z,"Jader Martins Camboim de S√°, Marcos Da Silveira, C√©dric Pruski","Live languages continuously evolve to integrate the cultural change of human
societies. This evolution manifests through neologisms (new words) or
\textbf{semantic changes} of words (new meaning to existing words).
Understanding the meaning of words is vital for interpreting texts coming from
different cultures (regionalism or slang), domains (e.g., technical terms), or
periods. In computer science, these words are relevant to computational
linguistics algorithms such as translation, information retrieval, question
answering, etc. Semantic changes can potentially impact the quality of the
outcomes of these algorithms. Therefore, it is important to understand and
characterize these changes formally. The study of this impact is a recent
problem that has attracted the attention of the computational linguistics
community. Several approaches propose methods to detect semantic changes with
good precision, but more effort is needed to characterize how the meaning of
words changes and to reason about how to reduce the impact of semantic change.
This survey provides an understandable overview of existing approaches to the
\textit{characterization of semantic changes} and also formally defines three
classes of characterizations: if the meaning of a word becomes more general or
narrow (change in dimension) if the word is used in a more pejorative or
positive/ameliorated sense (change in orientation), and if there is a trend to
use the word in a, for instance, metaphoric or metonymic context (change in
relation). We summarized the main aspects of the selected publications in a
table and discussed the needs and trends in the research activities on semantic
change characterization.",http://arxiv.org/abs/2402.19088v3
Predicting Future Change-points in Time Series,2024-05-15T16:25:44Z,"Chak Fung Choi, Chunxue Li, Chun Yip Yau, Zifeng Zhao","Change-point detection and estimation procedures have been widely developed
in the literature. However, commonly used approaches in change-point analysis
have mainly been focusing on detecting change-points within an entire time
series (off-line methods), or quickest detection of change-points in
sequentially observed data (on-line methods). Both classes of methods are
concerned with change-points that have already occurred. The arguably more
important question of when future change-points may occur, remains largely
unexplored. In this paper, we develop a novel statistical model that describes
the mechanism of change-point occurrence. Specifically, the model assumes a
latent process in the form of a random walk driven by non-negative innovations,
and an observed process which behaves differently when the latent process
belongs to different regimes. By construction, an occurrence of a change-point
is equivalent to hitting a regime threshold by the latent process. Therefore,
by predicting when the latent process will hit the next regime threshold,
future change-points can be forecasted. The probabilistic properties of the
model such as stationarity and ergodicity are established. A composite
likelihood-based approach is developed for parameter estimation and model
selection. Moreover, we construct the predictor and prediction interval for
future change points based on the estimated model.",http://arxiv.org/abs/2405.09485v2
"Automated GIS-Based Framework for Detecting Crosswalk Changes from
  Bi-Temporal High-Resolution Aerial Images",2024-06-14T05:36:50Z,"Richard Boadu Antwi, Samuel Takyi, Alican Karaer, Eren Erman Ozguven, Michael Kimollo, Ren Moses, Maxim A. Dulebenets, Thobias Sando","Identification of changes in pavement markings has become crucial for
infrastructure monitoring, maintenance, development, traffic management, and
safety. Automated extraction of roadway geometry is critical in helping with
this, given the increasing availability of high-resolution images and
advancements in computer vision and object detection. Specifically, due to the
substantial volume of satellite and high-resolution aerial images captured at
different time instances, change detection has become a viable solution. In
this study, an automated framework is developed to detect changes in crosswalks
of Orange, Osceola, and Seminole counties in Florida, utilizing data extracted
from high-resolution images obtained at various time intervals. Specifically,
for Orange County, crosswalk changes between 2019 and 2021 were manually
extracted, verified, and categorized as either new or modified crosswalks. For
Seminole County, the developed model was used to automatically extract
crosswalk changes between 2018 and 2021, while for Osceola County, changes
between 2019 and 2020 were extracted. Findings indicate that Orange County
witnessed approximately 2,094 crosswalk changes, with 312 occurring on state
roads. In Seminole and Osceola counties, on the other hand, 1,040 and 1,402
crosswalk changes were observed on both local and state roads, respectively.
Among these, 340 and 344 were identified on state roads in Seminole and
Osceola, respectively. Spatiotemporal changes observed in crosswalks can be
utilized to regularly update the existing crosswalk inventories, which is
essential for agencies engaged in traffic and safety studies. Data extracted
from these crosswalk changes can be combined with traffic and crash data to
provide valuable insights to policymakers.",http://arxiv.org/abs/2406.09731v1
"Which One Changes More? A Novel Radial Visualization for State Change
  Comparison",2024-06-19T17:38:05Z,"Shaolun Ruan, Yong Wang, Qiang Guan","It is common to compare state changes of multiple data items and identify
which data items have changed more in various applications (e.g., annual GDP
growth of different countries and daily increase of new COVID-19 cases in
different regions). Grouped bar charts and slope graphs can visualize both
state changes and their initial and final states of multiple data items, and
are thus widely used for state change comparison. But they leverage implicit
bar differences or line slopes to indicate state changes, which has been proven
less effective for visual comparison. Both visualizations also suffer from
visual scalability issues when an increasing number of data items need to be
compared. This paper fills the research gap by proposing a novel radial
visualization called Intercept Graph to facilitate visual comparison of
multiple state changes. It consists of inner and outer axes, and leverages the
lengths of line segments intercepted by the inner axis to explicitly encode the
state changes. Users can interactively adjust the inner axis to filter large
changes of their interest and magnify the difference of relatively-similar
state changes, enhancing its visual scalability and comparison accuracy. We
extensively evaluate the Intercept Graph in comparison with baseline methods
through two usage scenarios, quantitative metric evaluations, and well-designed
crowdsourcing user studies with 50 participants. Our results demonstrate the
usefulness and effectiveness of the Intercept Graph.",http://arxiv.org/abs/2406.13721v1
Rethinking Remote Sensing Change Detection With A Mask View,2024-06-21T17:27:58Z,"Xiaowen Ma, Zhenkai Wu, Rongrong Lian, Wei Zhang, Siyang Song","Remote sensing change detection aims to compare two or more images recorded
for the same area but taken at different time stamps to quantitatively and
qualitatively assess changes in geographical entities and environmental
factors. Mainstream models usually built on pixel-by-pixel change detection
paradigms, which cannot tolerate the diversity of changes due to complex scenes
and variation in imaging conditions. To address this shortcoming, this paper
rethinks the change detection with the mask view, and further proposes the
corresponding: 1) meta-architecture CDMask and 2) instance network
CDMaskFormer. Components of CDMask include Siamese backbone, change extractor,
pixel decoder, transformer decoder and normalized detector, which ensures the
proper functioning of the mask detection paradigm. Since the change query can
be adaptively updated based on the bi-temporal feature content, the proposed
CDMask can adapt to different latent data distributions, thus accurately
identifying regions of interest changes in complex scenarios. Consequently, we
further propose the instance network CDMaskFormer customized for the change
detection task, which includes: (i) a Spatial-temporal convolutional
attention-based instantiated change extractor to capture spatio-temporal
context simultaneously with lightweight operations; and (ii) a scene-guided
axial attention-instantiated transformer decoder to extract more spatial
details. State-of-the-art performance of CDMaskFormer is achieved on five
benchmark datasets with a satisfactory efficiency-accuracy trade-off. Code is
available at https://github.com/xwmaxwma/rschange.",http://arxiv.org/abs/2406.15320v1
"SeFi-CD: A Semantic First Change Detection Paradigm That Can Detect Any
  Change You Want",2024-07-13T12:49:58Z,"Ling Zhao, Zhenyang Huang, Dongsheng Kuang, Chengli Peng, Jun Gan, Haifeng Li","The existing change detection(CD) methods can be summarized as the
visual-first change detection (ViFi-CD) paradigm, which first extracts change
features from visual differences and then assigns them specific semantic
information. However, CD is essentially dependent on change regions of interest
(CRoIs), meaning that the CD results are directly determined by the semantics
changes of interest, making its primary image factor semantic of interest
rather than visual. The ViFi-CD paradigm can only assign specific semantics of
interest to specific change features extracted from visual differences, leading
to the inevitable omission of potential CRoIs and the inability to adapt to
different CRoI CD tasks. In other words, changes in other CRoIs cannot be
detected by the ViFi-CD method without retraining the model or significantly
modifying the method. This paper introduces a new CD paradigm, the
semantic-first CD (SeFi-CD) paradigm. The core idea of SeFi-CD is to first
perceive the dynamic semantics of interest and then visually search for change
features related to the semantics. Based on the SeFi-CD paradigm, we designed
Anything You Want Change Detection (AUWCD). Experiments on public datasets
demonstrate that the AUWCD outperforms the current state-of-the-art CD methods,
achieving an average F1 score 5.01\% higher than that of these advanced
supervised baselines on the SECOND dataset, with a maximum increase of 13.17\%.
The proposed SeFi-CD offers a novel CD perspective and approach.",http://arxiv.org/abs/2407.09874v1
"Semantic-CC: Boosting Remote Sensing Image Change Captioning via
  Foundational Knowledge and Semantic Guidance",2024-07-19T05:07:41Z,"Yongshuo Zhu, Lu Li, Keyan Chen, Chenyang Liu, Fugen Zhou, Zhenwei Shi","Remote sensing image change captioning (RSICC) aims to articulate the changes
in objects of interest within bi-temporal remote sensing images using natural
language. Given the limitations of current RSICC methods in expressing general
features across multi-temporal and spatial scenarios, and their deficiency in
providing granular, robust, and precise change descriptions, we introduce a
novel change captioning (CC) method based on the foundational knowledge and
semantic guidance, which we term Semantic-CC. Semantic-CC alleviates the
dependency of high-generalization algorithms on extensive annotations by
harnessing the latent knowledge of foundation models, and it generates more
comprehensive and accurate change descriptions guided by pixel-level semantics
from change detection (CD). Specifically, we propose a bi-temporal SAM-based
encoder for dual-image feature extraction; a multi-task semantic aggregation
neck for facilitating information interaction between heterogeneous tasks; a
straightforward multi-scale change detection decoder to provide pixel-level
semantic guidance; and a change caption decoder based on the large language
model (LLM) to generate change description sentences. Moreover, to ensure the
stability of the joint training of CD and CC, we propose a three-stage training
strategy that supervises different tasks at various stages. We validate the
proposed method on the LEVIR-CC and LEVIR-CD datasets. The experimental results
corroborate the complementarity of CD and CC, demonstrating that Semantic-CC
can generate more accurate change descriptions and achieve optimal performance
across both tasks.",http://arxiv.org/abs/2407.14032v1
Time Variation of the Solar Tachocline,2024-10-02T18:00:10Z,"Sarbani Basu, Wesley Ant√¥nio Machado Andrade de Aguiar, Sylvain G. Korzennik","We have used solar oscillation frequencies and frequency splittings obtained
over solar cycles 23, 24 and the rising phase of solar cycle 25 to investigate
whether the tachocline properties (jump i.e., the change in the rotation rate
across the tachocline, width and position) show any time variation. We confirm
that the change in rotation rate across the tachocline changes substantially,
however, the change does not show a simple correlation with solar cycle unlike,
for instance, changes in mode frequencies. The change during the ascending
phase of solar cycle 25 is almost a mirror image of the change during the
descending part of solar cycle 24, tempting us to speculate that the tachocline
has a much longer period than either the sunspot or the magnetic cycle. We also
find that the position of the tachocline, defined as the mid-point of the
change in rotation rate, showed significant changes during solar cycle 24. The
width of the tachocline, on the other hand, has showed significant changes
during solar cycle 23, but not later. The change in the tachocline becomes more
visible if we look at the upper and lower extents of the tachocline, defined as
(position +/- width). We find that for epochs around solar maxima and minima,
the extent decreases before increasing again - a few more years of data should
clarify this trend. Our results reinforce the need to continue helioseismic
monitoring of the Sun to understand solar activity and its evolution.",http://arxiv.org/abs/2410.01895v1
Chang's Conjectures and Easton collapses,2024-02-15T12:43:35Z,"Monroe Eskew, Masahiro Shioya","Using Easton collapses, we give a simplified construction of a model in which
Chang's Conjecture for triples holds.",http://arxiv.org/abs/2402.09917v1
"Weakly Supervised Change Detection via Knowledge Distillation and
  Multiscale Sigmoid Inference",2024-03-09T05:01:51Z,"Binghao Lu, Caiwen Ding, Jinbo Bi, Dongjin Song","Change detection, which aims to detect spatial changes from a pair of
multi-temporal images due to natural or man-made causes, has been widely
applied in remote sensing, disaster management, urban management, etc. Most
existing change detection approaches, however, are fully supervised and require
labor-intensive pixel-level labels. To address this, we develop a novel weakly
supervised change detection technique via Knowledge Distillation and Multiscale
Sigmoid Inference (KD-MSI) that leverages image-level labels. In our approach,
the Class Activation Maps (CAM) are utilized not only to derive a change
probability map but also to serve as a foundation for the knowledge
distillation process. This is done through a joint training strategy of the
teacher and student networks, enabling the student network to highlight
potential change areas more accurately than teacher network based on
image-level labels. Moreover, we designed a Multiscale Sigmoid Inference (MSI)
module as a post processing step to further refine the change probability map
from the trained student network. Empirical results on three public datasets,
i.e., WHU-CD, DSIFN-CD, and LEVIR-CD, demonstrate that our proposed technique,
with its integrated training strategy, significantly outperforms the
state-of-the-art.",http://arxiv.org/abs/2403.05796v1
Change Point Detection with Copula Entropy based Two-Sample Test,2024-02-03T20:36:48Z,Jian Ma,"Change point detection is a typical task that aim to find changes in time
series and can be tackled with two-sample test. Copula Entropy is a
mathematical concept for measuring statistical independence and a two-sample
test based on it was introduced recently. In this paper we propose a
nonparametric multivariate method for multiple change point detection with the
copula entropy-based two-sample test. The single change point detection is
first proposed as a group of two-sample tests on every points of time series
data and the change point is considered as with the maximum of the test
statistics. The multiple change point detection is then proposed by combining
the single change point detection method with binary segmentation strategy. We
verified the effectiveness of our method and compared it with the other similar
methods on the simulated univariate and multivariate data and the Nile data.",http://arxiv.org/abs/2403.07892v1
Sign-changing solutions of variational inequality,2024-04-15T09:09:06Z,"Xu Xian, Wang Taotao","In this paper, we are concerned with the sign-changing solutions of
variational inequality problems. In order to give the existence results of the
sign-changing solutions for variational inequality problems, we first construct
a suitable penalty problem related to the variational inequality, and prove the
existence of a sign-changing solution for this penalty problem using the
invariant set of descending flow method. Secondly, we perform a series of
estimates on the sign-changing solution sequence of the penalty problem, and
prove that the limit of convergence of the sign-changing solution sequence is
the sign-changing solution of the original variational inequality problem.
Variational inequality problems have extensive and important applications. In
this paper, we used penalty method and the method of invariant set of
descending flow to obtain the existence results for solutions of a variational
inequality. In particular, we obtain the existence results for sign-changing
solutions of variational inequalities for the first time based on the method of
invariant set of descending flow.",http://arxiv.org/abs/2404.11627v1
Confidence Estimation in Unsupervised Deep Change Vector Analysis,2024-05-16T08:36:20Z,Sudipan Saha,"Unsupervised transfer learning-based change detection methods exploit the
feature extraction capability of pre-trained networks to distinguish changed
pixels from the unchanged ones. However, their performance may vary
significantly depending on several geographical and model-related aspects. In
many applications, it is of utmost importance to provide trustworthy or
confident results, even if over a subset of pixels. The core challenge in this
problem is to identify changed pixels and confident pixels in an unsupervised
manner. To address this, we propose a two-network model - one tasked with mere
change detection and the other with confidence estimation. While the change
detection network can be used in conjunction with popular transfer
learning-based change detection methods such as Deep Change Vector Analysis,
the confidence estimation network operates similarly to a randomized smoothing
model. By ingesting ensembles of inputs perturbed by noise, it creates a
distribution over the output and assigns confidence to each pixel's outcome. We
tested the proposed method on three different Earth observation sensors:
optical, Synthetic Aperture Radar, and hyperspectral sensors.",http://arxiv.org/abs/2405.09896v1
"Reach and hold flexibility characterization and trade-off analysis for
  aggregations of thermostatically controlled loads",2024-05-21T01:41:44Z,"Mazen Elsaadany, Mads R. Almassalkhi","Thermostatically controlled loads (TCLs) have the potential to be flexible
and responsive loads to be used in demand response (DR) schemes. With
increasing renewable penetration, DR is playing an increasingly important role
in enhancing power grid reliability. The aggregate demand of a population of
TCLs can be modulated by changing their temperature setpoint. When and/or what
proportion of the population sees the setpoint change determines the change in
aggregate demand. However, since the TCL population is finite, not all changes
in aggregate demand can be maintained for arbitrarily long periods of time. In
this paper, the dynamic behavior of a TCL fleet is modeled and used to
characterize the set possible changes in aggregate demand that can be reached
and the corresponding time for which the demand change can be held, for a given
change in setpoint. This set is referred to, in this paper, as the reach and
hold set of a TCL fleet. Furthermore, the effect of the setpoint change and
ambient temperature on the reach and hold are analyzed. The characterized set
is then validated through simulation using both the population TCL models and
individual TCL micro-models.",http://arxiv.org/abs/2405.12444v1
"Post-selection inference for quantifying uncertainty in changes in
  variance",2024-05-24T16:06:19Z,"Rachel Carrington, Paul Fearnhead","Quantifying uncertainty in detected changepoints is an important problem.
However it is challenging as the naive approach would use the data twice, first
to detect the changes, and then to test them. This will bias the test, and can
lead to anti-conservative p-values. One approach to avoid this is to use ideas
from post-selection inference, which conditions on the information in the data
used to choose which changes to test. As a result this produces valid p-values;
that is, p-values that have a uniform distribution if there is no change.
Currently such methods have been developed for detecting changes in mean only.
This paper presents two approaches for constructing post-selection p-values for
detecting changes in variance. These vary depending on the method use to detect
the changes, but are general in terms of being applicable for a range of
change-detection methods and a range of hypotheses that we may wish to test.",http://arxiv.org/abs/2405.15670v1
"Utility maximisation and change of variable formulas for time-changed
  dynamics",2024-07-03T08:45:36Z,"Giulia Di Nunno, Hannes Haferkorn, Asma Khedher, Mich√®le Vanmaele","In this paper we derive novel change of variable formulas for stochastic
integrals w.r.t. a time-changed Brownian motion where we assume that the
time-change is a general increasing stochastic process with finitely many jumps
in a bounded set of the positive half-line and is independent of the Brownian
motion. As an application we consider the problem of maximising the expected
utility of the terminal wealth in a semimartingale setting, where the
semimartingale is written in terms of a time-changed Brownian motion and a
finite variation process. To solve this problem, we use an initial enlargement
of filtration and our change of variable formulas to shift the problem to a
maximisation problem under the enlarged filtration for models driven by a
Brownian motion and a finite variation process. The latter problem can be
solved by using martingale properties. Then applying again a change of variable
formula, we derive the optimal strategy for the original problem for a power
utility and for a logarithmic utility.",http://arxiv.org/abs/2407.02915v1
Generally-Occurring Model Change for Robust Counterfactual Explanations,2024-07-16T06:44:00Z,"Ao Xu, Tieru Wu","With the increasing impact of algorithmic decision-making on human lives, the
interpretability of models has become a critical issue in machine learning.
Counterfactual explanation is an important method in the field of interpretable
machine learning, which can not only help users understand why machine learning
models make specific decisions, but also help users understand how to change
these decisions. Naturally, it is an important task to study the robustness of
counterfactual explanation generation algorithms to model changes. Previous
literature has proposed the concept of Naturally-Occurring Model Change, which
has given us a deeper understanding of robustness to model change. In this
paper, we first further generalize the concept of Naturally-Occurring Model
Change, proposing a more general concept of model parameter changes,
Generally-Occurring Model Change, which has a wider range of applicability. We
also prove the corresponding probabilistic guarantees. In addition, we consider
a more specific problem, data set perturbation, and give relevant theoretical
results by combining optimization theory.",http://arxiv.org/abs/2407.11426v1
"Incorporating lane-change prediction into energy-efficient speed control
  of connected autonomous vehicles at intersections",2024-07-20T23:17:10Z,"Maziar Zamanpour, Suiyi He, Michael W. Levin, Zongxuan Sun","Connected and autonomous vehicles (CAVs) possess the capability of perception
and information broadcasting with other CAVs and connected intersections.
Additionally, they exhibit computational abilities and can be controlled
strategically, offering energy benefits. One potential control strategy is
real-time speed control, which adjusts the vehicle speed by taking advantage of
broadcasted traffic information, such as signal timings. However, the optimal
control is likely to increase the gap in front of the controlled CAV, which
induces lane changing by other drivers. This study proposes a modified traffic
flow model that aims to predict lane-changing occurrences and assess the impact
of lane changes on future traffic states. The primary objective is to improve
energy efficiency. The prediction model is based on a cell division platform
and is derived considering the additional flow during lane changing. An optimal
control strategy is then developed, subject to the predicted trajectory
generated for the preceding vehicle. Lane change prediction estimates future
speed and gap of vehicles, based on predicted traffic states. The proposed
framework outperforms the non-lane change traffic model, resulting in up to 13%
energy savings when lane changing is predicted 4-6 seconds in advance.",http://arxiv.org/abs/2407.15004v1
"Integrated Dynamic Phenological Feature for Remote Sensing Image Land
  Cover Change Detection",2024-08-08T01:07:28Z,"Yi Liu, Chenhao Sun, Hao Ye, Xiangying Liu, Weilong Ju","Remote sensing image change detection (CD) is essential for analyzing land
surface changes over time, with a significant challenge being the
differentiation of actual changes from complex scenes while filtering out
pseudo-changes. A primary contributor to this challenge is the intra-class
dynamic changes due to phenological characteristics in natural areas. To
overcome this, we introduce the InPhea model, which integrates phenological
features into a remote sensing image CD framework. The model features a
detector with a differential attention module for improved feature
representation of change information, coupled with high-resolution feature
extraction and spatial pyramid blocks to enhance performance. Additionally, a
constrainer with four constraint modules and a multi-stage contrastive learning
approach is employed to aid in the model's understanding of phenological
characteristics. Experiments on the HRSCD, SECD, and PSCD-Wuhan datasets reveal
that InPhea outperforms other models, confirming its effectiveness in
addressing phenological pseudo-changes and its overall model superiority.",http://arxiv.org/abs/2408.04144v1
"CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement
  Learning",2024-09-05T14:31:05Z,"John Birkbeck, Adam Sobey, Federico Cerutti, Katherine Heseltine Hurley Flynn, Timothy J. Norman","Reinforcement learning (RL) agents are costly to train and fragile to
environmental changes. They often perform poorly when there are many changing
tasks, prohibiting their widespread deployment in the real world. Many Lifelong
RL agent designs have been proposed to mitigate issues such as catastrophic
forgetting or demonstrate positive characteristics like forward transfer when
change occurs. However, no prior work has established whether the impact on
agent performance can be predicted from the change itself. Understanding this
relationship will help agents proactively mitigate a change's impact for
improved learning performance. We propose Change-Induced Regret Proxy (CHIRP)
metrics to link change to agent performance drops and use two environments to
demonstrate a CHIRP's utility in lifelong learning. A simple CHIRP-based agent
achieved $48\%$ higher performance than the next best method in one benchmark
and attained the best success rates in 8 of 10 tasks in a second benchmark
which proved difficult for existing lifelong RL agents.",http://arxiv.org/abs/2409.03577v2
Change point analysis with irregular signals,2024-09-13T14:26:31Z,"Tobias Kley, Yuhan Philip Liu, Hongyuan Cao, Wei Biao Wu","This paper considers the problem of testing and estimation of change point
where signals after the change point can be highly irregular, which departs
from the existing literature that assumes signals after the change point to be
piece-wise constant or vary smoothly. A two-step approach is proposed to
effectively estimate the location of the change point. The first step consists
of a preliminary estimation of the change point that allows us to obtain
unknown parameters for the second step. In the second step we use a new
procedure to determine the position of the change point. We show that, under
suitable conditions, the desirable $\mathcal{O}_P(1)$ rate of convergence of
the estimated change point can be obtained. We apply our method to analyze the
Baidu search index of COVID-19 related symptoms and find 8~December 2019 to be
the starting date of the COVID-19 pandemic.",http://arxiv.org/abs/2409.08863v1
Optimal Investment under the Influence of Decision-changing Imitation,2024-09-17T07:09:34Z,"Huisheng Wang, H. Vicky Zhao","Decision-changing imitation is a prevalent phenomenon in financial markets,
where investors imitate others' decision-changing rates when making their own
investment decisions. In this work, we study the optimal investment problem
under the influence of decision-changing imitation involving one leading expert
and one retail investor whose decisions are unilaterally influenced by the
leading expert. In the objective functional of the optimal investment problem,
we propose the integral disparity to quantify the distance between the two
investors' decision-changing rates. Due to the underdetermination of the
optimal investment problem, we first derive its general solution using the
variational method and find the retail investor's optimal decisions under two
special cases of the boundary conditions. We theoretically analyze the
asymptotic properties of the optimal decision as the influence of
decision-changing imitation approaches infinity, and investigate the impact of
decision-changing imitation on the optimal decision. Our analysis is validated
using numerical experiments on real stock data. This study is essential to
comprehend decision-changing imitation and devise effective mechanisms to guide
investors' decisions.",http://arxiv.org/abs/2409.10933v1
"A Latent Variable Model with Change Points and Its Application to Time
  Pressure Effects in Educational Assessment",2024-10-29T17:48:26Z,"Gabriel Wallin, Yunxiao Chen, Yi-Hsuan Lee, Xiaoou Li","Educational assessments are valuable tools for measuring student knowledge
and skills, but their validity can be compromised when test takers exhibit
changes in response behavior due to factors such as time pressure. To address
this issue, we introduce a novel latent factor model with change-points for
item response data, designed to detect and account for individual-level shifts
in response patterns during testing. This model extends traditional Item
Response Theory (IRT) by incorporating person-specific change-points, which
enables simultaneous estimation of item parameters, person latent traits, and
the location of behavioral changes. We evaluate the proposed model through
extensive simulation studies, which demonstrate its ability to accurately
recover item parameters, change-point locations, and individual ability
estimates under various conditions. Our findings show that accounting for
change-points significantly reduces bias in ability estimates, particularly for
respondents affected by time pressure. Application of the model to two
real-world educational testing datasets reveals distinct patterns of
change-point occurrence between high-stakes and lower-stakes tests, providing
insights into how test-taking behavior evolves during the tests. This approach
offers a more nuanced understanding of test-taking dynamics, with important
implications for test design, scoring, and interpretation.",http://arxiv.org/abs/2410.22300v1
"Enhancing Software Maintenance: A Learning to Rank Approach for
  Co-changed Method Identification",2024-11-28T12:23:02Z,"Yiping Jia, Safwat Hassan, Ying Zou","With the increasing complexity of large-scale software systems, identifying
all necessary modifications for a specific change is challenging. Co-changed
methods, which are methods frequently modified together, are crucial for
understanding software dependencies. However, existing methods often produce
large results with high false positives. Focusing on pull requests instead of
individual commits provides a more comprehensive view of related changes,
capturing essential co-change relationships. To address these challenges, we
propose a learning-to-rank approach that combines source code features and
change history to predict and rank co-changed methods at the pull-request
level. Experiments on 150 open-source Java projects, totaling 41.5 million
lines of code and 634,216 pull requests, show that the Random Forest model
outperforms other models by 2.5 to 12.8 percent in NDCG@5. It also surpasses
baselines such as file proximity, code clones, FCP2Vec, and StarCoder 2 by 4.7
to 537.5 percent. Models trained on longer historical data (90 to 180 days)
perform consistently, while accuracy declines after 60 days, highlighting the
need for bi-monthly retraining. This approach provides an effective tool for
managing co-changed methods, enabling development teams to handle dependencies
and maintain software quality.",http://arxiv.org/abs/2411.19099v1
Quantifying Lexical Semantic Shift via Unbalanced Optimal Transport,2024-12-17T06:00:54Z,"Ryo Kishino, Hiroaki Yamagiwa, Ryo Nagata, Sho Yokoi, Hidetoshi Shimodaira","Lexical semantic change detection aims to identify shifts in word meanings
over time. While existing methods using embeddings from a diachronic corpus
pair estimate the degree of change for target words, they offer limited insight
into changes at the level of individual usage instances. To address this, we
apply Unbalanced Optimal Transport (UOT) to sets of contextualized word
embeddings, capturing semantic change through the excess and deficit in the
alignment between usage instances. In particular, we propose Sense Usage Shift
(SUS), a measure that quantifies changes in the usage frequency of a word sense
at each usage instance. By leveraging SUS, we demonstrate that several
challenges in semantic change detection can be addressed in a unified manner,
including quantifying instance-level semantic change and word-level tasks such
as measuring the magnitude of semantic change and the broadening or narrowing
of meaning.",http://arxiv.org/abs/2412.12569v1
"Structural Changes and Percolation Transition in Networks after Aging
  Processes",2024-12-30T12:31:58Z,"Ryuho Sekikawa, Hiroshi Watanabe","In social networking services, users constantly change, and the network
structure changes simultaneously. As the network structure changes, so does the
word-of-mouth within it. To study how information transfer on the network
changes with the aging of the network, we investigated the relation of the
structure and the percolation in the aged networks. We first prepared the
Bianconi-Barab\'{a}si model as the initial network and observed the time
evolution of its properties by repeatedly deleting and adding nodes. We
introduced two tunable parameters, the deleting parameter $\alpha$ and the
adding parameter $\beta$, and observed the aging behavior for the various
parameter sets. We found that the network did not reach its steady state
depending on the parameters. We also found that when the network is stable,
there is a parameter region where the degree distribution changes from
power-law to exponential decay. We performed the percolation analyses and found
that the behavior of the percolation probability changes simultaneously with
changes in the network's structure. This study is expected to help control
network structure in steady-state aged networks.",http://arxiv.org/abs/2412.20904v3
Bug Priority Change: An Empirical Study on Apache Projects,2024-03-08T05:10:57Z,"Zengyang Li, Guangzong Cai, Qinyi Yu, Peng Liang, Ran Mo, Hui Liu","In issue tracking systems, each bug is assigned a priority level (e.g.,
Blocker, Critical, Major, Minor, or Trivial in JIRA from highest to lowest),
which indicates the urgency level of the bug. In this sense, understanding bug
priority changes helps to arrange the work schedule of participants reasonably,
and facilitates a better analysis and resolution of bugs. According to the data
extracted from JIRA deployed by Apache, a proportion of bugs in each project
underwent priority changes after such bugs were reported, which brings
uncertainty to the bug fixing process. However, there is a lack of indepth
investigation on the phenomenon of bug priority changes, which may negatively
impact the bug fixing process. Thus, we conducted a quantitative empirical
study on bugs with priority changes through analyzing 32 non-trivial Apache
open source software projects. The results show that: (1) 8.3% of the bugs in
the selected projects underwent priority changes; (2) the median priority
change time interval is merely a few days for most (28 out of 32) projects, and
half (50. 7%) of bug priority changes occurred before bugs were handled; (3)
for all selected projects, 87.9% of the bugs with priority changes underwent
only one priority change, most priority changes tend to shift the priority to
its adjacent priority, and a higher priority has a greater probability to
undergo priority change; (4) bugs that require bug-fixing changes of higher
complexity or that have more comments are likely to undergo priority changes;
and (5) priorities of bugs reported or allocated by a few specific participants
are more likely to be modified, and maximally only one participant in each
project tends to modify priorities.",http://arxiv.org/abs/2403.05059v1
Communication as a driver of change,2024-02-24T17:12:10Z,A. Fronzetti Colladon,"Chapter 2 explores the pivotal role of communication as a catalyst for change
and its profound influence on human behavior. The focus is on understanding
diverse forms of communication and human interaction that propel both
individual and collective transformations. Emphasizing the potency of effective
communication, the chapter unveils its capacity to unlock dormant mechanisms
and unleash untapped potential within individuals. The goal is to illuminate
the significant impact of communication in shaping our world and fostering
positive change. Chapter 3 discusses the challenges encountered during change
processes, providing practical strategies and exploring innovative ways to
overcome obstacles - by blending psychological insights, communication
strategies, and sociological perspectives.",http://arxiv.org/abs/2402.15863v1
Analyzing Semantic Change through Lexical Replacements,2024-04-29T10:20:41Z,"Francesco Periti, Pierluigi Cassotti, Haim Dubossarsky, Nina Tahmasebi","Modern language models are capable of contextualizing words based on their
surrounding context. However, this capability is often compromised due to
semantic change that leads to words being used in new, unexpected contexts not
encountered during pre-training. In this paper, we model \textit{semantic
change} by studying the effect of unexpected contexts introduced by
\textit{lexical replacements}. We propose a \textit{replacement schema} where a
target word is substituted with lexical replacements of varying relatedness,
thus simulating different kinds of semantic change. Furthermore, we leverage
the replacement schema as a basis for a novel \textit{interpretable} model for
semantic change. We are also the first to evaluate the use of LLaMa for
semantic change detection.",http://arxiv.org/abs/2404.18570v1
"Testing for changes in the error distribution in functional linear
  models",2024-11-07T08:24:17Z,"Natalie Neumeyer, Leonie Selk","We consider linear models with scalar responses and covariates from a
separable Hilbert space. The aim is to detect change points in the error
distribution, based on sequential residual empirical distribution functions.
Expansions for those estimated functions are more challenging in models with
infinite-dimensional covariates than in regression models with scalar or
vector-valued covariates due to a slower rate of convergence of the parameter
estimators. Yet the suggested change point test is asymptotically
distribution-free and consistent for one-change point alternatives. In the
latter case we also show consistency of a change point estimator.",http://arxiv.org/abs/2411.04522v1
"Why Studying Cut-ins? Comparing Cut-ins and Other Lane Changes Based on
  Naturalistic Driving Data",2024-02-13T08:40:20Z,"Yun Lu, Dejiang Zheng, Rong Su, Avalpreet Singh Brar, Niels de Boer, Yong Liang Guan","Extensive research has been conducted to explore vehicle lane changes, while
the study on cut-ins has not received sufficient attention. The existing
studies have not addressed the fundamental question of why studying cut-ins is
crucial, despite the extensive investigation into lane changes. To tackle this
issue, it is important to demonstrate how cut-ins, as a special type of lane
change, differ from other lane changes. In this paper, we explore to compare
driving characteristics of cut-ins and other lane changes based on naturalistic
driving data. The highD dataset is employed to conduct the comparison. We
extract all lane-change events from the dataset and exclude events that are not
suitable for our comparison. Lane-change events are then categorized into the
cut-in events and other lane-change events based on various gap-based rules.
Several performance metrics are designed to measure the driving characteristics
of the two types of events. We prove the significant differences between the
cut-in behavior and other lane-change behavior by using the Wilcoxon rank-sum
test. The results suggest the necessity of conducting specialized studies on
cut-ins, offering valuable insights for future research in this field.",http://arxiv.org/abs/2402.08289v2
"Change Guiding Network: Incorporating Change Prior to Guide Change
  Detection in Remote Sensing Imagery",2024-04-14T08:09:33Z,"Chengxi Han, Chen Wu, Haonan Guo, Meiqi Hu, Jiepan Li, Hongruixuan Chen","The rapid advancement of automated artificial intelligence algorithms and
remote sensing instruments has benefited change detection (CD) tasks. However,
there is still a lot of space to study for precise detection, especially the
edge integrity and internal holes phenomenon of change features. In order to
solve these problems, we design the Change Guiding Network (CGNet), to tackle
the insufficient expression problem of change features in the conventional
U-Net structure adopted in previous methods, which causes inaccurate edge
detection and internal holes. Change maps from deep features with rich semantic
information are generated and used as prior information to guide multi-scale
feature fusion, which can improve the expression ability of change features.
Meanwhile, we propose a self-attention module named Change Guide Module (CGM),
which can effectively capture the long-distance dependency among pixels and
effectively overcome the problem of the insufficient receptive field of
traditional convolutional neural networks. On four major CD datasets, we verify
the usefulness and efficiency of the CGNet, and a large number of experiments
and ablation studies demonstrate the effectiveness of CGNet. We're going to
open-source our code at https://github.com/ChengxiHAN/CGNet-CD.",http://arxiv.org/abs/2404.09179v1
"Explainable Self-Organizing Artificial Intelligence Captures Landscape
  Changes Correlated with Human Impact Data",2024-02-29T16:22:59Z,"John M. Wandeto, Birgitta Dresp-Langley","Novel methods of analysis are needed to help advance our understanding of the
intricate interplay between landscape changes, population dynamics, and
sustainable development. Self organized machine learning has been highly
successful in the analysis of visual data the human expert eye may not be able
to see. Thus, subtle but significant changes in fine visual detail in images
relating to trending alterations in natural or urban landscapes may remain
undetected. In the course of time, such changes may be the cause or the
consequence of measurable human impact. Capturing such change in imaging data
as early as possible can make critical information readily available to
citizens, professionals and policymakers. This promotes change awareness, and
facilitates early decision making for action. Here, we use unsupervised
Artificial Intelligence (AI) that exploits principles of self-organized
biological visual learning for the analysis of imaging time series. The
quantization error in the output of a Self Organizing Map prototype is
exploited as a computational metric of variability and change. Given the proven
sensitivity of this neural network metric to the intensity and polarity of
image pixel colour, it is shown to capture critical changes in urban
landscapes. This is achieved here on imaging data for two regions of geographic
interest in Las Vegas County, Nevada, USA. The SOM analysis is combined with
the statistical analysis of demographic data revealing human impacts. These
latter are significantly correlated with the structural change trends in the
numerical data for the specific regions of interest. By correlating data
relative to the impact of human activities with numerical data indicating
structural evolution, human footprint related environmental changes can be
predictably scaled.",http://arxiv.org/abs/2405.09547v1
ExelMap: Explainable Element-based HD-Map Change Detection and Update,2024-09-16T11:17:33Z,"Lena Wild, Ludvig Ericson, Rafael Valencia, Patric Jensfelt","Acquisition and maintenance are central problems in deploying high-definition
(HD) maps for autonomous driving, with two lines of research prevalent in
current literature: Online HD map generation and HD map change detection.
However, the generated map's quality is currently insufficient for safe
deployment, and many change detection approaches fail to precisely localize and
extract the changed map elements, hence lacking explainability and hindering a
potential fleet-based cooperative HD map update. In this paper, we propose the
novel task of explainable element-based HD map change detection and update. In
extending recent approaches that use online mapping techniques informed with an
outdated map prior for HD map updating, we present ExelMap, an explainable
element-based map updating strategy that specifically identifies changed map
elements. In this context, we discuss how currently used metrics fail to
capture change detection performance, while allowing for unfair comparison
between prior-less and prior-informed map generation methods. Finally, we
present an experimental study on real-world changes related to pedestrian
crossings of the Argoverse 2 Map Change Dataset. To the best of our knowledge,
this is the first comprehensive problem investigation of real-world end-to-end
element-based HD map change detection and update, and ExelMap the first
proposed solution.",http://arxiv.org/abs/2409.10178v1
"Show Me What and Where has Changed? Question Answering and Grounding for
  Remote Sensing Change Detection",2024-10-31T11:20:13Z,"Ke Li, Fuyu Dong, Di Wang, Shaofeng Li, Quan Wang, Xinbo Gao, Tat-Seng Chua","Remote sensing change detection aims to perceive changes occurring on the
Earth's surface from remote sensing data in different periods, and feed these
changes back to humans. However, most existing methods only focus on detecting
change regions, lacking the capability to interact with users to identify
changes that the users expect. In this paper, we introduce a new task named
Change Detection Question Answering and Grounding (CDQAG), which extends the
traditional change detection task by providing interpretable textual answers
and intuitive visual evidence. To this end, we construct the first CDQAG
benchmark dataset, termed QAG-360K, comprising over 360K triplets of questions,
textual answers, and corresponding high-quality visual masks. It encompasses 10
essential land-cover categories and 8 comprehensive question types, which
provides a valuable and diverse dataset for remote sensing applications.
Furthermore, we present VisTA, a simple yet effective baseline method that
unifies the tasks of question answering and grounding by delivering both visual
and textual answers. Our method achieves state-of-the-art results on both the
classic change detection-based visual question answering (CDVQA) and the
proposed CDQAG datasets. Extensive qualitative and quantitative experimental
results provide useful insights for developing better CDQAG models, and we hope
that our work can inspire further research in this important yet underexplored
research field. The proposed benchmark dataset and method are available at
https://github.com/like413/VisTA.",http://arxiv.org/abs/2410.23828v2
"Distributed Tracing for Cascading Changes of Objects in the Kubernetes
  Control Plane",2024-11-02T18:40:01Z,"Tomoyuki Ehira, Daisuke Kotani, Yasuo Okabe","Kubernetes is a container orchestration system that employs a declarative
configuration management approach. In Kubernetes, each desired and actual state
is represented by an ``object'', and multiple controllers autonomously monitor
related objects and update their objects towards the desired state in the
control plane. Because of this design, changes to one object propagate to other
objects in a chain. The cluster operators need to know the time required for
these cascading changes to complete, as it directly affects the quality of
service of applications running on the cluster. However, there is no practical
way to observe this kind of cascading change, including breakdown of the time
taken by each change. Distributed tracing techniques are commonly used in the
microservices architecture to monitor application performance, but they are not
directly applicable to the control plane of Kubernetes; the microservices
architecture relies on explicitly calling APIs on other services, but in
Kubernetes the controllers just monitor objects to know when to start
processing, and never call functions on other controllers directly. In this
paper, we propose a system that automatically traces changes to objects in the
control plane. Our method adds one identifier, a Change Propagation ID (CPID),
to the metadata of an object, and the controller that observes an object change
propagates its CPID to the objects that the controller is updated. When
multiple changes need to be merged on an object, a new CPID is generated, and
the relationship between the original CPID and the new CPID is sent to the
external trace server. We confirmed that change propagation can be visualized
and the required time measured. We also showed that this system's overhead is
not significant.",http://arxiv.org/abs/2411.01336v1
"Detect Changes like Humans: Incorporating Semantic Priors for Improved
  Change Detection",2024-12-22T08:27:15Z,"Yuhang Gan, Wenjie Xuan, Zhiming Luo, Lei Fang, Zengmao Wang, Juhua Liu, Bo Du","When given two similar images, humans identify their differences by comparing
the appearance ({\it e.g., color, texture}) with the help of semantics ({\it
e.g., objects, relations}). However, mainstream change detection models adopt a
supervised training paradigm, where the annotated binary change map is the main
constraint. Thus, these methods primarily emphasize the difference-aware
features between bi-temporal images and neglect the semantic understanding of
the changed landscapes, which undermines the accuracy in the presence of noise
and illumination variations. To this end, this paper explores incorporating
semantic priors to improve the ability to detect changes. Firstly, we propose a
Semantic-Aware Change Detection network, namely SA-CDNet, which transfers the
common knowledge of the visual foundation models ({\it i.e., FastSAM}) to
change detection. Inspired by the human visual paradigm, a novel dual-stream
feature decoder is derived to distinguish changes by combining semantic-aware
features and difference-aware features. Secondly, we design a single-temporal
semantic pre-training strategy to enhance the semantic understanding of
landscapes, which brings further increments. Specifically, we construct
pseudo-change detection data from public single-temporal remote sensing
segmentation datasets for large-scale pre-training, where an extra branch is
also introduced for the proxy semantic segmentation task. Experimental results
on five challenging benchmarks demonstrate the superiority of our method over
the existing state-of-the-art methods. The code is available at
\href{https://github.com/thislzm/SA-CD}{SA-CD}.",http://arxiv.org/abs/2412.16918v1
"Changen2: Multi-Temporal Remote Sensing Generative Change Foundation
  Model",2024-06-26T01:03:39Z,"Zhuo Zheng, Stefano Ermon, Dongjun Kim, Liangpei Zhang, Yanfei Zhong","Our understanding of the temporal dynamics of the Earth's surface has been
advanced by deep vision models, which often require lots of labeled
multi-temporal images for training. However, collecting, preprocessing, and
annotating multi-temporal remote sensing images at scale is non-trivial since
it is expensive and knowledge-intensive. In this paper, we present change data
generators based on generative models, which are cheap and automatic,
alleviating these data problems. Our main idea is to simulate a stochastic
change process over time. We describe the stochastic change process as a
probabilistic graphical model (GPCM), which factorizes the complex simulation
problem into two more tractable sub-problems, i.e., change event simulation and
semantic change synthesis. To solve these two problems, we present Changen2, a
GPCM with a resolution-scalable diffusion transformer which can generate time
series of images and their semantic and change labels from labeled or unlabeled
single-temporal images. Changen2 is a generative change foundation model that
can be trained at scale via self-supervision, and can produce change
supervisory signals from unlabeled single-temporal images. Unlike existing
foundation models, Changen2 synthesizes change data to train task-specific
foundation models for change detection. The resulting model possesses inherent
zero-shot change detection capabilities and excellent transferability.
Experiments suggest Changen2 has superior spatiotemporal scalability, e.g.,
Changen2 model trained on 256$^2$ pixel single-temporal images can yield time
series of any length and resolutions of 1,024$^2$ pixels. Changen2 pre-trained
models exhibit superior zero-shot performance (narrowing the performance gap to
3% on LEVIR-CD and approximately 10% on both S2Looking and SECOND, compared to
fully supervised counterparts) and transferability across multiple types of
change tasks.",http://arxiv.org/abs/2406.17998v1
A Change Language for Ontologies and Knowledge Graphs,2024-09-20T21:21:36Z,"Harshad Hegde, Jennifer Vendetti, Damien Goutte-Gattat, J Harry Caufield, John B Graybeal, Nomi L Harris, Naouel Karam, Christian Kindermann, Nicolas Matentzoglu, James A Overton, Mark A Musen, Christopher J Mungall","Ontologies and knowledge graphs (KGs) are general-purpose computable
representations of some domain, such as human anatomy, and are frequently a
crucial part of modern information systems. Most of these structures change
over time, incorporating new knowledge or information that was previously
missing. Managing these changes is a challenge, both in terms of communicating
changes to users, and providing mechanisms to make it easier for multiple
stakeholders to contribute.
  To fill that need, we have created KGCL, the Knowledge Graph Change Language,
a standard data model for describing changes to KGs and ontologies at a high
level, and an accompanying human-readable controlled natural language. This
language serves two purposes: a curator can use it to request desired changes,
and it can also be used to describe changes that have already happened,
corresponding to the concepts of ""apply patch"" and ""diff"" commonly used for
managing changes in text documents and computer programs. Another key feature
of KGCL is that descriptions are at a high enough level to be useful and
understood by a variety of stakeholders--for example, ontology edits can be
specified by commands like ""add synonym 'arm' to 'forelimb'"" or ""move
'Parkinson disease' under 'neurodegenerative disease'"".
  We have also built a suite of tools for managing ontology changes. These
include an automated agent that integrates with and monitors GitHub ontology
repositories and applies any requested changes, and a new component in the
BioPortal ontology resource that allows users to make change requests directly
from within the BioPortal user interface.
  Overall, the KGCL data model, its controlled natural language, and associated
tooling allow for easier management and processing of changes associated with
the development of ontologies and KGs.",http://arxiv.org/abs/2409.13906v1
"Simultaneous Change Point Detection and Identification for High
  Dimensional Linear Models",2024-01-16T07:31:05Z,"Bin Liu, Xinsheng Zhang, Yufeng Liu","In this article, we consider change point inference for high dimensional
linear models. For change point detection, given any subgroup of variables, we
propose a new method for testing the homogeneity of corresponding regression
coefficients across the observations. Under some regularity conditions, the
proposed new testing procedure controls the type I error asymptotically and is
powerful against sparse alternatives and enjoys certain optimality. For change
point identification, an argmax based change point estimator is proposed which
is shown to be consistent for the true change point location. Moreover,
combining with the binary segmentation technique, we further extend our new
method for detecting and identifying multiple change points. Extensive
numerical studies justify the validity of our new method and an application to
the Alzheimer's disease data analysis further demonstrate its competitive
performance.",http://arxiv.org/abs/2401.08173v1
The multi-wavelength phase curves of small bodies: Phase coloring,2024-02-16T22:34:23Z,Alvaro Alvarez-Candal,"Context. Small bodies change their brightness due to different motives:
Rotation along their axis or axes, combined with irregular shapes and/or
changing surface properties, or changes in the geometry of observations. In
this work, we tackle the problem of Phase curves, which show the change in
brightness due to changes in the fraction of illuminated surface as seen by the
observer. Aims. We aim to study the effect of the phase curves in the five
wavelengths of the Sloan Digital Sky Survey in scores of objects (several tens
of thousands), focusing particularly on the spectral slopes and the colors and
their changes with phase angle. Methods. We used a Bayesian inference method
and Monte Carlo techniques to retrieve the absolute magnitudes in five
wavelengths, using the results to study the phase coloring effect in different
bins of the semi-major axis. Results. We obtained absolute magnitudes in the
five filters for over 40 000 objects. Although some outliers are identified,
most of the usual color-color space is recovered by the data presented. We also
detect a dual behavior in the spectral slopes, with a change at
${\alpha\approx}$ 5 deg.",http://arxiv.org/abs/2402.11113v1
Non-linear Triple Changes Estimator for Targeted Policies,2024-02-19T22:34:00Z,"Sina Akbari, Negar Kiyavash","The renowned difference-in-differences (DiD) estimator relies on the
assumption of 'parallel trends,' which does not hold in many practical
applications. To address this issue, the econometrics literature has turned to
the triple difference estimator. Both DiD and triple difference are limited to
assessing average effects exclusively. An alternative avenue is offered by the
changes-in-changes (CiC) estimator, which provides an estimate of the entire
counterfactual distribution at the cost of relying on (stronger) distributional
assumptions. In this work, we extend the triple difference estimator to
accommodate the CiC framework, presenting the `triple changes estimator' and
its identification assumptions, thereby expanding the scope of the CiC
paradigm. Subsequently, we empirically evaluate the proposed framework and
apply it to a study examining the impact of Medicaid expansion on children's
preventive care.",http://arxiv.org/abs/2402.12583v1
"Semantic change detection for Slovene language: a novel dataset and an
  approach based on optimal transport",2024-02-26T14:27:06Z,"Marko Pranjiƒá, Kaja Dobrovoljc, Senja Pollak, Matej Martinc","In this paper, we focus on the detection of semantic changes in Slovene, a
less resourced Slavic language with two million speakers. Detecting and
tracking semantic changes provides insights into the evolution of the language
caused by changes in society and culture. Recently, several systems have been
proposed to aid in this study, but all depend on manually annotated gold
standard datasets for evaluation. In this paper, we present the first Slovene
dataset for evaluating semantic change detection systems, which contains
aggregated semantic change scores for 104 target words obtained from more than
3000 manually annotated sentence pairs. We evaluate several existing semantic
change detection methods on this dataset and also propose a novel approach
based on optimal transport that improves on the existing state-of-the-art
systems with an error reduction rate of 22.8%.",http://arxiv.org/abs/2402.16596v1
"Round Robin Active Sequential Change Detection for Dependent
  Multi-Channel Data",2024-03-24T21:05:28Z,"Anamitra Chaudhuri, Georgios Fellouris, Ali Tajer","This paper considers the problem of sequentially detecting a change in the
joint distribution of multiple data sources under a sampling constraint.
Specifically, the channels or sources generate observations that are
independent over time, but not necessarily independent at any given time
instant. The sources follow an initial joint distribution, and at an unknown
time instant, the joint distribution of an unknown subset of sources changes.
Importantly, there is a hard constraint that only a fixed number of sources are
allowed to be sampled at each time instant. The goal is to sequentially observe
the sources according to the constraint, and stop sampling as quickly as
possible after the change while controlling the false alarm rate below a
user-specified level. The sources can be selected dynamically based on the
already collected data, and thus, a policy for this problem consists of a joint
sampling and change-detection rule. A non-randomized policy is studied, and an
upper bound is established on its worst-case conditional expected detection
delay with respect to both the change point and the observations from the
affected sources before the change.",http://arxiv.org/abs/2403.16297v1
Change point localisation and inference in fragmented functional data,2024-05-09T12:43:43Z,"Gengyu Xue, Haotian Xu, Yi Yu","We study the problem of change point localisation and inference for
sequentially collected fragmented functional data, where each curve is observed
only over discrete grids randomly sampled over a short fragment. The sequence
of underlying covariance functions is assumed to be piecewise constant, with
changes happening at unknown time points. To localise the change points, we
propose a computationally efficient fragmented functional dynamic programming
(FFDP) algorithm with consistent change point localisation rates. With an extra
step of local refinement, we derive the limiting distributions for the refined
change point estimators in two different regimes where the minimal jump size
vanishes and where it remains constant as the sample size diverges. Such
results are the first time seen in the fragmented functional data literature.
As a byproduct of independent interest, we also present a non-asymptotic result
on the estimation error of the covariance function estimators over intervals
with change points inspired by Lin et al. (2021). Our result accounts for the
effects of the sampling grid size within each fragment under novel
identifiability conditions. Extensive numerical studies are also provided to
support our theoretical results.",http://arxiv.org/abs/2405.05730v1
"Global urban activity changes from COVID-19 physical distancing
  restrictions",2024-05-22T20:36:44Z,"Srija Chakraborty, Eleanor Stokes, Olivia Alexander","During the COVID-19 pandemic changes in human activity became widespread
through official policies and organically in response to the virus's
transmission, which in turn, impacted the environment and the economy. The
pandemic has been described as a natural experiment that tested how social and
economic disruptions impacted different components of the global Earth System.
To move this beyond hypotheses, locally-resolved, globally-available measures
of how, where, and when human activity changed are critically needed. Here we
use satellite-derived nighttime lights to quantify and map daily changes in
human activity that are atypical for each urban area globally for two years
after the onset of the pandemic using machine learning anomaly detectors.
Metrics characterizing changes in lights from pre-COVID baseline in human
settlements and quality assurance measures are reported. This dataset, TRacking
Anomalous COVID-19 induced changEs in NTL (TRACE-NTL), is the first to resolve
COVID-19 disruptions for all metropolitan regions globally, daily. It is
suitable to support a variety of post-pandemic studies that assess how changes
in human activity impact environmental systems.",http://arxiv.org/abs/2405.14902v1
"Nudging Users to Change Breached Passwords Using the Protection
  Motivation Theory",2024-05-24T07:51:15Z,"Yixin Zou, Khue Le, Peter Mayer, Alessandro Acquisti, Adam J. Aviv, Florian Schaub","We draw on the Protection Motivation Theory (PMT) to design nudges that
encourage users to change breached passwords. Our online experiment
($n$=$1,386$) compared the effectiveness of a threat appeal (highlighting
negative consequences of breached passwords) and a coping appeal (providing
instructions on how to change the breached password) in a 2x2 factorial design.
Compared to the control condition, participants receiving the threat appeal
were more likely to intend to change their passwords, and participants
receiving both appeals were more likely to end up changing their passwords;
both comparisons have a small effect size. Participants' password change
behaviors are further associated with other factors such as their security
attitudes (SA-6) and time passed since the breach, suggesting that PMT-based
nudges are useful but insufficient to fully motivate users to change their
passwords. Our study contributes to PMT's application in security research and
provides concrete design implications for improving compromised credential
notifications.",http://arxiv.org/abs/2405.15308v1
"Investigating the Contextualised Word Embedding Dimensions Specified for
  Contextual and Temporal Semantic Changes",2024-07-03T05:42:20Z,"Taichi Aida, Danushka Bollegala","The sense-aware contextualised word embeddings (SCWEs) encode semantic
changes of words within the contextualised word embedding (CWE) spaces. Despite
the superior performance of SCWEs in contextual/temporal semantic change
detection (SCD) benchmarks, it remains unclear as to how the meaning changes
are encoded in the embedding space. To study this, we compare pre-trained CWEs
and their fine-tuned versions on contextual and temporal semantic change
benchmarks under Principal Component Analysis (PCA) and Independent Component
Analysis (ICA) transformations. Our experimental results reveal (a) although
there exist a smaller number of axes that are specific to semantic changes of
words in the pre-trained CWE space, this information gets distributed across
all dimensions when fine-tuned, and (b) in contrast to prior work studying the
geometry of CWEs, we find that PCA to better represent semantic changes than
ICA within the top 10% of axes. These findings encourage the development of
more efficient SCD methods with a small number of SCD-aware dimensions. Source
code is available at https://github.com/LivNLP/svp-dims .",http://arxiv.org/abs/2407.02820v2
A Mamba-based Siamese Network for Remote Sensing Change Detection,2024-07-08T17:05:48Z,"Jay N. Paranjape, Celso de Melo, Vishal M. Patel","Change detection in remote sensing images is an essential tool for analyzing
a region at different times. It finds varied applications in monitoring
environmental changes, man-made changes as well as corresponding
decision-making and prediction of future trends. Deep learning methods like
Convolutional Neural Networks (CNNs) and Transformers have achieved remarkable
success in detecting significant changes, given two images at different times.
In this paper, we propose a Mamba-based Change Detector (M-CD) that segments
out the regions of interest even better. Mamba-based architectures demonstrate
linear-time training capabilities and an improved receptive field over
transformers. Our experiments on four widely used change detection datasets
demonstrate significant improvements over existing state-of-the-art (SOTA)
methods. Our code and pre-trained models are available at
https://github.com/JayParanjape/M-CD",http://arxiv.org/abs/2407.06839v1
"On the impossibility of detecting a late change-point in the
  preferential attachment random graph model",2024-07-26T12:06:29Z,"Ibrahim Kaddouri, Zacharie Naulet, √âlisabeth Gassiat","We consider the problem of late change-point detection under the preferential
attachment random graph model with time dependent attachment function. This can
be formulated as a hypothesis testing problem where the null hypothesis
corresponds to a preferential attachment model with a constant affine
attachment parameter $\delta_0$ and the alternative corresponds to a
preferential attachment model where the affine attachment parameter changes
from $\delta_0$ to $\delta_1$ at a time $\tau_n = n - \Delta_n$ where $0\leq
\Delta_n \leq n$ and $n$ is the size of the graph. It was conjectured in Bet et
al. that when observing only the unlabeled graph, detection of the change is
not possible for $\Delta_n = o(n^{1/2})$. In this work, we make a step towards
proving the conjecture by proving the impossibility of detecting the change
when $\Delta_n = o(n^{1/3})$. We also study change-point detection in the case
where the labeled graph is observed and show that change-point detection is
possible if and only if $\Delta_n \to \infty$, thereby exhibiting a strong
difference between the two settings.",http://arxiv.org/abs/2407.18685v3
A Lane Change Assistance System Based on Prediction of Driver Intention,2024-09-02T09:51:12Z,"Foghor Tanshi, Dirk S√∂ffker","Lane change assistance system increase safety by providing warnings and other
stability assistance to drivers to avert traffic dangers. In this contribution,
lane change intention recognition was performed and applied to generate
warnings for drivers to avoid eminent collision. Previous studies have not yet
integrated driver's intended lane change actions as an input for determining
when to warn drivers about eminent traffic dangers. Thus, if a driver's
intended action may result in a collision, the driver should be warned in
advance. In this contribution, lane change to left and right and lane keeping
intentions were utilized to warn drivers of potential collision using an audio
visual interface. The results indicate reduced risk of collision during lane
change to left and right except lane keeping maneuvers. Moreover several
participant feedback indicate an increased need for improved warnings by
additional situational analysis that anticipate other vehicle behaviors such as
intended lane changes.",http://arxiv.org/abs/2409.10551v1
Higher-criticism for sparse multi-sensor change-point detection,2024-09-23T23:04:47Z,"Tingnan Gong, Alon Kipnis, Yao Xie","We present a procedure based on higher criticism (Dohono \& Jin 2004) to
address the sparse multi-sensor quickest change-point detection problem.
Namely, we aim to detect a change in the distribution of the multi-sensor that
might affect a few sensors out of potentially many, while those affected
sensors, if they exist, are unknown to us in advance. Our procedure involves
testing for a change point in individual sensors and combining multiple tests
using higher criticism. As a by-product, our procedure also indicates a set of
sensors suspected to be affected by the change. We demonstrate the
effectiveness of our method compared to other procedures using extensive
numerical evaluations. We analyze our procedure under a theoretical framework
involving normal data sensors that might experience a change in both mean and
variance. We consider individual tests based on the likelihood ratio or the
generalized likelihood ratio statistics and show that our procedure attains the
information-theoretic limits of detection. These limits coincide with existing
litereature when the change is only in the mean.",http://arxiv.org/abs/2409.15597v1
"Quantum Bayes' rule and Petz transpose map from the minimal change
  principle",2024-10-01T01:37:48Z,"Ge Bai, Francesco Buscemi, Valerio Scarani","Bayes' rule, which is routinely used to update beliefs based on new evidence,
arises from a principle of minimal change. This principle states that updated
beliefs must be consistent with new data, while deviating minimally from the
prior belief. Here, we introduce a quantum analog of the minimal change
principle and use it to derive a quantum Bayes' rule by minimizing the change
between two quantum input-output processes, not just their marginals. This is
analogous to the classical case where Bayes' rule is obtained by minimizing the
change between the joint input-output distributions. When the change is
quantified by the fidelity, the quantum minimal change principle has a unique
solution, and the resulting quantum Bayes' rule recovers the Petz transpose
map.",http://arxiv.org/abs/2410.00319v2
Signature-based IaaS Performance Change Detection,2024-10-23T07:27:26Z,"Sheik Mohammad Mostakim Fattah, Athman Bouguettaya","We propose a novel change detection framework to identify changes in the
long-term performance behavior of an IaaS service. An IaaS service's long-term
performance behavior is represented by an IaaS performance signature. The
proposed framework leverages time series similarity measures and a sliding
window technique to detect changes in IaaS performance signatures. We introduce
a new IaaS performance noise model that enables the proposed framework to
distinguish between performance noise and actual changes in performance. The
proposed framework utilizes a novel Signal-to-Noise Ratio (SNR) based approach
to detect changes when prior knowledge about performance noise is available. A
set of experiments is conducted using real-world datasets to demonstrate the
effectiveness of the proposed change detection framework.",http://arxiv.org/abs/2410.17623v2
"Gaussian Derivative Change-point Detection for Early Warnings of
  Industrial System Failures",2024-10-29T23:14:03Z,"Hao Zhao, Rong Pan","An early warning of future system failure is essential for conducting
predictive maintenance and enhancing system availability. This paper introduces
a three-step framework for assessing system health to predict imminent system
breakdowns. First, the Gaussian Derivative Change-Point Detection (GDCPD)
algorithm is proposed for detecting changes in the high-dimensional feature
space. GDCPD conducts a multivariate Change-Point Detection (CPD) by
implementing Gaussian derivative processes for identifying change locations on
critical system features, as these changes eventually will lead to system
failure. To assess the significance of these changes, Weighted Mahalanobis
Distance (WMD) is applied in both offline and online analyses. In the offline
setting, WMD helps establish a threshold that determines significant system
variations, while in the online setting, it facilitates real-time monitoring,
issuing alarms for potential future system breakdowns. Utilizing the insights
gained from the GDCPD and monitoring scheme, Long Short-Term Memory (LSTM)
network is then employed to estimate the Remaining Useful Life (RUL) of the
system. The experimental study of a real-world system demonstrates the
effectiveness of the proposed methodology in accurately forecasting system
failures well before they occur. By integrating CPD with real-time monitoring
and RUL prediction, this methodology significantly advances system health
monitoring and early warning capabilities.",http://arxiv.org/abs/2410.22594v2
ViewDelta: Text-Prompted Change Detection in Unaligned Images,2024-12-10T15:51:17Z,"Subin Varghese, Joshua Gao, Vedhus Hoskere","Detecting changes between images is a fundamental problem in computer vision
with broad applications in situational awareness, infrastructure assessment,
environment monitoring, and industrial automation. Existing supervised models
are typically limited to detecting specific types of changes, necessitating
retraining for new tasks. To address these limitations with a single approach,
we propose a novel change detection method that is the first to utilize
unaligned images and textual prompts to output a binary segmentation of changes
relevant to user-provided text. Our architecture not only enables flexible
detection across diverse change detection use cases, but also yields
state-of-the art performance on established benchmarks. Additionally, we
release an accompanying dataset comprising of 100,311 pairs of images with text
prompts and the corresponding change detection labels. We demonstrate the
effectiveness of our method both quantitatively and qualitatively on datasets
with a wide variety of viewpoints in indoor, outdoor, street level, synthetic,
and satellite images.",http://arxiv.org/abs/2412.07612v1
"Prompting in the Wild: An Empirical Study of Prompt Evolution in
  Software Repositories",2024-12-23T05:41:01Z,"Mahan Tafreshipour, Aaron Imani, Eric Huang, Eduardo Almeida, Thomas Zimmermann, Iftekhar Ahmed","The adoption of Large Language Models (LLMs) is reshaping software
development as developers integrate these LLMs into their applications. In such
applications, prompts serve as the primary means of interacting with LLMs.
Despite the widespread use of LLM-integrated applications, there is limited
understanding of how developers manage and evolve prompts. This study presents
the first empirical analysis of prompt evolution in LLM-integrated software
development. We analyzed 1,262 prompt changes across 243 GitHub repositories to
investigate the patterns and frequencies of prompt changes, their relationship
with code changes, documentation practices, and their impact on system
behavior. Our findings show that developers primarily evolve prompts through
additions and modifications, with most changes occurring during feature
development. We identified key challenges in prompt engineering: only 21.9% of
prompt changes are documented in commit messages, changes can introduce logical
inconsistencies, and misalignment often occurs between prompt changes and LLM
responses. These insights emphasize the need for specialized testing
frameworks, automated validation tools, and improved documentation practices to
enhance the reliability of LLM-integrated applications.",http://arxiv.org/abs/2412.17298v2
fastcpd: Fast Change Point Detection in R,2024-04-09T01:33:57Z,"Xingchi Li, Xianyang Zhang","Change point analysis is concerned with detecting and locating structure
breaks in the underlying model of a sequence of observations ordered by time,
space or other variables. A widely adopted approach for change point analysis
is to minimize an objective function with a penalty term on the number of
change points. This framework includes several well-established procedures,
such as the penalized log-likelihood using the (modified) Bayesian information
criterion (BIC) or the minimum description length (MDL). The resulting
optimization problem can be solved in polynomial time by dynamic programming or
its improved version, such as the Pruned Exact Linear Time (PELT) algorithm
(Killick, Fearnhead, and Eckley 2012). However, existing computational methods
often suffer from two primary limitations: (1) methods based on direct
implementation of dynamic programming or PELT are often time-consuming for long
data sequences due to repeated computation of the cost value over different
segments of the data sequence; (2) state-of-the-art R packages do not provide
enough flexibility for users to handle different change point settings and
models. In this work, we present the fastcpd package, aiming to provide an
efficient and versatile framework for change point detection in several
commonly encountered settings. The core of our algorithm is built upon PELT and
the sequential gradient descent method recently proposed by Zhang and Dawn
(2023). We illustrate the usage of the fastcpd package through several
examples, including mean/variance changes in a (multivariate) Gaussian
sequence, parameter changes in regression models, structural breaks in
ARMA/GARCH/VAR models, and changes in user-specified models.",http://arxiv.org/abs/2404.05933v1
"MaskCD: A Remote Sensing Change Detection Network Based on Mask
  Classification",2024-04-18T11:05:15Z,"Weikang Yu, Xiaokang Zhang, Samiran Das, Xiao Xiang Zhu, Pedram Ghamisi","Change detection (CD) from remote sensing (RS) images using deep learning has
been widely investigated in the literature. It is typically regarded as a
pixel-wise labeling task that aims to classify each pixel as changed or
unchanged. Although per-pixel classification networks in encoder-decoder
structures have shown dominance, they still suffer from imprecise boundaries
and incomplete object delineation at various scenes. For high-resolution RS
images, partly or totally changed objects are more worthy of attention rather
than a single pixel. Therefore, we revisit the CD task from the mask prediction
and classification perspective and propose MaskCD to detect changed areas by
adaptively generating categorized masks from input image pairs. Specifically,
it utilizes a cross-level change representation perceiver (CLCRP) to learn
multiscale change-aware representations and capture spatiotemporal relations
from encoded features by exploiting deformable multihead self-attention
(DeformMHSA). Subsequently, a masked-attention-based detection transformers
(MA-DETR) decoder is developed to accurately locate and identify changed
objects based on masked attention and self-attention mechanisms. It
reconstructs the desired changed objects by decoding the pixel-wise
representations into learnable mask proposals and making final predictions from
these candidates. Experimental results on five benchmark datasets demonstrate
the proposed approach outperforms other state-of-the-art models. Codes and
pretrained models are available online (https://github.com/EricYu97/MaskCD).",http://arxiv.org/abs/2404.12081v1
"Predicting Likely-Vulnerable Code Changes: Machine Learning-based
  Vulnerability Protections for Android Open Source Project",2024-05-26T18:17:46Z,Keun Soo Yim,"This paper presents a framework that selectively triggers security reviews
for incoming source code changes. Functioning as a review bot within a code
review service, the framework can automatically request additional security
reviews at pre-submit time before the code changes are submitted to a source
code repository. Because performing such secure code reviews add cost, the
framework employs a classifier trained to identify code changes with a high
likelihood of vulnerabilities. The online classifier leverages various types of
input features to analyze the review patterns, track the software engineering
process, and mine specific text patterns within given code changes. The
classifier and its features are meticulously chosen and optimized using data
from the submitted code changes and reported vulnerabilities in Android Open
Source Project (AOSP). The evaluation results demonstrate that our
Vulnerability Prevention (VP) framework identifies approximately 80% of the
vulnerability-inducing code changes in the dataset with a precision ratio of
around 98% and a false positive rate of around 1.7%. We discuss the
implications of deploying the VP framework in multi-project settings and future
directions for Android security research. This paper explores and validates our
approach to code change-granularity vulnerability prediction, offering a
preventive technique for software security by preemptively detecting vulnerable
code changes before submission.",http://arxiv.org/abs/2405.16655v1
ChangeViT: Unleashing Plain Vision Transformers for Change Detection,2024-06-18T17:59:08Z,"Duowang Zhu, Xiaohu Huang, Haiyan Huang, Zhenfeng Shao, Qimin Cheng","Change detection in remote sensing images is essential for tracking
environmental changes on the Earth's surface. Despite the success of vision
transformers (ViTs) as backbones in numerous computer vision applications, they
remain underutilized in change detection, where convolutional neural networks
(CNNs) continue to dominate due to their powerful feature extraction
capabilities. In this paper, our study uncovers ViTs' unique advantage in
discerning large-scale changes, a capability where CNNs fall short.
Capitalizing on this insight, we introduce ChangeViT, a framework that adopts a
plain ViT backbone to enhance the performance of large-scale changes. This
framework is supplemented by a detail-capture module that generates detailed
spatial features and a feature injector that efficiently integrates
fine-grained spatial information into high-level semantic learning. The feature
integration ensures that ChangeViT excels in both detecting large-scale changes
and capturing fine-grained details, providing comprehensive change detection
across diverse scales. Without bells and whistles, ChangeViT achieves
state-of-the-art performance on three popular high-resolution datasets (i.e.,
LEVIR-CD, WHU-CD, and CLCD) and one low-resolution dataset (i.e., OSCD), which
underscores the unleashed potential of plain ViTs for change detection.
Furthermore, thorough quantitative and qualitative analyses validate the
efficacy of the introduced modules, solidifying the effectiveness of our
approach. The source code is available at
https://github.com/zhuduowang/ChangeViT.",http://arxiv.org/abs/2406.12847v1
"Continuous Urban Change Detection from Satellite Image Time Series with
  Temporal Feature Refinement and Multi-Task Integration",2024-06-25T10:53:57Z,"Sebastian Hafner, Heng Fang, Hossein Azizpour, Yifang Ban","Urbanization advances at unprecedented rates, resulting in negative effects
on the environment and human well-being. Remote sensing has the potential to
mitigate these effects by supporting sustainable development strategies with
accurate information on urban growth. Deep learning-based methods have achieved
promising urban change detection results from optical satellite image pairs
using convolutional neural networks (ConvNets), transformers, and a multi-task
learning setup. However, transformers have not been leveraged for urban change
detection with multi-temporal data, i.e., >2 images, and multi-task learning
methods lack integration approaches that combine change and segmentation
outputs. To fill this research gap, we propose a continuous urban change
detection method that identifies changes in each consecutive image pair of a
satellite image time series (SITS). Specifically, we propose a temporal feature
refinement (TFR) module that utilizes self-attention to improve ConvNet-based
multi-temporal building representations. Furthermore, we propose a multi-task
integration (MTI) module that utilizes Markov networks to find an optimal
building map time series based on segmentation and dense change outputs. The
proposed method effectively identifies urban changes based on high-resolution
SITS acquired by the PlanetScope constellation (F1 score 0.551) and Gaofen-2
(F1 score 0.440). Moreover, our experiments on two challenging datasets
demonstrate the effectiveness of the proposed method compared to bi-temporal
and multi-temporal urban change detection and segmentation methods.",http://arxiv.org/abs/2406.17458v2
Exploring the Capabilities of LLMs for Code Change Related Tasks,2024-07-03T05:49:18Z,"Lishui Fan, Jiakun Liu, Zhongxin Liu, David Lo, Xin Xia, Shanping Li","Developers deal with code-change-related tasks daily, e.g., reviewing code.
Pre-trained code and code-change-oriented models have been adapted to help
developers with such tasks. Recently, large language models (LLMs) have shown
their effectiveness in code-related tasks. However, existing LLMs for code
focus on general code syntax and semantics rather than the differences between
two code versions. Thus, it is an open question how LLMs perform on
code-change-related tasks.
  To answer this question, we conduct an empirical study using \textgreater 1B
parameters LLMs on three code-change-related tasks, i.e., code review
generation, commit message generation, and just-in-time comment update, with
in-context learning (ICL) and parameter-efficient fine-tuning (PEFT, including
LoRA and prefix-tuning). We observe that the performance of LLMs is poor
without examples and generally improves with examples, but more examples do not
always lead to better performance. LLMs tuned with LoRA have comparable
performance to the state-of-the-art small pre-trained models. Larger models are
not always better, but \textsc{Llama~2} and \textsc{Code~Llama} families are
always the best. The best LLMs outperform small pre-trained models on the code
changes that only modify comments and perform comparably on other code changes.
We suggest future work should focus more on guiding LLMs to learn the knowledge
specific to the changes related to code rather than comments for
code-change-related tasks.",http://arxiv.org/abs/2407.02824v1
"An Empirical Analysis of Git Commit Logs for Potential Inconsistency in
  Code Clones",2024-09-13T06:14:50Z,"Reishi Yokomori, Katsuro Inoue","Code clones are code snippets that are identical or similar to other snippets
within the same or different files. They are often created through
copy-and-paste practices and modified during development and maintenance
activities. Since a pair of code clones, known as a clone pair, has a possible
logical coupling between them, it is expected that changes to each snippet are
made simultaneously (co-changed) and consistently. There is extensive research
on code clones, including studies related to the co-change of clones; however,
detailed analysis of commit logs for code clone pairs has been limited.
  In this paper, we investigate the commit logs of code snippets from clone
pairs, using the git-log command to extract changes to cloned code snippets. We
analyzed 45 repositories owned by the Apache Software Foundation on GitHub and
addressed three research questions regarding commit frequency, co-change ratio,
and commit patterns. Our findings indicate that (1) on average, clone snippets
are changed infrequently, typically only two or three times throughout their
lifetime, (2) the ratio of co-changes is about half of all clone changes, with
10-20\% of co-changed commits being concerning (potentially inconsistent), and
(3) 35-65\% of all clone pairs being classified as concerning clone pairs
(potentially inconsistent clone pairs). These results suggest the need for a
consistent management system through the commit timeline of clones.",http://arxiv.org/abs/2409.08555v1
"ChARLES: Change-Aware Recovery of Latent Evolution Semantics in
  Relational Data",2024-09-27T02:05:02Z,"Shiyi He, Alexandra Meliou, Anna Fariha","Data-driven decision-making is at the core of many modern applications, and
understanding the data is critical in supporting trust in these decisions.
However, data is dynamic and evolving, just like the real-world entities it
represents. Thus, an important component of understanding data is analyzing and
drawing insights from the changes it undergoes. Existing methods for exploring
data change list differences exhaustively, which are not interpretable by
humans and lack salient insights regarding change trends. For example, an
explanation that semantically summarizes changes to highlight gender
disparities in performance rewards is more human-consumable than a long list of
employee salary changes. We demonstrate ChARLES, a system that derives semantic
summaries of changes between two snapshots of an evolving database, in an
effective, concise, and interpretable way. Our key observation is that, while
datasets often evolve through point and other small-batch updates, rich data
features can reveal latent semantics that can intuitively summarize the
changes. Under the hood, ChARLES compares database versions, infers feasible
transformations by fitting multiple regression lines over different data
partitions to derive change summaries, and ranks them. ChARLES allows users to
customize it to obtain their preferred explanation by navigating the
accuracy-interpretability tradeoff, and offers a proof of concept for reasoning
about data evolution over real-world datasets.",http://arxiv.org/abs/2409.18386v1
"A Robust Topological Framework for Detecting Regime Changes in
  Multi-Trial Experiments with Application to Predictive Maintenance",2024-10-27T13:40:39Z,"Anass B. El-Yaagoubi, Jean-Marc Freyermuth, Hernando Ombao","We present a general and flexible framework for detecting regime changes in
complex, non-stationary data across multi-trial experiments. Traditional change
point detection methods focus on identifying abrupt changes within a single
time series (single trial), targeting shifts in statistical properties such as
the mean, variance, and spectrum over time within that sole trial. In contrast,
our approach considers changes occurring across trials, accommodating changes
that may arise within individual trials due to experimental inconsistencies,
such as varying delays or event duration. By leveraging diverse metrics to
analyze time-frequency characteristics specifically topological changes in the
spectrum and spectrograms, our approach offers a comprehensive framework for
detecting such variations. Our approach can handle different scenarios with
various statistical assumptions, including varying levels of stationarity
within and across trials, making our framework highly adaptable. We validate
our approach through simulations using time-varying autoregressive processes
that exhibit different regime changes. Our results demonstrate the
effectiveness of detecting changes across trials under diverse conditions.
Furthermore, we illustrate the effectiveness of our method by applying it to
predictive maintenance using the NASA bearing dataset. By analyzing the
time-frequency characteristics of vibration signals recorded by accelerometers,
our approach accurately identifies bearing failures, showcasing its strong
potential for early fault detection in mechanical systems.",http://arxiv.org/abs/2410.20443v1
"Predicting the Impact of Scope Changes on Project Cost and Schedule
  Using Machine Learning Techniques",2024-12-02T23:54:00Z,Soheila Sadeghi,"In the dynamic landscape of project management, scope changes are an
inevitable reality that can significantly impact project performance. These
changes, whether initiated by stakeholders, external factors, or internal
project dynamics, can lead to cost overruns and schedule delays. Accurately
predicting the consequences of these changes is crucial for effective project
control and informed decision-making. This study aims to develop predictive
models to estimate the impact of scope changes on project cost and schedule
using machine learning techniques. The research utilizes a comprehensive
dataset containing detailed information on project tasks, including the Work
Breakdown Structure (WBS), task type, productivity rate, estimated cost, actual
cost, duration, task dependencies, scope change magnitude, and scope change
timing. Multiple machine learning models are developed and evaluated to predict
the impact of scope changes on project cost and schedule. These models include
Linear Regression, Decision Tree, Ridge Regression, Random Forest, Gradient
Boosting, and XGBoost. The dataset is split into training and testing sets, and
the models are trained using the preprocessed data. Model robustness and
generalization are assessed using cross-validation techniques. To evaluate the
performance of models, we use Mean Squared Error (MSE) and R2. Residual plots
are generated to assess the goodness of fit and identify any patterns or
outliers. Hyperparameter tuning is performed to optimize the XGBoost model and
improve its predictive accuracy. The study identifies the most influential
project attributes in determining the magnitude of cost and schedule deviations
caused by scope modifications. It is identified that productivity rate, scope
change magnitude, task dependencies, estimated cost, actual cost, duration, and
specific WBS elements are powerful predictors.",http://arxiv.org/abs/2412.02041v1
The Metaplectic Representation is Faithful,2024-01-09T14:36:31Z,"Christopher Chang, Simeon Hellsten, Mario Marcos Losada, Sergiu Novac","We develop methods to show that infinite-dimensional modules over the Iwasawa
algebra $KG$ of a uniform pro-p group are faithful and apply them to show that
the metaplectic representation for the symplectic group is faithful.",http://arxiv.org/abs/2401.04581v2
On a sign-change conjecture of Schlosser and Zhou,2024-06-05T16:54:07Z,"Kathrin Bringmann, Bernhard Heim, Ben Kane","In this paper, we investigate the signs changes of Fourier coefficients of
infinite products of $q$-series of Rogers--Ramanujan type. In particular, we
prove a conjecture made by Schlosser--Zhou pertaining to such sign changes for
products of modulus $10$.",http://arxiv.org/abs/2406.03453v2
Genus non-increasing totally positive unknotting number,2024-06-21T07:25:02Z,Tetsuya Ito,"The genus non-increasing totally positive unknotting number is the minimum
number of crossing changes that transform a knot into the unknot, such that all
the crossing changes are positive-to-negative crossing changes that do not
increase the genus. We show that the genus non-increasing totally positive
unknotting number can be arbitrary large for genus one knots.",http://arxiv.org/abs/2406.14918v1
A change-point problem for $m$-dependent multivariate random field,2024-06-27T15:55:50Z,"Vitalii Makogin, Duc Nguyen","In this paper, we consider a change-point problem for a centered, stationary
and $m$-dependent multivariate random field. Under the distribution free
assumption, a change-point test using CUSUM statistic is proposed to detect
anomalies within a multidimensional random field, controlling the false
positive rate as well as the Family-Wise Error in the multiple hypotheses
testing context.",http://arxiv.org/abs/2406.19282v1
"Collaborative Design and Planning of Software Architecture Changes via
  Software City Visualization",2024-08-15T13:37:41Z,"Alexander Krause-Glau, Malte Hansen, Wilhelm Hasselbring","Developers usually use diagrams and source code to jointly discuss and plan
software architecture changes. With this poster, we present our on-going work
on a novel approach that enables developers to collaboratively use software
city visualization to design and plan software architecture changes.",http://arxiv.org/abs/2408.16777v1
"Dynamic embedded topic models and change-point detection for exploring
  literary-historical hypotheses",2024-01-25T02:50:03Z,"Hale Sirin, Tom Lippincott","We present a novel combination of dynamic embedded topic models and
change-point detection to explore diachronic change of lexical semantic
modality in classical and early Christian Latin. We demonstrate several methods
for finding and characterizing patterns in the output, and relating them to
traditional scholarship in Comparative Literature and Classics. This simple
approach to unsupervised models of semantic change can be applied to any
suitable corpus, and we conclude with future directions and refinements aiming
to allow noisier, less-curated materials to meet that threshold.",http://arxiv.org/abs/2401.13905v1
Signature change by an isomorphism of spectral triples,2024-02-08T17:15:42Z,Gaston Nieuviarts,"We present a duality between spectral triples and pseudo-Riemannian spectral
triples. This duality is based on the fundamental connection between twists and
Krein products. A concept of isomorphism of spectral triples is introduced,
transforming one spectral triple to the dual one. In the case of
even-dimensional manifolds, we show how this implements a signature change
through the parity operator, given by the twist itself. The signature change
transformation is then only parametrized by one operator, given by the unitary
that implements the twist. This unitary is a central ingredient of the
approach, being directly connected to the Krein product, the twist and the
parity operator that implement the signature change.",http://arxiv.org/abs/2402.05839v2
A Change Detection Reality Check,2024-02-10T17:02:53Z,"Isaac Corley, Caleb Robinson, Anthony Ortiz","In recent years, there has been an explosion of proposed change detection
deep learning architectures in the remote sensing literature. These approaches
claim to offer state-of-the-art performance on different standard benchmark
datasets. However, has the field truly made significant progress? In this paper
we perform experiments which conclude a simple U-Net segmentation baseline
without training tricks or complicated architectural changes is still a top
performer for the task of change detection.",http://arxiv.org/abs/2402.06994v2
"Stability of multiphase mean curvature flow beyond circular topology
  changes",2024-04-03T17:38:04Z,"Julian Fischer, Sebastian Hensel, Alice Marveggio, Maximilian Moser","We prove a weak-strong uniqueness principle for varifold-BV solutions to
planar multiphase mean curvature flow beyond a circular topology change:
Assuming that there exists a classical solution with an interface that becomes
increasingly circular and shrinks to a point, any varifold-BV solution with the
same initial interface must coincide with it, and any varifold-BV solution with
similar initial data must undergo the same type of topology change. Our result
illustrates the robustness of the relative energy method for establishing
weak-strong uniqueness principles for interface evolution equations, showing
that it may also be applied beyond certain topological changes.",http://arxiv.org/abs/2404.02884v1
The Multiple Change-in-Gaussian-Mean Problem,2024-05-10T20:09:52Z,"Paul Fearnhead, Piotr Fryzlewicz","A manuscript version of the chapter ""The Multiple Change-in-Gaussian-Mean
Problem"" from the book ""Change-Point Detection and Data Segmentation"" by
Fearnhead and Fryzlewicz, currently in preparation. All R code and data to
accompany this chapter and the book are gradually being made available through
https://github.com/pfryz/cpdds.",http://arxiv.org/abs/2405.06796v1
The Size-Change Principle for Mixed Inductive and Coinductive types,2024-07-08T08:19:07Z,Pierre Hyvernat,"This paper shows how to use Lee, Jones and Ben Amram's size-change principle
to check correctness of arbitrary recursive definitions in an ML / Haskell like
programming language with inductive and coinductive types.Naively using the
size-change principle to check productivity and termination is straightforward
but unsound when inductive and coinductive types arenested. We can however
adapt the size-change principle to check ``totality'', which corresponds
exactly to correctness with respect to the corresponding (co)inductive type.",http://arxiv.org/abs/2407.05715v1
Query maintenance under batch changes with small-depth circuits,2024-07-29T14:11:32Z,"Samir Datta, Asif Khan, Anish Mukherjee, Felix Tschirbs, Nils Vortmeier, Thomas Zeume","Which dynamic queries can be maintained efficiently? For constant-size
changes, it is known that constant-depth circuits or, equivalently, first-order
updates suffice for maintaining many important queries, among them
reachability, tree isomorphism, and the word problem for context-free
languages. In other words, these queries are in the dynamic complexity class
DynFO. We show that most of the existing results for constant-size changes can
be recovered for batch changes of polylogarithmic size if one allows circuits
of depth O(log log n) or, equivalently, first-order updates that are iterated
O(log log n) times.",http://arxiv.org/abs/2407.20031v1
Unveiling Parkinson's Disease-like Changes Triggered by Spaceflight,2024-08-25T05:56:57Z,"Nilufar Ali, Afshin Beheshti, Greg Hampikian","A meta-analysis of spaceflight data from both mouse and human flights reveals
a striking overlap with Parkinson's disease (PD). Parallels include: changes in
gait, loss of dopamine, sustained changes in the basal ganglia, loss of
tyrosine hydroxylase in the substantia nigra, and systemic mitochondrial
dysfunction. We identified specific Parkinson's genes differentially expressed
post-spaceflight. These evidences indicate that spaceflight stressor-induced
changes in the brain may become permanent during deep space exploration, posing
a risk of PD in astronauts",http://arxiv.org/abs/2408.15021v1
Minimising changes to audit when updating decision trees,2024-08-29T07:48:55Z,"Anj Simmons, Scott Barnett, Anupam Chaudhuri, Sankhya Singh, Shangeetha Sivasothy","Interpretable models are important, but what happens when the model is
updated on new training data? We propose an algorithm for updating a decision
tree while minimising the number of changes to the tree that a human would need
to audit. We achieve this via a greedy approach that incorporates the number of
changes to the tree as part of the objective function. We compare our algorithm
to existing methods and show that it sits in a sweet spot between final
accuracy and number of changes to audit.",http://arxiv.org/abs/2408.16321v1
"Dynamic Bayesian Networks with Conditional Dynamics in Edge Addition and
  Deletion",2024-09-13T16:30:37Z,"Lupe S. H. Chan, Amanda M. Y. Chu, Mike K. P. So","This study presents a dynamic Bayesian network framework that facilitates
intuitive gradual edge changes. We use two conditional dynamics to model the
edge addition and deletion, and edge selection separately. Unlike previous
research that uses a mixture network approach, which restricts the number of
possible edge changes, or structural priors to induce gradual changes, which
can lead to unclear network evolution, our model induces more frequent and
intuitive edge change dynamics. We employ Markov chain Monte Carlo (MCMC)
sampling to estimate the model structures and parameters and demonstrate the
model's effectiveness in a portfolio selection application.",http://arxiv.org/abs/2409.08965v1
"Moving sum procedure for multiple change point detection in large factor
  models",2024-10-03T19:11:19Z,"Matteo Barigozzi, Haeran Cho, Lorenzo Trapani","The paper proposes a moving sum methodology for detecting multiple change
points in high-dimensional time series under a factor model, where changes are
attributed to those in loadings as well as emergence or disappearance of
factors. We establish the asymptotic null distribution of the proposed test for
family-wise error control, and show the consistency of the procedure for
multiple change point estimation. Simulation studies and an application to a
large dataset of volatilities demonstrate the competitive performance of the
proposed method.",http://arxiv.org/abs/2410.02918v1
"Exploring Foundation Models in Remote Sensing Image Change Detection: A
  Comprehensive Survey",2024-10-10T11:16:05Z,"Zihan Yu, Tianxiao Li, Yuxin Zhu, Rongze Pan","Change detection, as an important and widely applied technique in the field
of remote sensing, aims to analyze changes in surface areas over time and has
broad applications in areas such as environmental monitoring, urban
development, and land use analysis.In recent years, deep learning, especially
the development of foundation models, has provided more powerful solutions for
feature extraction and data fusion, effectively addressing these complexities.
This paper systematically reviews the latest advancements in the field of
change detection, with a focus on the application of foundation models in
remote sensing tasks.",http://arxiv.org/abs/2410.07824v1
Physics Informed Neural Networks for heat conduction with phase change,2024-10-18T07:06:04Z,"Bahae-Eddine Madir, Francky Luddens, Corentin Lothod√©, Ionut Danaila","We study numerical algorithms to solve a specific Partial Differential
Equation (PDE), namely the Stefan problem, using Physics Informed Neural
Networks (PINNs). This problem describes the heat propagation in a liquid-solid
phase change system. It implies a heat equation and a discontinuity at the
interface where the phase change occurs. In the context of PINNs, this model
leads to difficulties in the learning process, especially near the interface of
phase change. We present different strategies that can be used in this context.
We illustrate our results and compare with classical solvers for PDEs (finite
differences).",http://arxiv.org/abs/2410.14216v1
Changes-In-Changes For Discrete Treatment,2024-11-03T16:07:26Z,Onil Boussim,"This paper generalizes the changes-in-changes (CIC) model to handle discrete
treatments with more than two categories, extending the binary case of Athey
and Imbens (2006). While the original CIC model is well-suited for binary
treatments, it cannot accommodate multi-category discrete treatments often
found in economic and policy settings. Although recent work has extended CIC to
continuous treatments, there remains a gap for multi-category discrete
treatments. I introduce a generalized CIC model that adapts the rank invariance
assumption to multiple treatment levels, allowing for robust modeling while
capturing the distinct effects of varying treatment intensities.",http://arxiv.org/abs/2411.01617v1
"The Thousand-Pulsar-Array programme on MeerKAT -- XII. Discovery of
  long-term pulse profile evolution in 7 young pulsars",2024-02-14T10:22:58Z,"A. Basu, P. Weltevrede, M. J. Keith, S. Johnston, A. Karastergiou, L. S. Oswald, B. Posselt, X. Song, A. D. Cameron","A number of pulsars are known to have profile evolution on timescales of
months, often correlated with spin-down rate changes. Here, we present the
first result from 3 years of monitoring observations from MeerKAT as part of
the Thousand Pulsar Array programme. This programme obtains high-fidelity pulse
profiles for $\sim$ 500 pulsars, which enabled the detection of subtle changes
in seven sources not previously known to exhibit long-term profile evolution. A
2D Gaussian convolution is used to highlight correlated emission variability in
both the pulse phase and observing epoch direction. Simulations show that for
one additional source the observed profile variability is likely to originate
from stochastic single-pulse shape variability (jitter). We find that it is
common for long-term profile variability to be associated with changes in
polarization fractions, but not with polarisation position angle (PA) changes.
PA changes are expected if emission height changes or precession is responsible
for the profile variability. PSR J1741$-$3927 is the only pulsar in our sample
that shows correlated PA variability, and this is associated with orthogonal
polarization mode activity. For the six %the rest, without correlated PA
variability, other pulsars limits on possible emission height changes and
impact angle changes are derived. These limits are consistent with the small
changes in the total intensity profile shape. None of the sources show
detectable spin-down variability correlated with the emission changes, which
are thought to be driven by magnetospheric current fluctuations. Therefore the
absence of correlated spin-down rate variability allows upper limits to be
placed on changes in the magnetospheric charge density.",http://arxiv.org/abs/2402.09065v1
"Solutions to some sign change problems on the functions involving sums
  of divisors",2024-01-18T09:56:55Z,"Yuchen Ding, Hao Pan, Yu--Chen Sun","In this note, we solve some sign change problems on the functions involving
sums of divisors posed by Pongsriiam recently.",http://arxiv.org/abs/2401.09842v1
"Designing for Sustained Motivation: A Review of Self-Determination
  Theory in Behaviour Change Technologies",2024-01-31T19:00:08Z,"Lize Alberts, Ulrik Lyngs, Kai Lukoff","Recent years have seen a surge in applications and technologies aimed at
motivating users to achieve personal goals and improve their wellbeing.
However, these often fail to promote long-term behaviour change, and sometimes
even backfire. We consider how self-determination theory (SDT), a metatheory of
human motivation and wellbeing, can help explain why such technologies fail,
and how they may better help users internalise the motivation behind their
goals and make enduring changes in their behaviour. In this work, we
systematically reviewed 15 papers in the ACM Digital Library that apply SDT to
the design of behaviour change technologies (BCTs). We identified 50
suggestions for design features in BCTs, grounded in SDT, that researchers have
applied to enhance user motivation. However, we find that SDT is often
leveraged to optimise engagement with the technology itself rather than with
the targeted behaviour change per se. When interpreted through the lens of SDT,
the implication is that BCTs may fail to cultivate sustained changes in
behaviour, as users' motivation depends on their enjoyment of the intervention,
which may wane over time. An underexplored opportunity remains for designers to
leverage SDT to support users to internalise the ultimate goals and value of
certain behaviour changes, enhancing their motivation to sustain these changes
in the long term.",http://arxiv.org/abs/2402.00121v1
"LaserSAM: Zero-Shot Change Detection Using Visual Segmentation of
  Spinning LiDAR",2024-02-15T20:46:52Z,"Alexander Krawciw, Sven Lilge, Timothy D. Barfoot","This paper presents an approach for applying camera perception techniques to
spinning LiDAR data. To improve the robustness of long-term change detection
from a 3D LiDAR, range and intensity information are rendered into virtual
perspectives using a pinhole camera model. Hue-saturation-value image encoding
is used to colourize the images by range and near-IR intensity. The LiDAR's
active scene illumination makes it invariant to ambient brightness, which
enables night-to-day change detection without additional processing. Using the
range-colourized, perspective image allows existing foundation models to detect
semantic regions. Specifically, the Segment Anything Model detects semantically
similar regions in both a previously acquired map and live view from a
path-repeating robot. By comparing the masks in both views, changes in the live
scan are detected. Results indicate that the Segment Anything Model accurately
captures the shape of arbitrary changes introduced into scenes. The proposed
method achieves a segmentation intersection over union of 73.3% when evaluated
in unstructured environments and 80.4% when evaluated within the planning
corridor. Changes can be detected reliably through day-to-night illumination
variations. After pixel-level masks are generated, the one-to-one
correspondence with 3D points means that the 2D masks can be used directly to
recover the 3D location of the changes. The detected 3D changes are avoided in
a closed loop by treating them as obstacles in a local motion planner.
Experiments on an unmanned ground vehicle demonstrate the performance of the
method.",http://arxiv.org/abs/2402.10321v2
OSCaR: Object State Captioning and State Change Representation,2024-02-27T01:48:19Z,"Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu","The capability of intelligent models to extrapolate and comprehend changes in
object states is a crucial yet demanding aspect of AI research, particularly
through the lens of human interaction in real-world settings. This task
involves describing complex visual environments, identifying active objects,
and interpreting their changes as conveyed through language. Traditional
methods, which isolate object captioning and state change detection, offer a
limited view of dynamic environments. Moreover, relying on a small set of
symbolic words to represent changes has restricted the expressiveness of the
language. To address these challenges, in this paper, we introduce the Object
State Captioning and State Change Representation (OSCaR) dataset and benchmark.
OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique
objects from various egocentric video collections. It sets a new testbed for
evaluating multimodal large language models (MLLMs). Our experiments
demonstrate that while MLLMs show some skill, they lack a full understanding of
object state changes. The benchmark includes a fine-tuned model that, despite
initial capabilities, requires significant improvements in accuracy and
generalization ability for effective understanding of these changes. Our code
and dataset are available at https://github.com/nguyennm1024/OSCaR.",http://arxiv.org/abs/2402.17128v4
Generating all time-changes preserving dynamical zeta functions,2024-03-20T19:17:46Z,"Sawian Jaidee, Jakub Byszewski, Thomas Ward","We construct a set of topological generators for the monoid of time-changes
preserving the space of dynamical zeta functions.",http://arxiv.org/abs/2403.13932v1
Relational Network Verification,2024-03-25T23:47:55Z,"Xieyang Xu, Yifei Yuan, Zachary Kincaid, Arvind Krishnamurthy, Ratul Mahajan, David Walker, Ennan Zhai","Relational network verification is a new approach to validating network
changes. In contrast to traditional network verification, which analyzes
specifications for a single network snapshot, relational network verification
analyzes specifications concerning two network snapshots (e.g., pre- and
post-change snapshots) and captures their similarities and differences.
Relational change specifications are compact and precise because they specify
the flows or paths that change between snapshots and then simply mandate that
other behaviors of the network ""stay the same"", without enumerating them. To
achieve similar guarantees, single-snapshot specifications need to enumerate
all flow and path behaviors that are not expected to change, so we can check
that nothing has accidentally changed. Thus, precise single-snapshot
specifications are proportional to network size, which makes them impractical
to generate for many real-world networks.
  To demonstrate the value of relational reasoning, we develop a high-level
relational specification language and a tool called Rela to validate network
changes. Rela first compiles input specifications and network snapshot
representations to finite state transducers. It then checks compliance using
decision procedures for automaton equivalence. Our experiments using data on
complex changes to a global backbone (with over 10^3 routers) find that Rela
specifications need fewer than 10 terms for 93% of them and it validates 80% of
them within 20 minutes.",http://arxiv.org/abs/2403.17277v1
"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions
  with Large Language Models",2024-03-27T08:34:55Z,"Mingxing Peng, Xusen Guo, Xianda Chen, Meixin Zhu, Kehua Chen","To ensure safe driving in dynamic environments, autonomous vehicles should
possess the capability to accurately predict lane change intentions of
surrounding vehicles in advance and forecast their future trajectories.
Existing motion prediction approaches have ample room for improvement,
particularly in terms of long-term prediction accuracy and interpretability. In
this paper, we address these challenges by proposing LC-LLM, an explainable
lane change prediction model that leverages the strong reasoning capabilities
and self-explanation abilities of Large Language Models (LLMs). Essentially, we
reformulate the lane change prediction task as a language modeling problem,
processing heterogeneous driving scenario information as natural language
prompts for LLMs and employing supervised fine-tuning to tailor LLMs
specifically for lane change prediction task. Additionally, we finetune the
Chain-of-Thought (CoT) reasoning to improve prediction transparency and
reliability, and include explanatory requirements in the prompts during
inference stage. Therefore, our LC-LLM model not only predicts lane change
intentions and trajectories but also provides CoT reasoning and explanations
for its predictions, enhancing its interpretability. Extensive experiments
based on the large-scale highD dataset demonstrate the superior performance and
interpretability of our LC-LLM in lane change prediction task. To the best of
our knowledge, this is the first attempt to utilize LLMs for predicting lane
change behavior. Our study shows that LLMs can effectively encode comprehensive
interaction information for driving behavior understanding.",http://arxiv.org/abs/2403.18344v2
"MFDS-Net: Multi-Scale Feature Depth-Supervised Network for Remote
  Sensing Change Detection with Global Semantic and Detail Information",2024-05-02T07:44:11Z,"Zhenyang Huang, Zhaojin Fu, Song Jintao, Genji Yuan, Jinjiang Li","Change detection as an interdisciplinary discipline in the field of computer
vision and remote sensing at present has been receiving extensive attention and
research. Due to the rapid development of society, the geographic information
captured by remote sensing satellites is changing faster and more complex,
which undoubtedly poses a higher challenge and highlights the value of change
detection tasks. We propose MFDS-Net: Multi-Scale Feature Depth-Supervised
Network for Remote Sensing Change Detection with Global Semantic and Detail
Information (MFDS-Net) with the aim of achieving a more refined description of
changing buildings as well as geographic information, enhancing the
localisation of changing targets and the acquisition of weak features. To
achieve the research objectives, we use a modified ResNet_34 as backbone
network to perform feature extraction and DO-Conv as an alternative to
traditional convolution to better focus on the association between feature
information and to obtain better training results. We propose the Global
Semantic Enhancement Module (GSEM) to enhance the processing of high-level
semantic information from a global perspective. The Differential Feature
Integration Module (DFIM) is proposed to strengthen the fusion of different
depth feature information, achieving learning and extraction of differential
features. The entire network is trained and optimized using a deep supervision
mechanism.
  The experimental outcomes of MFDS-Net surpass those of current mainstream
change detection networks. On the LEVIR dataset, it achieved an F1 score of
91.589 and IoU of 84.483, on the WHU dataset, the scores were F1: 92.384 and
IoU: 86.807, and on the GZ-CD dataset, the scores were F1: 86.377 and IoU:
76.021. The code is available at https://github.com/AOZAKIiii/MFDS-Net",http://arxiv.org/abs/2405.01065v1
"Achieving Narrative Change Through AR: Displacing the Single Story to
  Create Spatial Justice",2024-05-05T15:31:08Z,Janice Tisha Samuels,"The ability of Augmented Reality to overcome the bias of single stories
through multidimensionality is explored in the artifacts of a youth gun
violence prevention project and its goal of narrative change.",http://arxiv.org/abs/2405.02971v1
"Cluster Magnification, Root Capacity, Unique Chains and Base Change",2024-05-10T21:55:40Z,"Chandrasheel Bhagwat, Shubham Jaiswal","This article is inspired from the work of M Krithika and P Vanchinathan on
Cluster Magnification and the work of Alexander Perlis on Cluster Size. We
establish the existence of polynomials for given degree and cluster size over
number fields which generalises a result of Perlis. We state the Strong cluster
magnification problem and establish an equivalent criterion for that. We also
discuss the notion of weak cluster magnification and prove some properties. We
provide an important example answering a question about Cluster Towers. We
introduce the concept of Root capacity and prove some of its properties. We
also introduce the concept of unique descending and ascending chains for
extensions and establish some properties and explicitly compute some
interesting examples. Finally we establish results about all these phenomena
under a particular type of base change. The article concludes with results
about strong cluster magnification and unique chains and some properties of the
ascending index for a field extension.",http://arxiv.org/abs/2405.06825v3
Anticipating Object State Changes in Long Procedural Videos,2024-05-21T13:40:30Z,"Victoria Manousaki, Konstantinos Bacharidis, Filippos Gouidis, Konstantinos Papoutsakis, Dimitris Plexousakis, Antonis Argyros","In this work, we introduce (a) the new problem of anticipating object state
changes in images and videos during procedural activities, (b) new curated
annotation data for object state change classification based on the Ego4D
dataset, and (c) the first method for addressing this challenging problem.
Solutions to this new task have important implications in vision-based scene
understanding, automated monitoring systems, and action planning. The proposed
novel framework predicts object state changes that will occur in the near
future due to yet unseen human actions by integrating learned visual features
that represent recent visual information with natural language (NLP) features
that represent past object state changes and actions. Leveraging the extensive
and challenging Ego4D dataset which provides a large-scale collection of
first-person perspective videos across numerous interaction scenarios, we
introduce an extension noted Ego4D-OSCA that provides new curated annotation
data for the object state change anticipation task (OSCA). An extensive
experimental evaluation is presented demonstrating the proposed method's
efficacy in predicting object state changes in dynamic scenarios. The
performance of the proposed approach also underscores the potential of
integrating video and linguistic cues to enhance the predictive performance of
video understanding systems and lays the groundwork for future research on the
new task of object state change anticipation. The source code and the new
annotation data (Ego4D-OSCA) will be made publicly available.",http://arxiv.org/abs/2405.12789v3
"ComFace: Facial Representation Learning with Synthetic Data for
  Comparing Faces",2024-05-25T02:44:07Z,"Yusuke Akamatsu, Terumi Umematsu, Hitoshi Imaoka, Shizuko Gomi, Hideo Tsurushima","Daily monitoring of intra-personal facial changes associated with health and
emotional conditions has great potential to be useful for medical, healthcare,
and emotion recognition fields. However, the approach for capturing
intra-personal facial changes is relatively unexplored due to the difficulty of
collecting temporally changing face images. In this paper, we propose a facial
representation learning method using synthetic images for comparing faces,
called ComFace, which is designed to capture intra-personal facial changes. For
effective representation learning, ComFace aims to acquire two feature
representations, i.e., inter-personal facial differences and intra-personal
facial changes. The key point of our method is the use of synthetic face images
to overcome the limitations of collecting real intra-personal face images.
Facial representations learned by ComFace are transferred to three extensive
downstream tasks for comparing faces: estimating facial expression changes,
weight changes, and age changes from two face images of the same individual.
Our ComFace, trained using only synthetic data, achieves comparable to or
better transfer performance than general pre-training and state-of-the-art
representation learning methods trained using real images.",http://arxiv.org/abs/2405.16016v2
Context-aware Difference Distilling for Multi-change Captioning,2024-05-31T14:07:39Z,"Yunbin Tu, Liang Li, Li Su, Zheng-Jun Zha, Chenggang Yan, Qingming Huang","Multi-change captioning aims to describe complex and coupled changes within
an image pair in natural language. Compared with single-change captioning, this
task requires the model to have higher-level cognition ability to reason an
arbitrary number of changes. In this paper, we propose a novel context-aware
difference distilling (CARD) network to capture all genuine changes for
yielding sentences. Given an image pair, CARD first decouples context features
that aggregate all similar/dissimilar semantics, termed common/difference
context features. Then, the consistency and independence constraints are
designed to guarantee the alignment/discrepancy of common/difference context
features. Further, the common context features guide the model to mine locally
unchanged features, which are subtracted from the pair to distill locally
difference features. Next, the difference context features augment the locally
difference features to ensure that all changes are distilled. In this way, we
obtain an omni-representation of all changes, which is translated into
linguistic sentences by a transformer decoder. Extensive experiments on three
public datasets show CARD performs favourably against state-of-the-art
methods.The code is available at https://github.com/tuyunbin/CARD.",http://arxiv.org/abs/2405.20810v2
Evaluation of Temporal Change in IR Test Collections,2024-07-01T15:25:31Z,"J√ºri Keller, Timo Breuer, Philipp Schaer","Information retrieval systems have been evaluated using the Cranfield
paradigm for many years. This paradigm allows a systematic, fair, and
reproducible evaluation of different retrieval methods in fixed experimental
environments. However, real-world retrieval systems must cope with dynamic
environments and temporal changes that affect the document collection, topical
trends, and the individual user's perception of what is considered relevant.
Yet, the temporal dimension in IR evaluations is still understudied.
  To this end, this work investigates how the temporal generalizability of
effectiveness evaluations can be assessed. As a conceptual model, we generalize
Cranfield-type experiments to the temporal context by classifying the change in
the essential components according to the create, update, and delete operations
of persistent storage known from CRUD. From the different types of change
different evaluation scenarios are derived and it is outlined what they imply.
Based on these scenarios, renowned state-of-the-art retrieval systems are
tested and it is investigated how the retrieval effectiveness changes on
different levels of granularity.
  We show that the proposed measures can be well adapted to describe the
changes in the retrieval results. The experiments conducted confirm that the
retrieval effectiveness strongly depends on the evaluation scenario
investigated. We find that not only the average retrieval performance of single
systems but also the relative system performance are strongly affected by the
components that change and to what extent these components changed.",http://arxiv.org/abs/2407.01373v1
Causal Discovery-Driven Change Point Detection in Time Series,2024-07-10T00:54:42Z,"Shanyun Gao, Raghavendra Addanki, Tong Yu, Ryan A. Rossi, Murat Kocaoglu","Change point detection in time series seeks to identify times when the
probability distribution of time series changes. It is widely applied in many
areas, such as human-activity sensing and medical science. In the context of
multivariate time series, this typically involves examining the joint
distribution of high-dimensional data: If any one variable changes, the whole
time series is assumed to have changed. However, in practical applications, we
may be interested only in certain components of the time series, exploring
abrupt changes in their distributions in the presence of other time series.
Here, assuming an underlying structural causal model that governs the
time-series data generation, we address this problem by proposing a two-stage
non-parametric algorithm that first learns parts of the causal structure
through constraint-based discovery methods. The algorithm then uses conditional
relative Pearson divergence estimation to identify the change points. The
conditional relative Pearson divergence quantifies the distribution disparity
between consecutive segments in the time series, while the causal discovery
method enables a focus on the causal mechanism, facilitating access to
independent and identically distributed (IID) samples. Theoretically, the
typical assumption of samples being IID in conventional change point detection
methods can be relaxed based on the Causal Markov Condition. Through
experiments on both synthetic and real-world datasets, we validate the
correctness and utility of our approach.",http://arxiv.org/abs/2407.07290v1
"On the precise quantification of the impact of a single discretionary
  lane change on surrounding traffic",2024-07-26T07:11:03Z,"Kangning Hou, Jia Zou, Fangfang Zheng, Xiaobo Liu, Zhengbing He","Lane-changing is a critical maneuver of vehicle driving, and a comprehensive
understanding of its impact on traffic is essential for effective traffic
management and optimization. Unfortunately, existing studies fail to adequately
distinguish the impact of lane changes from those resulting from natural
traffic dynamics. Additionally, there is a lack of precise methods for
measuring the spatial extent and duration of the impact of a single
discretionary lane change, as well as a definitive metric to quantify the
overall spatiotemporal impact. To address these gaps, this study introduces a
quantitative indicator called the Corrected Travel Distance Bias (CTDB), which
accounts for variable speeds due to inherent traffic dynamics, providing a more
accurate assessment of lane-changing impacts. A comprehensive methodology is
developed to compare vehicle trajectory data before and after lane-changing
events, measuring both the magnitude and spatiotemporal extent of the
lane-changing impact. The results, based on the Zen traffic data from Japan,
indicate that the impact of a lane change in the target lane lasts an average
of 23.8 seconds, affecting approximately 5.6 vehicles, with a CTDB value of
-10.8 meters. In contrast, in the original lane, the impact lasts 25 seconds,
affects 5.3 vehicles, and yields a CTDB value of 4.7 meters.",http://arxiv.org/abs/2407.18557v1
"Reinforcement Learning from Human Feedback for Lane Changing of
  Autonomous Vehicles in Mixed Traffic",2024-08-08T13:18:53Z,"Yuting Wang, Lu Liu, Maonan Wang, Xi Xiong","The burgeoning field of autonomous driving necessitates the seamless
integration of autonomous vehicles (AVs) with human-driven vehicles, calling
for more predictable AV behavior and enhanced interaction with human drivers.
Human-like driving, particularly during lane-changing maneuvers on highways, is
a critical area of research due to its significant impact on safety and traffic
flow. Traditional rule-based decision-making approaches often fail to
encapsulate the nuanced boundaries of human behavior in diverse driving
scenarios, while crafting reward functions for learning-based methods
introduces its own set of complexities. This study investigates the application
of Reinforcement Learning from Human Feedback (RLHF) to emulate human-like
lane-changing decisions in AVs. An initial RL policy is pre-trained to ensure
safe lane changes. Subsequently, this policy is employed to gather data, which
is then annotated by humans to train a reward model that discerns lane changes
aligning with human preferences. This human-informed reward model supersedes
the original, guiding the refinement of the policy to reflect human-like
preferences. The effectiveness of RLHF in producing human-like lane changes is
demonstrated through the development and evaluation of conservative and
aggressive lane-changing models within obstacle-rich environments and mixed
autonomy traffic scenarios. The experimental outcomes underscore the potential
of RLHF to diversify lane-changing behaviors in AVs, suggesting its viability
for enhancing the integration of AVs into the fabric of human-driven traffic.",http://arxiv.org/abs/2408.04447v1
Periodic sign changes for weakly holomorphic $Œ∑$-quotients,2024-09-11T10:22:41Z,"Kathrin Bringmann, Guoniu Han, Bernhard Heim, Ben Kane","In this paper, we study sign changes of weakly holomorphic modular forms
which are given as $\eta$-quotients. We give representative examples for forms
of negative weight, weight zero, and positive weight.",http://arxiv.org/abs/2409.07164v1
"Structural Change, Employment, and Inequality in Europe: an Economic
  Complexity Approach",2024-10-10T13:36:33Z,"Bernardo Caldarola, Dario Mazzilli, Aurelio Patelli, Angelica Sbardella","Structural change consists of industrial diversification towards more
productive, knowledge intensive activities. However, changes in the productive
structure bear inherent links with job creation and income distribution. In
this paper, we investigate the consequences of structural change, defined in
terms of labour shifts towards more complex industries, on employment growth,
wage inequality, and functional distribution of income. The analysis is
conducted for European countries using data on disaggregated industrial
employment shares over the period 2010-2018. First, we identify patterns of
industrial specialisation by validating a country-industry industrial
employment matrix using a bipartite weighted configuration model (BiWCM).
Secondly, we introduce a country-level measure of labour-weighted Fitness,
which can be decomposed in such a way as to isolate a component that identifies
the movement of labour towards more complex industries, which we define as
structural change. Thirdly, we link structural change to i) employment growth,
ii) wage inequality, and iii) labour share of the economy. The results indicate
that our structural change measure is associated negatively with employment
growth. However, it is also associated with lower income inequality. As
countries move to more complex industries, they drop the least complex ones, so
the (low-paid) jobs in the least complex sectors disappear. Finally, structural
change predicts a higher labour ratio of the economy; however, this is likely
to be due to the increase in salaries rather than by job creation.",http://arxiv.org/abs/2410.07906v1
"On Efficient Topology Management in Service-Oriented 6G Networks: An
  Edge Video Distribution Case Study",2024-10-14T09:50:08Z,"Zied Ennaceur, Mounir Bensalem, Admela Jukan, Claus Keuker, Huanzhuo Wu, Rastin Pries","An efficient topology management in future 6G networks is one of the
fundamental challenges for a dynamic network creation based on location
services, whereby each autonomous network entity, i.e., a sub-network, can be
created for a specific application scenario. In this paper, we study the
performance of a novel topology changes management system in a sample 6G
network being dynamically organized in autonomous sub-networks. We propose and
analyze an algorithm for intelligent prediction of topology changes and provide
a comparative analysis with topology monitoring based approach. To this end, we
present an industrially relevant case study on edge video distribution, as it
is envisioned to be implemented in line with the 3GPP and ETSI MEC
(Multi-access Edge Computing) standards. For changes prediction, we implement
and analyze a novel topology change prediction algorithm, which can
automatically optimize, train and, finally, select the best of different
machine learning models available, based on the specific scenario under study.
For link change scenario, the results show that three selected ML models
exhibit high accuracy in detecting changes in link delay and bandwidth using
measured throughput and RTT. ANN demonstrates the best performance in
identifying cases with no changes, slightly outperforming random forest and
XGBoost. For user mobility scenario, XGBoost is more efficient in learning
patterns for topology change prediction while delivering much faster results
compared to the more computationally demanding deep learning models, such as
LSTM and CNN. In terms of cost efficiency, our ML-based approach represents a
significantly cost-effective alternative to traditional monitoring approaches.",http://arxiv.org/abs/2410.10338v2
Sign changes of Fourier coefficients for holomorphic eta-quotients,2024-10-15T06:34:39Z,"Kathrin Bringmann, Guoniu Han, Bernhard Heim, Ben Kane","In this paper we study sign changes of an infinite class of $\eta$-quotients
which are holomorphic modular forms. There is also a relation to Hurwitz class
numbers.",http://arxiv.org/abs/2410.11318v1
"Just-In-Time Software Defect Prediction via Bi-modal Change
  Representation Learning",2024-10-15T23:13:29Z,"Yuze Jiang, Beijun Shen, Xiaodong Gu","For predicting software defects at an early stage, researchers have proposed
just-in-time defect prediction (JIT-DP) to identify potential defects in code
commits. The prevailing approaches train models to represent code changes in
history commits and utilize the learned representations to predict the presence
of defects in the latest commit. However, existing models merely learn editions
in source code, without considering the natural language intentions behind the
changes. This limitation hinders their ability to capture deeper semantics. To
address this, we introduce a novel bi-modal change pre-training model called
BiCC-BERT. BiCC-BERT is pre-trained on a code change corpus to learn bi-modal
semantic representations. To incorporate commit messages from the corpus, we
design a novel pre-training objective called Replaced Message Identification
(RMI), which learns the semantic association between commit messages and code
changes. Subsequently, we integrate BiCC-BERT into JIT-DP and propose a new
defect prediction approach -- JIT-BiCC. By leveraging the bi-modal
representations from BiCC-BERT, JIT-BiCC captures more profound change
semantics. We train JIT-BiCC using 27,391 code changes and compare its
performance with 8 state-of-the-art JIT-DP approaches. The results demonstrate
that JIT-BiCC outperforms all baselines, achieving a 10.8% improvement in
F1-score. This highlights its effectiveness in learning the bi-modal semantics
for JIT-DP.",http://arxiv.org/abs/2410.12107v1
"HCDN: A Change Detection Network for Construction Housekeeping Using
  Feature Fusion and Large Vision Models",2024-10-23T02:36:47Z,"Kailai Sun, Zherui Shao, Yang Miang Goh, Jing Tian, Vincent J. L. Gan","Workplace safety has received increasing attention as millions of workers
worldwide suffer from work-related accidents. Despite poor housekeeping is a
significant contributor to construction accidents, there remains a significant
lack of technological research focused on improving housekeeping practices in
construction sites. Recognizing and locating poor housekeeping in a dynamic
construction site is an important task that can be improved through computer
vision approaches. Despite advances in AI and computer vision, existing methods
for detecting poor housekeeping conditions face many challenges, including
limited explanations, lack of locating of poor housekeeping, and lack of
annotated datasets. On the other hand, change detection which aims to detect
the changed environmental conditions (e.g., changing from good to poor
housekeeping) and 'where' the change has occurred (e.g., location of objects
causing poor housekeeping), has not been explored to the problem of
housekeeping management. To address these challenges, we propose the
Housekeeping Change Detection Network (HCDN), an advanced change detection
neural network that integrates a feature fusion module and a large vision
model, achieving state-of-the-art performance. Additionally, we introduce the
approach to establish a novel change detection dataset (named Housekeeping-CCD)
focused on housekeeping in construction sites, along with a housekeeping
segmentation dataset. Our contributions include significant performance
improvements compared to existing methods, providing an effective tool for
enhancing construction housekeeping and safety. To promote further development,
we share our source code and trained models for global researchers:
https://github.com/NUS-DBE/Housekeeping-CD.",http://arxiv.org/abs/2410.17513v1
Sharp Bounds for Neighborhood degree based indices of Graphs,2024-11-20T02:24:29Z,"Sanju Vaidya, Jeff Chang","In this paper, we will construct formulas and bounds for Neighborhood
Degree-based indices of graphs and describe graphs that attain the bounds.
Furthermore, we will establish a lower bound for the spectral radius of any
graph.",http://arxiv.org/abs/2411.12984v1
"Sign changes on $\sum \frac{f(n)}{\sqrt{n}}$ for random completely
  multiplicative $f$",2024-11-06T23:34:49Z,Rodrigo Angelo,"If $f: \mathbb{N} \rightarrow \{\pm1\}$ is a sample of the random completely
multiplicative function, we show that almost surely $\sum_{n \le x}
\frac{f(n)}{\sqrt{n}}$ changes signs infinitely many times, answering a
question of Aymone.",http://arxiv.org/abs/2411.14447v1
"Reinforcement Learning for Freeway Lane-Change Regulation via Connected
  Vehicles",2024-12-05T16:59:31Z,"Ke Sun, Huan Yu","Lane change decision-making is a complex task due to intricate
vehicle-vehicle and vehicle-infrastructure interactions. Existing algorithms
for lane-change control often depend on vehicles with a certain level of
autonomy (e.g., autonomous or connected autonomous vehicles). To address the
challenges posed by low penetration rates of autonomous vehicles and the high
costs of precise data collection, this study proposes a dynamic lane change
regulation design based on multi-agent reinforcement learning (MARL) to enhance
freeway traffic efficiency. The proposed framework leverages multi-lane
macroscopic traffic models that describe spatial-temporal dynamics of the
density and speed for each lane. Lateral traffic flow between adjacent lanes,
resulting from aggregated lane-changing behaviors, is modeled as source terms
exchanged between the partial differential equations (PDEs). We propose a lane
change regulation strategy using MARL, where one agent is placed at each
discretized lane grid. The state of each agent is represented by aggregated
vehicle attributes within its grid, generated from the SUMO microscopic
simulation environment. The agent's actions are lane-change regulations for
vehicles in its grid. Specifically, lane-change regulation signals are computed
at a centralized traffic management center and then broadcast to connected
vehicles in the corresponding lane grids. Compared to vehicle-level maneuver
control, this approach achieves a higher regulation rate by leveraging vehicle
connectivity while introducing no critical safety concerns, and accommodating
varying levels of connectivity and autonomy within the traffic system. The
proposed model is simulated and evaluated in varied traffic scenarios and
demand conditions. Experimental results demonstrate that the method improves
overall traffic efficiency with minimal additional energy consumption while
maintaining driving safety.",http://arxiv.org/abs/2412.04341v2
Modeling Processes of Neighborhood Change,2024-01-06T21:08:41Z,"J. Carlos Mart√≠nez Mori, Zhanzhan Zhao","An urban planner might design the spatial layout of transportation amenities
so as to improve accessibility for underserved communities -- a fairness
objective. However, implementing such a design might trigger processes of
neighborhood change that change who benefits from these amenities in the long
term. If so, has the planner really achieved their fairness objective? Can
algorithmic decision-making anticipate second order effects? In this paper, we
take a step in this direction by formulating processes of neighborhood change
as instances of no-regret dynamics; a collective learning process in which a
set of strategic agents rapidly reach a state of approximate equilibrium. We
mathematize concepts of neighborhood change to model the incentive structures
impacting individual dwelling-site decision-making. Our model accounts for
affordability, access to relevant transit amenities, community ties, and site
upkeep. We showcase our model with computational experiments that provide
semi-quantitative insights on the spatial economics of neighborhood change,
particularly on the influence of residential zoning policy and the placement of
transit amenities.",http://arxiv.org/abs/2401.03307v2
Double tunable metamaterial perfect absorber,2024-01-07T08:52:22Z,"Mahyar Radak, Saeed Mirzanejhad","This paper focuses on the simulation of a tunable metamaterial absorber
designed for the infrared region. Adsorbents offer three different mechanisms
to adjust their absorption characteristics. The first method involves changes
in temperature. When the temperature changes, the electrical conductivity of
the active material, vanadium dioxide (VO2), also changes. This transition
between insulating and conducting phases at different temperatures provides the
possibility to adjust the absorption spectrum of the metamaterial. By
exploiting the thermochromic properties of VO2, the conductivity of the
material can be dynamically adjusted over a wide range. Through temperature
control, the conductivity of VO2 changes from 200 S/m to 2*105 S/m, resulting
in a continuous adjustment of the absorption peak intensity. The second method
relies on applying an electric field. By applying an electric field in the
transverse direction of the lead zirconate titanate )PZT( material, the size of
the piezoelectric material undergoes changes and bending. Consequently, the
entire structure bends, leading to a change in the shape of the absorber. This
mechanism allows for precise control of the absorber's shape by applying
different voltages, which in turn enables the modification of the absorption
peak.",http://arxiv.org/abs/2401.03421v1
"BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method
  guided by multi-scale feature information aggregation",2024-01-09T02:53:06Z,"Yonghui Tan, Xiaolong Li, Yishu Chen, Jinquan Ai","The purpose of remote sensing image change detection (RSCD) is to detect
differences between bi-temporal images taken at the same place. Deep learning
has been extensively used to RSCD tasks, yielding significant results in terms
of result recognition. However, due to the shooting angle of the satellite, the
impacts of thin clouds, and certain lighting conditions, the problem of fuzzy
edges in the change region in some remote sensing photographs cannot be
properly handled using current RSCD algorithms. To solve this issue, we
proposed a Body Decouple Multi-Scale by fearure Aggregation change detection
(BD-MSA), a novel model that collects both global and local feature map
information in the channel and space dimensions of the feature map during the
training and prediction phases. This approach allows us to successfully extract
the change region's boundary information while also divorcing the change
region's main body from its boundary. Numerous studies have shown that the
assessment metrics and evaluation effects of the model described in this paper
on the publicly available datasets DSIFN-CD, S2Looking and WHU-CD are the best
when compared to other models.",http://arxiv.org/abs/2401.04330v2
(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection,2024-01-25T09:36:58Z,"Francesco Periti, Haim Dubossarsky, Nina Tahmasebi","In the universe of Natural Language Processing, Transformer-based language
models like BERT and (Chat)GPT have emerged as lexical superheroes with great
power to solve open research problems. In this paper, we specifically focus on
the temporal problem of semantic change, and evaluate their ability to solve
two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and
HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf
technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a
family of models that currently stand as the state-of-the-art for modeling
semantic change. Our experiments represent the first attempt to assess the use
of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT
performs significantly worse than the foundational GPT version. Furthermore,
our results demonstrate that (Chat)GPT achieves slightly lower performance than
BERT in detecting long-term changes but performs significantly worse in
detecting short-term changes.",http://arxiv.org/abs/2401.14040v3
"Automating Sound Change Prediction for Phylogenetic Inference: A
  Tukanoan Case Study",2024-02-02T17:20:16Z,"Kalvin Chang, Nathaniel R. Robinson, Anna Cai, Ting Chen, Annie Zhang, David R. Mortensen","We describe a set of new methods to partially automate linguistic
phylogenetic inference given (1) cognate sets with their respective protoforms
and sound laws, (2) a mapping from phones to their articulatory features and
(3) a typological database of sound changes. We train a neural network on these
sound change data to weight articulatory distances between phones and predict
intermediate sound change steps between historical protoforms and their modern
descendants, replacing a linguistic expert in part of a parsimony-based
phylogenetic inference algorithm. In our best experiments on Tukanoan
languages, this method produces trees with a Generalized Quartet Distance of
0.12 from a tree that used expert annotations, a significant improvement over
other semi-automated baselines. We discuss potential benefits and drawbacks to
our neural approach and parsimony-based tree prediction. We also experiment
with a minimal generalization learner for automatic sound law induction,
finding it comparably effective to sound laws from expert annotation. Our code
is publicly available at https://github.com/cmu-llab/aiscp.",http://arxiv.org/abs/2402.01582v1
"A Comprehensive Review of Machine Learning Advances on Data Change: A
  Cross-Field Perspective",2024-02-20T01:16:01Z,"Jeng-Lin Li, Chih-Fan Hsu, Ming-Ching Chang, Wei-Chao Chen","Recent artificial intelligence (AI) technologies show remarkable evolution in
various academic fields and industries. However, in the real world, dynamic
data lead to principal challenges for deploying AI models. An unexpected data
change brings about severe performance degradation in AI models. We identify
two major related research fields, domain shift and concept drift according to
the setting of the data change. Although these two popular research fields aim
to solve distribution shift and non-stationary data stream problems, the
underlying properties remain similar which also encourages similar technical
approaches. In this review, we regroup domain shift and concept drift into a
single research problem, namely the data change problem, with a systematic
overview of state-of-the-art methods in the two research fields. We propose a
three-phase problem categorization scheme to link the key ideas in the two
technical fields. We thus provide a novel scope for researchers to explore
contemporary technical strategies, learn industrial applications, and identify
future directions for addressing data change challenges.",http://arxiv.org/abs/2402.12627v1
The Physical Properties of Changing-look Blazars,2024-02-29T19:10:43Z,"Shi-Ju Kang, Bing Lyu, Qingwen Wu, Yong-Gang Zheng, Junhui Fan","Changing-look active galactic nuclei (AGNs) are a special class of AGNs that
change their spectral type from type 1 to type 2 or vice versa. In recent
years, a number of changing-look blazars (CLBs) were also reported, which
transition between flat-spectrum radio quasars and BL Lacs. The physical
properties of CLBs are still unclear. Using the $mclust$ R package for Gaussian
Mixture Modeling, we performed a clustering analysis for a sample of 105 CLBs
selected from the literature. Three kinds of analysis found that CLBs lie in
between the parameter distributions of FSRQs and BL Lacs: (i) univariate
analysis; (ii) bivariate analysis; and (iii) multivariate analysis, carried out
with a dimension reduction approach of the physical properties of the three
types of blazars. Our results suggest that CLBs belong to a transition type
between FSRQs and BL Lacs, which may be regulated by the change of accretion
process and may be similar to other changing-look AGNs.",http://arxiv.org/abs/2403.00078v1
"Sign-changing bubbling solutions for an exponential nonlinearity in
  $\mathbb{R}^2$",2024-03-12T13:25:22Z,Yibin Zhang,"Quite differently from those perturbation techniques in \cite{DM}, we use the
assumption of a $C^1$-stable critical point to construct positive or
sign-changing bubbling solutions to the boundary value problem $-\Delta
u=\lambda u|u|^{p-2}e^{|u|^p}$ with homogeneous Dirichlet boundary condition in
a bounded, smooth planar domain $\Omega$, when $0<p<2$ and $\lambda>0$ is a
small parameter. A sufficient condition on the intersection between the nodal
line of these sign-changing solutions and the boundary of the domain is
founded. Moreover, for $\lambda$ small enough, we prove that when $\Omega$ is
an arbitrary bounded domain, this problem has not only at least two pairs of
bubbling solutions which change sign exactly once and whose nodal lines
intersect the boundary, but also a bubbling solution which changes sign exactly
twice or three times; when $\Omega$ has an axial symmetry, this problem has a
bubbling solution which alternately changes sign arbitrarily many times along
the axis of symmetry through the domain.",http://arxiv.org/abs/2403.07641v2
"Strategic Responses to Technological Change: Evidence from ChatGPT and
  Upwork",2024-03-22T15:00:42Z,"Shun Yiu, Rob Seamans, Manav Raj, Ted Liu","AI technologies have the potential to affect labor market outcomes by both
increasing worker productivity and reducing the demand for certain skills or
tasks. Such changes may have important implications for the ways in which
workers seek jobs and position themselves. In this project, we examine how
freelancers change their strategic behavior on an online work platform
following the launch of ChatGPT in December 2022. We present evidence
suggesting that, in response to technological change, freelancers increase the
concentration of job applications across specializations and differentiate
themselves from both their past behavior and their peers. We show that such
changes are shaped by the extent to which a freelancer experiences growth in
demand or supply in the domains they specialize in. We document heterogeneity
in this effect across freelancer characteristics and document how strategic
repositioning can help mitigate the negative effects of technological change on
freelancer performance on the platform.",http://arxiv.org/abs/2403.15262v2
Model-free Change-point Detection Using Modern Classifiers,2024-04-10T13:19:28Z,"Rohit Kanrar, Feiyu Jiang, Zhanrui Cai","In contemporary data analysis, it is increasingly common to work with
non-stationary complex datasets. These datasets typically extend beyond the
classical low-dimensional Euclidean space, making it challenging to detect
shifts in their distribution without relying on strong structural assumptions.
This paper introduces a novel offline change-point detection method that
leverages modern classifiers developed in the machine-learning community. With
suitable data splitting, the test statistic is constructed through sequential
computation of the Area Under the Curve (AUC) of a classifier, which is trained
on data segments on both ends of the sequence. It is shown that the resulting
AUC process attains its maxima at the true change-point location, which
facilitates the change-point estimation. The proposed method is characterized
by its complete nonparametric nature, significant versatility, considerable
flexibility, and absence of stringent assumptions pertaining to the underlying
data or any distributional shifts. Theoretically, we derive the limiting
pivotal distribution of the proposed test statistic under null, as well as the
asymptotic behaviors under both local and fixed alternatives. The weak
consistency of the change-point estimator is provided. Extensive simulation
studies and the analysis of two real-world datasets illustrate the superior
performance of our approach compared to existing model-free change-point
detection methods.",http://arxiv.org/abs/2404.06995v1
SNSeg: An R Package for Time Series Segmentation via Self-Normalization,2024-04-11T03:14:46Z,"Shubo Sun, Zifeng Zhao, Feiyu Jiang, Xiaofeng Shao","Time series segmentation aims to identify potential change-points in a
sequence of temporally dependent data, so that the original sequence can be
partitioned into several homogeneous subsequences. It is useful for modeling
and predicting non-stationary time series and is widely applied in natural and
social sciences. Existing segmentation methods primarily focus on only one type
of parameter changes such as mean and variance, and they typically depend on
laborious tuning or smoothing parameters, which can be challenging to choose in
practice. The self-normalization based change-point estimation framework SNCP
by Zhao et al. (2022), however, offers users more flexibility and convenience
as it allows for change-point estimation of different types of parameters (e.g.
mean, variance, quantile and autocovariance) in a unified fashion, and requires
effortless tuning. In this paper, the R package SNSeg is introduced to
implement SNCP for segmentation of univariate and multivariate time series. An
extension of SNCP, named SNHD, is also designed and implemented for
change-point estimation in the mean vector of high-dimensional time series. The
estimated changepoints as well as segmented time series are available with
graphical tools. Detailed examples of SNSeg are given in simulations of
multivariate autoregressive processes with change-points.",http://arxiv.org/abs/2404.07451v1
Automatic Recommendations for Evolving Relational Databases Schema,2024-04-12T15:14:38Z,"Anne Etien, Nicolas Anquetil","Relational databases play a central role in many information systems. Their
schema contains structural (e.g. tables and columns) and behavioral (e.g.
stored procedures or views) entity descriptions. Then, just like for ``normal''
software, changes in legislation, offered functionalities, or functional
contexts, impose to evolve databases and their schemas. But in some scenarios,
it is not so easy to deconstruct a wished evolution of the schema into a
precise sequence of operations. Changing a database schema may impose manually
dropping and recreating dependent entities, or manually searching for
dependencies in stored procedures. This is important because getting even the
order of application of the operators can be difficult and have profound
consequences. This meta-model allows us to compute the impact of planned
changes and recommend additional changes that will ensure that the RDBMS
constraints are always verified. The recommendations can then be compiled into
a valid SQL patch actually updating the database schema in an orderly way. We
replicated a past evolution showing that, without detailed knowledge of the
database, we could perform the same change in 75\% less time than the expert
database architect. We also exemplify the use of our approach on other planned
changes.",http://arxiv.org/abs/2404.08525v1
An Adaptive Metaheuristic Framework for Changing Environments,2024-04-18T13:47:53Z,Bestoun S. Ahmed,"The rapidly changing landscapes of modern optimization problems require
algorithms that can be adapted in real-time. This paper introduces an Adaptive
Metaheuristic Framework (AMF) designed for dynamic environments. It is capable
of intelligently adapting to changes in the problem parameters. The AMF
combines a dynamic representation of problems, a real-time sensing system, and
adaptive techniques to navigate continuously changing optimization
environments. Through a simulated dynamic optimization problem, the AMF's
capability is demonstrated to detect environmental changes and proactively
adjust its search strategy. This framework utilizes a differential evolution
algorithm that is improved with an adaptation module that adjusts solutions in
response to detected changes. The capability of the AMF to adjust is tested
through a series of iterations, demonstrating its resilience and robustness in
sustaining solution quality despite the problem's development. The
effectiveness of AMF is demonstrated through a series of simulations on a
dynamic optimization problem. Robustness and agility characterize the
algorithm's performance, as evidenced by the presented fitness evolution and
solution path visualizations. The findings show that AMF is a practical
solution to dynamic optimization and a major step forward in the creation of
algorithms that can handle the unpredictability of real-world problems.",http://arxiv.org/abs/2404.12185v1
"Made to Order: Discovering monotonic temporal changes via
  self-supervised video ordering",2024-04-25T17:59:56Z,"Charig Yang, Weidi Xie, Andrew Zisserman","Our objective is to discover and localize monotonic temporal changes in a
sequence of images. To achieve this, we exploit a simple proxy task of ordering
a shuffled image sequence, with `time' serving as a supervisory signal, since
only changes that are monotonic with time can give rise to the correct
ordering. We also introduce a transformer-based model for ordering of image
sequences of arbitrary length with built-in attribution maps. After training,
the model successfully discovers and localizes monotonic changes while ignoring
cyclic and stochastic ones. We demonstrate applications of the model in
multiple domains covering different scene and object types, discovering both
object-level and environmental changes in unseen sequences. We also demonstrate
that the attention-based attribution maps function as effective prompts for
segmenting the changing regions, and that the learned representations can be
used for downstream applications. Finally, we show that the model achieves the
state-of-the-art on standard benchmarks for image ordering.",http://arxiv.org/abs/2404.16828v3
Data-adaptive structural change-point detection via isolation,2024-04-30T08:09:24Z,"Andreas Anastasiou, Sophia Loizidou","In this paper, a new data-adaptive method, called DAIS (Data Adaptive
ISolation), is introduced for the estimation of the number and the location of
change-points in a given data sequence. The proposed method can detect changes
in various different signal structures; we focus on the examples of
piecewise-constant and continuous, piecewise-linear signals. The novelty of the
proposed algorithm comes from the data-adaptive nature of the methodology. At
each step, and for the data under consideration, we search for the most
prominent change-point in a targeted neighborhood of the data sequence that
contains this change-point with high probability. Using a suitably chosen
contrast function, the change-point will then get detected after being isolated
in an interval. The isolation feature enhances estimation accuracy, while the
data-adaptive nature of DAIS is advantageous regarding, mainly, computational
complexity. The methodology can be applied to both univariate and multivariate
signals. The simulation results presented indicate that DAIS is at least as
accurate as state-of-the-art competitors and in many cases significantly less
computationally expensive.",http://arxiv.org/abs/2404.19344v2
"Estimation and Inference for Change Points in Functional Regression Time
  Series",2024-05-08T23:27:23Z,"Shivam Kumar, Haotian Xu, Haeran Cho, Daren Wang","In this paper, we study the estimation and inference of change points under a
functional linear regression model with changes in the slope function. We
present a novel Functional Regression Binary Segmentation (FRBS) algorithm
which is computationally efficient as well as achieving consistency in multiple
change point detection. This algorithm utilizes the predictive power of
piece-wise constant functional linear regression models in the reproducing
kernel Hilbert space framework. We further propose a refinement step that
improves the localization rate of the initial estimator output by FRBS, and
derive asymptotic distributions of the refined estimators for two different
regimes determined by the magnitude of a change. To facilitate the construction
of confidence intervals for underlying change points based on the limiting
distribution, we propose a consistent block-type long-run variance estimator.
Our theoretical justifications for the proposed approach accommodate temporal
dependence and heavy-tailedness in both the functional covariates and the
measurement errors. Empirical effectiveness of our methodology is demonstrated
through extensive simulation studies and an application to the Standard and
Poor's 500 index dataset.",http://arxiv.org/abs/2405.05459v1
Mobile Sequencers,2024-05-09T12:39:50Z,Cem Bozsahin,"The article is an attempt to contribute to explorations of a common origin
for language and planned-collaborative action. It gives `semantics of change'
the central stage in the synthesis, from its history and recordkeeping to its
development, its syntax, delivery and reception, including substratal aspects.
  It is suggested that to arrive at a common core, linguistic semantics must be
understood as studying through syntax mobile agent's representing, tracking and
coping with change and no change. Semantics of actions can be conceived the
same way, but through plans instead of syntax. The key point is the following:
Sequencing itself, of words and action sequences, brings in more structural
interpretation to the sequence than which is immediately evident from the
sequents themselves. Mobile sequencers can be understood as subjects
structuring reporting, understanding and keeping track of change and no change.
The idea invites rethinking of the notion of category, both in language and in
planning.
  Understanding understanding change by mobile agents is suggested to be about
human extended practice, not extended-human practice. That's why linguistics is
as important as computer science in the synthesis. It must rely on
representational history of acts, thoughts and expressions, personal and
public, crosscutting overtness and covertness of these phenomena. It has
implication for anthropology in the extended practice, which is covered
briefly.",http://arxiv.org/abs/2405.06710v1
"Self-Supervised Learning of Visual Servoing for Low-Rigidity Robots
  Considering Temporal Body Changes",2024-05-20T05:44:11Z,"Kento Kawaharazuka, Naoaki Kanazawa, Kei Okada, Masayuki Inaba","In this study, we investigate object grasping by visual servoing in a
low-rigidity robot. It is difficult for a low-rigidity robot to handle its own
body as intended compared to a rigid robot, and calibration between vision and
body takes some time. In addition, the robot must constantly adapt to changes
in its body, such as the change in camera position and change in joints due to
aging. Therefore, we develop a method for a low-rigidity robot to autonomously
learn visual servoing of its body. We also develop a mechanism that can
adaptively change its visual servoing according to temporal body changes. We
apply our method to a low-rigidity 6-axis arm, MyCobot, and confirm its
effectiveness by conducting object grasping experiments based on visual
servoing.",http://arxiv.org/abs/2405.11798v1
"A Behavior-Aware Approach for Deep Reinforcement Learning in
  Non-stationary Environments without Known Change Points",2024-05-23T06:17:26Z,"Zihe Liu, Jie Lu, Guangquan Zhang, Junyu Xuan","Deep reinforcement learning is used in various domains, but usually under the
assumption that the environment has stationary conditions like transitions and
state distributions. When this assumption is not met, performance suffers. For
this reason, tracking continuous environmental changes and adapting to
unpredictable conditions is challenging yet crucial because it ensures that
systems remain reliable and flexible in practical scenarios. Our research
introduces Behavior-Aware Detection and Adaptation (BADA), an innovative
framework that merges environmental change detection with behavior adaptation.
The key inspiration behind our method is that policies exhibit different global
behaviors in changing environments. Specifically, environmental changes are
identified by analyzing variations between behaviors using Wasserstein
distances without manually set thresholds. The model adapts to the new
environment through behavior regularization based on the extent of changes. The
results of a series of experiments demonstrate better performance relative to
several current algorithms. This research also indicates significant potential
for tackling this long-standing challenge.",http://arxiv.org/abs/2405.14214v1
"Using Synchronic Definitions and Semantic Relations to Classify Semantic
  Change Types",2024-06-05T16:52:21Z,"Pierluigi Cassotti, Stefano De Pascale, Nina Tahmasebi","There is abundant evidence of the fact that the way words change their
meaning can be classified in different types of change, highlighting the
relationship between the old and new meanings (among which generalization,
specialization and co-hyponymy transfer). In this paper, we present a way of
detecting these types of change by constructing a model that leverages
information both from synchronic lexical relations and definitions of word
meanings. Specifically, we use synset definitions and hierarchy information
from WordNet and test it on a digitized version of Blank's (1997) dataset of
semantic change types. Finally, we show how the sense relationships can improve
models for both approximation of human judgments of semantic relatedness as
well as binary Lexical Semantic Change Detection.",http://arxiv.org/abs/2406.03452v3
"Some things never change: how far generative AI can really change
  software engineering practice",2024-06-14T05:26:42Z,"Aline de Campos, Jorge Melegati, Nicolas Nascimento, Rafael Chanin, Afonso Sales, Igor Wiese","Generative Artificial Intelligence (GenAI) has become an emerging technology
with the availability of several tools that could impact Software Engineering
(SE) activities. As any other disruptive technology, GenAI led to the
speculation that its full potential can deeply change SE. However, an overfocus
on improving activities for which GenAI is more suitable could negligent other
relevant areas of the process. In this paper, we aim to explore which SE
activities are not expected to be profoundly changed by GenAI. To achieve this
goal, we performed a survey with SE practitioners to identify their
expectations regarding GenAI in SE, including impacts, challenges, ethical
issues, and aspects they do not expect to change. We compared our results with
previous roadmaps proposed in SE literature. Our results show that although
practitioners expect an increase in productivity, coding, and process quality,
they envision that some aspects will not change, such as the need for human
expertise, creativity, and project management. Our results point to SE areas
for which GenAI is probably not so useful, and future research could tackle
them to improve SE practice.",http://arxiv.org/abs/2406.09725v1
"A Late-Stage Bitemporal Feature Fusion Network for Semantic Change
  Detection",2024-06-15T16:02:10Z,"Chenyao Zhou, Haotian Zhang, Han Guo, Zhengxia Zou, Zhenwei Shi","Semantic change detection is an important task in geoscience and earth
observation. By producing a semantic change map for each temporal phase, both
the land use land cover categories and change information can be interpreted.
Recently some multi-task learning based semantic change detection methods have
been proposed to decompose the task into semantic segmentation and binary
change detection subtasks. However, previous works comprise triple branches in
an entangled manner, which may not be optimal and hard to adopt foundation
models. Besides, lacking explicit refinement of bitemporal features during
fusion may cause low accuracy. In this letter, we propose a novel late-stage
bitemporal feature fusion network to address the issue. Specifically, we
propose local global attentional aggregation module to strengthen feature
fusion, and propose local global context enhancement module to highlight
pivotal semantics. Comprehensive experiments are conducted on two public
datasets, including SECOND and Landsat-SCD. Quantitative and qualitative
results show that our proposed model achieves new state-of-the-art performance
on both datasets.",http://arxiv.org/abs/2406.10678v1
Zero-Shot Scene Change Detection,2024-06-17T05:03:44Z,"Kyusik Cho, Dong Yeop Kim, Euntai Kim","We present a novel, training-free approach to scene change detection. Our
method leverages tracking models, which inherently perform change detection
between consecutive frames of video by identifying common objects and detecting
new or missing objects. Specifically, our method takes advantage of the change
detection effect of the tracking model by inputting reference and query images
instead of consecutive frames. Furthermore, we focus on the content gap and
style gap between two input images in change detection, and address both issues
by proposing adaptive content threshold and style bridging layers,
respectively. Finally, we extend our approach to video, leveraging rich
temporal information to enhance the performance of scene change detection. We
compare our approach and baseline through various experiments. While existing
train-based baseline tend to specialize only in the trained domain, our method
shows consistent performance across various domains, proving the
competitiveness of our approach.",http://arxiv.org/abs/2406.11210v3
"DDLNet: Boosting Remote Sensing Change Detection with Dual-Domain
  Learning",2024-06-19T14:54:09Z,"Xiaowen Ma, Jiawei Yang, Rui Che, Huanting Zhang, Wei Zhang","Remote sensing change detection (RSCD) aims to identify the changes of
interest in a region by analyzing multi-temporal remote sensing images, and has
an outstanding value for local development monitoring. Existing RSCD methods
are devoted to contextual modeling in the spatial domain to enhance the changes
of interest. Despite the satisfactory performance achieved, the lack of
knowledge in the frequency domain limits the further improvement of model
performance. In this paper, we propose DDLNet, a RSCD network based on
dual-domain learning (i.e., frequency and spatial domains). In particular, we
design a Frequency-domain Enhancement Module (FEM) to capture frequency
components from the input bi-temporal images using Discrete Cosine Transform
(DCT) and thus enhance the changes of interest. Besides, we devise a
Spatial-domain Recovery Module (SRM) to fuse spatiotemporal features for
reconstructing spatial details of change representations. Extensive experiments
on three benchmark RSCD datasets demonstrate that the proposed method achieves
state-of-the-art performance and reaches a more satisfactory
accuracy-efficiency trade-off. Our code is publicly available at
https://github.com/xwmaxwma/rschange.",http://arxiv.org/abs/2406.13606v1
"AXOLOTL'24 Shared Task on Multilingual Explainable Semantic Change
  Modeling",2024-07-04T17:41:32Z,"Mariia Fedorova, Timothee Mickus, Niko Partanen, Janine Siewert, Elena Spaziani, Andrey Kutuzov","This paper describes the organization and findings of AXOLOTL'24, the first
multilingual explainable semantic change modeling shared task. We present new
sense-annotated diachronic semantic change datasets for Finnish and Russian
which were employed in the shared task, along with a surprise test-only German
dataset borrowed from an existing source. The setup of AXOLOTL'24 is new to the
semantic change modeling field, and involves subtasks of identifying unknown
(novel) senses and providing dictionary-like definitions to these senses. The
methods of the winning teams are described and compared, thus paving a path
towards explainability in computational approaches to historical change of
meaning.",http://arxiv.org/abs/2407.04079v1
"Change-Point Detection in Industrial Data Streams based on Online
  Dynamic Mode Decomposition with Control",2024-07-08T14:18:33Z,"Marek Wadinger, Michal Kvasnica, Yoshinobu Kawahara","We propose a novel change-point detection method based on online Dynamic Mode
Decomposition with control (ODMDwC). Leveraging ODMDwC's ability to find and
track linear approximation of a non-linear system while incorporating control
effects, the proposed method dynamically adapts to its changing behavior due to
aging and seasonality. This approach enables the detection of changes in
spatial, temporal, and spectral patterns, providing a robust solution that
preserves correspondence between the score and the extent of change in the
system dynamics. We formulate a truncated version of ODMDwC and utilize
higher-order time-delay embeddings to mitigate noise and extract broad-band
features. Our method addresses the challenges faced in industrial settings
where safety-critical systems generate non-uniform data streams while requiring
timely and accurate change-point detection to protect profit and life. Our
results demonstrate that this method yields intuitive and improved detection
results compared to the Singular-Value-Decomposition-based method. We validate
our approach using synthetic and real-world data, showing its competitiveness
to other approaches on complex systems' benchmark datasets. Provided guidelines
for hyperparameters selection enhance our method's practical applicability.",http://arxiv.org/abs/2407.05976v2
Transformer for Multitemporal Hyperspectral Image Unmixing,2024-07-15T04:02:01Z,"Hang Li, Qiankun Dong, Xueshuo Xie, Xia Xu, Tao Li, Zhenwei Shi","Multitemporal hyperspectral image unmixing (MTHU) holds significant
importance in monitoring and analyzing the dynamic changes of surface. However,
compared to single-temporal unmixing, the multitemporal approach demands
comprehensive consideration of information across different phases, rendering
it a greater challenge. To address this challenge, we propose the Multitemporal
Hyperspectral Image Unmixing Transformer (MUFormer), an end-to-end unsupervised
deep learning model. To effectively perform multitemporal hyperspectral image
unmixing, we introduce two key modules: the Global Awareness Module (GAM) and
the Change Enhancement Module (CEM). The Global Awareness Module computes
self-attention across all phases, facilitating global weight allocation. On the
other hand, the Change Enhancement Module dynamically learns local temporal
changes by comparing endmember changes between adjacent phases. The synergy
between these modules allows for capturing semantic information regarding
endmember and abundance changes, thereby enhancing the effectiveness of
multitemporal hyperspectral image unmixing. We conducted experiments on one
real dataset and two synthetic datasets, demonstrating that our model
significantly enhances the effect of multitemporal hyperspectral image
unmixing.",http://arxiv.org/abs/2407.10427v1
"Instagram versus women of color: Why are women of color protesting
  Instagram's algorithmic changes?",2024-07-25T00:35:39Z,Ankolika De,"Instagram has been appropriated by communities for several contemporary
social struggles, often translating into real world action. Likewise, women of
color (WOC) have used it to protest, share information and support one another
through its various affordances. However, Instagram is known to have frequent
updates, and recently the updates have been more drastic. The newest update
changed the recommendation algorithm such that it showed video-oriented content
(reels) from unknown accounts over static media from a user's own network.
Several marginalized communities, and especially WOC resisted this change and
others that led to it. Due to the backlash, Instagram rolled back its changes.
Drawing from past HCI work on digital platforms for marginalised communities, I
propose a qualitative study informed by the open research strategy to
understand why WOC are resisting these changes, and eventually provide
implications for design that can help implement changes in a more inclusive
manner.",http://arxiv.org/abs/2407.17679v1
"The Good, the Bad, and the Monstrous: Predicting Highly Change-Prone
  Source Code Methods at Their Inception",2024-08-11T06:20:48Z,Shaiful Chowdhury,"The cost of software maintenance often surpasses the initial development
expenses, making it a significant concern for the software industry. A key
strategy for alleviating future maintenance burdens is the early prediction and
identification of change-prone code components, which allows for timely
optimizations. While prior research has largely concentrated on predicting
change-prone files and classes, an approach less favored by practitioners, this
paper shifts focus to predicting highly change-prone methods, aligning with the
preferences of both practitioners and researchers. We analyzed 774,051 source
code methods from 49 prominent open-source Java projects. Our findings reveal
that approximately 80% of changes are concentrated in just 20% of the methods,
demonstrating the Pareto 80/20 principle. Moreover, this subset of methods is
responsible for the majority of the identified bugs in these projects. After
establishing their critical role in mitigating software maintenance costs, our
study shows that machine learning models can effectively identify these highly
change-prone methods from their inception. Additionally, we conducted a
thorough manual analysis to uncover common patterns (or concepts) among the
more difficult-to-predict methods. These insights can help future research
develop new features and enhance prediction accuracy.",http://arxiv.org/abs/2408.05704v2
"A Probabilistic Framework for Adapting to Changing and Recurring
  Concepts in Data Streams",2024-08-18T01:53:52Z,"Ben Halstead, Yun Sing Koh, Patricia Riddle, Mykola Pechenizkiy, Albert Bifet","The distribution of streaming data often changes over time as conditions
change, a phenomenon known as concept drift. Only a subset of previous
experience, collected in similar conditions, is relevant to learning an
accurate classifier for current data. Learning from irrelevant experience
describing a different concept can degrade performance. A system learning from
streaming data must identify which recent experience is irrelevant when
conditions change and which past experience is relevant when concepts reoccur,
\textit{e.g.,} when weather events or financial patterns repeat. Existing
streaming approaches either do not consider experience to change in relevance
over time and thus cannot handle concept drift, or only consider the recency of
experience and thus cannot handle recurring concepts, or only sparsely evaluate
relevance and thus fail when concept drift is missed. To enable learning in
changing conditions, we propose SELeCT, a probabilistic method for continuously
evaluating the relevance of past experience. SELeCT maintains a distinct
internal state for each concept, representing relevant experience with a unique
classifier. We propose a Bayesian algorithm for estimating state relevance,
combining the likelihood of drawing recent observations from a given state with
a transition pattern prior based on the system's current state.",http://arxiv.org/abs/2408.09324v1
"Localization in Dynamic Indoor MIMO-OFDM Wireless Systems using Domain
  Adaptation",2024-08-23T12:11:36Z,"Rafail Ismayilov, Renato L. G. Cavalcante, Slawomir Stanczak","We propose a method for predicting the location of user equipment (UE) using
wireless fingerprints in dynamic indoor non-line-of-sight (NLoS) environments.
In particular, our method copes with the challenges posed by the drift, birth,
and death of scattering clusters resulting from dynamic changes in the wireless
environment. Prominent examples of such dynamic wireless environments include
factory floors or offices, where the geometry of the environment undergoes
changes over time. These changes affect the distribution of wireless
fingerprints, demonstrating some similarity between the distributions before
and after the change. Consequently, the performance of a location estimator
initially designed for a specific environment may degrade significantly when
applied after changes have occurred in that environment. To address this
limitation, we propose a domain adaptation framework that utilizes neural
networks to align the distributions of wireless fingerprints collected both
before and after environmental changes. By aligning these distributions, we
design an estimator capable of predicting UE locations from their wireless
fingerprints in the new environment. Experiments validate the effectiveness of
the proposed methods in localizing UEs in dynamic wireless environments.",http://arxiv.org/abs/2408.13017v2
CDChat: A Large Multimodal Model for Remote Sensing Change Description,2024-09-24T17:31:02Z,"Mubashir Noman, Noor Ahsan, Muzammal Naseer, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Shahbaz Khan","Large multimodal models (LMMs) have shown encouraging performance in the
natural image domain using visual instruction tuning. However, these LMMs
struggle to describe the content of remote sensing images for tasks such as
image or region grounding, classification, etc. Recently, GeoChat make an
effort to describe the contents of the RS images. Although, GeoChat achieves
promising performance for various RS tasks, it struggles to describe the
changes between bi-temporal RS images which is a key RS task. This necessitates
the development of an LMM that can describe the changes between the bi-temporal
RS images. However, there is insufficiency of datasets that can be utilized to
tune LMMs. In order to achieve this, we introduce a change description
instruction dataset that can be utilized to finetune an LMM and provide better
change descriptions for RS images. Furthermore, we show that the LLaVA-1.5
model, with slight modifications, can be finetuned on the change description
instruction dataset and achieve favorably better performance.",http://arxiv.org/abs/2409.16261v1
"Robust Quickest Correlation Change Detection in High-Dimensional Random
  Vectors",2024-10-04T16:53:21Z,"Assma Alghamdi, Taposh Banerjee, Jayant Rajgopal","Detecting changes in high-dimensional vectors presents significant
challenges, especially when the post-change distribution is unknown and
time-varying. This paper introduces a novel robust algorithm for correlation
change detection in high-dimensional data. The approach utilizes the summary
statistic of the maximum magnitude correlation coefficient to detect the
change. This summary statistic captures the level of correlation present in the
data but also has an asymptotic density. The robust test is designed using the
asymptotic density. The proposed approach is robust because it can help detect
a change in correlation level from some known level to unknown, time-varying
levels. The proposed test is also computationally efficient and valid for a
broad class of data distributions. The effectiveness of the proposed algorithm
is demonstrated on simulated data.",http://arxiv.org/abs/2410.03593v1
"Changing Redshifts caused by a Changing Expansion Velocity of the
  Universe",2024-10-11T12:00:39Z,"Nico Roos, Eric Sluimer, Bert van den Broek","With the next generation of big telescopes such as the ELT and SKA it might
become possible to measure changes in the expansion rate of the Universe in
real time by measuring the change of the redshifts of a large number of
galaxies over a period of the order of 10 years. This phenomenon, known as
'redshift drift,' will provide a crucial direct test of cosmological models.
The change in redshift is readily explained using the concept of conformal time
which is the comoving distance of a galaxy in lightyears. We emphasize that the
redshift drift is directly proportional to the average change in the cosmic
expansion rate between the time of a galaxy's light emission and its
absorption. This phenomenon is illustrated within the framework of the
concordance model, the Lambda-CDM model of the universe.",http://arxiv.org/abs/2410.08741v1
"LCD-Net: A Lightweight Remote Sensing Change Detection Network Combining
  Feature Fusion and Gating Mechanism",2024-10-14T10:33:30Z,"Wenyu Liu, Jindong Li, Haoji Wang, Run Tan, Yali Fu, Qichuan Tian","Remote sensing image change detection (RSCD) is crucial for monitoring
dynamic surface changes, with applications ranging from environmental
monitoring to disaster assessment. While traditional CNN-based methods have
improved detection accuracy, they often suffer from high computational
complexity and large parameter counts, limiting their use in
resource-constrained environments. To address these challenges, we propose a
Lightweight remote sensing Change Detection Network (LCD-Net in short) that
reduces model size and computational cost while maintaining high detection
performance. LCD-Net employs MobileNetV2 as the encoder to efficiently extract
features from bitemporal images. A Temporal Interaction and Fusion Module (TIF)
enhances the interaction between bitemporal features, improving temporal
context awareness. Additionally, the Feature Fusion Module (FFM) aggregates
multiscale features to better capture subtle changes while suppressing
background noise. The Gated Mechanism Module (GMM) in the decoder further
enhances feature learning by dynamically adjusting channel weights, emphasizing
key change regions. Experiments on LEVIR-CD+, SYSU, and S2Looking datasets show
that LCD-Net achieves competitive performance with just 2.56M parameters and
4.45G FLOPs, making it well-suited for real-time applications in
resource-limited settings. The code is available at
https://github.com/WenyuLiu6/LCD-Net.",http://arxiv.org/abs/2410.11580v1
MV-CC: Mask Enhanced Video Model for Remote Sensing Change Caption,2024-10-31T14:02:40Z,"Ruixun Liu, Kaiyu Li, Jiayi Song, Dongwei Sun, Xiangyong Cao","Remote sensing image change caption (RSICC) aims to provide natural language
descriptions for bi-temporal remote sensing images. Since Change Caption (CC)
task requires both spatial and temporal features, previous works follow an
encoder-fusion-decoder architecture. They use an image encoder to extract
spatial features and the fusion module to integrate spatial features and
extract temporal features, which leads to increasingly complex manual design of
the fusion module. In this paper, we introduce a novel video model-based
paradigm without design of the fusion module and propose a Mask-enhanced Video
model for Change Caption (MV-CC). Specifically, we use the off-the-shelf video
encoder to simultaneously extract the temporal and spatial features of
bi-temporal images. Furthermore, the types of changes in the CC are set based
on specific task requirements, and to enable the model to better focus on the
regions of interest, we employ masks obtained from the Change Detection (CD)
method to explicitly guide the CC model. Experimental results demonstrate that
our proposed method can obtain better performance compared with other
state-of-the-art RSICC methods. The code is available at
https://github.com/liuruixun/MV-CC.",http://arxiv.org/abs/2410.23946v1
Variational Neural Stochastic Differential Equations with Change Points,2024-11-01T14:46:17Z,"Yousef El-Laham, Zhongchang Sun, Haibei Zhu, Tucker Balch, Svitlana Vyetrenko","In this work, we explore modeling change points in time-series data using
neural stochastic differential equations (neural SDEs). We propose a novel
model formulation and training procedure based on the variational autoencoder
(VAE) framework for modeling time-series as a neural SDE. Unlike existing
algorithms training neural SDEs as VAEs, our proposed algorithm only
necessitates a Gaussian prior of the initial state of the latent stochastic
process, rather than a Wiener process prior on the entire latent stochastic
process. We develop two methodologies for modeling and estimating change points
in time-series data with distribution shifts. Our iterative algorithm
alternates between updating neural SDE parameters and updating the change
points based on either a maximum likelihood-based approach or a change point
detection algorithm using the sequential likelihood ratio test. We provide a
theoretical analysis of this proposed change point detection scheme. Finally,
we present an empirical evaluation that demonstrates the expressive power of
our proposed model, showing that it can effectively model both classical
parametric SDEs and some real datasets with distribution shifts.",http://arxiv.org/abs/2411.00635v1
"Automation Will Set Occupational Mobility Free: Structural Changes in
  the Occupation Network",2024-11-06T01:56:14Z,"Soohyoung Lee, Dawoon Jeong, Jeong-Dong Lee","Occupational mobility is an emergent strategy to cope with technological
unemployment by facilitating efficient labor redeployment. However, previous
studies analyzing networks show that the boundaries to smooth mobility are
constrained by a fragmented structure in the occupation network. In this study,
positing that this structure will significantly change due to automation, we
propose the skill automation view, which asserts that automation substitutes
for skills, not for occupations, and simulate a scenario of skill automation
drawing on percolation theory. We sequentially remove skills from the
occupation-skill bipartite network and investigate the structural changes in
the projected occupation network. The results show that the accumulation of
small changes (the emergence of bridges between occupations due to skill
automation) triggers significant structural changes in the occupation network.
The structural changes accelerate as the components integrate into a new giant
component. This result suggests that automation mitigates the bottlenecks to
smooth occupational mobility.",http://arxiv.org/abs/2411.03605v1
"On-chip rewritable phase-change metasurface for programmable diffractive
  deep neural networks",2024-11-08T17:32:46Z,Sanaz Zarei,"Photonic neural networks capable of rapid programming are indispensable to
realize many functionalities. Phase change technology can provide nonvolatile
programmability in photonic neural networks. Integrating direct laser writing
technique with phase change material (PCM) can potentially enable programming
and in-memory computing for on-chip photonic neural networks. Sb2Se3 is a newly
introduced ultralow-loss phase change material with a large refractive index
contrast over the telecommunication transmission band. Compact, low-loss,
rewritable, and nonvolatile on-chip phase-change metasurfaces can be created by
using direct laser writing on a Sb2Se3 thin film. Here, by cascading multiple
layers of on-chip phase-change metasurfaces, an ultra-compact on-chip
programmable diffractive deep neural network is demonstrated at the wavelength
of 1.55um and benchmarked on two machine learning tasks of pattern recognition
and MNIST (Modified National Institute of Standards and Technology) handwritten
digits classification and accuracies comparable to the state of the art are
achieved. The proposed on-chip programmable diffractive deep neural network is
also advantageous in terms of power consumption because of the ultralow-loss of
the Sb2Se3 and its nonvolatility which requires no constant power supply to
maintain its programmed state.",http://arxiv.org/abs/2411.05723v1
The Greedy Coin Change Problem,2024-11-27T08:37:47Z,"Shreya Gupta, Boyang Huang, Russell Impagliazzo","The Coin Change problem, also known as the Change-Making problem, is a
well-studied combinatorial optimization problem, which involves minimizing the
number of coins needed to make a specific change amount using a given set of
coin denominations. A natural and intuitive approach to this problem is the
greedy algorithm. While the greedy algorithm is not universally optimal for all
sets of coin denominations, it yields optimal solutions under most real-world
coin systems currently in use, making it an efficient heuristic with broad
practical applicability. Researchers have been studying ways to determine
whether a given coin system guarantees optimal solutions under the greedy
approach, but surprisingly little attention has been given to understanding the
general computational behavior of the greedy algorithm applied to the coin
change problem.
  To address this gap, we introduce the Greedy Coin Change problem and
formalize its decision version: given a target amount $W$ and a set of
denominations $C$, determine whether a specific coin is included in the greedy
solution. We prove that this problem is $\mathbf P$-complete under log-space
reductions, which implies it is unlikely to be efficiently parallelizable or
solvable in limited space.",http://arxiv.org/abs/2411.18137v1
"Robust Quickest Change Detection in Multi-Stream Non-Stationary
  Processes",2024-11-27T17:57:49Z,"Yingze Hou, Hoda Bidkhori, Taposh Banerjee","The problem of robust quickest change detection (QCD) in non-stationary
processes under a multi-stream setting is studied. In classical QCD theory,
optimal solutions are developed to detect a sudden change in the distribution
of stationary data. Most studies have focused on single-stream data. In
non-stationary processes, the data distribution both before and after change
varies with time and is not precisely known. The multi-dimension data even
complicates such issues. It is shown that if the non-stationary family for each
dimension or stream has a least favorable law (LFL) or distribution in a
well-defined sense, then the algorithm designed using the LFLs is robust
optimal. The notion of LFL defined in this work differs from the classical
definitions due to the dependence of the post-change model on the change point.
Examples of multi-stream non-stationary processes encountered in public health
monitoring and aviation applications are provided. Our robust algorithm is
applied to simulated and real data to show its effectiveness.",http://arxiv.org/abs/2412.04493v1
"How permanent are metadata for research data? Understanding changes in
  DataCite DOI metadata",2024-12-06T15:35:19Z,Dorothea Strecker,"With the move towards open research information, the DOI registration agency
DataCite is increasingly used as a source for metadata describing research
data, for example to perform scientometric analyses. However, there is a lack
of research on how DOI metadata describing research data are created and
maintained. This paper adresses this gap by using DataCite metadata provenance
information to analyze the overall prevalence and patterns of change to
DataCite DOI metadata records. The results show that change of DataCite DOI
metadata records is common, but it tends to be incremental and not extensive.
DataCite DOI metadata records offer reliable descriptions of datasets and are
stable enough to be used in scientometric research. The findings mirror
insights from previous studies of metadata change in other contexts, suggesting
that there are similarities in metadata practices between research data
repositories and more traditional cataloging environments. However, the
observed changes don't seem to fully align with idealized conceptualizations of
metadata creation and maintenance for research data. In particular, the data
does not show that metadata records are maintained continuously, and metadata
change has a limited effect on metadata completeness.",http://arxiv.org/abs/2412.05128v1
Robust Quickest Change Detection with Sampling Control,2024-12-28T16:37:18Z,"Yingze Hou, Hoda Bidkhori, Taposh Banerjee","The problem of quickest detection of a change in the distribution of a
sequence of random variables is studied. The objective is to detect the change
with the minimum possible delay, subject to constraints on the rate of false
alarms and the cost of observations used in the decision-making process. The
post-change distribution of the data is known only within a distribution
family. It is shown that if the post-change family has a distribution that is
least favorable in a well-defined sense, then a computationally efficient
algorithm can be designed that uses an on-off observation control strategy to
save the cost of observations. In addition, the algorithm can detect the change
robustly while avoiding unnecessary false alarms. It is shown that the
algorithm is also asymptotically robust optimal as the rate of false alarms
goes to zero for every fixed constraint on the cost of observations. The
algorithm's effectiveness is validated on simulated data and real public health
data.",http://arxiv.org/abs/2412.20207v1
"Empirical Analysis on CI/CD Pipeline Evolution in Machine Learning
  Projects",2024-03-18T19:14:38Z,"Alaa Houerbi, Rahul Ghanshyam Chavan, Dhia Elhaq Rzig, Foyzul Hassan","The growing popularity of machine learning (ML) and the integration of ML
components with other software artifacts has led to the use of continuous
integration and delivery (CI/CD) tools, such as Travis CI, GitHub Actions, etc.
that enable faster integration and testing for ML projects. Such CI/CD
configurations and services require synchronization during the life cycle of
the projects. Several works discussed how CI/CD configuration and services
change during their usage in traditional software systems. However, there is
very limited knowledge of how CI/CD configuration and services change in ML
projects.
  To fill this knowledge gap, this work presents the first empirical analysis
of how CI/CD configuration evolves for ML software systems. We manually
analyzed 343 commits collected from 508 open-source ML projects to identify
common CI/CD configuration change categories in ML projects and devised a
taxonomy of 14 co-changes in CI/CD and ML components. Moreover, we developed a
CI/CD configuration change clustering tool that identified frequent CI/CD
configuration change patterns in 15,634 commits. Furthermore, we measured the
expertise of ML developers who modify CI/CD configurations. Based on this
analysis, we found that 61.8% of commits include a change to the build policy
and minimal changes related to performance and maintainability compared to
general open-source projects. Additionally, the co-evolution analysis
identified that CI/CD configurations, in many cases, changed unnecessarily due
to bad practices such as the direct inclusion of dependencies and a lack of
usage of standardized testing frameworks. More practices were found through the
change patterns analysis consisting of using deprecated settings and reliance
on a generic build language. Finally, our developer's expertise analysis
suggests that experienced developers are more inclined to modify CI/CD
configurations.",http://arxiv.org/abs/2403.12199v3
"A Change Point Detection Integrated Remaining Useful Life Estimation
  Model under Variable Operating Conditions",2024-01-09T04:35:17Z,"Anushiya Arunan, Yan Qin, Xiaoli Li, Chau Yuen","By informing the onset of the degradation process, health status evaluation
serves as a significant preliminary step for reliable remaining useful life
(RUL) estimation of complex equipment. This paper proposes a novel temporal
dynamics learning-based model for detecting change points of individual
devices, even under variable operating conditions, and utilises the learnt
change points to improve the RUL estimation accuracy. During offline model
development, the multivariate sensor data are decomposed to learn fused
temporal correlation features that are generalisable and representative of
normal operation dynamics across multiple operating conditions. Monitoring
statistics and control limit thresholds for normal behaviour are dynamically
constructed from these learnt temporal features for the unsupervised detection
of device-level change points. The detected change points then inform the
degradation data labelling for training a long short-term memory (LSTM)-based
RUL estimation model. During online monitoring, the temporal correlation
dynamics of a query device is monitored for breach of the control limit derived
in offline training. If a change point is detected, the device's RUL is
estimated with the well-trained offline model for early preventive action.
Using C-MAPSS turbofan engines as the case study, the proposed method improved
the accuracy by 5.6\% and 7.5\% for two scenarios with six operating
conditions, when compared to existing LSTM-based RUL estimation models that do
not consider heterogeneous change points.",http://arxiv.org/abs/2401.04351v1
"The Impact of Elicitation and Contrasting Narratives on Engagement,
  Recall and Attitude Change with News Articles Containing Data Visualization",2024-01-10T19:09:52Z,"Milad Rogha, Subham Sah, Alireza Karduni, Douglas Markant, Wenwen Dou","News articles containing data visualizations play an important role in
informing the public on issues ranging from public health to politics. Recent
research on the persuasive appeal of data visualizations suggests that prior
attitudes can be notoriously difficult to change. Inspired by an NYT article,
we designed two experiments to evaluate the impact of elicitation and
contrasting narratives on attitude change, recall, and engagement. We
hypothesized that eliciting prior beliefs leads to more elaborative thinking
that ultimately results in higher attitude change, better recall, and
engagement. Our findings revealed that visual elicitation leads to higher
engagement in terms of feelings of surprise. While there is an overall attitude
change across all experiment conditions, we did not observe a significant
effect of belief elicitation on attitude change. With regard to recall error,
while participants in the draw trend elicitation exhibited significantly lower
recall error than participants in the categorize trend condition, we found no
significant difference in recall error when comparing elicitation conditions to
no elicitation. In a follow-up study, we added contrasting narratives with the
purpose of making the main visualization (communicating data on the focal
issue) appear strikingly different. Compared to the results of study 1, we
found that contrasting narratives improved engagement in terms of surprise and
interest but interestingly resulted in higher recall error and no significant
change in attitude. We discuss the effects of elicitation and contrasting
narratives in the context of topic involvement and the strengths of temporal
trends encoded in the data visualization.",http://arxiv.org/abs/2401.05511v1
"Measurement of changes in muscle viscoelasticity during static
  stretching using stress-relaxation data",2024-01-24T04:08:16Z,"Yo Kobayashi, Daiki Matsuyama","This study investigates how the viscoelasticity of the muscle changes during
static stretching by measuring the state of the muscle during stretching using
continuous time-series data. We used a device that applied a force to the
muscle during stretching and measured the reaction force. The device was
attached to the participants, and time-series data of the reaction force
(stress-relaxation data) during stretching were obtained. A model using
fractional calculus (spring-pot model) was selected as the viscoelastic model
for the muscle, in which the data for stress relaxation were fitted on a
straight line on a both logarithmic plot. The experimental stress-relaxation
results showed that viscoelasticity tended to change abruptly at a particular
time during static stretching because the stress-relaxation data were
represented by a broken line comprising two segments on the both logarithmic
plot. Considering two states of viscoelasticity, before and after the change,
the stress-relaxation curve was fitted to the spring-pot model with high
accuracy using segment regression (R2 = 0.99). We compared the parameters of
the spring-pot model before and after the change in muscle viscoelasticity. By
examining these continuous time-series data, we also investigated the time
taken for the effects of stretching to become apparent. Furthermore, by
measuring the changes in muscle viscoelasticity during static stretching before
and after a short-term exercise load of running on a treadmill, we examined the
effects of short-term exercise load on the changes in viscoelasticity during
static stretching.",http://arxiv.org/abs/2401.13217v1
"Machine Learning-Based Vehicle Intention Trajectory Recognition and
  Prediction for Autonomous Driving",2024-02-25T09:28:20Z,"Hanyi Yu, Shuning Huo, Mengran Zhu, Yulu Gong, Yafei Xiang","In recent years, the expansion of internet technology and advancements in
automation have brought significant attention to autonomous driving technology.
Major automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have
progressively introduced products ranging from assisted-driving vehicles to
semi-autonomous vehicles. However, this period has also witnessed several
traffic safety incidents involving self-driving vehicles. For instance, in
March 2016, a Google self-driving car was involved in a minor collision with a
bus. At the time of the accident, the autonomous vehicle was attempting to
merge into the right lane but failed to dynamically respond to the real-time
environmental information during the lane change. It incorrectly assumed that
the approaching bus would slow down to avoid it, leading to a low-speed
collision with the bus. This incident highlights the current technological
shortcomings and safety concerns associated with autonomous lane-changing
behavior, despite the rapid advancements in autonomous driving technology.
Lane-changing is among the most common and hazardous behaviors in highway
driving, significantly impacting traffic safety and flow. Therefore,
lane-changing is crucial for traffic safety, and accurately predicting drivers'
lane change intentions can markedly enhance driving safety. This paper
introduces a deep learning-based prediction method for autonomous driving lane
change behavior, aiming to facilitate safe lane changes and thereby improve
road safety.",http://arxiv.org/abs/2402.16036v1
"Safe Hybrid-Action Reinforcement Learning-Based Decision and Control for
  Discretionary Lane Change",2024-03-01T11:03:17Z,"Ruichen Xu, Xiao Liu, Jinming Xu, Yuan Lin","Autonomous lane-change, a key feature of advanced driver-assistance systems,
can enhance traffic efficiency and reduce the incidence of accidents. However,
safe driving of autonomous vehicles remains challenging in complex
environments. How to perform safe and appropriate lane change is a popular
topic of research in the field of autonomous driving. Currently, few papers
consider the safety of reinforcement learning in autonomous lane-change
scenarios. We introduce safe hybrid-action reinforcement learning into
discretionary lane change for the first time and propose Parameterized Soft
Actor-Critic with PID Lagrangian (PASAC-PIDLag) algorithm. Furthermore, we
conduct a comparative analysis of the Parameterized Soft Actor-Critic (PASAC),
which is an unsafe version of PASAC-PIDLag. Both algorithms are employed to
train the lane-change strategy of autonomous vehicles to output discrete
lane-change decision and longitudinal vehicle acceleration. Our simulation
results indicate that at a traffic density of 15 vehicles per kilometer (15
veh/km), the PASAC-PIDLag algorithm exhibits superior safety with a collision
rate of 0%, outperforming the PASAC algorithm, which has a collision rate of
1%. The outcomes of the generalization assessments reveal that at low traffic
density levels, both the PASAC-PIDLag and PASAC algorithms are proficient in
attaining a 0% collision rate. Under conditions of high traffic flow density,
the PASAC-PIDLag algorithm surpasses PASAC in terms of both safety and
optimality.",http://arxiv.org/abs/2403.00446v1
"SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional
  Videos",2024-03-03T19:53:06Z,"Yulei Niu, Wenliang Guo, Long Chen, Xudong Lin, Shih-Fu Chang","We study the problem of procedure planning in instructional videos, which
aims to make a goal-oriented sequence of action steps given partial visual
state observations. The motivation of this problem is to learn a structured and
plannable state and action space. Recent works succeeded in sequence modeling
of steps with only sequence-level annotations accessible during training, which
overlooked the roles of states in the procedures. In this work, we point out
that State CHangEs MAtter (SCHEMA) for procedure planning in instructional
videos. We aim to establish a more structured state space by investigating the
causal relations between steps and states in procedures. Specifically, we
explicitly represent each step as state changes and track the state changes in
procedures. For step representation, we leveraged the commonsense knowledge in
large language models (LLMs) to describe the state changes of steps via our
designed chain-of-thought prompting. For state change tracking, we align visual
state observations with language state descriptions via cross-modal contrastive
learning, and explicitly model the intermediate states of the procedure using
LLM-generated state descriptions. Experiments on CrossTask, COIN, and NIV
benchmark datasets demonstrate that our proposed SCHEMA model achieves
state-of-the-art performance and obtains explainable visualizations.",http://arxiv.org/abs/2403.01599v1
"ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing
  Change Detection",2024-03-26T17:46:25Z,"Mubashir Noman, Mustansar Fiaz, Hisham Cholakkal, Salman Khan, Fahad Shahbaz Khan","Deep learning has shown remarkable success in remote sensing change detection
(CD), aiming to identify semantic change regions between co-registered
satellite image pairs acquired at distinct time stamps. However, existing
convolutional neural network and transformer-based frameworks often struggle to
accurately segment semantic change regions. Moreover, transformers-based
methods with standard self-attention suffer from quadratic computational
complexity with respect to the image resolution, making them less practical for
CD tasks with limited training data. To address these issues, we propose an
efficient change detection framework, ELGC-Net, which leverages rich contextual
information to precisely estimate change regions while reducing the model size.
Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The
focus of our design is the introduction of an Efficient Local-Global Context
Aggregator module within the encoder, capturing enhanced global context and
local spatial information through a novel pooled-transpose (PT) attention and
depthwise convolution, respectively. The PT attention employs pooling
operations for robust feature extraction and minimizes computational cost with
transposed attention. Extensive experiments on three challenging CD datasets
demonstrate that ELGC-Net outperforms existing methods. Compared to the recent
transformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in
intersection over union metric on the LEVIR-CD dataset, while significantly
reducing trainable parameters. Our proposed ELGC-Net sets a new
state-of-the-art performance in remote sensing change detection benchmarks.
Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly
reduced computational complexity, suitable for resource-constrained settings,
while achieving comparable performance. Project url
https://github.com/techmn/elgcnet.",http://arxiv.org/abs/2403.17909v1
"An incremental hybrid adaptive network-based IDS in Software Defined
  Networks to detect stealth attacks",2024-04-01T13:33:40Z,Abdullah H Alqahtani,"Network attacks have became increasingly more sophisticated and stealthy due
to the advances in technologies and the growing sophistication of attackers.
Advanced Persistent Threats (APTs) are a type of attack that implement a wide
range of strategies to evade detection and be under the defence radar. Software
Defined Network (SDN) is a network paradigm that implements dynamic
configuration by separating the control plane from the network plane. This
approach improves security aspects by facilitating the employment of network
intrusion detection systems. Implementing Machine Learning (ML) techniques in
Intrusion Detection Systems (IDSs) is widely used to detect such attacks but
has a challenge when the data distribution changes. Concept drift is a term
that describes the change in the relationship between the input data and the
target value (label or class). The model is expected to degrade as certain
forms of change occur. In this paper, the primary form of change will be in
user behaviour (particularly changes in attacker behaviour). It is essential
for a model to adapt itself to deviations in data distribution. SDN can help in
monitoring changes in data distribution. This paper discusses changes in
stealth attacker behaviour. The work described here investigates various
concept drift detection algorithms. An incremental hybrid adaptive Network
Intrusion Detection System (NIDS) is proposed to tackle the issue of concept
drift in SDN. It can detect known and unknown attacks. The model is evaluated
over different datasets showing promising results.",http://arxiv.org/abs/2404.01109v1
A multi-phase thermo-mechanical model for rock-ice avalanche,2024-04-09T08:56:48Z,Shiva P. Pudasaini,"We propose a novel physically-based multi-phase thermo-mechanical model for
rock-ice avalanche. The model is built on a multi-phase mass flow model and
extends a two-phase rock-ice avalanche model. It considers rock, ice and fluid;
includes the mechanism of ice-melting and a rigorously derived dynamically
changing general temperature equation for avalanching bulk mass, the first of
its kind. It explains advection-diffusion of heat including the heat exchange
across the rock-ice avalanche body, basal heat conduction, production and loss
of heat due to frictional shearing and changing temperature, a general
formulation of the ice melting rate and enhancement of temperature due to basal
entrainment. The temperature equation includes a composite term containing
coupled dynamics: rate of change of thermal conductivity and temperature. Ice
melt intensity determines these rates as mixture conductivity evolves,
characterizing distinctive thermo-mechanical processes. The model highlights
essential aspects of rock-ice avalanches. Lateral heat productions play an
important role in temperature evolution. Fast moving avalanches produce higher
amount of heat. Fast ice melting results in substantial change in temperature.
We formally derive the melting efficiency dependent general fluid production
rate. The model includes internal mass and momentum exchanges between the
phases and mass and momentum productions due to entrainment. The latter
significantly changes the state of temperature; yet, the former exclusively
characterizes rock-ice avalanche. Temperature changes are rapid when heat
entrainment across the avalanche boundary is substantial. It also applies to
basal heat conduction. A strong coupling exists between phase mass and momentum
balances and the temperature equation. The new model offers the first-ever
complete dynamical solution for simulating rock-ice avalanche with changing
temperature.",http://arxiv.org/abs/2404.06130v2
"Learning of Balance Controller Considering Changes in Body State for
  Musculoskeletal Humanoids",2024-05-20T05:50:25Z,"Kento Kawaharazuka, Yoshimoto Ribayashi, Akihiro Miki, Yasunori Toshimitsu, Temma Suzuki, Kei Okada, Masayuki Inaba","The musculoskeletal humanoid is difficult to modelize due to the flexibility
and redundancy of its body, whose state can change over time, and so balance
control of its legs is challenging. There are some cases where ordinary PID
controls may cause instability. In this study, to solve these problems, we
propose a method of learning a correlation model among the joint angle, muscle
tension, and muscle length of the ankle and the zero moment point to perform
balance control. In addition, information on the changing body state is
embedded in the model using parametric bias, and the model estimates and adapts
to the current body state by learning this information online. This makes it
possible to adapt to changes in upper body posture that are not directly taken
into account in the model, since it is difficult to learn the complete dynamics
of the whole body considering the amount of data and computation. The model can
also adapt to changes in body state, such as the change in footwear and change
in the joint origin due to recalibration. The effectiveness of this method is
verified by a simulation and by using an actual musculoskeletal humanoid,
Musashi.",http://arxiv.org/abs/2405.11803v1
AI Alignment with Changing and Influenceable Reward Functions,2024-05-28T00:08:46Z,"Micah Carroll, Davis Foote, Anand Siththaranjan, Stuart Russell, Anca Dragan","Existing AI alignment approaches assume that preferences are static, which is
unrealistic: our preferences change, and may even be influenced by our
interactions with AI systems themselves. To clarify the consequences of
incorrectly assuming static preferences, we introduce Dynamic Reward Markov
Decision Processes (DR-MDPs), which explicitly model preference changes and the
AI's influence on them. We show that despite its convenience, the
static-preference assumption may undermine the soundness of existing alignment
techniques, leading them to implicitly reward AI systems for influencing user
preferences in ways users may not truly want. We then explore potential
solutions. First, we offer a unifying perspective on how an agent's
optimization horizon may partially help reduce undesirable AI influence. Then,
we formalize different notions of AI alignment that account for preference
change from the outset. Comparing the strengths and limitations of 8 such
notions of alignment, we find that they all either err towards causing
undesirable AI influence, or are overly risk-averse, suggesting that a
straightforward solution to the problems of changing preferences may not exist.
As there is no avoiding grappling with changing preferences in real-world
settings, this makes it all the more important to handle these issues with
care, balancing risks and capabilities. We hope our work can provide conceptual
clarity and constitute a first step towards AI alignment practices which
explicitly account for (and contend with) the changing and influenceable nature
of human preferences.",http://arxiv.org/abs/2405.17713v1
On an Interaction Model of General Language Change,2024-06-28T09:24:25Z,"Alfred Fuchs, Martin Schwingenheuer, Elisabeth Steinegger, Thomas Voglmaier","In the following article, we construct an interaction model (a variant of the
SIR-model) of general language change. In the context of language change it is
desirable to deduce the long-term behaviour of the corresponding dynamical
system (for example to decide if complete of reversible language change are
going to happen). We analyse this dynamical system by first proving
non-existence of periodic orbits and then invoking the Poincar\'{e}-Bendixson
theorem to show convergence to critical points only. Non-existence of periodic
orbits is established by contradiction in showing that the average position of
a potential periodic orbit must coincide with a certain critical point $C$
which cannot be encircled by the flow of the dynamical system so that the
average position would be pulled to that side. Thus the long-term behaviour of
the model for any given initial constellation of speakers depends only on four
interaction parameters and can be easily analysed by looking at the four
critical points. Subsequent numerical analysis of real data on language change
is used to justify the relevance of the constructed model for the practicing
quantitative linguist. We show how data-fitting methods can be used to
determine the four interaction parameters and predict from them the long-term
behaviour of the system, i.e. if complete language change or reversible
language change will take place.",http://arxiv.org/abs/2406.19775v1
"Using Photoplethysmography to Detect Real-time Blood Pressure Changes
  with a Calibration-free Deep Learning Model",2024-07-03T17:00:47Z,"Jingyuan Hong, Manasi Nandi, Weiwei Jin, Jordi Alastruey","Blood pressure (BP) changes are linked to individual health status in both
clinical and non-clinical settings. This study developed a deep learning model
to classify systolic (SBP), diastolic (DBP), and mean (MBP) BP changes using
photoplethysmography (PPG) waveforms. Data from the Vital Signs Database
(VitalDB) comprising 1,005 ICU patients with synchronized PPG and BP recordings
was used. BP changes were categorized into three labels: Spike (increase above
a threshold), Stable (change within a plus or minus threshold), and Dip
(decrease below a threshold). Four time-series classification models were
studied: multi-layer perceptron, convolutional neural network, residual
network, and Encoder. A subset of 500 patients was randomly selected for
training and validation, ensuring a uniform distribution across BP change
labels. Two test datasets were compiled: Test-I (n=500) with a uniform
distribution selection process, and Test-II (n=5) without. The study also
explored the impact of including second-deviation PPG (sdPPG) waveforms as
additional input information. The Encoder model with a Softmax weighting
process using both PPG and sdPPG waveforms achieved the highest detection
accuracy--exceeding 71.3% and 85.4% in Test-I and Test-II, respectively, with
thresholds of 30 mmHg for SBP, 15 mmHg for DBP, and 20 mmHg for MBP.
Corresponding F1-scores were over 71.8% and 88.5%. These findings confirm that
PPG waveforms are effective for real-time monitoring of BP changes in ICU
settings and suggest potential for broader applications.",http://arxiv.org/abs/2407.03274v1
"A consistent, volume preserving, and adaptive mesh refinement-based
  framework for modeling non-isothermal gas-liquid-solid flows with phase
  change",2024-07-08T03:49:41Z,"Ramakrishnan Thirumalaisamy, Amneet Pal Singh Bhalla","This work expands on our recently introduced low Mach enthalpy method [1] for
simulating the melting and solidification of a phase change material (PCM)
alongside (or without) an ambient gas phase. The method captures PCM's volume
change (shrinkage or expansion) by accounting for density change-induced flows.
We present several improvements to the original work. First, we introduce
consistent time integration schemes for the mass, momentum, and enthalpy
equations, which enhance the method stability. Demonstrating the effectiveness
of this scheme, we show that a system free of external forces and heat sources
can conserve its initial mass, momentum, enthalpy, and phase composition. This
allows the system to transition from a non-isothermal, non-equilibrium,
phase-changing state to an isothermal, equilibrium state without exhibiting
unrealistic behavior. Furthermore, we show that the low Mach enthalpy method
accurately simulates thermocapillary flows without introducing spurious phase
changes. We propose an analytical model to validate advanced CFD codes for
simulating metal manufacturing processes like welding and 3D printing. These
processes involve a heat source melting metal or alloy in an inert gas
environment. Traditionally, validation relied on manipulating material
properties to match complex experiments. Our model uses the Stefan problem with
a density jump to provide a straightforward method for validating multiphysics
simulations involving heat sources and phase changes in three-phase
flows.Lastly, we demonstrate the practical utility of the method in modeling
porosity defects (gas bubble trapping) during metal solidification. A field
extension technique is used to accurately apply surface tension forces in a
three-phase flow situation. This is where part of the bubble surface is trapped
within the (moving) solidification front.",http://arxiv.org/abs/2407.05588v2
Actuation without production bias,2024-07-15T19:38:03Z,"James Kirby, Morgan Sonderegger","Phonetic production bias is the external force most commonly invoked in
computational models of sound change, despite the fact that it is not
responsible for all, or even most, sound changes. Furthermore, the existence of
production bias alone cannot account for how changes do or do not propagate
throughout a speech community. While many other factors have been invoked by
(socio)phoneticians, including but not limited to contact (between
subpopulations) and differences in social evaluation (of variants, groups, or
individuals), these are not typically modeled in computational simulations of
sound change. In this paper, we consider whether production biases have a
unique dynamics in terms of how they impact the population-level spread of
change in a setting where agents learn from multiple teachers. We show that,
while the dynamics conditioned by production bias are not unique, it is not the
case that all perturbing forces have the same dynamics: in particular, if
social weight is a function of individual teachers and the correlation between
a teacher's social weight and the extent to which they realize a production
bias is weak, change is unlikely to propagate. Nevertheless, it remains the
case that changes initiated from different sources may display a similar
dynamics. A more nuanced understanding of how population structure interacts
with individual biases can thus provide a (partial) solution to the
`non-phonologization problem'.",http://arxiv.org/abs/2407.11202v1
Identification of changes in gene expression,2024-07-19T18:58:01Z,"Lucia Ameis, Kathrin M√∂llenhoff","Evaluating the change in gene expression is a common goal in many research
areas, such as in toxicological studies as well as in clinical trials. In
practice, the analysis is often based on multiple t-tests evaluated at the
observed time points. This severely limits the accuracy of determining the time
points at which the gene changes in expression. Even if a parametric approach
is chosen, the analysis is often restricted to identifying the onset of an
effect. In this paper, we propose a parametric method to identify the time
frame where the gene expression significantly changes. This is achieved by
fitting a parametric model to the time-response data and constructing a
confidence band for its first derivative. The confidence band is derived by a
flexible two step bootstrap approach, which can be applied to a wide variety of
possible curves. Our method focuses on the first derivative, since it provides
an easy to compute and reliable measure for the change in response. It is
summarised in terms of a hypothesis test, such that rejecting the null
hypothesis means detecting a significant change in gene expression.
Furthermore, a method for calculating confidence intervals for time points of
interest (e.g. the beginning and end of significant change) is developed. We
demonstrate the validity of our approach through a simulation study and present
a variety of different applications to mouse gene expression data from a study
investigating the effect of a Western diet on the progression of non-alcoholic
fatty liver disease.",http://arxiv.org/abs/2407.14630v1
"Counterfactual Explanations with Probabilistic Guarantees on their
  Robustness to Model Change",2024-08-09T03:35:53Z,"Ignacy Stƒôpka, Mateusz Lango, Jerzy Stefanowski","Counterfactual explanations (CFEs) guide users on how to adjust inputs to
machine learning models to achieve desired outputs. While existing research
primarily addresses static scenarios, real-world applications often involve
data or model changes, potentially invalidating previously generated CFEs and
rendering user-induced input changes ineffective. Current methods addressing
this issue often support only specific models or change types, require
extensive hyperparameter tuning, or fail to provide probabilistic guarantees on
CFE robustness to model changes. This paper proposes a novel approach for
generating CFEs that provides probabilistic guarantees for any model and change
type, while offering interpretable and easy-to-select hyperparameters. We
establish a theoretical framework for probabilistically defining robustness to
model change and demonstrate how our BetaRCE method directly stems from it.
BetaRCE is a post-hoc method applied alongside a chosen base CFE generation
method to enhance the quality of the explanation beyond robustness. It
facilitates a transition from the base explanation to a more robust one with
user-adjusted probability bounds. Through experimental comparisons with
baselines, we show that BetaRCE yields robust, most plausible, and closest to
baseline counterfactual explanations.",http://arxiv.org/abs/2408.04842v4
Cyclical period changes in cataclysmic variables: a statistical study,2024-08-14T23:31:26Z,"Leandro Souza, Raymundo Baptista","We report the results of a statistical study of cyclical period changes in
cataclysmic variables (CVs). Assuming the third-body hypothesis as the cause of
period changes, we estimate the third-body mass, $m_3$, and its separation from
the binary, $a_3$, for 21 CVs showing cyclical period changes from well-sampled
observed-minus-calculated diagrams covering more than a decade of observations.
The inferred $a_3$ values are independent of the binary orbital period,
$P_\mathrm{orb}$, whereas the $m_3$ values increase with $P_\mathrm{orb}$ by an
order of magnitude from the shortest period (oldest) to the longest period
(youngest) systems, implying significant mass loss from the third body with
time. A model for the time evolution of the triple system is not able to
simultaneously explain the observed behavior of the $m_3(P_\mathrm{orb})$ and
$a_3(P_\mathrm{orb})$ distributions because the combined mass loss from the
binary and the third body demands an increase in orbital separation by factors
$\sim 140$ as the binary evolves toward shorter $P_\mathrm{orb}$'s, in clear
disagreement with the observed distribution. We conclude that the third-body
hypothesis is statistically inconsistent and cannot be used to explain cyclical
period changes observed in CVs. On the other hand, the diagram of the amplitude
of the period change versus the CV donor-star mass is consistent both with the
alternative hypothesis that the observed cyclical period changes are a
consequence of magnetic activity in the solar-type donor star, and with the
standard evolutionary scenario for CVs.",http://arxiv.org/abs/2408.07850v1
"AnomalyCD: A benchmark for Earth anomaly change detection with
  high-resolution and time-series observations",2024-09-09T14:47:57Z,"Jingtao Li, Qian Zhu, Xinyu Wang, Hengwei Zhao, Yanfei Zhong","Various Earth anomalies have destroyed the stable, balanced state, resulting
in fatalities and serious destruction of property. With the advantages of
large-scale and precise observation, high-resolution remote sensing images have
been widely used for anomaly monitoring and localization. Powered by the deep
representation, the existing methods have achieved remarkable advances,
primarily in classification and change detection techniques. However, labeled
samples are difficult to acquire due to the low probability of anomaly
occurrence, and the trained models are limited to fixed anomaly categories,
which hinders the application for anomalies with few samples or unknown
anomalies. In this paper, to tackle this problem, we propose the anomaly change
detection (AnomalyCD) technique, which accepts time-series observations and
learns to identify anomalous changes by learning from the historical normal
change pattern. Compared to the existing techniques, AnomalyCD processes an
unfixed number of time steps and can localize the various anomalies in a
unified manner, without human supervision. To benchmark AnomalyCD, we
constructed a high-resolution dataset with time-series images dedicated to
various Earth anomalies (the AnomalyCDD dataset). AnomalyCDD contains
high-resolution (from 0.15 to 2.39 m/pixel), time-series (from 3 to 7 time
steps), and large-scale images (1927.93 km2 in total) collected globally
Furthermore, we developed a zero-shot baseline model (AnomalyCDM), which
implements the AnomalyCD technique by extracting a general representation from
the segment anything model (SAM) and conducting temporal comparison to
distinguish the anomalous changes from normal changes. AnomalyCDM is designed
as a two-stage workflow to enhance the efficiency, and has the ability to
process the unseen images directly, without retraining for each scene.",http://arxiv.org/abs/2409.05679v1
Detecting Change Points of Covariance Matrices in High Dimensions,2024-09-23T22:44:41Z,"Nina D√∂rnemann, Holger Dette","Testing for change points in sequences of high-dimensional covariance
matrices is an important and equally challenging problem in statistical
methodology with applications in various fields. Motivated by the observation
that even in cases where the ratio between dimension and sample size is as
small as $0.05$, tests based on a fixed-dimension asymptotics do not keep their
preassigned level, we propose to derive critical values of test statistics
using an asymptotic regime where the dimension diverges at the same rate as the
sample size.
  This paper introduces a novel and well-founded statistical methodology for
detecting change points in a sequence of high-dimensional covariance matrices.
Our approach utilizes a min-type statistic based on a sequential process of
likelihood ratio statistics. This is used to construct a test for the
hypothesis of the existence of a change point with a corresponding estimator
for its location. We provide theoretical guarantees for these inference tools
by thoroughly analyzing the asymptotic properties of the sequential process of
likelihood ratio statistics in the case where the dimension and sample size
converge with the same rate to infinity. In particular, we prove weak
convergence towards a Gaussian process under the null hypothesis of no change.
To identify the challenging dependency structure between consecutive test
statistics, we employ tools from random matrix theory and stochastic processes.
Moreover, we show that the new test attains power under a class of alternatives
reflecting changes in the bulk of the spectrum, and we prove consistency of the
estimator for the change-point location.",http://arxiv.org/abs/2409.15588v1
"Device for detection of activity-dependent changes in neural spheroids
  at MHz and GHz frequencies",2024-09-25T01:52:10Z,"Saeed Omidi, Gianluca Fabi, Xiaopeng Wang, James C. M. Hwang, Yevgeny Berdichevsky","Intracellular processes triggered by neural activity include changes in ionic
concentrations, protein release, and synaptic vesicle cycling. These processes
play significant roles in neurological disorders. The beneficial effects of
brain stimulation may also be mediated through intracellular changes. There is
a lack of label-free techniques for monitoring activity-dependent intracellular
changes. Electromagnetic (EM) waves at frequencies larger than 1x10^6 Hz (1
MHz) were previously used to probe intracellular contents of cells, as cell
membrane becomes transparent at this frequency range. EM waves interact with
membranes of intracellular organelles, proteins, and water in the MHz-GHz
range. In this work, we developed a device for probing the interaction between
intracellular contents of active neurons and EM waves. The device used an array
of grounded coplanar waveguides (GCPWs) to deliver EM waves to a
three-dimensional (3D) spheroid of rat cortical neurons. Neural activity was
evoked using optogenetics, with synchronous detection of propagation of EM
waves. Broadband measurements were conducted in the MHz-GHz range to track
changes in transmission coefficients. Neuronal activity was found to reversibly
alter EM wave transmission. Pharmacological suppression of neuronal activity
abolished changes in transmission. Time constants of changes in transmission
were in the range of seconds to tens of seconds, suggesting the presence of
relatively slow, activity-dependent intracellular processes. This study
provides the first evidence that EM transmission through neuronal tissue is
activity-dependent in MHz-GHz range. Device developed in this work may find
future applications in studies of the mechanisms of neurological disorders and
the development of new therapies.",http://arxiv.org/abs/2409.16552v1
"VFDelta: A Framework for Detecting Silent Vulnerability Fixes by
  Enhancing Code Change Learning",2024-09-25T04:13:08Z,"Xu Yang, Shaowei Wang, Jiayuan Zhou, Xing Hu","Vulnerability fixes in open source software (OSS) usually follow the
coordinated vulnerability disclosure model and are silently fixed. This delay
can expose OSS users to risks as malicious parties might exploit the software
before fixes are publicly known. Therefore, it is important to identify
vulnerability fixes early and automatically. Existing methods classify
vulnerability fixes by learning code change representations from commits,
typically by concatenating code changes, which does not effectively highlight
nuanced differences. Additionally, previous approaches fine-tune code embedding
models and classification models separately, which limits overall
effectiveness. We propose VFDelta, a lightweight yet effective framework that
embeds code before and after changes using independent models with surrounding
code as context. By performing element-wise subtraction on these embeddings, we
capture fine-grain changes. Our architecture allows joint training of embedding
and classification models, optimizing overall performance. Experiments
demonstrate that VFDelta achieves up to 0.33 F1 score and 0.63 CostEffort@5,
improving over state-of-the-art methods by 77.4% and 7.1%, respectively.
Ablation analysis confirms the importance of our code change representation in
capturing small changes. We also expanded the dataset and introduced a temporal
split to simulate real-world scenarios; VFDelta significantly outperforms
baselines VulFixMiner and MiDas across all metrics in this setting.",http://arxiv.org/abs/2409.16606v1
"3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical
  Object Rearrangement",2024-11-06T07:08:41Z,"Ziqi Lu, Jianbo Ye, John Leonard","We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for
detecting physical object rearrangements in 3D scenes. Our approach estimates
3D object-level changes by comparing two sets of unaligned images taken at
different times. Leveraging 3DGS's novel view rendering and EfficientSAM's
zero-shot segmentation capabilities, we detect 2D object-level changes, which
are then associated and fused across views to estimate 3D change masks and
object transformations. Our method can accurately identify changes in cluttered
environments using sparse (as few as one) post-change images within as little
as 18s. It does not rely on depth input, user instructions, pre-defined object
classes, or object models -- An object is recognized simply if it has been
re-arranged. Our approach is evaluated on both public and self-collected
real-world datasets, achieving up to 14% higher accuracy and three orders of
magnitude faster performance compared to the state-of-the-art
radiance-field-based change detection method. This significant performance
boost enables a broad range of downstream applications, where we highlight
three key use cases: object reconstruction, robot workspace reset, and 3DGS
model update. Our code and data will be made available at
https://github.com/520xyxyzq/3DGS-CD.",http://arxiv.org/abs/2411.03706v2
"Investigating Changing-Look Active Galactic Nuclei with Long-term
  Optical and X-Ray Observations",2024-11-13T15:10:05Z,"A. Jana, C. Ricci, M. J. Temple, H. -K. Chang, E. Shablovinskaya, B. Trakhtenbrot, Y. Diaz, D. Ilic, P. Nandi, M. Koss","Changing-look active galactic nuclei (CLAGNs) show the appearance and
disappearance of broad emission lines in their UV/optical spectra on timescales
of months to decades. Here, we investigate how CL transitions depend on several
AGN parameters such as accretion rate, obscuration properties and black hole
mass. We study a sample of 20 nearby optically-identified CLAGNs from the BAT
AGN Spectroscopic Survey (BASS), using quasi-simultaneous optical and X-ray
observations taken in the last $\sim 40$ years. We find that for all CLAGNs,
the transition is accompanied by a change in Eddington ratio. The CL
transitions are not associated with changes in the obscuration properties of
the AGN. CLAGNs are found to have a median Eddington ratio lower than the AGNs
in the BASS sample in which CL transitions were not detected. The median of the
transition Eddington ratio (Eddington ratio at which AGN changes its state) is
found to be $\sim 0.01$ for type 1 $\leftrightarrow$ 1.8/1.9/2 transition,
which is consistent with the hard $\leftrightarrow$ soft state transition in
black hole X-ray binaries. Most CL events are constrained to occur within 3-4
years, which is considerably shorter than the expected viscous timescale in AGN
accretion disk. The transitions of the optical CLAGNs studied here are likely
associated to state changes in the accretion flow, possibly driven by
disk-instability.",http://arxiv.org/abs/2411.08676v1
O-MAGIC: Online Change-Point Detection for Dynamic Systems,2024-11-19T06:56:40Z,"Yan Sun, Yeping Wang, Zhaohui Li, Shihao Yang","The capture of changes in dynamic systems, especially ordinary differential
equations (ODEs), is an important and challenging task, with multiple
applications in biomedical research and other scientific areas. This article
proposes a fast and mathematically rigorous online method, called ODE-informed
MAnifold-constrained Gaussian process Inference for Change point
detection(O-MAGIC), to detect changes of parameters in the ODE system using
noisy and sparse observation data. O-MAGIC imposes a Gaussian process prior to
the time series of system components with a latent manifold constraint, induced
by restricting the derivative process to satisfy ODE conditions. To detect the
parameter changes from the observation, we propose a procedure based on a
two-sample generalized likelihood ratio (GLR) test that can detect multiple
change points in the dynamic system automatically. O-MAGIC bypasses
conventional numerical integration and achieves substantial savings in
computation time. By incorporating the ODE structures through manifold
constraints, O-MAGIC enjoys a significant advantage in detection delay, while
following principled statistical construction under the Bayesian paradigm,
which further enables it to handle systems with missing data or unobserved
components. O-MAGIC can also be applied to general nonlinear systems.
Simulation studies on three challenging examples: SEIRD model, Lotka-Volterra
model and Lorenz model are provided to illustrate the robustness and efficiency
of O-MAGIC, compared with numerical integration and other popular
time-series-based change point detection benchmark methods.",http://arxiv.org/abs/2411.12277v1
"LaVIDE: A Language-Vision Discriminator for Detecting Changes in
  Satellite Image with Map References",2024-11-29T15:04:40Z,"Shuguo Jiang, Fang Xu, Sen Jia, Gui-Song Xia","Change detection, which typically relies on the comparison of bi-temporal
images, is significantly hindered when only a single image is available.
Comparing a single image with an existing map, such as OpenStreetMap, which is
continuously updated through crowd-sourcing, offers a viable solution to this
challenge. Unlike images that carry low-level visual details of ground objects,
maps convey high-level categorical information. This discrepancy in abstraction
levels complicates the alignment and comparison of the two data types. In this
paper, we propose a \textbf{La}nguage-\textbf{VI}sion \textbf{D}iscriminator
for d\textbf{E}tecting changes in satellite image with map references, namely
\ours{}, which leverages language to bridge the information gap between maps
and images. Specifically, \ours{} formulates change detection as the problem of
``{\textit Does the pixel belong to [class]?}'', aligning maps and images
within the feature space of the language-vision model to associate high-level
map categories with low-level image details. Moreover, we build a
mixture-of-experts discriminative module, which compares linguistic features
from maps with visual features from images across various semantic
perspectives, achieving comprehensive semantic comparison for change detection.
Extensive evaluation on four benchmark datasets demonstrates that \ours{} can
effectively detect changes in satellite image with map references,
outperforming state-of-the-art change detection algorithms, e.g., with gains of
about $13.8$\% on the DynamicEarthNet dataset and $4.3$\% on the SECOND
dataset.",http://arxiv.org/abs/2411.19758v1
"Syntactic Language Change in English and German: Metrics, Parsers, and
  Convergences",2024-02-18T11:46:16Z,"Yanran Chen, Wei Zhao, Anne Breitbarth, Manuel Stoeckel, Alexander Mehler, Steffen Eger","Many studies have shown that human languages tend to optimize for lower
complexity and increased communication efficiency. Syntactic dependency
distance, which measures the linear distance between dependent words, is often
considered a key indicator of language processing difficulty and working memory
load. The current paper looks at diachronic trends in syntactic language change
in both English and German, using corpora of parliamentary debates from the
last c. 160 years. We base our observations on five dependency parsers,
including the widely used Stanford CoreNLP as well as 4 newer alternatives. Our
analysis of syntactic language change goes beyond linear dependency distance
and explores 15 metrics relevant to dependency distance minimization (DDM)
and/or based on tree graph properties, such as the tree height and degree
variance. Even though we have evidence that recent parsers trained on modern
treebanks are not heavily affected by data 'noise' such as spelling changes and
OCR errors in our historic data, we find that results of syntactic language
change are sensitive to the parsers involved, which is a caution against using
a single parser for evaluating syntactic language change as done in previous
work. We also show that syntactic language change over the time period
investigated is largely similar between English and German for the different
metrics explored: only 4% of cases we examine yield opposite conclusions
regarding upwards and downtrends of syntactic metrics across German and
English. We also show that changes in syntactic measures seem to be more
frequent at the tails of sentence length distributions. To our best knowledge,
ours is the most comprehensive analysis of syntactic language change using
modern NLP technology in recent corpora of English and German.",http://arxiv.org/abs/2402.11549v2
"ObjectCompose: Evaluating Resilience of Vision-Based Models on
  Object-to-Background Compositional Changes",2024-03-07T17:48:48Z,"Hashmat Shadab Malik, Muhammad Huzaifa, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan","Given the large-scale multi-modal training of recent vision-based models and
their generalization capabilities, understanding the extent of their robustness
is critical for their real-world deployment. In this work, we evaluate the
resilience of current vision-based models against diverse object-to-background
context variations. The majority of robustness evaluation methods have
introduced synthetic datasets to induce changes to object characteristics
(viewpoints, scale, color) or utilized image transformation techniques
(adversarial changes, common corruptions) on real images to simulate shifts in
distributions. Recent works have explored leveraging large language models and
diffusion models to generate changes in the background. However, these methods
either lack in offering control over the changes to be made or distort the
object semantics, making them unsuitable for the task. Our method, on the other
hand, can induce diverse object-to-background changes while preserving the
original semantics and appearance of the object. To achieve this goal, we
harness the generative capabilities of text-to-image, image-to-text, and
image-to-segment models to automatically generate a broad spectrum of
object-to-background changes. We induce both natural and adversarial background
changes by either modifying the textual prompts or optimizing the latents and
textual embedding of text-to-image models. We produce various versions of
standard vision datasets (ImageNet, COCO), incorporating either diverse and
realistic backgrounds into the images or introducing color, texture, and
adversarial changes in the background. We conduct extensive experiments to
analyze the robustness of vision-based models against object-to-background
context variations across diverse tasks. Code
https://github.com/Muhammad-Huzaifaa/ObjectCompose.",http://arxiv.org/abs/2403.04701v4
"CityPulse: Fine-Grained Assessment of Urban Change with Street View Time
  Series",2024-01-02T08:57:09Z,"Tianyuan Huang, Zejia Wu, Jiajun Wu, Jackelyn Hwang, Ram Rajagopal","Urban transformations have profound societal impact on both individuals and
communities at large. Accurately assessing these shifts is essential for
understanding their underlying causes and ensuring sustainable urban planning.
Traditional measurements often encounter constraints in spatial and temporal
granularity, failing to capture real-time physical changes. While street view
imagery, capturing the heartbeat of urban spaces from a pedestrian point of
view, can add as a high-definition, up-to-date, and on-the-ground visual proxy
of urban change. We curate the largest street view time series dataset to date,
and propose an end-to-end change detection model to effectively capture
physical alterations in the built environment at scale. We demonstrate the
effectiveness of our proposed method by benchmark comparisons with previous
literature and implementing it at the city-wide level. Our approach has the
potential to supplement existing dataset and serve as a fine-grained and
accurate assessment of urban change.",http://arxiv.org/abs/2401.01107v2
"Change Detection Between Optical Remote Sensing Imagery and Map Data via
  Segment Anything Model (SAM)",2024-01-17T07:30:52Z,"Hongruixuan Chen, Jian Song, Naoto Yokoya","Unsupervised multimodal change detection is pivotal for time-sensitive tasks
and comprehensive multi-temporal Earth monitoring. In this study, we explore
unsupervised multimodal change detection between two key remote sensing data
sources: optical high-resolution imagery and OpenStreetMap (OSM) data.
Specifically, we propose to utilize the vision foundation model Segmentation
Anything Model (SAM), for addressing our task. Leveraging SAM's exceptional
zero-shot transfer capability, high-quality segmentation maps of optical images
can be obtained. Thus, we can directly compare these two heterogeneous data
forms in the so-called segmentation domain. We then introduce two strategies
for guiding SAM's segmentation process: the 'no-prompt' and 'box/mask prompt'
methods. The two strategies are designed to detect land-cover changes in
general scenarios and to identify new land-cover objects within existing
backgrounds, respectively. Experimental results on three datasets indicate that
the proposed approach can achieve more competitive results compared to
representative unsupervised multimodal change detection methods.",http://arxiv.org/abs/2401.09019v1
"Graph-based Clustering for Detecting Semantic Change Across Time and
  Languages",2024-02-01T21:27:19Z,"Xianghe Ma, Michael Strube, Wei Zhao","Despite the predominance of contextualized embeddings in NLP, approaches to
detect semantic change relying on these embeddings and clustering methods
underperform simpler counterparts based on static word embeddings. This stems
from the poor quality of the clustering methods to produce sense clusters --
which struggle to capture word senses, especially those with low frequency.
This issue hinders the next step in examining how changes in word senses in one
language influence another. To address this issue, we propose a graph-based
clustering approach to capture nuanced changes in both high- and low-frequency
word senses across time and languages, including the acquisition and loss of
these senses over time. Our experimental results show that our approach
substantially surpasses previous approaches in the SemEval2020 binary
classification task across four languages. Moreover, we showcase the ability of
our approach as a versatile visualization tool to detect semantic changes in
both intra-language and inter-language setups. We make our code and data
publicly available.",http://arxiv.org/abs/2402.01025v1
Bivariate change point detection in movement direction and speed,2024-02-04T13:32:34Z,"Solveig Plomer, Theresa Ernst, Philipp Gebhardt, Enrico Schleiff, Ralph Neininger, Gaby Schneider","Biological movement patterns can sometimes be quasi linear with abrupt
changes in direction and speed, as in plastids in root cells investigated here.
For the analysis of such changes we propose a new stochastic model for movement
along linear structures. Maximum likelihood estimators are provided, and due to
serial dependencies of increments, the classical MOSUM statistic is replaced by
a moving kernel estimator. Convergence of the resulting difference process and
strong consistency of the variance estimator are shown. We estimate the change
points and propose a graphical technique to distinguish between change points
in movement direction and speed.",http://arxiv.org/abs/2402.02489v2
"A Systematic Comparison of Contextualized Word Embeddings for Lexical
  Semantic Change",2024-02-19T10:04:59Z,"Francesco Periti, Nina Tahmasebi","Contextualized embeddings are the preferred tool for modeling Lexical
Semantic Change (LSC). Current evaluations typically focus on a specific task
known as Graded Change Detection (GCD). However, performance comparison across
work are often misleading due to their reliance on diverse settings. In this
paper, we evaluate state-of-the-art models and approaches for GCD under equal
conditions. We further break the LSC problem into Word-in-Context (WiC) and
Word Sense Induction (WSI) tasks, and compare models across these different
levels. Our evaluation is performed across different languages on eight
available benchmarks for LSC, and shows that (i) APD outperforms other
approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for
WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need
for improving the modeling of word meanings, as well as focus on how, when, and
why these meanings change, rather than solely focusing on the extent of
semantic change.",http://arxiv.org/abs/2402.12011v3
"Sequential Change-point Detection for Compositional Time Series with
  Exogenous Variables",2024-02-28T07:44:23Z,"Yajun Liu, Beth Andrews","Sequential change-point detection for time series enables us to sequentially
check the hypothesis that the model still holds as more and more data are
observed. It is widely used in data monitoring in practice. In this work, we
consider sequential change-point detection for compositional time series, time
series in which the observations are proportions. For fitting compositional
time series, we propose a generalized Beta AR(1) model, which can incorporate
exogenous variables upon which the time series observations are dependent. We
show the compositional time series are strictly stationary and geometrically
ergodic and consider maximum likelihood estimation for model parameters. We
show the partial MLEs are consistent and asymptotically normal and propose a
parametric sequential change-point detection method for the compositional time
series model. The change-point detection method is illustrated using a time
series of Covid-19 positivity rates.",http://arxiv.org/abs/2402.18130v1
"A New Dynamic Distributed Planning Approach: Application to DPDP
  Problems",2024-02-24T10:06:04Z,Zakaria Tolba,"In this work, we proposed a new dynamic distributed planning approach that is
able to take into account the changes that the agent introduces on his set of
actions to be planned in order to take into account the changes that occur in
his environment. Our approach fits into the context of distributed planning for
distributed plans where each agent can produce its own plans. According to our
approach the generation of the plans is based on the satisfaction of the
constraints by the use of the genetic algorithms. Our approach is to generate,
a new plan by each agent, whenever there is a change in its set of actions to
plan. This in order to take into account the new actions introduced in its new
plan. In this new plan, the agent takes, each time, as a new action set to plan
all the old un-executed actions of the old plan and the new actions engendered
by the changes and as a new initial state; the state in which the set of
actions of the agent undergoes a change. In our work, we used a concrete case
to illustrate and demonstrate the utility of our approach.",http://arxiv.org/abs/2403.00805v1
Topology Change from Pointlike Sources,2024-03-07T07:24:41Z,"Yasha Neiman, David O'Connell","In this paper we study topology-changing spacetimes occurring from pointlike
sources. Following an old idea of Penrose, we will opt for a non-Hausdorff
model of topology change in which an initial pointlike source is ``doubled"" and
allowed to propagate along null rays into an eventual cobordism. By appealing
to recent developments in non-Hausdorff differential geometry, we will describe
and evaluate gravitational actions on these topology-changing spacetimes.
Motivated by analogous results for the Trousers space, we describe a sign
convention for Lorentzian angles that will ensure the dampening of our
non-Hausdorff topology-changing spacetimes within a two-dimensional path
integral for gravity.",http://arxiv.org/abs/2403.04281v2
Counterfactual Reasoning with Knowledge Graph Embeddings,2024-03-11T17:21:39Z,"Lena Zellinger, Andreas Stephan, Benjamin Roth","Knowledge graph embeddings (KGEs) were originally developed to infer true but
missing facts in incomplete knowledge repositories. In this paper, we link
knowledge graph completion and counterfactual reasoning via our new task CFKGR.
We model the original world state as a knowledge graph, hypothetical scenarios
as edges added to the graph, and plausible changes to the graph as inferences
from logical rules. We create corresponding benchmark datasets, which contain
diverse hypothetical scenarios with plausible changes to the original knowledge
graph and facts that should be retained. We develop COULDD, a general method
for adapting existing knowledge graph embeddings given a hypothetical premise,
and evaluate it on our benchmark. Our results indicate that KGEs learn patterns
in the graph without explicit training. We further observe that KGEs adapted
with COULDD solidly detect plausible counterfactual changes to the graph that
follow these patterns. An evaluation on human-annotated data reveals that KGEs
adapted with COULDD are mostly unable to recognize changes to the graph that do
not follow learned inference rules. In contrast, ChatGPT mostly outperforms
KGEs in detecting plausible changes to the graph but has poor knowledge
retention. In summary, CFKGR connects two previously distinct areas, namely KG
completion and counterfactual reasoning.",http://arxiv.org/abs/2403.06936v1
"Small Price Changes, Sales Volume, and Menu Cost",2024-03-11T21:09:30Z,"Doron Sayag, Avichai Snir, Daniel Levy","The finding of small price changes in many retail price datasets is often
viewed as a puzzle. We show that a possible explanation for the presence of
small price changes is related to sales volume, an observation that has been
overlooked in the existing literature. Analyzing a large retail scanner price
dataset that contains information on both prices and sales volume, we find that
small price changes are more frequent when products sales volume is high. This
finding holds across product categories, within product categories, and for
individual products. It is also robust to various sensitivity analyses such as
measurement errors, the definition of small price changes, the inclusion of
measures of price synchronization, the size of producers, the time horizon used
to compute the average sales volume, the revenues, the competition, shoppers
characteristics, etc.",http://arxiv.org/abs/2403.07166v1
Bilocal holography and locality in the bulk,2024-03-12T12:46:24Z,"Robert de Mello Koch, Garreth Kemp, Hendrik J. R. Van Zyl","Bilocal holography provides a constructive approach to the vector
model/higher spin gravity duality. It has two ingredients: a change of field
variables and a change of space time coordinates. The change of field variables
ensures that the loop expansion parameter becomes ${1\over N}$. The change of
coordinates solves the Clebsch-Gordan problem of moving from the tensor product
basis (in which the collective bilocal field is written) to the direct sum
basis (appropriate for the description of the gravity fields). We argue that
the change of space time coordinates can be deduced by requiring that operators
constructed in the bilocal collective field theory are dual to local operators
in the AdS bulk.",http://arxiv.org/abs/2403.07606v2
Multiscale Quantile Regression with Local Error Control,2024-03-17T22:12:48Z,"Zhi Liu, Housen Li","For robust and efficient detection of change points, we introduce a novel
methodology MUSCLE (multiscale quantile segmentation controlling local error)
that partitions serial data into multiple segments, each sharing a common
quantile. It leverages multiple tests for quantile changes over different
scales and locations, and variational estimation. Unlike the often adopted
global error control, MUSCLE focuses on local errors defined on individual
segments, significantly improving detection power in finding change points.
Meanwhile, due to the built-in model complexity penalty, it enjoys the finite
sample guarantee that its false discovery rate (or the expected proportion of
falsely detected change points) is upper bounded by its unique tuning
parameter. Further, we obtain the consistency and the localisation error rates
in estimating change points, under mild signal-to-noise-ratio conditions. Both
match (up to log factors) the minimax optimality results in the Gaussian setup.
All theories hold under the only distributional assumption of serial
independence. Incorporating the wavelet tree data structure, we develop an
efficient dynamic programming algorithm for computing MUSCLE. Extensive
simulations as well as real data applications in electrophysiology and
geophysics demonstrate its competitiveness and effectiveness. An implementation
via R package muscle is available from GitHub.",http://arxiv.org/abs/2403.11356v1
"Advanced Feature Manipulation for Enhanced Change Detection Leveraging
  Natural Language Models",2024-03-23T22:07:32Z,"Zhenglin Li, Yangchen Huang, Mengran Zhu, Jingyu Zhang, JingHao Chang, Houze Liu","Change detection is a fundamental task in computer vision that processes a
bi-temporal image pair to differentiate between semantically altered and
unaltered regions. Large language models (LLMs) have been utilized in various
domains for their exceptional feature extraction capabilities and have shown
promise in numerous downstream applications. In this study, we harness the
power of a pre-trained LLM, extracting feature maps from extensive datasets,
and employ an auxiliary network to detect changes. Unlike existing LLM-based
change detection methods that solely focus on deriving high-quality feature
maps, our approach emphasizes the manipulation of these feature maps to enhance
semantic relevance.",http://arxiv.org/abs/2403.15943v2
Clustering Change Sign Detection by Fusing Mixture Complexity,2024-03-27T05:50:23Z,"Kento Urano, Ryo Yuki, Kenji Yamanishi","This paper proposes an early detection method for cluster structural changes.
Cluster structure refers to discrete structural characteristics, such as the
number of clusters, when data are represented using finite mixture models, such
as Gaussian mixture models. We focused on scenarios in which the cluster
structure gradually changed over time. For finite mixture models, the concept
of mixture complexity (MC) measures the continuous cluster size by considering
the cluster proportion bias and overlap between clusters. In this paper, we
propose MC fusion as an extension of MC to handle situations in which multiple
mixture numbers are possible in a finite mixture model. By incorporating the
fusion of multiple models, our approach accurately captured the cluster
structure during transitional periods of gradual change. Moreover, we introduce
a method for detecting changes in the cluster structure by examining the
transition of MC fusion. We demonstrate the effectiveness of our method through
empirical analysis using both artificial and real-world datasets.",http://arxiv.org/abs/2403.18269v1
"Partially-Observable Sequential Change-Point Detection for
  Autocorrelated Data via Upper Confidence Region",2024-03-30T02:32:53Z,"Haijie Xu, Xiaochen Xian, Chen Zhang, Kaibo Liu","Sequential change point detection for multivariate autocorrelated data is a
very common problem in practice. However, when the sensing resources are
limited, only a subset of variables from the multivariate system can be
observed at each sensing time point. This raises the problem of partially
observable multi-sensor sequential change point detection. For it, we propose a
detection scheme called adaptive upper confidence region with state space model
(AUCRSS). It models multivariate time series via a state space model (SSM), and
uses an adaptive sampling policy for efficient change point detection and
localization. A partially-observable Kalman filter algorithm is developed for
online inference of SSM, and accordingly, a change point detection scheme based
on a generalized likelihood ratio test is developed. How its detection power
relates to the adaptive sampling strategy is analyzed. Meanwhile, by treating
the detection power as a reward, its connection with the online combinatorial
multi-armed bandit (CMAB) problem is formulated and an adaptive upper
confidence region algorithm is proposed for adaptive sampling policy design.
Theoretical analysis of the asymptotic average detection delay is performed,
and thorough numerical studies with synthetic data and real-world data are
conducted to demonstrate the effectiveness of our method.",http://arxiv.org/abs/2404.00220v1
"HANet: A Hierarchical Attention Network for Change Detection With
  Bitemporal Very-High-Resolution Remote Sensing Images",2024-04-14T08:01:27Z,"Chengxi Han, Chen Wu, Haonan Guo, Meiqi Hu, Hongruixuan Chen","Benefiting from the developments in deep learning technology,
deep-learning-based algorithms employing automatic feature extraction have
achieved remarkable performance on the change detection (CD) task. However, the
performance of existing deep-learning-based CD methods is hindered by the
imbalance between changed and unchanged pixels. To tackle this problem, a
progressive foreground-balanced sampling strategy on the basis of not adding
change information is proposed in this article to help the model accurately
learn the features of the changed pixels during the early training process and
thereby improve detection performance.Furthermore, we design a discriminative
Siamese network, hierarchical attention network (HANet), which can integrate
multiscale features and refine detailed features. The main part of HANet is the
HAN module, which is a lightweight and effective self-attention mechanism.
Extensive experiments and ablation studies on two CDdatasets with extremely
unbalanced labels validate the effectiveness and efficiency of the proposed
method.",http://arxiv.org/abs/2404.09178v1
"Single-temporal Supervised Remote Change Detection for Domain
  Generalization",2024-04-17T12:38:58Z,"Qiangang Du, Jinlong Peng, Xu Chen, Qingdong He, Liren He, Qiang Nie, Wenbing Zhu, Mingmin Chi, Yabiao Wang, Chengjie Wang","Change detection is widely applied in remote sensing image analysis. Existing
methods require training models separately for each dataset, which leads to
poor domain generalization. Moreover, these methods rely heavily on large
amounts of high-quality pair-labelled data for training, which is expensive and
impractical. In this paper, we propose a multimodal contrastive learning
(ChangeCLIP) based on visual-language pre-training for change detection domain
generalization. Additionally, we propose a dynamic context optimization for
prompt learning. Meanwhile, to address the data dependency issue of existing
methods, we introduce a single-temporal and controllable AI-generated training
strategy (SAIN). This allows us to train the model using a large number of
single-temporal images without image pairs in the real world, achieving
excellent generalization. Extensive experiments on series of real change
detection datasets validate the superiority and strong generalization of
ChangeCLIP, outperforming state-of-the-art change detection methods. Code will
be available.",http://arxiv.org/abs/2404.11326v4
"Change-point analysis for binomial autoregressive model with application
  to price stability counts",2024-04-22T01:58:19Z,"Danshu Sheng, Chang Liu, Yao Kang","The first-order binomial autoregressive (BAR(1)) model is the most frequently
used tool to analyze the bounded count time series. The BAR(1) model is
stationary and assumes process parameters to remain constant throughout the
time period, which may be incompatible with the non-stationary real data, which
indicates piecewise stationary characteristic. To better analyze the
non-stationary bounded count time series, this article introduces the BAR(1)
process with multiple change-points, which contains the BAR(1) model as a
special case. Our primary goals are not only to detect the change-points, but
also to give a solution to estimate the number and locations of the
change-points. For this, the cumulative sum (CUSUM) test and minimum
description length (MDL) principle are employed to deal with the testing and
estimation problems. The proposed approaches are also applied to analysis of
the Harmonised Index of Consumer Prices of the European Union.",http://arxiv.org/abs/2404.13825v1
"Inference for multiple change-points in generalized integer-valued
  autoregressive model",2024-04-22T02:21:26Z,"Danshu Sheng, Dehui Wang","In this paper, we propose a computationally valid and theoretically justified
methods, the likelihood ratio scan method (LRSM), for estimating multiple
change-points in a piecewise stationary generalized conditional integer-valued
autoregressive process. LRSM with the usual window parameter $h$ is more
satisfied to be used in long-time series with few and even change-points vs.
LRSM with the multiple window parameter $h_{mix}$ performs well in short-time
series with large and dense change-points. The computational complexity of LRSM
can be efficiently performed with order $O((\log n)^3 n)$. Moreover, two
bootstrap procedures, namely parametric and block bootstrap, are developed for
constructing confidence intervals (CIs) for each of the change-points.
Simulation experiments and real data analysis show that the LRSM and bootstrap
procedures have excellent performance and are consistent with the theoretical
analysis.",http://arxiv.org/abs/2404.13834v1
"Zero-shot Degree of Ill-posedness Estimation for Active Small Object
  Change Detection",2024-05-10T01:56:39Z,"Koji Takeda, Kanji Tanaka, Yoshimasa Nakamura, Asako Kanezaki","In everyday indoor navigation, robots often needto detect non-distinctive
small-change objects (e.g., stationery,lost items, and junk, etc.) to maintain
domain knowledge. Thisis most relevant to ground-view change detection (GVCD),
a recently emerging research area in the field of computer vision.However,
these existing techniques rely on high-quality class-specific object priors to
regularize a change detector modelthat cannot be applied to semantically
nondistinctive smallobjects. To address ill-posedness, in this study, we
explorethe concept of degree-of-ill-posedness (DoI) from the newperspective of
GVCD, aiming to improve both passive and activevision. This novel DoI problem
is highly domain-dependent,and manually collecting fine-grained annotated
training datais expensive. To regularize this problem, we apply the conceptof
self-supervised learning to achieve efficient DoI estimationscheme and
investigate its generalization to diverse datasets.Specifically, we tackle the
challenging issue of obtaining self-supervision cues for semantically
non-distinctive unseen smallobjects and show that novel ""oversegmentation cues""
from openvocabulary semantic segmentation can be effectively exploited.When
applied to diverse real datasets, the proposed DoI modelcan boost
state-of-the-art change detection models, and it showsstable and consistent
improvements when evaluated on real-world datasets.",http://arxiv.org/abs/2405.06185v1
Trustworthy Actionable Perturbations,2024-05-18T06:14:00Z,"Jesse Friedbaum, Sudarshan Adiga, Ravi Tandon","Counterfactuals, or modified inputs that lead to a different outcome, are an
important tool for understanding the logic used by machine learning classifiers
and how to change an undesirable classification. Even if a counterfactual
changes a classifier's decision, however, it may not affect the true underlying
class probabilities, i.e. the counterfactual may act like an adversarial attack
and ``fool'' the classifier. We propose a new framework for creating modified
inputs that change the true underlying probabilities in a beneficial way which
we call Trustworthy Actionable Perturbations (TAP). This includes a novel
verification procedure to ensure that TAP change the true class probabilities
instead of acting adversarially. Our framework also includes new cost, reward,
and goal definitions that are better suited to effectuating change in the real
world. We present PAC-learnability results for our verification procedure and
theoretically analyze our new method for measuring reward. We also develop a
methodology for creating TAP and compare our results to those achieved by
previous counterfactual methods.",http://arxiv.org/abs/2405.11195v1
"Active Object Detection with Knowledge Aggregation and Distillation from
  Large Models",2024-05-21T05:39:31Z,"Dejie Yang, Yang Liu","Accurately detecting active objects undergoing state changes is essential for
comprehending human interactions and facilitating decision-making. The existing
methods for active object detection (AOD) primarily rely on visual appearance
of the objects within input, such as changes in size, shape and relationship
with hands. However, these visual changes can be subtle, posing challenges,
particularly in scenarios with multiple distracting no-change instances of the
same category. We observe that the state changes are often the result of an
interaction being performed upon the object, thus propose to use informed
priors about object related plausible interactions (including semantics and
visual appearance) to provide more reliable cues for AOD. Specifically, we
propose a knowledge aggregation procedure to integrate the aforementioned
informed priors into oracle queries within the teacher decoder, offering more
object affordance commonsense to locate the active object. To streamline the
inference process and reduce extra knowledge inputs, we propose a knowledge
distillation approach that encourages the student decoder to mimic the
detection capabilities of the teacher decoder using the oracle query by
replicating its predictions and attention. Our proposed framework achieves
state-of-the-art performance on four datasets, namely Ego4D, Epic-Kitchens,
MECCANO, and 100DOH, which demonstrates the effectiveness of our approach in
improving AOD.",http://arxiv.org/abs/2405.12509v1
CoLa-DCE -- Concept-guided Latent Diffusion Counterfactual Explanations,2024-06-03T14:27:46Z,"Franz Motzkus, Christian Hellert, Ute Schmid","Recent advancements in generative AI have introduced novel prospects and
practical implementations. Especially diffusion models show their strength in
generating diverse and, at the same time, realistic features, positioning them
well for generating counterfactual explanations for computer vision models.
Answering ""what if"" questions of what needs to change to make an image
classifier change its prediction, counterfactual explanations align well with
human understanding and consequently help in making model behavior more
comprehensible. Current methods succeed in generating authentic
counterfactuals, but lack transparency as feature changes are not directly
perceivable. To address this limitation, we introduce Concept-guided Latent
Diffusion Counterfactual Explanations (CoLa-DCE). CoLa-DCE generates
concept-guided counterfactuals for any classifier with a high degree of control
regarding concept selection and spatial conditioning. The counterfactuals
comprise an increased granularity through minimal feature changes. The
reference feature visualization ensures better comprehensibility, while the
feature localization provides increased transparency of ""where"" changed ""what"".
We demonstrate the advantages of our approach in minimality and
comprehensibility across multiple image classification models and datasets and
provide insights into how our CoLa-DCE explanations help comprehend model
errors like misclassification cases.",http://arxiv.org/abs/2406.01649v1
"Structure Phase Change Induced by Nonequilibrium Effects in Molecular
  Scale Junctions",2024-06-13T01:12:08Z,"Hao Wang, Kah-Meng Yam, Zhuoling Jiang, Na Guo, Chun Zhang","The interrelationship between a material's structure and its properties lies
at the heart of materials-related research. Finding how the changes of one
affect the other is of primary importance in theoretical and computational
materials studies. In this work, based on Hershfield nonequilibrium quantum
statistics and the mean-field approach with steady-state density functional
theory, we derive a first-principles method to calculate nonequilibrium effects
induced forces acting on atoms, enabling structure optimizations and molecular
dynamics simulations for molecular junctions under external biases. By applying
the method to a few molecular devices, we found that in general, the external
bias can induce profound nonequilibrium effects on both electronic/transport
properties and the geometric structure of these devices, and consequent changes
in electronic properties and geometric structure are closely interrelated.
Particularly, when the bias voltage is above 1.0 V, significant structure phase
changes could occur, causing dramatic changes in I-V characteristics and
vibrational spectra. These findings greatly broaden our understanding of
quantum electronic devices and provide a new avenue for discovering novel
transport phenomena at molecular scale.",http://arxiv.org/abs/2406.08729v1
Definition generation for lexical semantic change detection,2024-06-20T10:13:08Z,"Mariia Fedorova, Andrey Kutuzov, Yves Scherrer","We use contextualized word definitions generated by large language models as
semantic representations in the task of diachronic lexical semantic change
detection (LSCD). In short, generated definitions are used as `senses', and the
change score of a target word is retrieved by comparing their distributions in
two time periods under comparison. On the material of five datasets and three
languages, we show that generated definitions are indeed specific and general
enough to convey a signal sufficient to rank sets of words by the degree of
their semantic change over time. Our approach is on par with or outperforms
prior non-supervised sense-based LSCD methods. At the same time, it preserves
interpretability and allows to inspect the reasons behind a specific shift in
terms of discrete definitions-as-senses. This is another step in the direction
of explainable semantic change modeling.",http://arxiv.org/abs/2406.14167v2
"On the Multivariate Generalized Counting Process and its Time-Changed
  Variants",2024-07-08T17:30:16Z,"K. K. Kataria, M. Dhillon","In this paper, we study a multivariate version of the generalized counting
process (GCP) and discuss its various time-changed variants. The time is
changed using random processes such as the stable subordinator, inverse stable
subordinator, and their composition, tempered stable subordinator, gamma
subordinator $etc.$ Several distributional properties that include the
probability generating function, probability mass function and their governing
differential equations are obtained for these variants. It is shown that some
of these time-changed processes are L\'evy and for such processes we have
derived the associated L\'evy measure. The explicit expressions for the
covariance and codifference of the component processes for some of these
time-changed variants are obtained. An application of the multivariate
generalized space fractional counting process to shock models is discussed.",http://arxiv.org/abs/2407.06156v2
"RIO-CPD: A Riemannian Geometric Method for Correlation-aware Online
  Change Point Detection",2024-07-12T21:42:51Z,"Chengyuan Deng, Zhengzhang Chen, Xujiang Zhao, Haoyu Wang, Junxiang Wang, Haifeng Chen, Jie Gao","Change point detection aims to identify abrupt shifts occurring at multiple
points within a data sequence. This task becomes particularly challenging in
the online setting, where different types of changes can occur, including
shifts in both the marginal and joint distributions of the data. In this paper,
we address these challenges by tracking the Riemannian geometry of correlation
matrices, allowing Riemannian metrics to compute the geodesic distance as an
accurate measure of correlation dynamics. We introduce Rio-CPD, a
non-parametric, correlation-aware online change point detection framework that
integrates the Riemannian geometry of the manifold of symmetric positive
definite matrices with the cumulative sum (CUSUM) statistic for detecting
change points. Rio-CPD employs a novel CUSUM design by computing the geodesic
distance between current observations and the Fr\'echet mean of prior
observations. With appropriate choices of Riemannian metrics, Rio-CPD offers a
simple yet effective and computationally efficient algorithm. Experimental
results on both synthetic and real-world datasets demonstrate that Rio-CPD
outperforms existing methods on detection accuracy, average detection delay and
efficiency.",http://arxiv.org/abs/2407.09698v2
"A Transformation Theorem for Transverse Signature-Type Changing
  Semi-Riemannian Manifolds",2024-07-12T21:45:23Z,"W. Hasse, N. E. Rieger","In the early eighties Hartle and Hawking put forth that signature-type change
may be conceptually interesting, paving the way to the so-called 'no boundary'
proposal for the initial conditions for the universe. Such singularity-free
universes have no beginning, but they do have an origin of time. In
mathematical terms, we are dealing with signature-type changing manifolds where
a Riemannian region (i.e., a region with a positive definite metric) is
smoothly joined to a Lorentzian region at the surface of transition where time
begins.
  We present a transformation prescription to transform an arbitrary Lorentzian
manifold into a singular signature-type changing manifold. Then we establish
the Transformation Theorem, asserting that, conversely, under certain
conditions, such a metric $(M,\tilde{g})$ can be obtained from some Lorentz
metric $g$ through the aforementioned transformation procedure. By augmenting
the assumption by certain constraints, mutatis mutandis, the global version of
the Transformation Theorem can be proven as well. In conclusion, we make use of
the Transformation Prescription to demonstrate that the induced metric on the
hypersurface of signature change is either Riemannian or a positive
semi-definite pseudo metric.",http://arxiv.org/abs/2407.09699v1
"Distractors-Immune Representation Learning with Cross-modal Contrastive
  Regularization for Change Captioning",2024-07-16T13:00:33Z,"Yunbin Tu, Liang Li, Li Su, Chenggang Yan, Qingming Huang","Change captioning aims to succinctly describe the semantic change between a
pair of similar images, while being immune to distractors (illumination and
viewpoint changes). Under these distractors, unchanged objects often appear
pseudo changes about location and scale, and certain objects might overlap
others, resulting in perturbational and discrimination-degraded features
between two images. However, most existing methods directly capture the
difference between them, which risk obtaining error-prone difference features.
In this paper, we propose a distractors-immune representation learning network
that correlates the corresponding channels of two image representations and
decorrelates different ones in a self-supervised manner, thus attaining a pair
of stable image representations under distractors. Then, the model can better
interact them to capture the reliable difference features for caption
generation. To yield words based on the most related difference features, we
further design a cross-modal contrastive regularization, which regularizes the
cross-modal alignment by maximizing the contrastive alignment between the
attended difference features and generated words. Extensive experiments show
that our method outperforms the state-of-the-art methods on four public
datasets. The code is available at https://github.com/tuyunbin/DIRL.",http://arxiv.org/abs/2407.11683v1
"EfficientCD: A New Strategy For Change Detection Based With Bi-temporal
  Layers Exchanged",2024-07-22T19:11:50Z,"Sijun Dong, Yuwei Zhu, Geng Chen, Xiaoliang Meng","With the widespread application of remote sensing technology in environmental
monitoring, the demand for efficient and accurate remote sensing image change
detection (CD) for natural environments is growing. We propose a novel deep
learning framework named EfficientCD, specifically designed for remote sensing
image change detection. The framework employs EfficientNet as its backbone
network for feature extraction. To enhance the information exchange between
bi-temporal image feature maps, we have designed a new Feature Pyramid Network
module targeted at remote sensing change detection, named ChangeFPN.
Additionally, to make full use of the multi-level feature maps in the decoding
stage, we have developed a layer-by-layer feature upsampling module combined
with Euclidean distance to improve feature fusion and reconstruction during the
decoding stage. The EfficientCD has been experimentally validated on four
remote sensing datasets: LEVIR-CD, SYSU-CD, CLCD, and WHUCD. The experimental
results demonstrate that EfficientCD exhibits outstanding performance in change
detection accuracy. The code and pretrained models will be released at
https://github.com/dyzy41/mmrscd.",http://arxiv.org/abs/2407.15999v1
Change Point Detection in Pairwise Comparison Data with Covariates,2024-08-24T18:10:52Z,"Yi Han, Thomas C. M. Lee","This paper introduces the novel piecewise stationary covariate-assisted
ranking estimation (PS-CARE) model for analyzing time-evolving pairwise
comparison data, enhancing item ranking accuracy through the integration of
covariate information. By partitioning the data into distinct, stationary
segments, the PS-CARE model adeptly detects temporal shifts in item rankings,
known as change points, whose number and positions are initially unknown.
Leveraging the minimum description length (MDL) principle, this paper
establishes a statistically consistent model selection criterion to estimate
these unknowns. The practical optimization of this MDL criterion is done with
the pruned exact linear time (PELT) algorithm. Empirical evaluations reveal the
method's promising performance in accurately locating change points across
various simulated scenarios. An application to an NBA dataset yielded
meaningful insights that aligned with significant historical events,
highlighting the method's practical utility and the MDL criterion's
effectiveness in capturing temporal ranking changes. To the best of the
authors' knowledge, this research pioneers change point detection in pairwise
comparison data with covariate information, representing a significant leap
forward in the field of dynamic ranking analysis.",http://arxiv.org/abs/2408.13642v1
"The Effect of Ga-Ion Irradiation on Sub-Micron-Wavelength Spin Waves in
  Yttrium-Iron-Garnet Films",2024-08-26T19:02:15Z,"Johannes Greil, Martina Kiechle, Adam Papp, Peter Neumann, Zolt√°n Kov√°cs, Janos Volk, Frank Schulz, Sebastian Wintz, Markus Weigand, Gy√∂rgy Csaba, Markus Becherer","We investigate the effect of focused-ion-beam (FIB) irradiation on spin waves
with sub-micron wavelengths in Yttrium-Iron-Garnet (YIG) films. Time-resolved
scanning transmission X-ray (TR-STXM) microscopy was used to image the spin
waves in irradiated regions and deduce corresponding changes in the magnetic
parameters of the film. We find that the changes of Ga$^+$ irradiation can be
understood by assuming a few percent change in the effective magnetization
$M_\mathrm{eff}$ of the film due to a trade-off between changes in anisotropy
and effective film thickness. Our results demonstrate that FIB irradiation can
be used to locally alter the dispersion relation and the effective refractive
index $n_\textrm{eff}$ of the film, even for submicron wavelengths. To achieve
the same change in $n_\textrm{eff}$ for shorter wavelengths, a higher dose is
required, but no significant deterioration of spin wave propagation length in
the irradiated regions was observed, even at the highest applied doses.",http://arxiv.org/abs/2408.14580v1
"Tracking the Electron Density Changes in Excited States -- A
  Computational Study on Pyrazine",2024-08-28T02:41:19Z,"Sebastian V. Pios, Jiaji Zhang, Maxim F. Gelin, Hong-Guang Duan, Lipeng Chen","The development of X-ray free-electron lasers (XFELs) has enabled ultrafast
X-ray diffraction (XRD) experiments, which are capable of resolving
electronic/vibrational transitions and structural changes in molecules, or
capturing molecular movies. While time-resolved XRD has received increasing
attention, the extraction of information content from signals is challenging
and requires theoretical support. In this work, we combined X-ray scattering
theory and trajectory surface hopping approach to resolve dynamical changes in
the electronic structure of photo-excited molecules by studying time evolution
of electron density changes between electronic excited states and ground state.
Using pyrazine molecule as an example, we show that key features of reaction
pathways can be identified, enabling the capture of structural changes
associated with electronic transitions for a photo-excited molecule.",http://arxiv.org/abs/2408.15494v1
Volume Changing Symmetries by Matrix Product Operators,2024-08-28T09:20:32Z,"M√°rton Borsi, Bal√°zs Pozsgay","We consider spin chain models with exotic symmetries that change the length
of the spin chain. It is known that the XXZ Heisenberg spin chain at the
supersymmetric point $\Delta=-1/2$ possesses such a symmetry: it is given by
the supersymmetry generators, which change the length of the chain by one unit.
We show that volume changing symmetries exist also in other spin chain models,
and that they can be constructed using a special tensor network, which is a
simple generalization of a Matrix Product Operator. As examples we consider the
folded XXZ model and its perturbations, and also a new hopping model that is
defined on constrained Hilbert spaces. We show that the volume changing
symmetries are not related to integrability: the symmetries can survive even
non-integrable perturbations. We also show that the known supersymmetry
generator of the XXZ chain with $\Delta=-1/2$ can also be expressed as a
generalized Matrix Product Operator.",http://arxiv.org/abs/2408.15659v1
Towards Generalizable Scene Change Detection,2024-09-10T04:45:25Z,"Jaewoo Kim, Uehwan Kim","Scene Change Detection (SCD) is vital for applications such as visual
surveillance and mobile robotics. However, current SCD methods exhibit a bias
to the temporal order of training datasets and limited performance on unseen
domains; coventional SCD benchmarks are not able to evaluate generalization or
temporal consistency. To tackle these limitations, we introduce a Generalizable
Scene Change Detection Framework (GeSCF) in this work. The proposed GeSCF
leverages localized semantics of a foundation model without any re-training or
fine-tuning -- for generalization over unseen domains. Specifically, we design
an adaptive thresholding of the similarity distribution derived from facets of
the pre-trained foundation model to generate initial pseudo-change mask. We
further utilize Segment Anything Model's (SAM) class-agnostic masks to refine
pseudo-masks. Moreover, our proposed framework maintains commutative operations
in all settings to ensure complete temporal consistency. Finally, we define new
metrics, evaluation dataset, and evaluation protocol for Generalizable Scene
Change Detection (GeSCD). Extensive experiments demonstrate that GeSCF excels
across diverse and challenging environments -- establishing a new benchmark for
SCD performance.",http://arxiv.org/abs/2409.06214v1
"A Framework for Predicting the Impact of Game Balance Changes through
  Meta Discovery",2024-09-11T15:20:43Z,"Akash Saravanan, Matthew Guzdial","A metagame is a collection of knowledge that goes beyond the rules of a game.
In competitive, team-based games like Pok\'emon or League of Legends, it refers
to the set of current dominant characters and/or strategies within the player
base. Developer changes to the balance of the game can have drastic and
unforeseen consequences on these sets of meta characters. A framework for
predicting the impact of balance changes could aid developers in making more
informed balance decisions. In this paper we present such a Meta Discovery
framework, leveraging Reinforcement Learning for automated testing of balance
changes. Our results demonstrate the ability to predict the outcome of balance
changes in Pok\'emon Showdown, a collection of competitive Pok\'emon tiers,
with high accuracy.",http://arxiv.org/abs/2409.07340v1
"Investigation of Electrical Conductivity Changes during Brain Functional
  Activity in 3T MRI",2024-09-12T07:34:44Z,"Kyu-Jin Jung, Chuanjiang Cui, Soo-Hyung Lee, Chan-Hee Park, Ji-Won Chun, Dong-Hyun Kim","Blood oxygenation level-dependent (BOLD) functional magnetic resonance
imaging (fMRI) is widely used to visualize brain activation regions by
detecting hemodynamic responses associated with increased metabolic demand.
While alternative MRI methods have been employed to monitor functional
activities, the investigation of in-vivo electrical property changes during
brain function remains limited. In this study, we explored the relationship
between fMRI signals and electrical conductivity (measured at the Larmor
frequency) changes using phase-based electrical properties tomography (EPT).
Our results revealed consistent patterns: conductivity changes showed negative
correlations, with conductivity decreasing in the functionally active regions
whereas B1 phase mapping exhibited positive correlations around activation
regions. These observations were consistent across both motor and visual cortex
activations. To further substantiate these findings, we conducted
electromagnetic radio-frequency simulations that modeled activation states with
varying conductivity, which demonstrated trends similar to our in-vivo results
for both B1 phase and conductivity. These findings suggest that in-vivo
electrical conductivity changes can indeed be measured during brain activity.
However, further investigation is needed to fully understand the underlying
mechanisms driving these measurements.",http://arxiv.org/abs/2409.07806v1
Quickest Change Detection Using Mismatched CUSUM,2024-09-12T11:19:07Z,"Austin Cooper, Sean Meyn","The field of quickest change detection (QCD) concerns design and analysis of
algorithms to estimate in real time the time at which an important event takes
place and identify properties of the post-change behavior. The goal is to
devise a stopping time adapted to the observations that minimizes an $L_1$
loss.
  Approximately optimal solutions are well known under a variety of
assumptions. In the work surveyed here we consider the CUSUM statistic, which
is defined as a one-dimensional reflected random walk driven by a functional of
the observations. It is known that the optimal functional is a log likelihood
ratio subject to special statical assumptions.
  The paper concerns model free approaches to detection design, considering the
following questions:
  1. What is the performance for a given functional of the observations?
  2. How do the conclusions change when there is dependency between pre- and
post-change behavior?
  3. How can techniques from statistics and machine learning be adapted to
approximate the best functional in a given class?",http://arxiv.org/abs/2409.07948v1
"Safe Control of Quadruped in Varying Dynamics via Safety Index
  Adaptation",2024-09-15T22:27:37Z,"Kai S. Yun, Rui Chen, Chase Dunaway, John M. Dolan, Changliu Liu","Varying dynamics pose a fundamental difficulty when deploying safe control
laws in the real world. Safety Index Synthesis (SIS) deeply relies on the
system dynamics and once the dynamics change, the previously synthesized safety
index becomes invalid. In this work, we show the real-time efficacy of Safety
Index Adaptation (SIA) in varying dynamics. SIA enables real-time adaptation to
the changing dynamics so that the adapted safe control law can still guarantee
1) forward invariance within a safe region and 2) finite time convergence to
that safe region. This work employs SIA on a package-carrying quadruped robot,
where the payload weight changes in real-time. SIA updates the safety index
when the dynamics change, e.g., a change in payload weight, so that the
quadruped can avoid obstacles while achieving its performance objectives.
Numerical study provides theoretical guarantees for SIA and a series of
hardware experiments demonstrate the effectiveness of SIA in real-world
deployment in avoiding obstacles under varying dynamics.",http://arxiv.org/abs/2409.09882v1
Conjugate Bayesian Two-step Change Point Detection for Hawkes Process,2024-09-26T07:16:38Z,"Zeyue Zhang, Xiaoling Lu, Feng Zhou","The Bayesian two-step change point detection method is popular for the Hawkes
process due to its simplicity and intuitiveness. However, the non-conjugacy
between the point process likelihood and the prior requires most existing
Bayesian two-step change point detection methods to rely on non-conjugate
inference methods. These methods lack analytical expressions, leading to low
computational efficiency and impeding timely change point detection. To address
this issue, this work employs data augmentation to propose a conjugate Bayesian
two-step change point detection method for the Hawkes process, which proves to
be more accurate and efficient. Extensive experiments on both synthetic and
real data demonstrate the superior effectiveness and efficiency of our method
compared to baseline methods. Additionally, we conduct ablation studies to
explore the robustness of our method concerning various hyperparameters. Our
code is publicly available at https://github.com/Aurora2050/CoBay-CPD.",http://arxiv.org/abs/2409.17591v4
Detecting Change-points in Mean of Multivariate Time Series,2024-09-28T10:46:47Z,Ramkrishna Jyoti Samanta,"This work delves into presenting a probabilistic method for analyzing linear
process data with weakly dependent innovations, focusing on detecting
change-points in the mean and estimating its spectral density. We develop a
test for identifying change-points in the mean of data coming from such a
model, aiming to detect shifts in the underlying distribution. Additionally, we
propose a consistent estimator for the spectral density of the data, contingent
upon fundamental assumptions, notably the long-run variance. By leveraging
probabilistic techniques, our approach provides reliable tools for
understanding temporal changes in linear process data. Through theoretical
analysis and empirical evaluation, we demonstrate the efficacy and consistency
of our proposed methods, offering valuable insights for practitioners in
various fields dealing with time series data analysis. Finally, we implemented
our method on bitcoin data for identifying the time points of significant
changes in its stock price.",http://arxiv.org/abs/2409.19312v1
"Comparative Analysis of Static and Contextual Embeddings for Analyzing
  Semantic Changes in Medieval Latin Charters",2024-10-11T22:19:17Z,"Yifan Liu, Gelila Tilahun, Xinxiang Gao, Qianfeng Wen, Michael Gervers","The Norman Conquest of 1066 C.E. brought profound transformations to
England's administrative, societal, and linguistic practices. The DEEDS
(Documents of Early England Data Set) database offers a unique opportunity to
explore these changes by examining shifts in word meanings within a vast
collection of Medieval Latin charters. While computational linguistics
typically relies on vector representations of words like static and contextual
embeddings to analyze semantic changes, existing embeddings for scarce and
historical Medieval Latin are limited and may not be well-suited for this task.
This paper presents the first computational analysis of semantic change pre-
and post-Norman Conquest and the first systematic comparison of static and
contextual embeddings in a scarce historical data set. Our findings confirm
that, consistent with existing studies, contextual embeddings outperform static
word embeddings in capturing semantic change within a scarce historical corpus.",http://arxiv.org/abs/2410.09283v1
"ChangeMinds: Multi-task Framework for Detecting and Describing Changes
  in Remote Sensing",2024-10-13T23:43:10Z,"Yuduo Wang, Weikang Yu, Michael Kopp, Pedram Ghamisi","Recent advancements in Remote Sensing (RS) for Change Detection (CD) and
Change Captioning (CC) have seen substantial success by adopting deep learning
techniques. Despite these advances, existing methods often handle CD and CC
tasks independently, leading to inefficiencies from the absence of synergistic
processing. In this paper, we present ChangeMinds, a novel unified multi-task
framework that concurrently optimizes CD and CC processes within a single,
end-to-end model. We propose the change-aware long short-term memory module
(ChangeLSTM) to effectively capture complex spatiotemporal dynamics from
extracted bi-temporal deep features, enabling the generation of universal
change-aware representations that effectively serve both CC and CD tasks.
Furthermore, we introduce a multi-task predictor with a cross-attention
mechanism that enhances the interaction between image and text features,
promoting efficient simultaneous learning and processing for both tasks.
Extensive evaluations on the LEVIR-MCI dataset, alongside other standard
benchmarks, show that ChangeMinds surpasses existing methods in multi-task
learning settings and markedly improves performance in individual CD and CC
tasks. Codes and pre-trained models will be available online.",http://arxiv.org/abs/2410.10047v2
Analyzing the Evolution of Graphs and Texts,2024-11-09T21:39:41Z,Xingzhi Guo,"With the recent advance of representation learning algorithms on graphs
(e.g., DeepWalk/GraphSage) and natural languages (e.g., Word2Vec/BERT) , the
state-of-the art models can even achieve human-level performance over many
downstream tasks, particularly for the task of node and sentence
classification. However, most algorithms focus on large-scale models for static
graphs and text corpus without considering the inherent dynamic characteristics
or discovering the reasons behind the changes. This dissertation aims to
efficiently model the dynamics in graphs (such as social networks and citation
graphs) and understand the changes in texts (specifically news titles and
personal biographies). To achieve this goal, we utilize the renowned
Personalized PageRank algorithm to create effective dynamic network embeddings
for evolving graphs. Our proposed approaches significantly improve the running
time and accuracy for both detecting network abnormal intruders and discovering
entity meaning shifts over large-scale dynamic graphs. For text changes, we
analyze the post-publication changes in news titles to understand the intents
behind the edits and discuss the potential impact of titles changes from
information integrity perspective. Moreover, we investigate self-presented
occupational identities in Twitter users' biographies over five years,
investigating job prestige and demographics effects in how people disclose
jobs, quantifying over-represented jobs and their transitions over time.",http://arxiv.org/abs/2411.06295v1
How do Machine Learning Models Change?,2024-11-14T18:14:32Z,"Joel Casta√±o, Rafael Caba√±as, Antonio Salmer√≥n, David Lo, Silverio Mart√≠nez-Fern√°ndez","The proliferation of Machine Learning (ML) models and their open-source
implementations has transformed Artificial Intelligence research and
applications. Platforms like Hugging Face (HF) enable the development, sharing,
and deployment of these models, fostering an evolving ecosystem. While previous
studies have examined aspects of models hosted on platforms like HF, a
comprehensive longitudinal study of how these models change remains
underexplored. This study addresses this gap by utilizing both repository
mining and longitudinal analysis methods to examine over 200,000 commits and
1,200 releases from over 50,000 models on HF. We replicate and extend an ML
change taxonomy for classifying commits and utilize Bayesian networks to
uncover patterns in commit and release activities over time. Our findings
indicate that commit activities align with established data science
methodologies, such as CRISP-DM, emphasizing iterative refinement and
continuous improvement. Additionally, release patterns tend to consolidate
significant updates, particularly in documentation, distinguishing between
granular changes and milestone-based releases. Furthermore, projects with
higher popularity prioritize infrastructure enhancements early in their
lifecycle, and those with intensive collaboration practices exhibit improved
documentation standards. These and other insights enhance the understanding of
model changes on community platforms and provide valuable guidance for best
practices in model maintenance.",http://arxiv.org/abs/2411.09645v1
Bayesian optimal change point detection in high-dimensions,2024-11-22T11:24:23Z,"Jaehoon Kim, Kyoungjae Lee, Lizhen Lin","We propose the first Bayesian methods for detecting change points in
high-dimensional mean and covariance structures. These methods are constructed
using pairwise Bayes factors, leveraging modularization to identify significant
changes in individual components efficiently. We establish that the proposed
methods consistently detect and estimate change points under much milder
conditions than existing approaches in the literature. Additionally, we
demonstrate that their localization rates are nearly optimal in terms of rates.
The practical performance of the proposed methods is evaluated through
extensive simulation studies, where they are compared to state-of-the-art
techniques. The results show comparable or superior performance across most
scenarios. Notably, the methods effectively detect change points whenever
signals of sufficient magnitude are present, irrespective of the number of
signals. Finally, we apply the proposed methods to genetic and financial
datasets, illustrating their practical utility in real-world applications.",http://arxiv.org/abs/2411.14864v1
Text Change Detection in Multilingual Documents Using Image Comparison,2024-12-05T13:04:10Z,"Doyoung Park, Naresh Reddy Yarram, Sunjin Kim, Minkyu Kim, Seongho Cho, Taehee Lee","Document comparison typically relies on optical character recognition (OCR)
as its core technology. However, OCR requires the selection of appropriate
language models for each document and the performance of multilingual or hybrid
models remains limited. To overcome these challenges, we propose text change
detection (TCD) using an image comparison model tailored for multilingual
documents. Unlike OCR-based approaches, our method employs word-level text
image-to-image comparison to detect changes. Our model generates bidirectional
change segmentation maps between the source and target documents. To enhance
performance without requiring explicit text alignment or scaling preprocessing,
we employ correlations among multi-scale attention features. We also construct
a benchmark dataset comprising actual printed and scanned word pairs in various
languages to evaluate our model. We validate our approach using our benchmark
dataset and public benchmarks Distorted Document Images and the LRDE Document
Binarization Dataset. We compare our model against state-of-the-art semantic
segmentation and change detection models, as well as to conventional OCR-based
models.",http://arxiv.org/abs/2412.04137v1
"Enhanced Sampling of Protein Conformational Changes via True Reaction
  Coordinates from Energy Relaxation",2024-12-05T18:14:49Z,"Huiyu Li, Ao Ma","The bottleneck in enhanced sampling lies in finding collective variables
(CVs) that can effectively accelerate protein conformational changes. True
reaction coordinates (tRCs) that can predict the committor are considered the
optimal CVs, but identifying them requires unbiased natural reactive
trajectories, which, paradoxically, depend on effective enhanced sampling.
Using the generalized work functional method, we found that tRCs control both
conformational changes and energy relaxation, enabling us to compute tRCs from
energy relaxation simulations. Applying bias to tRCs accelerated conformational
changes and ligand dissociation in HIV-1 protease and the PDZ2 domain by 10^5
to 10^15-fold. The resulting trajectories follow natural transition pathways,
enabling efficient generation of natural reactive trajectories. In contrast,
biased trajectories from empirical CVs often display non-physical features.
Furthermore, by computing tRCs from a single protein structure, our method
enables predictive sampling of conformational changes. These findings
significantly broaden the range of protein functional processes accessible to
molecular dynamics simulations.",http://arxiv.org/abs/2412.04400v1
"Network Optimization in Dynamic Systems: Fast Adaptation via Zero-Shot
  Lagrangian Update",2024-12-10T19:11:10Z,I-Hong Hou,"This paper addresses network optimization in dynamic systems, where factors
such as user composition, service requirements, system capacity, and channel
conditions can change abruptly and unpredictably. Unlike existing studies that
focus primarily on optimizing long-term performance in steady states, we
develop online learning algorithms that enable rapid adaptation to sudden
changes. Recognizing that many current network optimization algorithms rely on
dual methods to iteratively learn optimal Lagrange multipliers, we propose
zero-shot updates for these multipliers using only information available at the
time of abrupt changes. By combining Taylor series analysis with complementary
slackness conditions, we theoretically derive zero-shot updates applicable to
various abrupt changes in two distinct network optimization problems. These
updates can be integrated with existing algorithms to significantly improve
performance during transitory phases in terms of total utility, operational
cost, and constraint violations. Simulation results demonstrate that our
zero-shot updates substantially improve transitory performance, often achieving
near-optimal outcomes without additional learning, even under severe system
changes.",http://arxiv.org/abs/2412.07865v1
"Sequential Change Point Detection in High-dimensional Vector
  Auto-regressive Models",2024-12-13T02:21:36Z,"Yuhan Tian, Abolfazl Safikhani","Sequential (online) change-point detection involves continuously monitoring
time-series data and triggering an alarm when shifts in the data distribution
are detected. We propose an algorithm for real-time identification of
alterations in the transition matrices of high-dimensional vector
autoregressive models. The algorithm estimates transition matrices and error
term variances using regularization techniques applied to training data, then
computes a specific test statistic to detect changes in transition matrices as
new data batches arrive. We establish the asymptotic normality of the test
statistic under the scenario of no change points, subject to mild conditions.
An alarm is raised when the calculated test statistic exceeds a predefined
quantile of the standard normal distribution. We demonstrate that, as the size
of the change (jump size) increases, the test power approaches one. The
effectiveness of the algorithm is validated empirically across various
simulation scenarios. Finally, we present two applications of the proposed
methodology: analyzing shocks in S&P 500 data and detecting the timing of
seizures in EEG data.",http://arxiv.org/abs/2412.09794v1
A car-following framework for traffic instability and lane changes,2024-12-30T20:42:10Z,"Nicholas Mankowski, Hassan Mushtaq, Hanliang Guo","This paper develops a computational framework based on a car-following model
to study traffic instability and lane changes. Building upon Newell's classical
first-order car-following model, we show that, both analytically and
numerically, there exists a vehicle-density-dependent critical reaction time
that determines the stability of single-lane traffic. Specifically,
perturbations to the equilibrium system decay with time for low reaction time
and grow for high reaction time. This critical reaction time converges to
Newell's original result in the continuum limit. Additionally, we propose a
psychology-based lane-changing mechanism that builds a quantitative connection
between the driver's psychological factor (frustration level) and the driving
condition. We show that our stochastic lane-changing model can faithfully
reproduce interesting phenomena like load-balancing of different lanes. Our
model supports the result that more frequent lane changes only marginally
benefit the driver's overall velocity.",http://arxiv.org/abs/2501.01988v1
"Precision Franck-Condon spectroscopy from highly-excited vibrational
  states",2024-01-18T21:44:37Z,"Sindhana Pannir-Sivajothi, Joel Yuen-Zhou","As per the Franck-Condon principle, absorption spectroscopy reveals changes
in nuclear geometry in molecules or solids upon electronic excitation. It is
often assumed these changes cannot be resolved beyond the ground vibrational
wavefunction width ($\sqrt{\hbar/m\omega}$). Here, we show this resolution
dramatically improves with highly-excited vibrational initial states (with
occupation number $\langle n\rangle$). These states magnify changes in geometry
by $2\langle n\rangle +1$, a possibly counterintuitive result given the spatial
uncertainty of Fock states grows with $n$. We also discuss generalizations of
this result to multimode systems. Our result is relevant to optical
spectroscopy, polariton condensates, and quantum simulators ($\textit{e.g.}$,
boson samplers).",http://arxiv.org/abs/2401.10384v2
"Efficient and Interaction-Aware Trajectory Planning for Autonomous
  Vehicles with Particle Swarm Optimization",2024-02-02T17:13:03Z,"Lin Song, David Isele, Naira Hovakimyan, Sangjae Bae","This paper introduces a novel numerical approach to achieving smooth
lane-change trajectories in autonomous driving scenarios. Our trajectory
generation approach leverages particle swarm optimization (PSO) techniques,
incorporating Neural Network (NN) predictions for trajectory refinement. The
generation of smooth and dynamically feasible trajectories for the lane change
maneuver is facilitated by combining polynomial curve fitting with particle
propagation, which can account for vehicle dynamics. The proposed planning
algorithm is capable of determining feasible trajectories with real-time
computation capability. We conduct comparative analyses with two baseline
methods for lane changing, involving analytic solutions and heuristic
techniques in numerical simulations. The simulation results validate the
efficacy and effectiveness of our proposed approach.",http://arxiv.org/abs/2402.01575v1
Extracting Scalar Measures from Curves,2024-02-02T18:19:55Z,"Lanqiu Yao, Thaddeus Tarpey","The ability to order outcomes is necessary to make comparisons which is
complicated when there is no natural ordering on the space of outcomes, as in
the case of functional outcomes. This paper examines methods for extracting a
scalar summary from functional or longitudinal outcomes based on an average
rate of change which can be used to compare curves. Common approaches used in
practice use a change score or an analysis of covariance (ANCOVA) to make
comparisons. However, these standard approaches only use a fraction of the
available data and are inefficient. We derive measures of performance of an
averaged rate of change of a functional outcome and compare this measure to
standard measures. Simulations and data from a depression clinical trial are
used to illustrate results.",http://arxiv.org/abs/2402.01827v1
Extending the definition of set tolerances,2024-02-22T13:32:02Z,"Gerold J√§ger, Marcel Turkensteen","Optimal solutions of combinatorial optimization problems can be sensitive to
changes in the cost of one or more elements of the ground set E. Single and set
tolerances measure the supremum / infimum possible change such that the current
solution remains optimal for cost changes in one or more elements. The current
definition does not apply to all elements of E or to all subsets of E. In this
work, we broaden the definition to all elements for single tolerances and to
all subsets of elements for set tolerances, while proving that key theoretical
and computational properties still apply.",http://arxiv.org/abs/2402.14542v2
"Sequential Change-point Detection for Binomial Time Series with
  Exogenous Variables",2024-02-27T07:39:54Z,"Yajun Liu, Beth Andrews","Sequential change-point detection for time series enables us to sequentially
check the hypothesis that the model still holds as more and more data are
observed. It's widely used in data monitoring in practice. Meanwhile, binomial
time series, which depicts independent binary individual behaviors within a
group when the individual behaviors are dependent on past observations of the
whole group, is an important type of model in practice but hasn't been
developed well. We first propose a Binomial AR($1$) model, and then consider a
method for sequential change-point detection for the Binomial AR(1).",http://arxiv.org/abs/2402.17274v1
"Integrating Generative AI into Financial Market Prediction for Improved
  Decision Making",2024-04-04T15:26:26Z,"Chang Che, Zengyi Huang, Chen Li, Haotian Zheng, Xinyu Tian","This study provides an in-depth analysis of the model architecture and key
technologies of generative artificial intelligence, combined with specific
application cases, and uses conditional generative adversarial networks ( cGAN
) and time series analysis methods to simulate and predict dynamic changes in
financial markets. The research results show that the cGAN model can
effectively capture the complexity of financial market data, and the deviation
between the prediction results and the actual market performance is minimal,
showing a high degree of accuracy.",http://arxiv.org/abs/2404.03523v1
"Controlling the Flavour Changing Neutral Couplings of Multi-Higgs
  Doublets Models through Unitary Matrices",2024-04-29T10:04:53Z,Jo√£o M. Alves,"In this paper, we introduce Unitary Flavour Violation to produce Multi-Higgs
Doublets Models where all flavour parameters are contained within three unitary
matrices. After that, we identify two of its subclasses, the left and right
models, which have naturally suppressed tree-level Flavour Changing Neutral
Couplings that easily avoid the experimental constraints derived from neutral
meson mixing. Then, we show that left models can accomodate spontaneous CP
violation when all quarks have Flavour Changing Neutral Couplings. Finally, we
illustrate these concepts by considering a specific implementation with three
Higgs doublets.",http://arxiv.org/abs/2404.18559v1
Random walk in slowly changing environments,2024-06-21T07:16:36Z,"Bryan Park, Souvik Ray","A Random Walk in Changing Environment (RWCE) is a weighted random walk on a
locally finite, connected graph $G$ with random, time-dependent edge-weights.
This includes self-interacting random walks, where the edge-weights depend on
the history of the process. In general, even the basic question of recurrence
or transience for RWCEs is difficult, especially when the underlying graph
contains cycles. In this note, we derive a condition for recurrence or
transience that is too restrictive for classical RWCEs but instead works for
any graph $G.$ Namely, we show that any bounded RWCE on $G$ with ""slowly""
changing edge-weights inherits the recurrence or transience of the initial
weighted graph.",http://arxiv.org/abs/2406.14914v1
Topologically protected quantized changes of the distance between atoms,2024-06-28T11:48:01Z,"Ali Emami Kopaei, Krzysztof Giergiel, Krzysztof Sacha","Thouless pumping enables the transport of particles in a one-dimensional
periodic potential if the potential is slowly and periodically modulated in
time. The change in the position of particles after each modulation period is
quantized and depends solely on the topology of the pump cycle, making it
robust against perturbations. Here, we demonstrate that Thouless pumping also
allows for the realization of topologically protected quantized changes of the
distance between atoms if the atomic s-wave scattering length is properly
modulated in time.",http://arxiv.org/abs/2406.19850v1
Reasoning about unpredicted change and explicit time,2024-07-09T07:49:57Z,"Florence Dupin de Saint-Cyr, J√©r√¥me Lang","Reasoning about unpredicted change consists in explaining observations by
events; we propose here an approach for explaining time-stamped observations by
surprises, which are simple events consisting in the change of the truth value
of a fluent. A framework for dealing with surprises is defined. Minimal sets of
surprises are provided together with time intervals where each surprise has
occurred, and they are characterized from a model-based diagnosis point of
view. Then, a probabilistic approach of surprise minimisation is proposed.",http://arxiv.org/abs/2407.06622v1
Towards Temporal Change Explanations from Bi-Temporal Satellite Images,2024-06-27T12:49:22Z,"Ryo Tsujimoto, Hiroki Ouchi, Hidetaka Kamigaito, Taro Watanabe","Explaining temporal changes between satellite images taken at different times
is important for urban planning and environmental monitoring. However, manual
dataset construction for the task is costly, so human-AI collaboration is
promissing. Toward the direction, in this paper, we investigate the ability of
Large-scale Vision-Language Models (LVLMs) to explain temporal changes between
satellite images. While LVLMs are known to generate good image captions, they
receive only a single image as input. To deal with a par of satellite images as
input, we propose three prompting methods. Through human evaluation, we found
the effectiveness of our step-by-step reasoning based prompting.",http://arxiv.org/abs/2407.09548v1
An Alexander Polynomial Obstruction to Cosmetic Crossing Changes,2024-07-17T17:41:20Z,Joe Boninger,"The cosmetic crossing conjecture posits that switching a non-trivial crossing
in a knot diagram always changes the knot type. Generalizing work of Balm,
Friedl, Kalfagianni and Powell, and of Lidman and Moore, we give an Alexander
polynomial condition that obstructs cosmetic crossing changes for knots with
$L$-space branched double covers, a family that includes all alternating knots.
As an application, we prove the cosmetic crossing conjecture for a
five-parameter infinite family of pretzel knots. We also discuss the state of
the conjecture for alternating knots with eleven crossings.",http://arxiv.org/abs/2407.12763v2
Distribution-Aware Replay for Continual MRI Segmentation,2024-07-30T21:59:02Z,"Nick Lemke, Camila Gonz√°lez, Anirban Mukhopadhyay, Martin Mundt","Medical image distributions shift constantly due to changes in patient
population and discrepancies in image acquisition. These distribution changes
result in performance deterioration; deterioration that continual learning aims
to alleviate. However, only adaptation with data rehearsal strategies yields
practically desirable performance for medical image segmentation. Such
rehearsal violates patient privacy and, as most continual learning approaches,
overlooks unexpected changes from out-of-distribution instances. To transcend
both of these challenges, we introduce a distribution-aware replay strategy
that mitigates forgetting through auto-encoding of features, while
simultaneously leveraging the learned distribution of features to detect model
failure. We provide empirical corroboration on hippocampus and prostate MRI
segmentation.",http://arxiv.org/abs/2407.21216v1
Graphical Structural Learning of rs-fMRI data in Heavy Smokers,2024-09-12T20:48:28Z,"Yiru Gong, Qimin Zhang, Huili Zheng, Zheyan Liu, Shaohan Chen","Recent studies revealed structural and functional brain changes in heavy
smokers. However, the specific changes in topological brain connections are not
well understood. We used Gaussian Undirected Graphs with the graphical lasso
algorithm on rs-fMRI data from smokers and non-smokers to identify significant
changes in brain connections. Our results indicate high stability in the
estimated graphs and identify several brain regions significantly affected by
smoking, providing valuable insights for future clinical research.",http://arxiv.org/abs/2409.08395v2
Boundary overlap in the open XXZ spin chain,2024-09-23T16:44:00Z,"Charbel Abetian, Nikolai Kitanine, Veronique Terras","In this paper we compute the overlaps of the ground states for the open spin
chains after a change of one of the boundary magnetic fields. It can be
considered as the first step toward the study of the boundary quench problem:
behaviour of an open spin chain after an abrupt change of one boundary magnetic
field.",http://arxiv.org/abs/2409.15194v3
"Base change conductors through intersection theory and quotient
  singularities",2024-10-20T12:07:23Z,"Dennis Eriksson, Lars Halvard Halle, Johannes Nicaise","We perform a systematic study of the base change conductor for Jacobians.
Through the lens of intersection theory and Deligne's Riemann-Roch theorem, we
present novel computational approaches for both the tame and wild parts of the
base change conductor. Our key results include a general formula of the tame
part, as well as a computation of the wild part in terms of Galois quotients of
semistable models of the curves. We treat in detail the case of potential good
reduction when the quotient only has weak wild quotient singularities, relying
on recent advances by Obus and Wewers.",http://arxiv.org/abs/2410.15370v2
"Supersymmetry-like tunneling current noise as a probe of Goldstino
  excitation in a Bose-Fermi mixture",2024-10-28T07:19:39Z,Tingyu Zhang,"The Goldstino, which is a fermionic Nambu-Goldstone mode, has been predicted
in a Bose-Fermi mixture when the supersymmetry is broken. To detect this
excitation mode, we theoretically investigate the shot noise of the
supersymmetry-like tunneling current in a weakly interacting ultracold
Bose-Fermi mixture. The Fano factor, which is defined by the noise-to-current
ratio, reflect the elementary carriers of the tunneling process. The change of
the Fano factor microscopically as the density changes evinces a crossover from
the quasiparticle transport to multiparticle (Goldstino) transport. The
tunneling channel can also be changed by tuning the potential barrier.",http://arxiv.org/abs/2410.20794v2
"Topological phase transitions in a constrained two-qubit quantum control
  landscape",2024-11-13T16:18:41Z,"Nicol√≤ Beato, Pranay Patil, Marin Bukov","In optimal quantum control, control landscape phase transitions (CLPTs)
indicate sharp changes occurring in the set of optimal protocols, as a physical
model parameter is varied. Here, we demonstrate the existence of a new class of
CLPTs, associated with changes in the topological properties of the optimal
level set in a two-qubit state-preparation problem. In particular, the distance
distribution of control protocols sampled through stochastic homotopic dynamics
reveals discontinuous changes in the number of connected components in the
optimal level set, as a function of the protocol duration. We demonstrate how
topological CLPTs can be detected in modern-day experiments.",http://arxiv.org/abs/2411.08736v1
"Prediction of cerebral blood volume change after resuscitation from
  hypoxic-ischemic insult for newborn piglets",2024-11-15T08:09:45Z,"Manabu Machida, Tsutomu Mitsuie, Shinji Nakamura, Takashi Kusaka","Neonatal hypoxic-ischemic encephalopathy (HIE) is a significant cause of
neonatal mortality and developmental disabilities. It has been revealed that
the temporal behavior of the cerebral blood volume (CBV) carries information on
the degree of hypoxia-ischemia. CBV can be estimated by means of near-infrared
spectroscopy. The change of CBV after the insult is related to the change of
CBV during the insult. In this paper, we consider a mathematical model which
governs the time evolution of CBV after the insult. We show that the temporal
behavior of CBV can be predicted with the Kalman filter which is based on the
mathematical model.",http://arxiv.org/abs/2411.10025v1
"Real quadratic base changes for $\mathrm{GL}_3$ and integral periods
  relations",2024-11-25T13:40:24Z,Tristan Ricoul,"We prove a $p$-adic divisibility between the automorphic periods of a
cuspidal automorphic representation of $\mathrm{GL}_3(\mathbb{Q})$ and the
periods of its Arthur-Clozel's base change to some real quadratic field $E$.
This generalizes earlier works of Tilouine-Urban and of Hida in the case of
classical modular forms. The divisibility we prove involves a new kind of
automorphic periods, defined using the middle degree of the cuspidal cohomology
of $\mathrm{GL}_3(E)$, instead of the top or bottom degrees. We also
investigate the Rogawski's stable base change from the quasi-split unitary
group $U_E$ associated with $E$ to $\mathrm{GL}_3(E)$. In this situation, we
also obtain some results toward a $p$-adic divisibility of automorphic periods.",http://arxiv.org/abs/2411.16381v1
Feller property and convergence for semigroups of time-changed processes,2024-11-29T08:43:09Z,"Ali BenAmor, Kazuhiro Kuwae","We give a substitute to Feller property for semigroups of time-changed
processes; under some conditions this leads to establish sufficient (new)
conditions for the semigroups to be Feller. Moreover, given a standard process
and a sequence of measures converging vaguely to a final measure, under some
assumptions, we establish convergence of the sequence of the semigroups and the
resolvents of the corresponding time changed-processes. Some applications are
given: convergence of solutions of evolution equations and convergence of
finite time distributions.",http://arxiv.org/abs/2411.19543v2
Economic Geography and Structural Change,2024-12-04T22:48:29Z,"Clement E. Bohr, Marti Mestieri, Frederic Robert-Nicoud","As countries develop, the relative importance of agriculture declines and
economic activity becomes spatially concentrated. We develop a model
integrating structural change and regional disparities to jointly capture these
phenomena. A key modeling innovation ensuring analytical tractability is the
introduction of non-homothetic Cobb-Douglas preferences, which are
characterized by constant unitary elasticity of substitution and non-constant
income elasticity. As labor productivity increases over time, economic
well-being rises, leading to a declining expenditure share on agricultural
goods. Labor reallocates away from agriculture, and industry concentrates
spatially, further increasing aggregate productivity: structural change and
regional disparities are two mutually reinforcing outcomes and propagators of
the growth process.",http://arxiv.org/abs/2412.03755v1
Contextuality of the probability current in quantum mechanics,2024-12-05T12:16:19Z,Franck Lalo√´,"We revisit an argument proposed by Hardy concerning local realistic theories,
but in terms of probability currents in standard quantum mechanics and of the
trajectories obtained from its flux lines. We emphasize surprising properties
of the trajectories in configuration space, in particular their (quasi)
discontinuous variations when the context (the experimental setup) is changed.
This occurs in both Galilean and Einsteinian relativity. In the latter case,
discontinuous variations also appear without changing the setup, when the
Lorentzian reference frame is changed.",http://arxiv.org/abs/2412.04104v1
"Leveraging Black-box Models to Assess Feature Importance in
  Unconditional Distribution",2024-12-07T23:00:21Z,"Jing Zhou, Chunlin Li","Understanding how changes in explanatory features affect the unconditional
distribution of the outcome is important in many applications. However,
existing black-box predictive models are not readily suited for analyzing such
questions. In this work, we develop an approximation method to compute the
feature importance curves relevant to the unconditional distribution of
outcomes, while leveraging the power of pre-trained black-box predictive
models. The feature importance curves measure the changes across quantiles of
outcome distribution given an external impact of change in the explanatory
features. Through extensive numerical experiments and real data examples, we
demonstrate that our approximation method produces sparse and faithful results,
and is computationally efficient.",http://arxiv.org/abs/2412.05759v1
Exception-aware Lifecycle Model Construction for Framework APIs,2024-01-05T06:35:47Z,"Jiwei Yan, Jinhao Huang, Hengqin Yang, Jun Yan","The implementation of complex software systems usually depends on low-level
frameworks or third-party libraries. During their evolution, the APIs adding
and removing behaviors may cause unexpected compatibility problems. So,
precisely analyzing and constructing the framework/ library's API lifecycle
model is of great importance. Existing works have proposed the API
existence-changing model for defect detection, while not considering the
influence of semantic changes in APIs. In some cases, developers will not
remove or deprecate APIs but modify their semantics by adding, removing, or
modifying their exception-thrown code, which may bring potential defects to
upper-level code. Therefore, besides the API existence model, it is also
necessary for developers to be concerned with the exception-related code
evolution in APIs, which requires the construction of exception-aware API
lifecycle models for framework/library projects. To achieve automatic
exception-aware API lifecycle model construction, this paper adopts a static
analysis technique to extract exception summary information in the framework
API code and adopts a multi-step matching strategy to obtain the changing
process of exceptions. Then, it generates exception-aware API lifecycle models
for the given framework/library project. With this approach, the API lifecycle
extraction tool, JavaExP, is implemented, which is based on Java bytecode
analysis. Compared to the state-of-the-art tool, JavaExP achieves both a higher
F1 score (+60%) and efficiency (+7x), whose precision of exception matching and
changing results is 98%. Compared to the exception-unaware API lifecycle
modeling on 60 versions, JavaExp can identify 18% times more API changes. Among
the 75,433 APIs under analysis, 20% of APIs have changed their
exception-throwing behavior at least once after API introduction, which may
bring many hidden compatibility issues.",http://arxiv.org/abs/2401.02660v1
"Siamese Meets Diffusion Network: SMDNet for Enhanced Change Detection in
  High-Resolution RS Imagery",2024-01-17T16:48:55Z,"Jia Jia, Geunho Lee, Zhibo Wang, Lyu Zhi, Yuchu He","Recently, the application of deep learning to change detection (CD) has
significantly progressed in remote sensing images. In recent years, CD tasks
have mostly used architectures such as CNN and Transformer to identify these
changes. However, these architectures have shortcomings in representing
boundary details and are prone to false alarms and missed detections under
complex lighting and weather conditions. For that, we propose a new network,
Siamese Meets Diffusion Network (SMDNet). This network combines the Siam-U2Net
Feature Differential Encoder (SU-FDE) and the denoising diffusion implicit
model to improve the accuracy of image edge change detection and enhance the
model's robustness under environmental changes. First, we propose an innovative
SU-FDE module that utilizes shared weight features to capture differences
between time series images and identify similarities between features to
enhance edge detail detection. Furthermore, we add an attention mechanism to
identify key coarse features to improve the model's sensitivity and accuracy.
Finally, the diffusion model of progressive sampling is used to fuse key coarse
features, and the noise reduction ability of the diffusion model and the
advantages of capturing the probability distribution of image data are used to
enhance the adaptability of the model in different environments. Our method's
combination of feature extraction and diffusion models demonstrates
effectiveness in change detection in remote sensing images. The performance
evaluation of SMDNet on LEVIR-CD, DSIFN-CD, and CDD datasets yields validated
F1 scores of 90.99%, 88.40%, and 88.47%, respectively. This substantiates the
advanced capabilities of our model in accurately identifying variations and
intricate details.",http://arxiv.org/abs/2401.09325v1
"Electric field dependent thermal conductivity of relaxor ferroelectric
  PMN-33PT through changes in the phonon spectrum",2024-02-13T15:13:36Z,"Delaram Rashadfar, Brandi L. Wooten, Joseph P. Heremans","In ferroelectric materials, an electric field has been shown to change the
phonon dispersion sufficiently to alter the lattice thermal conductivity,
opening the possibility that a heat gradient could drive a polarization flux,
and technologically, also opening a pathway towards voltage-driven, all
solid-state heat switching. In this report, we confirm the validity of the
theory originally developed for Pb(Zr,Ti)O_3 (PZT) on the ferroelectric relaxor
0.67Pb[Mg_(1/3)Nb_(2/3)]O_3-0.33PbTiO_3 (PMN-33PT). In the theory, the change
in sound velocity and thermal conductivity with electric field relates to the
piezoelectric coefficients and the Gruneisen parameter. It predicts that in
PMN-33PT the effect should be an order of magnitude larger, and of opposite
sign as in PZT; this is confirmed here experimentally. The effects are measured
on samples never poled before and on samples that underwent multiple field
sweep cycles and passed through two phase transitions with change in
temperature. The thermal conductivity changes are linked to variations in the
piezoelectric coefficients and can be as large as 8-11% at room temperature and
above. To date, this is the only means of heat conduction modulation that
utilizes changes in the phonon spectrum. While this technology is in its
infancy, it offers another path to future active thermal conduction control.",http://arxiv.org/abs/2402.08516v2
"Towards a dynamically reconfigurable pixelated reflective display:
  Focused ion beam for phase-change metapixel structures",2024-02-14T01:56:24Z,"Daniel T. Yimam, Minpeng Liang, Jianting Ye, Bart J. Kooi","The switching and optical properties of phase-change thin films are actively
investigated for future smart optical devices. The possibility of having more
than one stable state, the large optical contrast between phases, and the fast
and reversible switching are some attractive properties driving the research
interest. Optical devices based on phase change alloys are considered the
frontier contenders for tunable photonics. The combination of vivid structural
color formation, with partial amorphization/crystallization of phase change
alloys, and the associated optical tunability could be integrated into an
energy-efficient reflective display device with high pixel density. This work
demonstrates a contrast formation due to relative height differences from
isolated pixelated structures. A reflective heterostructure device consisting
of a low-loss Sb2Se3 alloy on a gold substrate was produced. With a focused ion
beam, a pixelated metasurface structure was produced. Moreover, the ability to
create local height differences using an ion beam was employed to create a
structural color combination mimicking traditional LED like RGB pixels. We
believe our approach in creating metapixels on phase change thin film surfaces
could open up research interest in phase change alloys and moving away from
semi/static plasmonic systems into truly dynamic display devices.",http://arxiv.org/abs/2402.08891v1
"An Evaluation of Real-time Adaptive Sampling Change Point Detection
  Algorithm using KCUSUM",2024-02-15T19:45:24Z,"Vijayalakshmi Saravanan, Perry Siehien, Shinjae Yoo, Hubertus Van Dam, Thomas Flynn, Christopher Kelly, Khaled Z Ibrahim","Detecting abrupt changes in real-time data streams from scientific
simulations presents a challenging task, demanding the deployment of accurate
and efficient algorithms. Identifying change points in live data stream
involves continuous scrutiny of incoming observations for deviations in their
statistical characteristics, particularly in high-volume data scenarios.
Maintaining a balance between sudden change detection and minimizing false
alarms is vital. Many existing algorithms for this purpose rely on known
probability distributions, limiting their feasibility. In this study, we
introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric
extension of the traditional Cumulative Sum (CUSUM) method, which has gained
prominence for its efficacy in online change point detection under less
restrictive conditions. KCUSUM splits itself by comparing incoming samples
directly with reference samples and computes a statistic grounded in the
Maximum Mean Discrepancy (MMD) non-parametric framework. This approach extends
KCUSUM's pertinence to scenarios where only reference samples are available,
such as atomic trajectories of proteins in vacuum, facilitating the detection
of deviations from the reference sample without prior knowledge of the data's
underlying distribution. Furthermore, by harnessing MMD's inherent random-walk
structure, we can theoretically analyze KCUSUM's performance across various use
cases, including metrics like expected delay and mean runtime to false alarms.
Finally, we discuss real-world use cases from scientific simulations such as
NWChem CODAR and protein folding data, demonstrating KCUSUM's practical
effectiveness in online change point detection.",http://arxiv.org/abs/2402.10291v2
Positive Lynden-Bell derivative as a ticket to the bar trap?,2024-03-13T08:22:53Z,"Viktor D. Zozulia, Anton A. Smirnov, Natalia Ya. Sotnikova","We have translated the results of $N$-body simulations of one barred model
into the language of action variables and frequencies. Using this language, we
analysed the behaviour of all orbits in the model on a large time scale at the
stage of a mature bar. We show that the orbits join the bar while preserving
their adiabatic invariant, which takes into account the 3D structure of the
orbits. This allows us to apply the concept of the Lynden-Bell derivative for
each of these orbits and trace how the sign of the derivative changes, i.e. how
asynchronous changes in angular momentum $L_z$ and orbital precession rate
$\Omega_\mathrm{pr}$ (normal orbital mode) change to synchronous (abnormal
mode). The transition to the abnormal mode occurs when $\Omega_\mathrm{pr}$
reaches the angular velocity of the pattern $\Omega_\mathrm{p}$, after which
the orbit becomes stuck in the bar trap. All this happens against the
background of secular changes in actions ($L_z$ decreases, $J_\mathrm{R}$ and
$J_z$ increase). At the same time, corotation particles near two stable
Lagrange points are also subject to secular changes in their actions. They
increase $L_z$ and drift to the periphery, shifting corotation outwards. We
also show that a change in the orbital mode from normal to abnormal and the
trapping of orbits in a bar is possible only when the bar speed decreases with
time, regardless of what is causing the bar to slow down. Our findings clarify
and expand the picture of bar formation and evolution in numerical models.",http://arxiv.org/abs/2403.08326v1
Belief Change based on Knowledge Measures,2024-03-15T17:40:11Z,"Umberto Straccia, Giovanni Casini","Knowledge Measures (KMs) aim at quantifying the amount of
knowledge/information that a knowledge base carries. On the other hand, Belief
Change (BC) is the process of changing beliefs (in our case, in terms of
contraction, expansion and revision) taking into account a new piece of
knowledge, which possibly may be in contradiction with the current belief. We
propose a new quantitative BC framework that is based on KMs by defining belief
change operators that try to minimise, from an information-theoretic point of
view, the surprise that the changed belief carries. To this end, we introduce
the principle of minimal surprise. In particular, our contributions are (i) a
general information-theoretic approach to KMs for which [1] is a special case;
(ii) KM-based BC operators that satisfy the so-called AGM postulates; and (iii)
a characterisation of any BC operator that satisfies the AGM postulates as a
KM-based BC operator, i.e., any BC operator satisfying the AGM postulates can
be encoded within our quantitative BC framework. We also introduce quantitative
measures that account for the information loss of contraction, information gain
of expansion and information change of revision. We also give a succinct look
into the problem of iterated revision, which deals with the application of a
sequence of revision operations in our framework, and also illustrate how one
may build from our KM-based contraction operator also one not satisfying the
(in)famous recovery postulate, by focusing on the so-called severe withdrawal
model as an illustrative example.",http://arxiv.org/abs/2403.10502v1
"Inferring Change Points in High-Dimensional Regression via Approximate
  Message Passing",2024-04-11T15:57:12Z,"Gabriel Arpino, Xiaoqi Liu, Julia Gontarek, Ramji Venkataramanan","We consider the problem of localizing change points in a generalized linear
model (GLM), a model that covers many widely studied problems in statistical
learning including linear, logistic, and rectified linear regression. We
propose a novel and computationally efficient Approximate Message Passing (AMP)
algorithm for estimating both the signals and the change point locations, and
rigorously characterize its performance in the high-dimensional limit where the
number of parameters $p$ is proportional to the number of samples $n$. This
characterization is in terms of a state evolution recursion, which allows us to
precisely compute performance measures such as the asymptotic Hausdorff error
of our change point estimates, and allows us to tailor the algorithm to take
advantage of any prior structural information on the signals and change points.
Moreover, we show how our AMP iterates can be used to efficiently compute a
Bayesian posterior distribution over the change point locations in the
high-dimensional limit. We validate our theory via numerical experiments, and
demonstrate the favorable performance of our estimators on both synthetic and
real data in the settings of linear, logistic, and rectified linear regression.",http://arxiv.org/abs/2404.07864v2
"ChangeAnywhere: Sample Generation for Remote Sensing Change Detection
  via Semantic Latent Diffusion Model",2024-04-13T03:46:35Z,"Kai Tang, Jin Chen","Remote sensing change detection (CD) is a pivotal technique that pinpoints
changes on a global scale based on multi-temporal images. With the recent
expansion of deep learning, supervised deep learning-based CD models have shown
satisfactory performance. However, CD sample labeling is very time-consuming as
it is densely labeled and requires expert knowledge. To alleviate this problem,
we introduce ChangeAnywhere, a novel CD sample generation method using the
semantic latent diffusion model and single-temporal images. Specifically,
ChangeAnywhere leverages the relative ease of acquiring large single-temporal
semantic datasets to generate large-scale, diverse, and semantically annotated
bi-temporal CD datasets. ChangeAnywhere captures the two essentials of CD
samples, i.e., change implies semantically different, and non-change implies
reasonable change under the same semantic constraints. We generated
ChangeAnywhere-100K, the largest synthesis CD dataset with 100,000 pairs of CD
samples based on the proposed method. The ChangeAnywhere-100K significantly
improved both zero-shot and few-shot performance on two CD benchmark datasets
for various deep learning-based CD models, as demonstrated by transfer
experiments. This paper delineates the enormous potential of ChangeAnywhere for
CD sample generation and demonstrates the subsequent enhancement of model
performance. Therefore, ChangeAnywhere offers a potent tool for remote sensing
CD. All codes and pre-trained models will be available at
https://github.com/tangkai-RS/ChangeAnywhere.",http://arxiv.org/abs/2404.08892v1
"Modeling the Lane-Change Reactions to Merging Vehicles for Highway
  On-Ramp Simulations",2024-04-15T15:02:44Z,"Dustin Holley, Jovin Dsa, Hossein Nourkhiz Mahjoub, Gibran Ali, Tyler Naes, Ehsan Moradi-Pari, Pawan Sai Kallepalli","Enhancing simulation environments to replicate real-world driver behavior is
essential for developing Autonomous Vehicle technology. While some previous
works have studied the yielding reaction of lag vehicles in response to a
merging car at highway on-ramps, the possible lane-change reaction of the lag
car has not been widely studied. In this work we aim to improve the simulation
of the highway merge scenario by including the lane-change reaction in addition
to yielding behavior of main-lane lag vehicles, and we evaluate two different
models for their ability to capture this reactive lane-change behavior. To tune
the payoff functions of these models, a novel naturalistic dataset was
collected on U.S. highways that provided several hours of merge-specific data
to learn the lane change behavior of U.S. drivers. To make sure that we are
collecting a representative set of different U.S. highway geometries in our
data, we surveyed 50,000 U.S. highway on-ramps and then selected eight
representative sites. The data were collected using roadside-mounted lidar
sensors to capture various merge driver interactions. The models were
demonstrated to be configurable for both keep-straight and lane-change
behavior. The models were finally integrated into a high-fidelity simulation
environment and confirmed to have adequate computation time efficiency for use
in large-scale simulations to support autonomous vehicle development.",http://arxiv.org/abs/2404.09851v2
"A biologically inspired computational trust model for open multi-agent
  systems which is resilient to trustor population changes",2024-04-13T10:56:32Z,"Zoi Lygizou, Dimitris Kalles","Current trust and reputation models continue to have significant limitations,
such as the inability to deal with agents constantly entering or exiting open
multi-agent systems (open MAS), as well as continuously changing behaviors. Our
study is based on CA, a previously proposed decentralized computational trust
model from the trustee's point of view, inspired by synaptic plasticity and the
formation of assemblies in the human brain. It is designed to meet the
requirements of highly dynamic and open MAS, and its main difference with most
conventional trust and reputation models is that the trustor does not select a
trustee to delegate a task; instead, the trustee determines whether it is
qualified to successfully execute it. We ran a series of simulations to compare
CA model to FIRE, a well-established, decentralized trust and reputation model
for open MAS under conditions of continuous trustee and trustor population
replacement, as well as continuous change of trustees' abilities to perform
tasks. The main finding is that FIRE is superior to changes in the trustee
population, whereas CA is resilient to the trustor population changes. When the
trustees switch performance profiles FIRE clearly outperforms despite the fact
that both models' performances are significantly impacted by this environmental
change. Findings lead us to conclude that learning to use the appropriate trust
model, according to the dynamic conditions in effect could maximize the
trustor's benefits.",http://arxiv.org/abs/2404.10014v1
"Leveraging Fine-Grained Information and Noise Decoupling for Remote
  Sensing Change Detection",2024-04-17T12:32:10Z,"Qiangang Du, Jinlong Peng, Changan Wang, Xu Chen, Qingdong He, Wenbing Zhu, Mingmin Chi, Yabiao Wang, Chengjie Wang","Change detection aims to identify remote sense object changes by analyzing
data between bitemporal image pairs. Due to the large temporal and spatial span
of data collection in change detection image pairs, there are often a
significant amount of task-specific and task-agnostic noise. Previous effort
has focused excessively on denoising, with this goes a great deal of loss of
fine-grained information. In this paper, we revisit the importance of
fine-grained features in change detection and propose a series of operations
for fine-grained information compensation and noise decoupling (FINO). First,
the context is utilized to compensate for the fine-grained information in the
feature space. Next, a shape-aware and a brightness-aware module are designed
to improve the capacity for representation learning. The shape-aware module
guides the backbone for more precise shape estimation, guiding the backbone
network in extracting object shape features. The brightness-aware module learns
a overall brightness estimation to improve the model's robustness to
task-agnostic noise. Finally, a task-specific noise decoupling structure is
designed as a way to improve the model's ability to separate noise interference
from feature similarity. With these training schemes, our proposed method
achieves new state-of-the-art (SOTA) results in multiple change detection
benchmarks. The code will be made available.",http://arxiv.org/abs/2404.11318v3
"Artificial intelligence for context-aware visual change detection in
  software test automation",2024-05-01T21:22:33Z,"Milad Moradi, Ke Yan, David Colwell, Rhona Asgari","Automated software testing is integral to the software development process,
streamlining workflows and ensuring product reliability. Visual testing within
this context, especially concerning user interface (UI) and user experience
(UX) validation, stands as one of crucial determinants of overall software
quality. Nevertheless, conventional methods like pixel-wise comparison and
region-based visual change detection fall short in capturing contextual
similarities, nuanced alterations, and understanding the spatial relationships
between UI elements. In this paper, we introduce a novel graph-based method for
visual change detection in software test automation. Leveraging a machine
learning model, our method accurately identifies UI controls from software
screenshots and constructs a graph representing contextual and spatial
relationships between the controls. This information is then used to find
correspondence between UI controls within screenshots of different versions of
a software. The resulting graph encapsulates the intricate layout of the UI and
underlying contextual relations, providing a holistic and context-aware model.
This model is finally used to detect and highlight visual regressions in the
UI. Comprehensive experiments on different datasets showed that our change
detector can accurately detect visual software changes in various simple and
complex test scenarios. Moreover, it outperformed pixel-wise comparison and
region-based baselines by a large margin in more complex testing scenarios.
This work not only contributes to the advancement of visual change detection
but also holds practical implications, offering a robust solution for
real-world software test automation challenges, enhancing reliability, and
ensuring the seamless evolution of software interfaces.",http://arxiv.org/abs/2405.00874v1
Towards Robust Physical-world Backdoor Attacks on Lane Detection,2024-05-09T05:23:34Z,"Xinwei Zhang, Aishan Liu, Tianyuan Zhang, Siyuan Liang, Xianglong Liu","Deep learning-based lane detection (LD) plays a critical role in autonomous
driving systems, such as adaptive cruise control. However, it is vulnerable to
backdoor attacks. Existing backdoor attack methods on LD exhibit limited
effectiveness in dynamic real-world scenarios, primarily because they fail to
consider dynamic scene factors, including changes in driving perspectives
(e.g., viewpoint transformations) and environmental conditions (e.g., weather
or lighting changes). To tackle this issue, this paper introduces BadLANE, a
dynamic scene adaptation backdoor attack for LD designed to withstand changes
in real-world dynamic scene factors. To address the challenges posed by
changing driving perspectives, we propose an amorphous trigger pattern composed
of shapeless pixels. This trigger design allows the backdoor to be activated by
various forms or shapes of mud spots or pollution on the road or lens, enabling
adaptation to changes in vehicle observation viewpoints during driving. To
mitigate the effects of environmental changes, we design a meta-learning
framework to train meta-generators tailored to different environmental
conditions. These generators produce meta-triggers that incorporate diverse
environmental information, such as weather or lighting conditions, as the
initialization of the trigger patterns for backdoor implantation, thus enabling
adaptation to dynamic environments. Extensive experiments on various commonly
used LD models in both digital and physical domains validate the effectiveness
of our attacks, outperforming other baselines significantly (+25.15% on average
in Attack Success Rate). Our codes will be available upon paper publication.",http://arxiv.org/abs/2405.05553v3
"TransAnaNet: Transformer-based Anatomy Change Prediction Network for
  Head and Neck Cancer Patient Radiotherapy",2024-05-09T11:00:06Z,"Meixu Chen, Kai Wang, Michael Dohopolski, Howard Morgan, David Sher, Jing Wang","Early identification of head and neck cancer (HNC) patients who would
experience significant anatomical change during radiotherapy (RT) is important
to optimize patient clinical benefit and treatment resources. This study aims
to assess the feasibility of using a vision-transformer (ViT) based neural
network to predict RT-induced anatomic change in HNC patients. We
retrospectively included 121 HNC patients treated with definitive RT/CRT. We
collected the planning CT (pCT), planned dose, CBCTs acquired at the initial
treatment (CBCT01) and fraction 21 (CBCT21), and primary tumor volume (GTVp)
and involved nodal volume (GTVn) delineated on both pCT and CBCTs for model
construction and evaluation. A UNet-style ViT network was designed to learn
spatial correspondence and contextual information from embedded CT, dose,
CBCT01, GTVp, and GTVn image patches. The model estimated the deformation
vector field between CBCT01 and CBCT21 as the prediction of anatomic change,
and deformed CBCT01 was used as the prediction of CBCT21. We also generated
binary masks of GTVp, GTVn, and patient body for volumetric change evaluation.
The predicted image from the proposed method yielded the best similarity to the
real image (CBCT21) over pCT, CBCT01, and predicted CBCTs from other comparison
models. The average MSE and SSIM between the normalized predicted CBCT to
CBCT21 are 0.009 and 0.933, while the average dice coefficient between body
mask, GTVp mask, and GTVn mask are 0.972, 0.792, and 0.821 respectively. The
proposed method showed promising performance for predicting
radiotherapy-induced anatomic change, which has the potential to assist in the
decision-making of HNC Adaptive RT.",http://arxiv.org/abs/2405.05674v2
"Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in
  Remote Sensing Images",2024-05-21T15:44:31Z,"Xiaofei Yu, Yitong Li, Jie Ma","Remote sensing image change captioning (RSICC) aims at generating human-like
language to describe the semantic changes between bi-temporal remote sensing
image pairs. It provides valuable insights into environmental dynamics and land
management. Unlike conventional change captioning task, RSICC involves not only
retrieving relevant information across different modalities and generating
fluent captions, but also mitigating the impact of pixel-level differences on
terrain change localization. The pixel problem due to long time span decreases
the accuracy of generated caption. Inspired by the remarkable generative power
of diffusion model, we propose a probabilistic diffusion model for RSICC to
solve the aforementioned problems. In training process, we construct a noise
predictor conditioned on cross modal features to learn the distribution from
the real caption distribution to the standard Gaussian distribution under the
Markov chain. Meanwhile, a cross-mode fusion and a stacking self-attention
module are designed for noise predictor in the reverse process. In testing
phase, the well-trained noise predictor helps to estimate the mean value of the
distribution and generate change captions step by step. Extensive experiments
on the LEVIR-CC dataset demonstrate the effectiveness of our Diffusion-RSCC and
its individual components. The quantitative results showcase superior
performance over existing methods across both traditional and newly augmented
metrics. The code and materials will be available online at
https://github.com/Fay-Y/Diffusion-RSCC.",http://arxiv.org/abs/2405.12875v1
"Content and Salient Semantics Collaboration for Cloth-Changing Person
  Re-Identification",2024-05-26T15:17:28Z,"Qizao Wang, Xuelin Qian, Bin Li, Lifeng Chen, Yanwei Fu, Xiangyang Xue","Cloth-changing person Re-IDentification (Re-ID) aims at recognizing the same
person with clothing changes across non-overlapping cameras. Conventional
person Re-ID methods usually bias the model's focus on cloth-related appearance
features rather than identity-sensitive features associated with biological
traits. Recently, advanced cloth-changing person Re-ID methods either resort to
identity-related auxiliary modalities (e.g., sketches, silhouettes, keypoints
and 3D shapes) or clothing labels to mitigate the impact of clothes. However,
relying on unpractical and inflexible auxiliary modalities or annotations
limits their real-world applicability. In this paper, we promote cloth-changing
person Re-ID by effectively leveraging abundant semantics present within
pedestrian images without the need for any auxiliaries. Specifically, we
propose the Content and Salient Semantics Collaboration (CSSC) framework,
facilitating cross-parallel semantics interaction and refinement. Our framework
is simple yet effective, and the vital design is the Semantics Mining and
Refinement (SMR) module. It extracts robust identity features about content and
salient semantics, while mitigating interference from clothing appearances
effectively. By capitalizing on the mined abundant semantic features, our
proposed approach achieves state-of-the-art performance on three cloth-changing
benchmarks as well as conventional benchmarks, demonstrating its superiority
over advanced competitors.",http://arxiv.org/abs/2405.16597v1
"MARS: Benchmarking the Metaphysical Reasoning Abilities of Language
  Models with a Multi-task Evaluation Dataset",2024-06-04T08:35:04Z,"Weiqi Wang, Yangqiu Song","To enable Large Language Models (LLMs) to function as conscious agents with
generalizable reasoning capabilities, it is crucial that they possess the
reasoning ability to comprehend situational changes (transitions) in
distribution triggered by environmental factors or actions from other agents.
Despite its fundamental significance, this ability remains underexplored due to
the complexity of modeling infinite possible changes in an event and their
associated distributions, coupled with the lack of benchmark data with
situational transitions. Addressing these gaps, we propose a novel formulation
of reasoning with distributional changes as a three-step discriminative
process, termed as MetAphysical ReaSoning. We then introduce the first-ever
benchmark, MARS, comprising three tasks corresponding to each step. These tasks
systematically assess LLMs' capabilities in reasoning the plausibility of (i)
changes in actions, (ii) states caused by changed actions, and (iii)
situational transitions driven by changes in action. Extensive evaluations with
20 (L)LMs of varying sizes and methods indicate that all three tasks in this
process pose significant challenges, even for state-of-the-art LLMs and LMs
after fine-tuning. Further analyses reveal potential causes for the
underperformance of LLMs and demonstrate that pre-training them on large-scale
conceptualization taxonomies can potentially enhance their metaphysical
reasoning capabilities. Our data and models are publicly accessible at
https://github.com/HKUST-KnowComp/MARS.",http://arxiv.org/abs/2406.02106v1
"SRC-Net: Bi-Temporal Spatial Relationship Concerned Network for Change
  Detection",2024-06-09T06:53:39Z,"Hongjia Chen, Xin Xu, Fangling Pu","Change detection (CD) in remote sensing imagery is a crucial task with
applications in environmental monitoring, urban development, and disaster
management. CD involves utilizing bi-temporal images to identify changes over
time. The bi-temporal spatial relationships between features at the same
location at different times play a key role in this process. However, existing
change detection networks often do not fully leverage these spatial
relationships during bi-temporal feature extraction and fusion. In this work,
we propose SRC-Net: a bi-temporal spatial relationship concerned network for
CD. The proposed SRC-Net includes a Perception and Interaction Module that
incorporates spatial relationships and establishes a cross-branch perception
mechanism to enhance the precision and robustness of feature extraction.
Additionally, a Patch-Mode joint Feature Fusion Module is introduced to address
information loss in current methods. It considers different change modes and
concerns about spatial relationships, resulting in more expressive fusion
features. Furthermore, we construct a novel network using these two
relationship concerned modules and conducted experiments on the LEVIR-CD and
WHU Building datasets. The experimental results demonstrate that our network
outperforms state-of-the-art (SOTA) methods while maintaining a modest
parameter count. We believe our approach sets a new paradigm for change
detection and will inspire further advancements in the field. The code and
models are publicly available at https://github.com/Chnja/SRCNet.",http://arxiv.org/abs/2406.05668v2
Streaming Algorithms with Few State Changes,2024-06-10T22:09:42Z,"Rajesh Jayaram, David P. Woodruff, Samson Zhou","In this paper, we study streaming algorithms that minimize the number of
changes made to their internal state (i.e., memory contents). While the design
of streaming algorithms typically focuses on minimizing space and update time,
these metrics fail to capture the asymmetric costs, inherent in modern hardware
and database systems, of reading versus writing to memory. In fact, most
streaming algorithms write to their memory on every update, which is
undesirable when writing is significantly more expensive than reading. This
raises the question of whether streaming algorithms with small space and number
of memory writes are possible.
  We first demonstrate that, for the fundamental $F_p$ moment estimation
problem with $p\ge 1$, any streaming algorithm that achieves a constant factor
approximation must make $\Omega(n^{1-1/p})$ internal state changes, regardless
of how much space it uses. Perhaps surprisingly, we show that this lower bound
can be matched by an algorithm that also has near-optimal space complexity.
Specifically, we give a $(1+\varepsilon)$-approximation algorithm for $F_p$
moment estimation that uses a near-optimal
$\widetilde{\mathcal{O}}_\varepsilon(n^{1-1/p})$ number of state changes, while
simultaneously achieving near-optimal space, i.e., for $p\in[1,2]$, our
algorithm uses $\text{poly}\left(\log n,\frac{1}{\varepsilon}\right)$ bits of
space, while for $p>2$, the algorithm uses
$\widetilde{\mathcal{O}}_\varepsilon(n^{1-2/p})$ space. We similarly design
streaming algorithms that are simultaneously near-optimal in both space
complexity and the number of state changes for the heavy-hitters problem,
sparse support recovery, and entropy estimation. Our results demonstrate that
an optimal number of state changes can be achieved without sacrificing space
complexity.",http://arxiv.org/abs/2406.06821v1
