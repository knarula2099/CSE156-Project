Title,Published,Authors,Abstract,Link
You are right. I am ALARMED -- But by Climate Change Counter Movement,2020-04-30T16:06:02Z,"Shraey Bhatia, Jey Han Lau, Timothy Baldwin","The world is facing the challenge of climate crisis. Despite the consensus in
scientific community about anthropogenic global warming, the web is flooded
with articles spreading climate misinformation. These articles are carefully
constructed by climate change counter movement (cccm) organizations to
influence the narrative around climate change. We revisit the literature on
climate misinformation in social sciences and repackage it to introduce in the
community of NLP. Despite considerable work in detection of fake news, there is
no misinformation dataset available that is specific to the domain.of climate
change. We try to bridge this gap by scraping and releasing articles with known
climate change misinformation.",http://arxiv.org/abs/2004.14907v1
Climate Monitoring using Internet of X-Things,2020-05-30T09:54:09Z,"Nasir Saeed, Tareq Y. Al-Naffouri, Mohamed-Slim Alouini","Global climate change is significantly affecting the life on planet Earth.
Predicting timely changes in the climate is a big challenge and requires great
attention from the scientific community. Research now suggests that using the
internet of X-things (X-IoT) helps in monitoring global climate change.",http://arxiv.org/abs/2006.00231v1
Future Climate Change Projections over the Indian Region,2020-12-02T07:46:57Z,"J. Sanjay, R. Krishnan, M. V. S. Ramarao, R. Mahesh, Bhupendra Bahadur Singh, Jayashri Patel, Sandip Ingle, Preethi Bhaskar, J. V. Revadekar, T. P. Sabin, M. Mujumdar","Assessments of impacts of climate change and future projections over the
Indian region, have so far relied on a single regional climate model (RCM) -
eg., the PRECIS RCM of the Hadley Centre, UK. While these assessments have
provided inputs to various reports (e.g., INCCA 2010; NATCOMM2 2012), it is
important to have an ensemble of climate projections drawn from multiple RCMs
due to large uncertainties in regional-scale climate projections. Ensembles of
multi-RCM projections driven under different perceivable socio-economic
scenarios are required to capture the probable path of growth, and provide the
behavior of future climate and impacts on various biophysical systems and
economic sectors dependent on such systems.
  The Centre for Climate Change Research, Indian Institute of Tropical
Meteorology (CCCR-IITM) has generated an ensemble of high resolution downscaled
projections of regional climate and monsoon over South Asia until 2100 for the
Intergovernmental Panel for Climate Change (IPCC)using a RCM (ICTP-RegCM4) at
50 km horizontal resolution, by driving the regional model with lateral and
lower boundary conditions from multiple global atmosphere-ocean coupled models
from the Coupled Model Intercomparison Project Phase 5 (CMIP5). The future
projections are based on three Representation Concentration Pathway (RCP)
scenarios (viz., RCP2.6, RCP4.5, RCP8.5) of the IPCC.",http://arxiv.org/abs/2012.10386v1
"The Human Effect Requires Affect: Addressing Social-Psychological
  Factors of Climate Change with Machine Learning",2020-11-24T23:34:54Z,"Kyle Tilbury, Jesse Hoey","Machine learning has the potential to aid in mitigating the human effects of
climate change. Previous applications of machine learning to tackle the human
effects in climate change include approaches like informing individuals of
their carbon footprint and strategies to reduce it. For these methods to be the
most effective they must consider relevant social-psychological factors for
each individual. Of social-psychological factors at play in climate change,
affect has been previously identified as a key element in perceptions and
willingness to engage in mitigative behaviours. In this work, we propose an
investigation into how affect could be incorporated to enhance machine learning
based interventions for climate change. We propose using affective agent-based
modelling for climate change as well as the use of a simulated climate change
social dilemma to explore the potential benefits of affective machine learning
interventions. Behavioural and informational interventions can be a powerful
tool in helping humans adopt mitigative behaviours. We expect that utilizing
affective ML can make interventions an even more powerful tool and help
mitigative behaviours become widely adopted.",http://arxiv.org/abs/2011.12443v1
The Role of Uncertainty in Controlling Climate Change,2020-03-03T16:00:26Z,Yongyang Cai,"Integrated Assessment Models (IAMs) of the climate and economy aim to analyze
the impact and efficacy of policies that aim to control climate change, such as
carbon taxes and subsidies. A major characteristic of IAMs is that their
geophysical sector determines the mean surface temperature increase over the
preindustrial level, which in turn determines the damage function. Most of the
existing IAMs are perfect-foresight forward-looking models, assuming that we
know all of the future information. However, there are significant
uncertainties in the climate and economic system, including parameter
uncertainty, model uncertainty, climate tipping risks, economic risks, and
ambiguity. For example, climate damages are uncertain: some researchers assume
that climate damages are proportional to instantaneous output, while others
assume that climate damages have a more persistent impact on economic growth.
Climate tipping risks represent (nearly) irreversible climate events that may
lead to significant changes in the climate system, such as the Greenland ice
sheet collapse, while the conditions, probability of tipping, duration, and
associated damage are also uncertain. Technological progress in carbon capture
and storage, adaptation, renewable energy, and energy efficiency are uncertain
too. In the face of these uncertainties, policymakers have to provide a
decision that considers important factors such as risk aversion, inequality
aversion, and sustainability of the economy and ecosystem. Solving this problem
may require richer and more realistic models than standard IAMs, and advanced
computational methods. The recent literature has shown that these uncertainties
can be incorporated into IAMs and may change optimal climate policies
significantly.",http://arxiv.org/abs/2003.01615v2
Regional Climate Change Datasets for South Asia,2020-12-02T07:48:36Z,"J. Sanjay, M. V. S. Ramarao, R. Mahesh, Sandip Ingle, Bhupendra Bahadur Singh, R. Krishnan","The Centre for Climate Change Research (CCCR;http://cccr.tropmet.res.in) at
the Indian Institute of Tropical Meteorology (IITM; http://www.tropmet.res.in),
Pune, launched in 2009 with the support of the Ministry of Earth Sciences
(MoES), Government of India, focuses on the development of new climate
modelling capabilities in India and South Asia to address issues concerning the
science of climate change. CCCR-IITM has the mandate of developing an Earth
System Model and to make the regional climate projections. An important
achievement was made by developing an Earth System Model at IITM, which is an
important step towards understanding global and regional climate response to
long-term climate variability and climate change. CCCR-IITM has also generated
an ensemble of high resolution dynamically downscaled future projections of
regional climate over South Asia and Indian monsoon, which are found useful for
impact assessment studies and for quantifying uncertainties in the regional
projections. A brief overview of these core climate change modeling activities
of CCCR-IITM was presented in an Interim Report on Climate Change over India
(available at http://cccr.tropmet.res.in/home/reports.jsp)",http://arxiv.org/abs/2012.10387v1
CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims,2020-12-01T16:32:54Z,"Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, Markus Leippold","We introduce CLIMATE-FEVER, a new publicly available dataset for verification
of climate change-related claims. By providing a dataset for the research
community, we aim to facilitate and encourage work on improving algorithms for
retrieving evidential support for climate-specific claims, addressing the
underlying language understanding challenges, and ultimately help alleviate the
impact of misinformation on climate change. We adapt the methodology of FEVER
[1], the largest dataset of artificially designed claims, to real-life claims
collected from the Internet. While during this process, we could rely on the
expertise of renowned climate scientists, it turned out to be no easy task. We
discuss the surprising, subtle complexity of modeling real-world
climate-related claims within the \textsc{fever} framework, which we believe
provides a valuable challenge for general natural language understanding. We
hope that our work will mark the beginning of a new exciting long-term joint
effort by the climate science and AI community.",http://arxiv.org/abs/2012.00614v2
"Introduction to the Special Issue on the Statistical Mechanics of
  Climate",2020-06-24T05:39:58Z,Valerio Lucarini,"We introduce the special issue on the Statistical Mechanics of Climate
published on the Journal of Statistical Physics by presenting an informal
discussion of some theoretical aspects of climate dynamics that make it a topic
of great interest for mathematicians and theoretical physicists. In particular,
we briefly discuss its nonequilibrium and multiscale properties, the
relationship between natural climate variability and climate change, the
different regimes of climate response to perturbations, and critical
transitions.",http://arxiv.org/abs/2006.13495v1
HECT: High-Dimensional Ensemble Consistency Testing for Climate Models,2020-10-08T15:16:16Z,"Niccol√≤ Dalmasso, Galen Vincent, Dorit Hammerling, Ann B. Lee","Climate models play a crucial role in understanding the effect of
environmental and man-made changes on climate to help mitigate climate risks
and inform governmental decisions. Large global climate models such as the
Community Earth System Model (CESM), developed by the National Center for
Atmospheric Research, are very complex with millions of lines of code
describing interactions of the atmosphere, land, oceans, and ice, among other
components. As development of the CESM is constantly ongoing, simulation
outputs need to be continuously controlled for quality. To be able to
distinguish a ""climate-changing"" modification of the code base from a true
climate-changing physical process or intervention, there needs to be a
principled way of assessing statistical reproducibility that can handle both
spatial and temporal high-dimensional simulation outputs. Our proposed work
uses probabilistic classifiers like tree-based algorithms and deep neural
networks to perform a statistically rigorous goodness-of-fit test of
high-dimensional spatio-temporal data.",http://arxiv.org/abs/2010.04051v2
"Toward an Effective Pedagogy of Climate Change: Lessons From a Physics
  Classroom",2020-08-01T15:19:26Z,Vandana Singh,"A major roadblock to effective climate education is the lack of radical
visions (Kwauk, 2020) for engaging with climate change and related
social-environmental crises in the classroom. Key aspects of the climate
system: its inherent complexity and transdisciplinarity, the entanglement of
social and natural systems, and the fact that it spans large scales of space
and time - all present serious challenges to our modern, compartmentalized
formal education systems. I present an argument for a justice-centered,
inter-to-transdisciplinary conceptualization of climate change at the nexus of
science, society and justice, which is designed to engage with the very
features of the climate system that make it challenging. Based on pedagogical
experiments in an undergraduate physics classroom for non-science majors, I
identify five general barriers to teaching climate change, and formulate four
dimensions of an effective climate pedagogy: the scientific-technological, the
transdisciplinary, the onto-epistemological, and the psychosocial action
dimensions. Within the context of a collaborative classroom culture informed by
transformational learning, I describe the use of a meta-conceptual framework
that, along with the issue of climate justice, seeks to realize all four
dimensions of an effective climate pedagogy. This broad holistic framework is
potentially adaptable to contexts and disciplines beyond undergraduate physics.",http://arxiv.org/abs/2008.00281v2
Climate change adaption in Chinese ancient architecture,2020-12-28T14:22:09Z,"Siyang Li, Ke Ding, Aijun Ding, Lejun He, Xin Huang, Quansheng Ge, Congbin Fu","As an important symbol of civilization and culture, architectures originally
were built for sheltering human beings from weather disasters and therefore
should be affected by climate change, particularly the associated change in the
occurrence of extreme weather events. However, although meteorology has been
considered as a factor in modern architecture design, it remains unclear
whether and how the ancients adapted to climate change from the perspective of
architecture design, particularly on a millennium time scale. Here we show that
the periodic change and trend of the roof slope of ancient architectures in
northern China clearly demonstrate the climate change adaptation over the past
thousand years. We show that the snowfall fluctuation caused by the
paleo-climate change was an essential driving factor within the roof
modification process of ancient Chinese timber architecture. Our study
indicates that climate change may act as a much more important factor on human
behaviour than we ever thought.",http://arxiv.org/abs/2012.14244v1
"Determining feature importance for actionable climate change mitigation
  policies",2020-03-19T01:06:52Z,"Romit Maulik, Junghwa Choi, Wesley Wehde, Prasanna Balaprakash","Given the importance of public support for policy change and implementation,
public policymakers and researchers have attempted to understand the factors
associated with this support for climate change mitigation policy. In this
article, we compare the feasibility of using different supervised learning
methods for regression using a novel socio-economic data set which measures
public support for potential climate change mitigation policies. Following this
model selection, we utilize gradient boosting regression, a well-known
technique in the machine learning community, but relatively uncommon in public
policy and public opinion research, and seek to understand what factors among
the several examined in previous studies are most central to shaping public
support for mitigation policies in climate change studies. The use of this
method provides novel insights into the most important factors for public
support for climate change mitigation policies. Using national survey data, we
find that the perceived risks associated with climate change are more decisive
for shaping public support for policy options promoting renewable energy and
regulating pollutants. However, we observe a very different behavior related to
public support for increasing the use of nuclear energy where climate change
risk perception is no longer the sole decisive feature. Our findings indicate
that public support for renewable energy is inherently different from that for
nuclear energy reliance with the risk perception of climate change, dominant
for the former, playing a subdued role for the latter.",http://arxiv.org/abs/2003.10234v1
Emergent constraints on climate sensitivities,2020-12-17T09:46:47Z,"Mark S. Williamson, Chad W. Thackeray, Peter M. Cox, Alex Hall, Chris Huntingford, Femke J. M. M. Nijsse","Despite major advances in climate science over the last 30 years, persistent
uncertainties in projections of future climate change remain. Climate
projections are produced with increasingly complex models which attempt to
represent key processes in the Earth system, including atmospheric and oceanic
circulations, convection, clouds, snow, sea-ice, vegetation and interactions
with the carbon cycle. Uncertainties in the representation of these processes
feed through into a range of projections from the many state-of-the-art climate
models now being developed and used worldwide. For example, despite major
improvements in climate models, the range of equilibrium global warming due to
doubling carbon dioxide still spans a range of more than three. Here we review
a promising way to make use of the ensemble of climate models to reduce the
uncertainties in the sensitivities of the real climate system. The emergent
constraint approach uses the model ensemble to identify a relationship between
an uncertain aspect of the future climate and an observable variation or trend
in the contemporary climate. This review summarises previous published work on
emergent constraints, and discusses the huge promise and potential dangers of
the approach. Most importantly, it argues that emergent constraints should be
based on well-founded physical principles such as the fluctuation-dissipation
theorem. It is hoped that this review will stimulate physicists to contribute
to the rapidly developing field of emergent constraints on climate projections,
bringing to it much needed rigour and physical insights.",http://arxiv.org/abs/2012.09468v1
ClimaText: A Dataset for Climate Change Topic Detection,2020-12-01T13:42:37Z,"Francesco S. Varini, Jordan Boyd-Graber, Massimiliano Ciaramita, Markus Leippold","Climate change communication in the mass media and other textual sources may
affect and shape public perception. Extracting climate change information from
these sources is an important task, e.g., for filtering content and
e-discovery, sentiment analysis, automatic summarization, question-answering,
and fact-checking. However, automating this process is a challenge, as climate
change is a complex, fast-moving, and often ambiguous topic with scarce
resources for popular text-based AI tasks. In this paper, we introduce
\textsc{ClimaText}, a dataset for sentence-based climate change topic
detection, which we make publicly available. We explore different approaches to
identify the climate change topic in various text sources. We find that popular
keyword-based models are not adequate for such a complex and evolving task.
Context-based algorithms like BERT \cite{devlin2018bert} can detect, in
addition to many trivial cases, a variety of complex and implicit topic
patterns. Nevertheless, our analysis reveals a great potential for improvement
in several directions, such as, e.g., capturing the discussion on indirect
effects of climate change. Hence, we hope this work can serve as a good
starting point for further research on this topic.",http://arxiv.org/abs/2012.00483v2
Affective Polarization in Online Climate Change Discourse on Twitter,2020-08-29T20:37:33Z,"Aman Tyagi, Joshua Uyheng, Kathleen M. Carley","Online social media has become an important platform to organize around
different socio-cultural and political topics. An extensive scholarship has
discussed how people are divided into echo-chamber-like groups. However, there
is a lack of work related to quantifying hostile communication or
\textit{affective polarization} between two competing groups. This paper
proposes a systematic, network-based methodology for examining affective
polarization in online conversations. Further, we apply our framework to 100
weeks of Twitter discourse about climate change. We find that deniers of
climate change (Disbelievers) are more hostile towards people who believe
(Believers) in the anthropogenic cause of climate change than vice versa.
Moreover, Disbelievers use more words and hashtags related to natural disasters
during more hostile weeks as compared to Believers. These findings bear
implications for studying affective polarization in online discourse,
especially concerning the subject of climate change. Lastly, we discuss our
findings in the context of increasingly important climate change communication
research.",http://arxiv.org/abs/2008.13051v1
Polarizing Tweets on Climate Change,2020-08-29T20:57:54Z,"Aman Tyagi, Matthew Babcock, Kathleen M. Carley, Douglas C. Sicker","We introduce a framework to analyze the conversation between two competing
groups of Twitter users, one who believe in the anthropogenic causes of climate
change (Believers) and a second who are skeptical (Disbelievers). As a case
study, we use Climate Change related tweets during the United Nation's (UN)
Climate Change Conference - COP24 (2018), Katowice, Poland. We find that both
Disbelievers and Believers talk within their group more than with the other
group; this is more so the case for Disbelievers than for Believers. The
Disbeliever messages focused more on attacking those personalities that believe
in the anthropogenic causes of climate change. On the other hand, Believer
messages focused on calls to combat climate change. We find that in both
Disbelievers and Believers bot-like accounts were equally active and that
unlike Believers, Disbelievers get their news from a concentrated number of
news sources.",http://arxiv.org/abs/2008.13054v1
"The Historical Impact of Anthropogenic Climate Change on Global
  Agricultural Productivity",2020-07-20T19:05:29Z,"Ariel Ortiz-Bobea, Toby R. Ault, Carlos M. Carrillo, Robert G. Chambers, David B. Lobell","Agricultural research has fostered productivity growth, but the historical
influence of anthropogenic climate change on that growth has not been
quantified. We develop a robust econometric model of weather effects on global
agricultural total factor productivity (TFP) and combine this model with
counterfactual climate scenarios to evaluate impacts of past climate trends on
TFP. Our baseline model indicates that anthropogenic climate change has reduced
global agricultural TFP by about 21% since 1961, a slowdown that is equivalent
to losing the last 9 years of productivity growth. The effect is substantially
more severe (a reduction of ~30-33%) in warmer regions such as Africa and Latin
America and the Caribbean. We also find that global agriculture has grown more
vulnerable to ongoing climate change.",http://arxiv.org/abs/2007.10415v2
"Modeling Climate Change Impact on Wind Power Resources Using Adaptive
  Neuro-Fuzzy Inference System",2020-01-09T17:35:56Z,"Narjes Nabipour, Amir Mosavi, Eva Hajnal, Laszlo Nadai, Shahab Shamshirband, Kwok-Wing Chau","Climate change impacts and adaptations are the subjects to ongoing issues
that attract the attention of many researchers. Insight into the wind power
potential in an area and its probable variation due to climate change impacts
can provide useful information for energy policymakers and strategists for
sustainable development and management of the energy. In this study, spatial
variation of wind power density at the turbine hub-height and its variability
under future climatic scenarios are taken under consideration. An ANFIS based
post-processing technique was employed to match the power outputs of the
regional climate model with those obtained from the reference data. The
near-surface wind data obtained from a regional climate model are employed to
investigate climate change impacts on the wind power resources in the Caspian
Sea. Subsequent to converting near-surface wind speed to turbine hub-height
speed and computation of wind power density, the results have been investigated
to reveal mean annual power, seasonal, and monthly variability for a 20-year
period in the present (1981-2000) and in the future (2081-2100). The findings
of this study indicated that the middle and northern parts of the Caspian Sea
are placed with the highest values of wind power. However, the results of the
post-processing technique using adaptive neuro-fuzzy inference system (ANFIS)
model showed that the real potential of the wind power in the area is lower
than those of projected from the regional climate model.",http://arxiv.org/abs/2001.04279v1
"From Talk to Action with Accountability: Monitoring the Public
  Discussion of Policy Makers with Deep Neural Networks and Topic Modelling",2020-10-16T12:21:01Z,"Vili H√§t√∂nen, Fiona Melzer","Decades of research on climate have provided a consensus that human activity
has changed the climate and we are currently heading into a climate crisis.
While public discussion and research efforts on climate change mitigation have
increased, potential solutions need to not only be discussed but also
effectively deployed. For preventing mismanagement and holding policy makers
accountable, transparency and degree of information about government processes
have been shown to be crucial. However, currently the quantity of information
about climate change discussions and the range of sources make it increasingly
difficult for the public and civil society to maintain an overview to hold
politicians accountable.
  In response, we propose a multi-source topic aggregation system (MuSTAS)
which processes policy makers speech and rhetoric from several publicly
available sources into an easily digestible topic summary. MuSTAS uses novel
multi-source hybrid latent Dirichlet allocation to model topics from a variety
of documents. This topic digest will serve the general public and civil society
in assessing where, how, and when politicians talk about climate and climate
policies, enabling them to hold politicians accountable for their actions to
mitigate climate change and lack thereof.",http://arxiv.org/abs/2010.08346v3
Dynamical Landscape and Multistability of a Climate Model,2020-10-20T15:31:38Z,"Georgios Margazoglou, Tobias Grafke, Alessandro Laio, Valerio Lucarini","We apply two independent data analysis methodologies to locate stable climate
states in an intermediate complexity climate model and analyze their interplay.
First, drawing from the theory of quasipotentials, and viewing the state space
as an energy landscape with valleys and mountain ridges, we infer the relative
likelihood of the identified multistable climate states, and investigate the
most likely transition trajectories as well as the expected transition times
between them. Second, harnessing techniques from data science, specifically
manifold learning, we characterize the data landscape of the simulation output
to find climate states and basin boundaries within a fully agnostic and
unsupervised framework. Both approaches show remarkable agreement, and reveal,
apart from the well known warm and snowball earth states, a third intermediate
stable state in one of the two climate models we consider. The combination of
our approaches allows to identify how the negative feedback of ocean heat
transport and entropy production via the hydrological cycle drastically change
the topography of the dynamical landscape of Earth's climate.",http://arxiv.org/abs/2010.10374v2
"FireSRnet: Geoscience-Driven Super-Resolution of Future Fire Risk from
  Climate Change",2020-11-24T20:19:51Z,"Tristan Ballard, Gopal Erinjippurath","With fires becoming increasingly frequent and severe across the globe in
recent years, understanding climate change's role in fire behavior is critical
for quantifying current and future fire risk. However, global climate models
typically simulate fire behavior at spatial scales too coarse for local risk
assessments. Therefore, we propose a novel approach towards super-resolution
(SR) enhancement of fire risk exposure maps that incorporates not only 2000 to
2020 monthly satellite observations of active fires but also local information
on land cover and temperature. Inspired by SR architectures, we propose an
efficient deep learning model trained for SR on fire risk exposure maps. We
evaluate this model on resolution enhancement and find it outperforms standard
image interpolation techniques at both 4x and 8x enhancement while having
comparable performance at 2x enhancement. We then demonstrate the
generalizability of this SR model over northern California and New South Wales,
Australia. We conclude with a discussion and application of our proposed model
to climate model simulations of fire risk in 2040 and 2100, illustrating the
potential for SR enhancement of fire risk maps from the latest state-of-the-art
climate models.",http://arxiv.org/abs/2011.12353v1
"Constraining Global Changes in Temperature and Precipitation From
  Observable Changes in Surface Radiative Heating",2020-04-29T12:46:47Z,Chirag Dhara,"Changes in the atmospheric composition alter the magnitude and partitioning
between the downward propagating solar and atmospheric longwave radiative
fluxes heating the Earth's surface. These changes are computed by radiative
transfer codes in Global Climate Models, and measured with high precision at
surface observation networks. Changes in radiative heating signify changes in
the global surface temperature and hydrologic cycle. Here, we develop a
conceptual framework using an Energy Balance Model to show that first order
changes in the hydrologic cycle are mainly associated with changes in solar
radiation, while that in surface temperature are mainly associated with changes
in atmospheric longwave radiation. These insights are used to explain a range
of phenomena including observed historical trends, biases in climate model
output, and the inter-model spread in climate change projections. These results
may help identify biases in future generations of climate models.",http://arxiv.org/abs/2004.14144v1
"Fingerprinting Heatwaves and Cold Spells and Assessing Their Response to
  Climate Change using Large Deviation Theory",2020-10-16T09:45:08Z,"Vera Melinda Galfi, Valerio Lucarini","Extreme events provide relevant insights into the dynamics of climate and
their understanding is key for mitigating the impact of climate variability and
climate change. By applying large deviation theory to a state-of-the-art Earth
system model, we define the climatology of persistent heatwaves and cold spells
in key target geographical regions by estimating the rate functions for the
surface temperature, and we assess the impact of increasing CO$_2$
concentration on such persistent anomalies. Hence, we can better quantify the
increasing hazard {\color{black}due} to heatwaves in a warmer climate. We show
that two 2010 high impact events - summer Russian heatwave and winter Dzud in
Mongolia - are associated with atmospheric patterns that are exceptional
compared to the typical ones, but typical compared to the climatology of
extremes. Their dynamics is encoded in the natural variability of the climate.
Finally, we propose and test an approximate formula for the return times of
large and persistent temperature fluctuations from easily accessible
statistical properties.",http://arxiv.org/abs/2010.08272v2
Deep Learning for Climate Model Output Statistics,2020-12-09T11:38:37Z,"Michael Steininger, Daniel Abel, Katrin Ziegler, Anna Krause, Heiko Paeth, Andreas Hotho","Climate models are an important tool for the assessment of prospective
climate change effects but they suffer from systematic and representation
errors, especially for precipitation. Model output statistics (MOS) reduce
these errors by fitting the model output to observational data with machine
learning. In this work, we explore the feasibility and potential of deep
learning with convolutional neural networks (CNNs) for MOS. We propose the CNN
architecture ConvMOS specifically designed for reducing errors in climate model
outputs and apply it to the climate model REMO. Our results show a considerable
reduction of errors and mostly improved performance compared to three commonly
used MOS approaches.",http://arxiv.org/abs/2012.10394v1
"Multi-Agent Reinforcement Learning and Human Social Factors in Climate
  Change Mitigation",2020-02-12T18:46:48Z,"Kyle Tilbury, Jesse Hoey","Many complex real-world problems, such as climate change mitigation, are
intertwined with human social factors. Climate change mitigation, a social
dilemma made difficult by the inherent complexities of human behavior, has an
impact at a global scale. We propose applying multi-agent reinforcement
learning (MARL) in this setting to develop intelligent agents that can
influence the social factors at play in climate change mitigation. There are
ethical, practical, and technical challenges that must be addressed when
deploying MARL in this way. In this paper, we present these challenges and
outline an approach to address them. Understanding how intelligent agents can
be used to impact human social factors is important to prevent their abuse and
can be beneficial in furthering our knowledge of these complex problems as a
whole. The challenges we present are not limited to our specific application
but are applicable to broader MARL. Thus, developing MARL for social factors in
climate change mitigation helps address general problems hindering MARL's
applicability to other real-world problems while also motivating discussion on
the social implications of MARL deployment.",http://arxiv.org/abs/2002.05147v1
"REME -- Renewable Energy and Materials Economy -- The Path to Energy
  Security, Prosperity and Climate Stability",2020-12-29T23:34:44Z,Peter Eisenberger,"A Renewable Energy and Materials Economy (REME) is proposed as the solution
to the climate change threat. REME mimics nature to produce carbon neutral
liquid fuels and chemicals as well as carbon negative materials by using water,
CO$_2$ from the atmosphere and renewable energy as inputs. By being in harmony
with nature REME has a positive feedback between economic development and
climate change protection. In REME the feedback driven accelerated rate of
economic growth enables the climate change threat to be addressed in a timely
manner. It is also cost-effective protection because it sequesters by
monetizing the carbon removed from the air in carbon-based building materials.
Thus, addressing the climate change threat is not a cost to the economy but a
result of REME driven prosperity.",http://arxiv.org/abs/2012.14976v1
"Zonally opposing shifts of the intertropical convergence zone in
  response to climate change",2020-07-01T04:48:26Z,"Antonios Mamalakis, James T. Randerson, Jin-Yi Yu, Michael S. Pritchard, Gudrun Magnusdottir, Padhraic Smyth, Paul A. Levine, Sungduk Yu, Efi Foufoula-Georgiou","Future changes in the location of the intertropical convergence zone (ITCZ)
due to climate change are of high interest since they could substantially alter
precipitation patterns in the tropics and subtropics. Although models predict a
future narrowing of the ITCZ during the 21st century in response to climate
warming, uncertainties remain large regarding its future position, with most
past work focusing on the zonal-mean ITCZ shifts. Here we use projections from
27 state-of-the-art climate models (CMIP6) to investigate future changes in
ITCZ location as a function of longitude and season, in response to climate
warming. We document a robust zonally opposing response of the ITCZ, with a
northward shift over eastern Africa and the Indian Ocean, and a southward shift
in the eastern Pacific and Atlantic Ocean by 2100, for the SSP3-7.0 scenario.
Using a two-dimensional energetics framework, we find that the revealed ITCZ
response is consistent with future changes in the divergent atmospheric energy
transport over the tropics, and sector-mean shifts of the energy flux equator
(EFE). The changes in the EFE appear to be the result of zonally opposing
imbalances in the hemispheric atmospheric heating over the two sectors,
consisting of increases in atmospheric heating over Eurasia and cooling over
the Southern Ocean, which contrast with atmospheric cooling over the North
Atlantic Ocean due to a model-projected weakening of the Atlantic meridional
overturning circulation.",http://arxiv.org/abs/2007.00239v1
"Multivariate Estimations of Equilibrium Climate Sensitivity from Short
  Transient Warming Simulations",2020-10-02T08:12:17Z,"Robbin Bastiaansen, Henk A. Dijkstra, Anna S. von der Heydt","One of the most used metrics to gauge the effects of climate change is the
equilibrium climate sensitivity, defined as the long-term (equilibrium)
temperature increase resulting from instantaneous doubling of atmospheric
CO$_2$. Since global climate models cannot be fully equilibrated in practice,
extrapolation techniques are used to estimate the equilibrium state from
transient warming simulations. Because of the abundance of climate feedbacks -
spanning a wide range of temporal scales - it is hard to extract long-term
behaviour from short-time series; predominantly used techniques are only
capable of detecting the single most dominant eigenmode, thus hampering their
ability to give accurate long-term estimates. Here, we present an extension to
those methods by incorporating data from multiple observables in a
multi-component linear regression model. This way, not only the dominant but
also the next-dominant eigenmodes of the climate system are captured, leading
to better long-term estimates from short, non-equilibrated time series.",http://arxiv.org/abs/2010.00845v1
Analyzing Sustainability Reports Using Natural Language Processing,2020-11-03T21:22:42Z,"Alexandra Luccioni, Emily Baylor, Nicolas Duchene","Climate change is a far-reaching, global phenomenon that will impact many
aspects of our society, including the global stock market
\cite{dietz2016climate}. In recent years, companies have increasingly been
aiming to both mitigate their environmental impact and adapt to the changing
climate context. This is reported via increasingly exhaustive reports, which
cover many types of climate risks and exposures under the umbrella of
Environmental, Social, and Governance (ESG). However, given this abundance of
data, sustainability analysts are obliged to comb through hundreds of pages of
reports in order to find relevant information. We leveraged recent progress in
Natural Language Processing (NLP) to create a custom model, ClimateQA, which
allows the analysis of financial reports in order to identify climate-relevant
sections based on a question answering approach. We present this tool and the
methodology that we used to develop it in the present article.",http://arxiv.org/abs/2011.08073v2
"On the abrupt change of the maximum likelihood state in a simplified
  stochastic thermohaline circulation system",2020-12-05T07:09:26Z,"Fang Yang, Xu Sun, Jinqiao Duan","The maximum likelihood state for a simplified stochastic thermohaline
circulation model is investigated. It is shown that a jump occurs for the
maximum likelihood state during transitions between two metastable states. The
jump helps to explain the mechanism of the abrupt change in the climate systems
as revealed by various climate records.",http://arxiv.org/abs/2012.02963v1
The impact of climate change on astronomical observations,2020-09-24T16:07:46Z,"Faustine Cantalloube, Julien Milli, Christoph B√∂hm, Susanne Crewell, Julio Navarrete, Kira Rehfeld, Marc Sarazin, Anna Sommani","Climate change is affecting and will increasingly affect astronomical
observations. In this paper, we investigated the role some key weather
parameters play in the quality of astronomical observations, and analysed their
long-term trends (longer than 30 years) in order to grasp the impact of climate
change on current and future observations. In this preliminary study, we
specifically analysed four parameters, the temperature, the surface layer
turbulence, the wind speed at the jetstream layer and the humidity. The
analyses is conducted with data from the Very Large Telescope (VLT), operated
by the European Southern Observatory (ESO), located at Cerro Paranal in the
Atacama desert, Chile, which is one of the driest places on Earth. To complete
the data from the various sensors installed at Paranal, we used the fifth
generation and 20th century European Centre Medium-Range Weather Forecasts
(ECMWF) atmospheric reanalysis of the global climate, ERA5 (from 1980 to now)
and ERA20C (from 1900 to 2010), which we interpolated at the Paranal
observatory location. In addition, we also explored climate projections in this
region, using the Coupled Model Intercomparison Project Phase 6 (CMIP6)
multi-model ensemble, under the worst-case climate change Shared Socio-Economic
Pathways (SSP5-8.5) scenario. Further investigation is needed to better
understand the underlying mechanisms of change, as well as to assess the
severity of the impact.",http://arxiv.org/abs/2009.11779v1
"Assessing biophysical and socio-economic impacts of climate change on
  avian biodiversity",2020-02-07T11:35:16Z,"Simon Kapitza, Pham Van Ha, Tom Kompas, Nick Golding, Natasha C. R. Cadenhead, Payal Bal, Brendan A. Wintle","Climate change threatens biodiversity directly by influencing biophysical
variables that drive species' geographic distributions and indirectly through
socio-economic changes that influence land use patterns, driven by global
consumption, production and climate. To date, no detailed analyses have been
produced that assess the relative importance of, or interaction between, these
direct and indirect climate change impacts on biodiversity at large scales.
Here, we apply a new integrated modelling framework to quantify the relative
influence of biophysical and socio-economically mediated impacts on avian
species in Vietnam and Australia. We find that socio-economically mediated
impacts on suitable ranges are largely outweighed by biophysical impacts, but
global shifts of production are likely to result in adverse impacts on habitats
worldwide. By translating economic futures and shocks into spatially explicit
predictions of biodiversity change, we now have the power to analyse in a
consistent way outcomes for nature and people of any change to policy,
regulation, trading conditions or consumption trend at any scale from
sub-national to global.",http://arxiv.org/abs/2002.02721v1
"Bias-corrected climate projections from Coupled Model Intercomparison
  Project-6 (CMIP6) for South Asia",2020-06-05T04:47:21Z,"Vimal Mishra, Udit Bhatia, Amar Deep Tiwari","Climate change is likely to pose enormous challenges for agriculture, water
resources, infrastructure, and livelihood of millions of people living in South
Asia. Here, we develop daily bias-corrected data of precipitation, maximum and
minimum temperatures at 0.25{\deg} spatial resolution for South Asia (India,
Pakistan, Bangladesh, Nepal, Bhutan, and Sri Lanka) and 18 river basins located
in the Indian sub-continent. The bias-corrected dataset is developed using
Empirical Quantile Mapping (EQM) for the historic (1951-2014) and projected
(2015-2100) climate for the four scenarios (SSP126, SSP245, SSP370, SSP585)
using output from 13 CMIP6-GCMs. The bias-corrected dataset was evaluated
against the observations for both mean and extremes of precipitation, maximum
and minimum temperatures. Bias corrected projections from 13 CMIP6-GCMs project
a warmer (3-5{\deg}C) and wetter (13-30%) climate in South Asia in the 21st
century. The bias-corrected projections from CMIP6-GCMs can be used for climate
change impact assessment in South Asia and hydrologic impact assessment in the
sub-continental river basins.",http://arxiv.org/abs/2006.12976v1
Entropy production rates of the climate,2020-08-05T14:02:54Z,"Goodwin Gibbins, Joanna D. Haigh","There is ongoing interest in the global entropy production rate as a climate
diagnostic and predictor, but progress has been limited by ambiguities in its
definition; different conceptual boundaries of the climate system give rise to
different internal production rates. Three viable options are described,
estimated and investigated here, two of which -- the material and the total
radiative (here 'planetary') entropy production rates -- are well-established
and a third which has only recently been considered but appears very promising.
This new option is labelled the 'transfer' entropy production rate and includes
all irreversible processes that transfer heat within the climate, radiative and
material, but not those involved in the exchange of radiation with space.
Estimates in three model climates put the material rate in the range $27$-$48$
mW/m$^2$K, the transfer rate $67$-$76$ mW/m$^2$K, and the planetary rate
$1279$-$1312$ mW/m$^2$K.
  The climate-relevance of each rate is probed by calculating their responses
to climate changes in a simple radiative-convective model. An increased
greenhouse effect causes a significant increase in the material and transfer
entropy production rates but has no direct impact on the planetary rate. When
the same surface temperature increase is forced by changing the albedo instead,
the material and transfer entropy production rates increase less dramatically
and the planetary rate also registers an increase. This is pertinent to solar
radiation management as it demonstrates the difficulty of reversing greenhouse
gas-mediated climate changes by albedo alterations. It is argued that the
transfer perspective has particular significance in the climate system and
warrants increased prominence.",http://arxiv.org/abs/2008.02141v1
"Understanding North Atlantic Climate Instabilities and Complex
  Interactions using Data Science",2020-01-28T05:06:49Z,"Alka Yadav, Sourish Das, Anirban Chakraborti, Sudeep Shukla","The North Atlantic Oscillation (NAO) index, a measure of sea-level
atmospheric pressure variability, holds significant influence over weather
patterns in North America and Northern Europe. A negative (positive) NAO value
signifies increased cold air outbreaks and storm occurrences (reduced
occurrences) in these regions. NAO, a product of multiple climate factors,
demonstrates intricate dynamics with sea surface temperature (SST) and sea ice
extent (SIE). In this study, we adopt a data-driven approach to explore the
complex interplay between NAO, SST, and SIE, revealing a critical instability
rooted in positive feedback loops among these climate variables. Our
statistical machine learning methodology examines the impacts of melting Arctic
SIE and rising SST on NAO, thereby understanding the weather patterns across
the North Atlantic region. The skewness analysis yields a negative skewness in
NAO across various time intervals -- daily, weekly, and monthly. This skewness,
coupled with NAO's mean zero stationary nature, accentuates system instability.
To capture these dynamics, we formulate a Bayesian Granger-causal dynamic
linear model, which effectively updates the predictor-dependent variable
relationship over time. The findings underscore an impending critical
instability, indicative of more frequent occurrences of intensely cold climates
in eastern North America and northern Europe, theory signifies a notable
climate shift. By delving into the intricate feedback mechanisms of NAO, SST,
and SIE, our study enhances our comprehension of climate variability, fostering
a more informed perspective on the imminent climate changes that lie ahead.",http://arxiv.org/abs/2001.10171v4
Atmospheric dynamics on terrestrial planets with eccentric orbits,2020-04-30T10:23:18Z,"Ilai Guendelman, Yohai Kaspi","The insolation a planet receives from its parent star is the main driver of
the climate and depends on the planet's orbital configuration. Planets with
non-zero obliquity and eccentricity experience seasonal insolation variations.
As a result, the climate exhibits a seasonal cycle, with its strength depending
on the orbital configuration and atmospheric characteristics. In this study,
using an idealized general circulation model, we examine the climate response
to changes in eccentricity for both zero and non-zero obliquity planets. In the
zero obliquity case, a comparison between the seasonal response to changes in
eccentricity and perpetual changes in the solar constant shows that the
seasonal response strongly depends on the orbital period and radiative
timescale. More specifically, using a simple energy balance model, we show the
importance of the latitudinal structure of the radiative timescale in the
climate response. We also show that the response strongly depends on the
atmospheric moisture content. The combination of an eccentric orbit with
non-zero obliquity is complex, as the insolation also depends on the perihelion
position. Although the detailed response of the climate to variations in
eccentricity, obliquity, and perihelion is involved, the circulation is
constrained mainly by the thermal Rossby number and the maximum temperature
latitude. Finally, we discuss the importance of different planetary parameters
that affect the climate response to orbital configuration variations.",http://arxiv.org/abs/2004.14673v2
"Hurricane-blackout-heatwave Compound Hazard Risk and Resilience in a
  Changing Climate",2020-12-08T14:40:06Z,"Kairui Feng, Ouyang Min, Ning Lin","Hurricanes have caused power outages and blackouts, affecting millions of
customers and inducing severe social and economic impacts. The impacts of
hurricane-caused blackouts may worsen due to increased heat extremes and
possibly increased hurricanes under climate change. We apply hurricane and
heatwave projections with power outage and recovery process analysis to
investigate how the emerging hurricane-blackout-heatwave compound hazard may
vary in a changing climate, for Harris County in Texas (including major part of
Houston City) as an example. We find that, under the high-emissions scenario
RCP8.5, the expected percent of customers experiencing at least one
longer-than-5-day hurricane-induced power outage in a 20-year period would
increase significantly from 14% at the end of the 20th century to 44% at the
end of the 21st century in Harris County. The expected percent of customers who
may experience at least one longer-than-5-day heatwave without power (to
provide air conditioning) would increase alarmingly, from 0.8% to 15.5%. These
increases of risk may be largely avoided if the climate is well controlled
under the stringent mitigation scenario RCP2.6. We also reveal that a moderate
enhancement of critical sectors of the distribution network can significantly
improve the resilience of the entire power grid and mitigate the risk of the
future compound hazard. Together these findings suggest that, in addition to
climate mitigation, climate adaptation actions are urgently needed to improve
the resilience of coastal power systems.",http://arxiv.org/abs/2012.04452v1
Reviewing climate change and agricultural market competitiveness,2020-08-31T16:45:03Z,"Bakhtmina Zia, Dr Muhammad Rafiq PhD Research Scholar, Institute of Management Sciences, Peshawar, Pakistan, Associate Professor, Institute of Management Sciences, Peshawar, Pakistan","The paper is a collection of knowledge regarding the phenomenon of climate
change, competitiveness, and literature linking the two phenomena to
agricultural market competitiveness. The objective is to investigate the peer
reviewed and grey literature on the subject to explore the link between climate
change and agricultural market competitiveness and also explore an appropriate
technique to validate the presumed relationship empirically. The paper
concludes by identifying implications for developing an agricultural
competitiveness index while incorporating the climate change impacts, to
enhance the potential of agricultural markets for optimizing the agricultural
sectors competitiveness.",http://arxiv.org/abs/2008.13726v1
"Indicator patterns of forced change learned by an artificial neural
  network",2020-05-25T18:16:36Z,"Elizabeth A. Barnes, Benjamin Toms, James W. Hurrell, Imme Ebert-Uphoff, Chuck Anderson, David Anderson","Many problems in climate science require the identification of signals
obscured by both the ""noise"" of internal climate variability and differences
across models. Following previous work, we train an artificial neural network
(ANN) to identify the year of input maps of temperature and precipitation from
forced climate model simulations. This prediction task requires the ANN to
learn forced patterns of change amidst a background of climate noise and model
differences. We then apply a neural network visualization technique (layerwise
relevance propagation) to visualize the spatial patterns that lead the ANN to
successfully predict the year. These spatial patterns thus serve as ""reliable
indicators"" of the forced change. The architecture of the ANN is chosen such
that these indicators vary in time, thus capturing the evolving nature of
regional signals of change. Results are compared to those of more standard
approaches like signal-to-noise ratios and multi-linear regression in order to
gain intuition about the reliable indicators identified by the ANN. We then
apply an additional visualization tool (backward optimization) to highlight
where disagreements in simulated and observed patterns of change are most
important for the prediction of the year. This work demonstrates that ANNs and
their visualization tools make a powerful pair for extracting climate patterns
of forced change.",http://arxiv.org/abs/2005.12322v1
"Classification and understanding of cloud structures via satellite
  images with EfficientUNet",2020-09-27T19:50:05Z,"Tashin Ahmed, Noor Hossain Nuri Sabab","Climate change has been a common interest and the forefront of crucial
political discussion and decision-making for many years. Shallow clouds play a
significant role in understanding the Earth's climate, but they are challenging
to interpret and represent in a climate model. By classifying these cloud
structures, there is a better possibility of understanding the physical
structures of the clouds, which would improve the climate model generation,
resulting in a better prediction of climate change or forecasting weather
update. Clouds organise in many forms, which makes it challenging to build
traditional rule-based algorithms to separate cloud features. In this paper,
classification of cloud organization patterns was performed using a new
scaled-up version of Convolutional Neural Network (CNN) named as EfficientNet
as the encoder and UNet as decoder where they worked as feature extractor and
reconstructor of fine grained feature map and was used as a classifier, which
will help experts to understand how clouds will shape the future climate. By
using a segmentation model in a classification task, it was shown that with a
good encoder alongside UNet, it is possible to obtain good performance from
this dataset. Dice coefficient has been used for the final evaluation metric,
which gave the score of 66.26\% and 66.02\% for public and private (test set)
leaderboard on Kaggle competition respectively.",http://arxiv.org/abs/2009.12931v4
"A model for Dansgaard-Oeschger events and millennial-scale abrupt
  climate change without external forcing",2020-05-31T01:40:10Z,Georg A. Gottwald,"We propose a conceptual model which generates abrupt climate changes akin to
Dansgaard-Oeschger events. In the model these abrupt climate changes are not
triggered by external perturbations but rather emerge in a dynamic
self-consistent model through complex interactions of the ocean, the atmosphere
and an intermittent process. The abrupt climate changes are caused in our model
by intermittencies in the sea-ice cover. The ocean is represented by a Stommel
two-box model, the atmosphere by a Lorenz-84 model and the sea-ice cover by a
deterministic approximation of correlated additive and multiplicative noise
(CAM) process. The key dynamical ingredients of the model are given by
stochastic limits of deterministic multi-scale systems and recent results in
deterministic homogenisation theory. The deterministic model reproduces
statistical features of actual ice-core data such as non-Gaussian
$\alpha$-stable behaviour. The proposed mechanism for abrupt millenial-scale
climate change only relies on the existence of a quantity, which exhibits
intermittent dynamics on an intermediate time scale. We consider as a
particular mechanism intermittent sea-ice cover where the intermittency is
generated by emergent atmospheric noise. However, other mechanisms such as
freshwater influxes may also be formulated within the proposed framework.",http://arxiv.org/abs/2006.00405v2
"Influence Of Climate Change On The Corn Yield In Ontario And Its Impact
  On Corn Farms Income At The 2068 Horizon",2020-03-03T00:52:37Z,"Antoine Kornprobst, Matt Davison","Our study aims at quantifying the impact of climate change on corn farming in
Ontario under several warming scenarios at the 2068 horizon. It is articulated
around a discrete-time dynamic model of corn farm income with an annual
time-step, corresponding to one agricultural cycle from planting to harvest. At
each period, we compute the income given the corn yield, which is highly
dependent on weather variables. We also provide a reproducible forecast of the
yearly distribution of corn yield for 10 cities in Ontario. The price of corn
futures at harvest time is taken into account and we fit our model by using 49
years of historical data. We then conduct out-of-sample Monte-Carlo simulations
to obtain the farm income forecasts under a given climate change scenario.",http://arxiv.org/abs/2003.01270v2
"Impact of climate change on surface stirring and transport in the
  Mediterranean Sea",2020-11-05T16:07:11Z,"Enrico Ser-Giacomi, Gabriel Jorda Sanchez, Javier Soto-Navarro, Soren Thomsen, Juliette Mignot, Florence Sevault, Vincent Rossi","Understanding how climate change will affect oceanic fluid transport is
crucial for environmental applications and human activities. However, a
synoptic characterization of the influence of climate change on mesoscale
stirring and transport in the surface ocean is missing. To bridge this gap, we
exploit a high-resolution, fully-coupled climate model of the Mediterranean
basin using a Network Theory approach. We project significant increases of
horizontal stirring and kinetic energies in the next century, likely due to
increments of available potential energy. The future evolution of basin-scale
transport patterns hint at a rearrangement of the main hydrodynamic provinces,
defined as regions of the surface ocean that are well-mixed internally but with
minimal cross-flow across their boundaries. This results in increased
heterogeneity of province sizes and stronger mixing in their interiors. Our
approach can be readily applied to other oceanic regions, providing information
for the present and future marine spatial planning.",http://arxiv.org/abs/2011.02941v1
Ozone depletion-induced climate change following a 50 pc supernova,2020-06-26T16:17:13Z,"Brian C. Thomas, Cody L. Ratterman","Ozone in Earth's atmosphere is known to have a radiative forcing effect on
climate. Motivated by geochemical evidence for one or more nearby supernovae
about 2.6 million years ago, we have investigated the question of whether a
supernova at about 50 pc could cause a change in Earth's climate through its
impact on atmospheric ozone concentrations. We used the ""Planet Simulator""
(PlaSim) intermediate-complexity climate model with prescribed ozone profiles
taken from existing atmospheric chemistry modeling. We found that the effect on
globally averaged surface temperature is small, but localized changes are
larger and differences in atmospheric circulation and precipitation patterns
could have regional impacts.",http://arxiv.org/abs/2006.15079v2
"High-resolution Probabilistic Precipitation Prediction for use in
  Climate Simulations",2020-12-17T18:42:48Z,"Sherman Lo, Peter Watson, Peter Dueben, Ritabrata Dutta","The accurate prediction of precipitation is important to allow for reliable
warnings of flood or drought risk in a changing climate. However, to make
trust-worthy predictions of precipitation, at a local scale, is one of the most
difficult challenges for today's weather and climate models. This is because
important features, such as individual clouds and high-resolution topography,
cannot be resolved explicitly within simulations due to the significant
computational cost of high-resolution simulations. Climate models are typically
run at $\sim$50-100 km resolution which is insufficient to represent local
precipitation events in satisfying detail. Here, we develop a method to make
probabilistic precipitation predictions based on features that climate models
can resolve well and that is not highly sensitive to the approximations used in
individual models. To predict, we will use a temporal compound Poisson
distribution dependent on the output of climate models at a location. We use
the output of Earth System models at coarse resolution $\sim$50 km as input and
train the statistical models towards precipitation observations over Wales at
$\sim$10 km resolution. A Bayesian inferential scheme is provided so that the
compound-Poisson model can be inferred using a
Gibbs-within-Metropolis-Elliptic-Slice sampling scheme which enables us to
quantify the uncertainty of our predictions. In addition, we use a Gaussian
process regressor on the posterior samples of the model parameters, to infer a
spatially coherent model and hence to produce spatially coherent rainfall
prediction. We illustrate the prediction performance of our model by training
over 5 years of the data up to 31st December 1999 and predicting precipitation
for 20 years afterwards for Cardiff and Wales.",http://arxiv.org/abs/2012.09821v2
Analysis of Greenhouse Gases,2020-03-21T17:50:30Z,Shalin Shah,"Climate change is a result of a complex system of interactions of greenhouse
gases (GHG), the ocean, land, ice, and clouds. Large climate change models use
several computers and solve several equations to predict the future climate.
The equations may include simple polynomials to partial differential equations.
Because of the uptake mechanism of the land and ocean, greenhouse gas emissions
can take a while to affect the climate. The IPCC has published reports on how
greenhouse gas emissions may affect the average temperature of the troposphere
and the predictions show that by the end of the century, we can expect a
temperature increase from 0.8 C to 5 C. In this article, I use Linear
Regression (LM), Quadratic Regression and Gaussian Process Regression (GPR) on
monthly GHG data going back several years and try to predict the temperature
anomalies based on extrapolation. The results are quite similar to the IPCC
reports.",http://arxiv.org/abs/2003.11916v2
"Shadow economy and populism-risk and uncertainty factors for
  establishing low-carbon economy of Balkan countries-case study for Bulgaria",2020-11-13T08:38:08Z,"Shteryo Nozharov, Nina Nikolova","The main purpose of the current publication is to formulate a scenario model
for analysis of the opportunities for low-carbon economy establishment in the
countries with transition economies.The model studies risk factors such as
shadow economy level and populism based on the implementation and development
of Balkan countries economic policy and at the same time shows future climate
changes tendencies and uncertainties of climate models.A transdisciplinary
approach is implemented in the study. Climate change perception and
understanding about low-carbon economy are examined through the public opinion
and analysis of mass-media publications.The results of the research are
important in order to clarify the multicultural divergences as a factor for
risk and uncertainty in the implementation process of the policy for climate
change.In this way geographical aspects of risk and uncertainty, which are not
only related to the economic development of the relevant countries, could be
brought out.",http://arxiv.org/abs/2011.06592v1
DeepClimGAN: A High-Resolution Climate Data Generator,2020-11-23T20:13:37Z,"Alexandra Puchko, Robert Link, Brian Hutchinson, Ben Kravitz, Abigail Snyder","Earth system models (ESMs), which simulate the physics and chemistry of the
global atmosphere, land, and ocean, are often used to generate future
projections of climate change scenarios. These models are far too
computationally intensive to run repeatedly, but limited sets of runs are
insufficient for some important applications, like adequately sampling
distribution tails to characterize extreme events. As a compromise, emulators
are substantially less expensive but may not have all of the complexity of an
ESM. Here we demonstrate the use of a conditional generative adversarial
network (GAN) to act as an ESM emulator. In doing so, we gain the ability to
produce daily weather data that is consistent with what ESM might output over
any chosen scenario. In particular, the GAN is aimed at representing a joint
probability distribution over space, time, and climate variables, enabling the
study of correlated extreme events, such as floods, droughts, or heatwaves.",http://arxiv.org/abs/2011.11705v1
"Application of ERA5 and MENA simulations to predict offshore wind energy
  potential",2020-02-24T00:25:29Z,"Shahab Shamshirband, Amir Mosavi, Narjes Nabipour, Kwok-wing Chau","This study explores wind energy resources in different locations through the
Gulf of Oman and also their future variability due climate change impacts. In
this regard, EC-EARTH near surface wind outputs obtained from CORDEX-MENA
simulations are used for historical and future projection of the energy. The
ERA5 wind data are employed to assess suitability of the climate model.
Moreover, the ERA5 wave data over the study area are applied to compute sea
surface roughness as an important variable for converting near surface wind
speeds to those of wind speed at turbine hub-height. Considering the power
distribution, bathymetry and distance from the coats, some spots as tentative
energy hotspots to provide detailed assessment of directional and temporal
variability and also to investigate climate change impact studies. RCP8.5 as a
common climatic scenario is used to project and extract future variation of the
energy in the selected sites. The results of this study demonstrate that the
selected locations have a suitable potential for wind power turbine plan and
constructions.",http://arxiv.org/abs/2002.10022v1
"Climate-driven trends in the streamflow records of a reference
  hydrologic network in Southern Spain",2020-08-18T07:06:31Z,"Patricio Yeste, Javier Dorador, Wenceslao Mart√≠n-Rosales, Emilio Molero, Mar√≠a Jes√∫s Esteban Parra","Monthly streamflow records from a set of gauging stations, selected to form a
reference hydrologic network, are analyzed together with precipitation and
temperature data to establish whether the streamflows in the Guadalquivir River
Basin have experienced changes during the last half of the XXth century that
can be attributed to hydrological forcing. The observed streamflows in the
reference network have undergone generalized and significant decreases both at
seasonal and annual scales during the study period. Annual rainfall, though,
did not experienced statistically significant changes. The observed trends in
streamflows can be attributed to either land-use changes, or to the
statistically significant changes exhibited both by yearly potential
evapotranspiration values and by the seasonal distribution of precipitation. In
the attribution work conducted using both data-based and simulation-based
methods, the intra-annual redistribution of precipitation is shown to be the
main statistically significant climate-driver of streamflow change. The
contributions of other non-climate factors, such as the change in land cover,
to the reduction in annual streamflows are shown to be minor in comparison.",http://arxiv.org/abs/2008.07771v1
"Augmented Convolutional LSTMs for Generation of High-Resolution Climate
  Change Projections",2020-09-23T17:52:09Z,"Nidhin Harilal, Udit Bhatia, Mayank Singh","Projection of changes in extreme indices of climate variables such as
temperature and precipitation are critical to assess the potential impacts of
climate change on human-made and natural systems, including critical
infrastructures and ecosystems. While impact assessment and adaptation planning
rely on high-resolution projections (typically in the order of a few
kilometers), state-of-the-art Earth System Models (ESMs) are available at
spatial resolutions of few hundreds of kilometers. Current solutions to obtain
high-resolution projections of ESMs include downscaling approaches that
consider the information at a coarse-scale to make predictions at local scales.
Complex and non-linear interdependence among local climate variables (e.g.,
temperature and precipitation) and large-scale predictors (e.g., pressure
fields) motivate the use of neural network-based super-resolution
architectures. In this work, we present auxiliary variables informed
spatio-temporal neural architecture for statistical downscaling. The current
study performs daily downscaling of precipitation variable from an ESM output
at 1.15 degrees (~115 km) to 0.25 degrees (25 km) over the world's most
climatically diversified country, India. We showcase significant improvement
gain against three popular state-of-the-art baselines with a better ability to
predict extreme events. To facilitate reproducible research, we make available
all the codes, processed datasets, and trained models in the public domain.",http://arxiv.org/abs/2009.11279v1
Deep learning the atmospheric boundary layer height,2020-04-09T03:42:50Z,"David R. Vivas, Estiven S√°nchez, John H. Reina","A question of global concern regarding the sustainable future of humankind
stems from the effect due to aerosols on the global climate. The quantification
of atmospheric aerosols and their relationship to climatic impacts are key to
understanding the dynamics of climate forcing and to improve our knowledge
about climate change. Due to its response to precipitation, temperature,
topography and human activity, one of the most dynamical atmospheric regions is
the atmospheric boundary layer (ABL): ABL aerosols have a sizable impact on the
evolution of the radiative forcing of climate change, human health, food
security, and, ultimately, on the local and global economy. The identification
of ABL pattern behaviour requires constant monitoring and the application of
instrumental and computational methods for its detection and analysis. Here, we
show a new method for the retrieval of ABL top arising from light detection and
ranging (LiDAR) signals, by training a convolutional neural network in a
supervised manner; forcing it to learn how to retrieve such a dynamical
parameter on real, non-ideal conditions and in a fully automated, unsupervised
way. Our findings pave the way for a full integration of LiDAR elastic,
inelastic, and depolarisation signal processing, and provide a novel approach
for real-time quantitative sensing of aerosols.",http://arxiv.org/abs/2004.04353v1
"Quantifying and attributing time step sensitivities in present-day
  climate simulations conducted with EAMv1",2020-10-15T02:39:39Z,"Hui Wan, Shixuan Zhang, Philip J. Rasch, Vincent E. Larson, Xubin Zeng, Huiping Yan","This study assesses the relative importance of time integration error in
present-day climate simulations conducted with the atmosphere component of the
Energy Exascale Earth System Model version 1 (EAMv1) at 1-degree horizontal
resolution. We show that a factor-of-6 reduction of time step size in all major
parts of the model leads to significant changes in the long-term mean climate.
These changes imply that the reduction of temporal truncation errors leads to a
notable although unsurprising degradation of agreement between the simulated
and observed present-day climate; the model would require retuning to regain
optimal climate fidelity in the absence of those truncation errors. A
coarse-grained attribution of the time step sensitivities is carried out by
separately shortening time steps used in various components of EAM or by
revising the numerical coupling between some processes. The results provide
useful clues to help better understand the root causes of time step
sensitivities in EAM. The experimentation strategy used here can also provide a
pathway for other models to identify and reduce time integration errors.",http://arxiv.org/abs/2010.07479v3
"Invasive species, extreme fire risk, and toxin release under a changing
  climate",2020-08-03T17:22:18Z,"Kimberley Miner, Laura Meyerson, . Climate Change Institute, School of Earth, Climate Sciences, University of Maine, Orono, ME 04469 2. Department of Natural Resources Science, University of Rhode Island, Kingston, RI 02881","Mediterranean ecosystems such as those found in California, Central Chile,
Southern Europe, and Southwest Australia host numerous, diverse, fire-adapted
micro-ecosystems. These micro-ecosystems are as diverse as mountainous conifer
to desert-like chaparral communities. Over the last few centuries, human
intervention, invasive species, and climate warming have drastically affected
the composition and health of Mediterranean ecosystems on almost every
continent. Increased fuel load from fire suppression policies and the continued
range expansion of non-native insects and plants, some driven by long-term
drought, produced the deadliest wildfire season on record in 2018. As a
consequence of these fires, a large number of structures are destroyed,
releasing household chemicals into the environment as uncontrolled toxins. The
mobilization of these materials can lead to health risks and disruption in both
human and natural systems. This article identifies drivers that led to a
structural weakening of the mosaic of fire-adapted ecosystems in California,
and subsequently increased the risk of destructive and explosive wildfires
throughout the state. Under a new climate regime, managing the impacts on
systems moving out-of-phase with natural processes may protect lives and ensure
the stability of ecosystem services.",http://arxiv.org/abs/2008.01035v1
Impacts of Solar Intermittency on Future Photovoltaic Reliability,2020-01-30T08:41:39Z,"Jun Yin, Annalisa Molini, Amilcare Porporato","As photovoltaic power is expanding rapidly worldwide, it is imperative to
assess its promise under future climate scenarios. While a great deal of
research has been devoted to trends of mean solar radiation, less attention has
been paid to its intermittent character, a key challenge when compounded with
uncertainties related to climate variability. Using both satellite data and
climate model outputs, here we characterize solar radiation intermittency to
assess future photovoltaic reliability. We find that the relation between the
future power supply and long-term trends of mean solar radiation is highly
nonlinear, thus making power reliability more sensitive to the fluctuations of
mean solar radiation in regions where insolation is the highest. Our results
highlight how reliability analysis must account simultaneously for the mean and
intermittency of solar inputs when assessing the impacts of climate change on
photovoltaics.",http://arxiv.org/abs/2001.11211v1
"Learning drivers of climate-induced human migrations with Gaussian
  processes",2020-11-17T19:38:26Z,"Jose M. Tarraga, Maria Piles, Gustau Camps-Valls","In the current context of climate change, extreme heatwaves, droughts, and
floods are not only impacting the biosphere and atmosphere but the
anthroposphere too. Human populations are forcibly displaced, which are now
referred to as climate-induced migrants. In this work, we investigate which
climate and structural factors forced major human displacements in the presence
of floods and storms during the years 2017-2019. We built, curated, and
harmonized a database of meteorological and remote sensing indicators along
with structural factors of 27 developing countries worldwide. We show how we
can use Gaussian Processes to learn what variables can explain the impact of
floods and storms in the context of forced displacements and to develop models
that reproduce migration flows. Our results at regional, global, and
disaster-specific scales show the importance of structural factors in the
determination of the magnitude of displacements. The study may have both
societal, political, and economical implications.",http://arxiv.org/abs/2011.08901v1
"Including climate system feedbacks in calculations of the social cost of
  methane",2020-12-07T21:07:50Z,"Kristina R. Colbert, Frank C. Errickson, David Anthoff, Chris E. Forest","Integrated assessment models (IAMs) are valuable tools that consider the
interactions between socioeconomic systems and the climate system.
Decision-makers and policy analysts employ IAMs to calculate the marginalized
monetary cost of climate damages resulting from an incremental emission of a
greenhouse gas. Used within the context of regulating anthropogenic methane
emissions, this metric is called the social cost of methane (SC-CH$_4$).
Because several key IAMs used for social cost estimation contain a simplified
model structure that prevents the endogenous modeling of non-CO$_2$ greenhouse
gases, very few estimates of the SC-CH$_4$ exist. For this reason, IAMs should
be updated to better represent methane cycle dynamics that are consistent with
comprehensive Earth System Models. We include feedbacks of climate change on
the methane cycle to estimate the SC-CH$_4$. Our expected value for the
SC-CH$_4$ is \$1163/t-CH$_4$ under a constant 3.0% discount rate. This
represents a 44% increase relative to a mean estimate without feedbacks on the
methane cycle.",http://arxiv.org/abs/2012.04062v1
A Predictive Model for Geographic Distributions of Mangroves,2020-12-30T22:52:50Z,"Lynn Wahab, Ezzat Chebaro, Jad Ismail, Amir Nasrelddine, Ali El-Zein","Climate change is an impending disaster which is of pressing concern more and
more every year. Countless efforts have been made to study the long-term
effects of climate change on agriculture, land resources, and biodiversity.
Studies involving marine life, however, are less prevalent in the literature.
Our research studies the available data on the population of mangroves (groups
of shrubs or small trees living in saline coastal intertidal zones) and their
correlations to climate change variables, specifically, temperature, heat
content, various sea levels, and sea salinity. Mangroves are especially
relevant to oceanic ecosystems because of their protective nature towards other
marine life, as well as their high absorption rate of carbon dioxide, and their
ability to withstand varying levels of salinity of our coasts. The change in
global distribution was studied based on global distributions of the previous
year, as well as ocean heat content, salinity, temperature, halosteric sea
level, thermosteric sea level, and total steric sea level. The best performing
predictive model was a support vector regressor, which yielded a correlation
coefficient of 0.9998.",http://arxiv.org/abs/2101.00967v2
Pathways to Sustainable Planetary Science,2020-09-09T17:06:27Z,"Matija ƒÜuk, Anne K. Virkki, Tom√°≈° Kohout, Emmanuel Lellouch, Jack J. Lissauer","Climate change is a major impending threat to the future of humanity.
According to the International Panel on Climate Change (IPCC), our emissions
are estimated to have caused 0.8 deg C-1.2 deg C of anthropogenic global
warming (AGW) above pre-industrial levels. AGW is likely to reach 1.5 degrees C
between 2030 and 2052 if it continues to increase at the current rate. As the
climate change is driven by the release of carbon dioxide and other greenhouse
gases (GHG) into the atmosphere, there is a broad consensus that the mitigation
of climate change requires transition to low GHG emission energy sources,
technologies and practices. Implementing such changes systematically from
individual to community-wide scales together with the resulting cultural
changes and leadership towards environmental consciousness and responsibility
are crucial to mitigate the looming damage of AGW. Given planetary scientists'
wide recognition of the realities of climate change, and the need for us to
maintain credibility by leading by example, it is appropriate to make own
professional behavior more environmentally responsible. While scientists are
few in numbers, and planetary scientists far fewer, high volumes of academic
travel to conferences, panels, colloquia, and research collaboration visits
together with extensive use of large, energetically demanding infrastructures
make the ""carbon footprint"" of scientists much higher than that of an average
citizen. This White Paper focuses on how modifying our activities, particularly
associated with academic travel, can affect the carbon footprint of the
planetary science community, and it makes recommendations on how the community
and the funding agencies could best participate in the cultural change required
to mitigate the damage that AGW will cause.",http://arxiv.org/abs/2009.04419v1
"Climate and structure of the 8.2 ka event reconstructed from three
  speleothems from Germany",2020-08-07T09:14:07Z,"S. Waltgenbach, D. Scholz, C. Sp√∂tl, D. F. C. Riechelmann, K. Jochum, J. Fohlmeister, A. Schr√∂der-Ritzrau","The most pronounced climate anomaly of the Holocene was the 8.2 ka cooling
event. We present new 230Th/U-ages as well as high-resolution stable isotope
and trace element data from three stalagmitesfrom two different cave systems in
Germany, which provide important information about the structure and climate
variability of the 8.2 ka event in central Europe. In all three speleothems,
the 8.2 ka event is clearly recorded as a pronounced negative excursion of the
{\delta}18O values and can be divided into a 'whole event' and a 'central
event'. All stalagmites show a similar structure of the event with a short
negative excursion prior to the 'central event', which marks the beginning of
the 'whole event'. The timing and duration of the 8.2.ka event are different
for the individual records, which may, however, be related to dating
uncertainties. Whereas stalagmite Bu4 from Bunker Cave also shows a negative
anomaly in the {\delta}13C values and Mg content during the event, the two
speleothems from the Herbstlabyrinth cave system do not show distinct peaks in
the other proxies. This may suggest that the speleothem {\delta}18O values
recorded in the three stalagmites do not primarily reflect climate change at
the cave site, but rather large-scale changes in the North Atlantic. This is
supported by comparison with climate modelling data, which suggest that the
negative peak in the speleothem {\delta}18O values is mainly due to lower
{\delta}18O values of precipitation above the cave and that temperature only
played a minor role. Alternatively, the other proxies may not be as sensitive
as {\delta}18O values to record this centennial-scale cooling event. This may
particularly be the case for speleothem {\delta}13C values as suggested by
comparison with a climate modelling study simulating vegetation changes in
Europe during the 8.2 ka event. ...",http://arxiv.org/abs/2008.03051v1
Helping Reduce Environmental Impact of Aviation with Machine Learning,2020-12-17T08:04:22Z,Ashish Kapoor,"Commercial aviation is one of the biggest contributors towards climate
change. We propose to reduce environmental impact of aviation by considering
solutions that would reduce the flight time. Specifically, we first consider
improving winds aloft forecast so that flight planners could use better
information to find routes that are efficient. Secondly, we propose an aircraft
routing method that seeks to find the fastest route to the destination by
considering uncertainty in the wind forecasts and then optimally trading-off
between exploration and exploitation.",http://arxiv.org/abs/2012.09433v1
Regional Flood Risk Projections under Climate Change,2020-10-10T03:13:25Z,"Sanjib Sharma, Michael Gomez, Klaus Keller, Robert Nicholas, Alfonso Mejia","Flood-related risks to people and property are expected to increase in the
future due to environmental and demographic changes. It is important to
quantify and effectively communicate flood hazards and exposure to inform the
design and implementation of flood risk management strategies. Here we develop
an integrated modeling framework to assess projected changes in regional
riverine flood inundation risks. The framework samples climate model outputs to
force a hydrologic model and generate streamflow projections. Together with a
statistical and hydraulic model, we use the projected streamflow to map the
uncertainty of flood inundation projections for extreme flood events. We
implement the framework for rivers across the state of Pennsylvania, United
States. Our projections suggest that flood hazards and exposure across
Pennsylvania are overall increasing with future climate change. Specific
regions, including the main stem Susquehanna River, lower portion of the
Allegheny basin and central portion of Delaware River basin, demonstrate higher
flood inundation risks. In our analysis, the climate uncertainty dominates the
overall uncertainty surrounding the flood inundation projection chain. The
combined hydrologic and hydraulic uncertainties can account for as much as 37%
of the total uncertainty. We discuss how this framework can provide regional
and dynamic flood-risk assessments and help to inform the design of
risk-management strategies.",http://arxiv.org/abs/2010.04886v1
"The effect of varying atmospheric pressure upon habitability and
  biosignatures of Earth-like planets",2020-06-09T12:05:24Z,"Engin Keles, John Lee Grenfell, Mareike Godolt, Barbara Stracke, Heike Rauer","Understanding the possible climatic conditions on rocky extrasolar planets,
and thereby their potential habitability, is one of the major subjects of
exoplanet research. Determining how the climate, as well as potential
atmospheric biosignatures, change under different conditions is a key aspect
when studying Earth-like exoplanets. One important property is the atmospheric
mass hence pressure and its influence on the climatic conditions. Therefore,
the aim of the present study is to understand the influence of atmospheric mass
on climate, hence habitability, and the spectral appearance of planets with
Earth-like, that is, N2-O2 dominated, atmospheres orbiting the Sun at 1
Astronomical Unit. This work utilizes a 1D coupled, cloud-free,
climate-photochemical atmospheric column model; varies atmospheric surface
pressure from 0.5 bar to 30 bar; and investigates temperature and key species
profiles, as well as emission and brightness temperature spectra in a range
between 2{\mu}m - 20{\mu}m. Increasing the surface pressure up to 4 bar leads
to an increase in the surface temperature due to increased greenhouse warming.
Above this point, Rayleigh scattering dominates and the surface temperature
decreases, reaching surface temperatures below 273K (approximately at ~34 bar
surface pressure). For ozone, nitrous oxide, water, methane, and carbon
dioxide, the spectral response either increases with surface temperature or
pressure depending on the species. Masking effects occur, for example, for the
bands of the biosignatures ozone and nitrous oxide by carbon dioxide, which
could be visible in low carbon dioxide atmospheres.",http://arxiv.org/abs/2006.05207v1
"Crop Yield Prediction Integrating Genotype and Weather Variables Using
  Deep Learning",2020-06-24T16:20:12Z,"Johnathon Shook, Tryambak Gangopadhyay, Linjiang Wu, Baskar Ganapathysubramanian, Soumik Sarkar, Asheesh K. Singh","Accurate prediction of crop yield supported by scientific and domain-relevant
insights, can help improve agricultural breeding, provide monitoring across
diverse climatic conditions and thereby protect against climatic challenges to
crop production including erratic rainfall and temperature variations. We used
historical performance records from Uniform Soybean Tests (UST) in North
America spanning 13 years of data to build a Long Short Term Memory - Recurrent
Neural Network based model to dissect and predict genotype response in
multiple-environments by leveraging pedigree relatedness measures along with
weekly weather parameters. Additionally, for providing explainability of the
important time-windows in the growing season, we developed a model based on
temporal attention mechanism. The combination of these two models outperformed
random forest (RF), LASSO regression and the data-driven USDA model for yield
prediction. We deployed this deep learning framework as a 'hypotheses
generation tool' to unravel GxExM relationships. Attention-based time series
models provide a significant advancement in interpretability of yield
prediction models. The insights provided by explainable models are applicable
in understanding how plant breeding programs can adapt their approaches for
global climate change, for example identification of superior varieties for
commercial release, intelligent sampling of testing environments in variety
development, and integrating weather parameters for a targeted breeding
approach. Using DL models as hypothesis generation tools will enable
development of varieties with plasticity response in variable climatic
conditions. We envision broad applicability of this approach (via conducting
sensitivity analysis and ""what-if"" scenarios) for soybean and other crop
species under different climatic conditions.",http://arxiv.org/abs/2006.13847v1
"Long-term and seasonal variability of wind and wave extremes in the
  Arctic Ocean",2020-06-12T12:58:31Z,"Isabela S. Cabral, Ian R. Young, Alessandro Toffoli","Over recent decades, the Arctic Ocean has experienced dramatic changes due to
climate change. Retreating sea ice has opened up large areas of ocean,
resulting in an enhanced wave climate. Taking into account the intense
seasonality and the rapid changes to the Arctic climate, a non-stationary
approach is applied to time-varying statistical properties to investigate
historical trends of extreme values. The analysis is based on a 28-year wave
hindcast (from 1991 to 2018) carried out with the WAVEWATCH III wave model
forced by ERA5 wind speed. The results show notable seasonal differences and
robust positive trends in extreme wave height and wind speed, especially in the
Beaufort and East Siberian seas, with increasing rates in areal-average of the
100-year return period of wind speed of approximately 4\% and significant wave
height up to 60%. It is concluded that the significant increases in extreme
significant wave height are largely associated with sea-ice retreat and the
enhanced fetches available for wave generation.",http://arxiv.org/abs/2006.07148v2
"Formatting the Landscape: Spatial conditional GAN for varying population
  in satellite imagery",2020-12-08T13:31:49Z,"Tomas Langer, Natalia Fedorova, Ron Hagensieker","Climate change is expected to reshuffle the settlement landscape: forcing
people in affected areas to migrate, to change their lifeways, and continuing
to affect demographic change throughout the world. Changes to the geographic
distribution of population will have dramatic impacts on land use and land
cover and thus constitute one of the major challenges of planning for climate
change scenarios. In this paper, we explore a generative model framework for
generating satellite imagery conditional on gridded population distributions.
We make additions to the existing ALAE architecture, creating a spatially
conditional version: SCALAE. This method allows us to explicitly disentangle
population from the model's latent space and thus input custom population
forecasts into the generated imagery. We postulate that such imagery could then
be directly used for land cover and land use change estimation using existing
frameworks, as well as for realistic visualisation of expected local change. We
evaluate the model by comparing pixel and semantic reconstructions, as well as
calculate the standard FID metric. The results suggest the model captures
population distributions accurately and delivers a controllable method to
generate realistic satellite imagery.",http://arxiv.org/abs/2101.05069v1
Shifting velocity of temperature extremes under climate change,2020-01-17T14:27:00Z,"Joan Rey, Guillaume Rohat, Marjorie Perroud, St√©phane Goyette, J√©r√¥me Kasparian","Rapid changes in climatic conditions threaten both socioeconomic and
ecological systems, as these might not be able to adapt or to migrate at the
same pace as that of global warming. In particular, an increase of weather and
climate extremes can lead to increased stress on human and natural systems, and
a tendency for serious adverse effects. We rely on the EURO-CORDEX simulations
and focus on the the screen-level daily mean temperature (T2m). We compare the
shifting velocities of the cold and hot extremes with these of the associated
central trends, i.e., the arithmetical mean or median. We define the extremes
relative to the T2m distribution as it evolves with time over the period of
1951--2100. We find that temperature extremes shift at a similar velocity
compared to that of the central trends. Accordingly, the T2m probability
distribution shifts mostly as a whole, as the tails of the distribution
increase together with the central trends. Exceptions occur however in specific
regions and for the clustering of warm days, which shifts slower than all other
extremes investigated in this study.",http://arxiv.org/abs/2001.06331v1
"Semantic Workflows and Machine Learning for the Assessment of Carbon
  Storage by Urban Trees",2020-09-22T01:30:29Z,"Juan Carrillo, Daniel Garijo, Mark Crowley, Rober Carrillo, Yolanda Gil, Katherine Borda","Climate science is critical for understanding both the causes and
consequences of changes in global temperatures and has become imperative for
decisive policy-making. However, climate science studies commonly require
addressing complex interoperability issues between data, software, and
experimental approaches from multiple fields. Scientific workflow systems
provide unparalleled advantages to address these issues, including
reproducibility of experiments, provenance capture, software reusability and
knowledge sharing. In this paper, we introduce a novel workflow with a series
of connected components to perform spatial data preparation, classification of
satellite imagery with machine learning algorithms, and assessment of carbon
stored by urban trees. To the best of our knowledge, this is the first study
that estimates carbon storage for a region in Africa following the guidelines
from the Intergovernmental Panel on Climate Change (IPCC).",http://arxiv.org/abs/2009.10263v1
"Global warNing: challenges, threats and opportunities for ground beetles
  (Coleoptera: Carabidae) in high altitude habitats",2020-11-13T08:15:32Z,Mauro Gobbi,"Aim of this paper is to provide the first comprehensive synthesis about
ground beetle (Coleoptera: Carabidae) distribution in high altitude habitats.
Specifically, the attention is focused on the species assemblages living on the
most common ice-related mountain landforms (glaciers, debris-covered glaciers,
glacier forelands and rock glaciers) and on the challenges, threats and
opportunities carabids living in these habitats have to face in relation to the
ongoing climate warming. The suggested role of the ice-related alpine
landforms, as present climatic refugia for cold-adapted ground beetles, is
discussed. Finally, the needs to develop a large-scale High-alpine Biodiversity
Monitoring Program to describe how the current climate change is shaping the
distribution of high altitude specialists, is highlighted.",http://arxiv.org/abs/2011.06804v1
"An analysis of the periodically forced PP04 climate model, using the
  theory of non-smooth dynamical systems",2020-05-18T10:17:47Z,"Kgomotso S. Morupisi, Chris J. Budd","In this paper we perform a careful analysis of the forced PP04 model for
climate change, in particular the behaviour of the ice-ages. This system models
the transition from a glacial to an inter-glacial state through a sudden
release of oceanic Carbon Dioxide into the atmosphere. This process can be cast
in terms of a Filippov dynamical system, with a discontinuous change in its
dynamics related to the Carbon Dioxide release. By using techniques from the
theory of non-smooth dynamical systems, we give an analysis of this model in
the cases of both no insolation forcing and also periodic insolation forcing.
This reveals a rich, and novel, dynamical structure to the solutions of the
PP04 model. In particular we see synchronised periodic solutions with subtle
regions of existence which depend on the amplitude and frequency of the
forcing. The orbits can be created/destroyed in both smooth and discontinuity
induced bifurcations. We study both the orbits and the transitions between them
and make comparisons with actual climate dynamics.",http://arxiv.org/abs/2005.08561v1
"A well-timed switch from local to global agreements accelerates climate
  change mitigation",2020-07-26T23:06:22Z,"Vadim A. Karatayev, V√≠tor V. Vasconcelos, Anne-Sophie Lafuite, Simon A. Levin, Chris T. Bauch, Madhur Anand","Recent attempts at cooperating on climate change mitigation highlight the
limited efficacy of large-scale agreements, when commitment to mitigation is
costly and initially rare. Bottom-up approaches using region-specific
mitigation agreements promise greater success, at the cost of slowing global
adoption. Here, we show that a well-timed switch from regional to global
negotiations dramatically accelerates climate mitigation compared to using only
local, only global, or both agreement types simultaneously. This highlights the
scale-specific roles of mitigation incentives: local incentives capitalize on
regional differences (e.g., where recent disasters incentivize mitigation) by
committing early-adopting regions, after which global agreements draw in
late-adopting regions. We conclude that global agreements are key to overcoming
the expenses of mitigation and economic rivalry among regions but should be
attempted once regional agreements are common. Gradually up-scaling efforts
could likewise accelerate mitigation at smaller scales, for instance when
costly ecosystem restoration initially faces limited public and legislative
support.",http://arxiv.org/abs/2007.13238v1
Meta-modeling strategy for data-driven forecasting,2020-11-14T19:25:58Z,"Dominic J. Skinner, Romit Maulik","Accurately forecasting the weather is a key requirement for climate change
mitigation. Data-driven methods offer the ability to make more accurate
forecasts, but lack interpretability and can be expensive to train and deploy
if models are not carefully developed. Here, we make use of two historical
climate data sets and tools from machine learning, to accurately predict
temperature fields. Furthermore, we are able to use low fidelity models that
are cheap to train and evaluate, to selectively avoid expensive high fidelity
function evaluations, as well as uncover seasonal variations in predictive
power. This allows for an adaptive training strategy for computationally
efficient geophysical emulation.",http://arxiv.org/abs/2012.00678v1
"Wildfire Smoke and Air Quality: How Machine Learning Can Guide Forest
  Management",2020-10-09T15:49:38Z,"Lorenzo Tomaselli, Coty Jen, Ann B. Lee","Prescribed burns are currently the most effective method of reducing the risk
of widespread wildfires, but a largely missing component in forest management
is knowing which fuels one can safely burn to minimize exposure to toxic smoke.
Here we show how machine learning, such as spectral clustering and manifold
learning, can provide interpretable representations and powerful tools for
differentiating between smoke types, hence providing forest managers with vital
information on effective strategies to reduce climate-induced wildfires while
minimizing production of harmful smoke.",http://arxiv.org/abs/2010.04651v2
"Climbing down Charney's ladder: Machine Learning and the post-Dennard
  era of computational climate science",2020-05-24T23:46:09Z,V. Balaji,"The advent of digital computing in the 1950s sparked a revolution in the
science of weather and climate. Meteorology, long based on extrapolating
patterns in space and time, gave way to computational methods in a decade of
advances in numerical weather forecasting. Those same methods also gave rise to
computational climate science, studying the behaviour of those same numerical
equations over intervals much longer than weather events, and changes in
external boundary conditions. Several subsequent decades of exponential growth
in computational power have brought us to the present day, where models ever
grow in resolution and complexity, capable of mastery of many small-scale
phenomena with global repercussions, and ever more intricate feedbacks in the
Earth system.
  The current juncture in computing, seven decades later, heralds an end to
what is called Dennard scaling, the physics behind ever smaller computational
units and ever faster arithmetic. This is prompting a fundamental change in our
approach to the simulation of weather and climate, potentially as revolutionary
as that wrought by John von Neumann in the 1950s. One approach could return us
to an earlier era of pattern recognition and extrapolation, this time aided by
computational power. Another approach could lead us to insights that continue
to be expressed in mathematical equations. In either approach, or any synthesis
of those, it is clearly no longer the steady march of the last few decades,
continuing to add detail to ever more elaborate models. In this prospectus, we
attempt to show the outlines of how this may unfold in the coming decades, a
new harnessing of physical knowledge, computation, and data.",http://arxiv.org/abs/2005.11862v2
How Ominous is the Future Global Warming Premonition?,2020-08-25T17:12:38Z,"Debashis Chatterjee, Sourabh Bhattacharya","Global warming, the phenomenon of increasing global average temperature in
the recent decades, is receiving wide attention due to its very significant
adverse effects on climate. Whether global warming will continue even in the
future, is a question that is most important to investigate. In this regard,
the so-called general circulation models (GCMs) have attempted to project the
future climate, and nearly all of them exhibit alarming rates of global
temperature rise in the future.
  Although global warming in the current time frame is undeniable, it is
important to assess the validity of the future predictions of the GCMs. In this
article, we attempt such a study using our recently-developed Bayesian multiple
testing paradigm for model selection in inverse regression problems. The model
we assume for the global temperature time series is based on Gaussian process
emulation of the black box scenario, realistically treating the dynamic
evolution of the time series as unknown.
  We apply our ideas to datasets available from the Intergovernmental Panel on
Climate Change (IPCC) website. The best GCM models selected by our method under
different assumptions on future climate change scenarios do not convincingly
support the present global warming pattern when only the future predictions are
considered known. Using our Gaussian process idea, we also forecast the future
temperature time series given the current one. Interestingly, our results do
not support drastic future global warming predicted by almost all the GCM
models.",http://arxiv.org/abs/2008.11175v1
"Regularized Fingerprinting in Detection and Attribution of Climate
  Change with Weight Matrix Optimizing the Efficiency in Scaling Factor
  Estimation",2020-12-08T04:07:59Z,"Yan Li, Kun Chen, Jun Yan, Xuebin Zhang","The optimal fingerprinting method for detection and attribution of climate
change is based on a multiple regression where each covariate has measurement
error whose covariance matrix is the same as that of the regression error up to
a known scale. Inferences about the regression coefficients are critical not
only for making statements about detection and attribution but also for
quantifying the uncertainty in important outcomes derived from detection and
attribution analyses. When there is no errors-in-variables (EIV), the optimal
weight matrix in estimating the regression coefficients is the precision matrix
of the regression error which, in practice, is never known and has to be
estimated from climate model simulations. We construct a weight matrix by
inverting a nonlinear shrinkage estimate of the error covariance matrix that
minimizes loss functions directly targeting the uncertainty of the resulting
regression coefficient estimator. The resulting estimator of the regression
coefficients is asymptotically optimal as the sample size of the climate model
simulations and the matrix dimension go to infinity together with a limiting
ratio. When EIVs are present, the estimator of the regression coefficients
based on the proposed weight matrix is asymptotically more efficient than that
based on the inverse of the existing linear shrinkage estimator of the error
covariance matrix. The performance of the method is confirmed in finite sample
simulation studies mimicking realistic situations in terms of the length of the
confidence intervals and empirical coverage rates for the regression
coefficients. An application to detection and attribution analyses of the mean
temperature at different spatial scales illustrates the utility of the method.",http://arxiv.org/abs/2012.04200v1
"The PCL Framework: A strategic approach to comprehensive risk management
  in response to climate change impacts",2020-04-13T18:24:34Z,Youssef Nassef,"The PCL framework provides a comprehensive climate risk management approach
grounded in the assessment of societal values of financial and non-financial
loss tolerability. The framework optimizes response action across three main
clusters, namely preemptive adaptation (P) or risk reduction, contingent
arrangements (C), and loss acceptance (L); without a predetermined hierarchy
across them. The PCL Framework aims at including the three clusters of outlay
within a single continuum, and with the main policy outcome being a balanced
portfolio of actions across the three clusters by way of an optimization
module, such that the aggregate outlay is optimized in the long-term. It is
proposed that the approach be applied separately for each hazard to which the
target community is exposed. While it is currently applied to climate-related
risk management, the methodology can be repurposed for use in other contexts
where societal buy-in is central.",http://arxiv.org/abs/2004.06144v1
Spread of Tweets in Climate Discussions,2020-10-19T19:22:57Z,"Yan Xia, Ted Hsuan Yun Chen, Mikko Kivel√§","Characterising the spreading of ideas within echo chambers is essential for
understanding polarisation. In this paper, we explore the characteristics of
popular and viral content in climate change discussions on Twitter around the
2019 announcement of the Nobel Peace Prize, where we find the retweet network
of users to be polarised into two well-separated groups of activists and
sceptics. Operationalising popularity as the number of retweets and virality as
the spreading probability inferred using an independent cascade model, we find
that the viral themes echo and differ from the popular themes in interesting
ways. Most importantly, we find that the most viral themes in the two groups
reflect different types of bonds that tie the community together, yet both
function to enhance ingroup connections while repulsing outgroup engagement.
With this, our study sheds light, from an information spreading perspective, on
the formation and upkeep of echo chambers in climate discussions.",http://arxiv.org/abs/2010.09801v2
"Triple Junction and Grain Boundary Influences on Climate Signals in
  Polar Ice",2020-05-28T20:06:03Z,"Thomas M. Beers, Sharon B. Sneed, Paul Andrew Mayewski, Andrei V. Kurbatov, Michael J. Handley","The Climate Change Institute W. M. Keck Laser Ice Facility laser ablation
inductively coupled plasma spectrometer (LA-ICP-MS) yields a sample every 121
micrometers, a resolution on the scale of ice crystal triple junctions and
grain boundaries in ice cores. Recent publications suggest that these features
can allow amplification of impurity concentrations, and allow migration through
veins potentially obscuring climate signals preserved in polar ice. LA-ICP-MS
data reveal that such features modify these signals by less than 6% in the case
of Na, Ca, and Fe based on the examination of GISP2 ice deposited at the
beginning of the Holocene.",http://arxiv.org/abs/2005.14268v1
Forecasting Black Sigatoka Infection Risks with Latent Neural ODEs,2020-12-01T21:59:53Z,"Yuchen Wang, Matthieu Chan Chee, Ziyad Edher, Minh Duc Hoang, Shion Fujimori, Sornnujah Kathirgamanathan, Jesse Bettencourt","Black Sigatoka disease severely decreases global banana production, and
climate change aggravates the problem by altering fungal species distributions.
Due to the heavy financial burden of managing this infectious disease, farmers
in developing countries face significant banana crop losses. Though scientists
have produced mathematical models of infectious diseases, adapting these models
to incorporate climate effects is difficult. We present MR. NODE (Multiple
predictoR Neural ODE), a neural network that models the dynamics of black
Sigatoka infection learnt directly from data via Neural Ordinary Differential
Equations. Our method encodes external predictor factors into the latent space
in addition to the variable that we infer, and it can also predict the
infection risk at an arbitrary point in time. Empirically, we demonstrate on
historical climate data that our method has superior generalization performance
on time points up to one month in the future and unseen irregularities. We
believe that our method can be a useful tool to control the spread of black
Sigatoka.",http://arxiv.org/abs/2012.00752v2
"Understanding Climate Impacts on Vegetation with Gaussian Processes in
  Granger Causality",2020-12-06T17:47:03Z,"Miguel Morata-Dolz, Diego Bueso, Maria Piles, Gustau Camps-Valls","Global warming is leading to unprecedented changes in our planet, with great
societal, economical and environmental implications, especially with the
growing demand of biofuels and food. Assessing the impact of climate on
vegetation is of pressing need. We approached the attribution problem with a
novel nonlinear Granger causal (GC) methodology and used a large data archive
of remote sensing satellite products, environmental and climatic variables
spatio-temporally gridded over more than 30 years. We generalize kernel Granger
causality by considering the variables cross-relations explicitly in Hilbert
spaces, and use the covariance in Gaussian processes. The method generalizes
the linear and kernel GC methods, and comes with tighter bounds of performance
based on Rademacher complexity. Spatially-explicit global Granger footprints of
precipitation and soil moisture on vegetation greenness are identified more
sharply than previous GC methods.",http://arxiv.org/abs/2012.03338v1
Statistical physics approaches to the complex Earth system,2020-09-05T10:10:45Z,"Jingfang Fan, Jun Meng, Josef Ludescher, Xiaosong Chen, Yosef Ashkenazy, Jurgen Kurths, Shlomo Havlin, Hans Joachim Schellnhuber","Global climate change, extreme climate events, earthquakes and their
accompanying natural disasters pose significant risks to humanity. Yet due to
the nonlinear feedbacks, strategic interactions and complex structure of the
Earth system, the understanding and in particular the predicting of such
disruptive events represent formidable challenges for both scientific and
policy communities. During the past years, the emergence and evolution of Earth
system science has attracted much attention and produced new concepts and
frameworks. Especially, novel statistical physics and complex networks-based
techniques have been developed and implemented to substantially advance our
knowledge for a better understanding of the Earth system, including climate
extreme events, earthquakes and Earth geometric relief features, leading to
substantially improved predictive performances. We present here a comprehensive
review on the recent scientific progress in the development and application of
how combined statistical physics and complex systems science approaches such
as, critical phenomena, network theory, percolation, tipping points analysis,
as well as entropy can be applied to complex Earth systems (climate,
earthquakes, etc.). Notably, these integrating tools and approaches provide new
insights and perspectives for understanding the dynamics of the Earth systems.
The overall aim of this review is to offer readers the knowledge on how
statistical physics approaches can be useful in the field of Earth system
science.",http://arxiv.org/abs/2009.04918v1
Social tipping processes for sustainability: An analytical framework,2020-10-09T10:29:16Z,"Ricarda Winkelmann, Jonathan F. Donges, E. Keith Smith, Manjana Milkoreit, Christina Eder, Jobst Heitzig, Alexia Katsanidou, Marc Wiedermann, Nico Wunderling, Timothy M. Lenton","Societal transformations are necessary to address critical global challenges,
such as mitigation of anthropogenic climate change and reaching UN sustainable
development goals. Recently, social tipping processes have received increased
attention, as they present a form of social change whereby a small change can
shift a sensitive social system into a qualitatively different state due to
strongly self-amplifying (mathematically positive) feedback mechanisms. Social
tipping processes have been suggested as key drivers of sustainability
transitions emerging in the fields of technological and energy systems,
political mobilization, financial markets and sociocultural norms and
behaviors.
  Drawing from expert elicitation and comprehensive literature review, we
develop a framework to identify and characterize social tipping processes
critical to facilitating rapid social transformations. We find that social
tipping processes are distinguishable from those of already more widely studied
climate and ecological tipping dynamics. In particular, we identify human
agency, social-institutional network structures, different spatial and temporal
scales and increased complexity as key distinctive features underlying social
tipping processes. Building on these characteristics, we propose a formal
definition for social tipping processes and filtering criteria for those
processes that could be decisive for future trajectories to global
sustainability in the Anthropocene. We illustrate this definition with the
European political system as an example of potential social tipping processes,
highlighting the potential role of the FridaysForFuture movement.
  Accordingly, this analytical framework for social tipping processes can be
utilized to illuminate mechanisms for necessary transformative climate change
mitigation policies and actions.",http://arxiv.org/abs/2010.04488v1
"The Trouble with Water: Condensation, Circulation and Climate",2020-06-03T16:28:09Z,Geoffrey K. Vallis,"This article discussesl a few of the problems that arise in geophysical fluid
dynamics and climate that are associated with the presence of moisture in the
air, its condensation and release of latent heat. Our main focus is Earth's
atmosphere but we also discuss how these problems might manifest themselves on
other planetary bodies, with particular attention to Titan where methane takes
on the role of water.
  GFD has traditionally been concerned with understanding the very basic
problems that lie at the foundation of dynamical meteorology and
ocean\-ography. Conventionally, and a little ironically, the subject mainly
considers `dry' fluids, meaning it does not concern itself overly much with
phase changes. The subject is often regarded as dry in another way, because it
does not consider problems perceived as relevant to the real world, such as
clouds or rainfall, which have typically been the province of complicated
numerical models. Those models often rely on parameterizations of unresolved
processes, parameterizations that may work very well but that often have a
semi-empirical basis. The consequent dichotomy between the foundations and the
applications prevents progress being made that has both a secure basis in
scientific understanding and a relevance to the Earth's climate, especially
where moisture is concerned. The dichotomy also inhibits progress in
understanding the climate of other planets, where observations are insufficient
to tune the parameterizations that weather and climate models for Earth rely
upon, and a more fundamental approach is called for. Here we discuss four
diverse examples of the problems with moisture: the determination of relative
humidity and cloudiness; the transport of water vapor and its possible change
under global warming; the moist shallow water equations and the Madden-Julian
Oscillation; and the hydrology cycle on other planetary bodies.",http://arxiv.org/abs/2006.02364v1
"Emergent risks in the Mt. Everest region in the time of anthropogenic
  climate change",2020-08-03T16:46:57Z,"Kimberley R. Miner, Paul A. Mayewski, Saraju K. Baidya, Kenneth Broad, Heather Clifford, Ananta P. Gajurel, Bibek Giri, Mary Hubbard, Corey Jaskolski, Heather Koldewey, Wei Li, Tom Matthews, Imogen Napper, Baker Perry, Mariusz Potocki, John C. Priscu, Alex Tait, Richard Thompson, Subash Tuladhar","In April and May 2019, as a part of the National Geographic and Roxel
Perpetual Planet Everest Expedition, the most interdisciplinary scientific ever
was launched. This research identified changing dynamics, including emergent
risks resulting from natural and anthropogenic change to the natural system. We
have identified compounded risks to ecosystem and human health, geologic
hazards, and changing climate conditions that impact the local community,
climbers, and trekkeers in the future. This review brings together perspectives
from across the biological, geological, and health sciences to better
understand emergent risks on Mt. Everest and in the Khumbu region.
Understanding and mitigating these risks is critical for the ~10,000 people
living in the Khumbu region, as well as the thousands of visiting trekkers and
the hundreds of climbers who attempt to summit each year.",http://arxiv.org/abs/2008.01010v1
"Cons{√©}quences du changement climatique pour les maladies {√†}
  transmission vectorielle et impact en assurance de personnes",2020-11-13T15:05:14Z,"Yannick Drif, Benjamin Roche, Pierre Valade","Climate change, which is largely linked to human activities, is already
having a considerable impact on our societies. Based on current trends, climate
change is expected to accelerate in the coming decades. Beyond its impact on
the pace of natural disasters (floods, hurricanes, etc.), climate change may
have catastrophic consequences for human life and health. One of the concerns
is the increase in the transmission of viruses spread by mosquitoes. Indeed,
rising temperatures have a direct positive impact on the viability of
mosquitoes in ecosystems, leading to their abundance and thus the risk of
exposure of human populations to these pathogens. This study quantifies the
consequences of global warming on the risk of epidemics of viruses transmitted
by the Aedes Albopictus mosquito in metropolitan France. This mosquito, which
is a vector for the Dengue, Chikungunya and Zika viruses, among others, arrived
in mainland France in 2004 and has since spread throughout the country. Thanks
to the association previously established between the probability of the
presence of the mosquito and the average temperature combined with a
mathematical model, the probability of an epidemic and the number of people who
could be infected and die during a season in each department are estimated. If
there is a high degree of heterogeneity in metropolitan France, nearly 2,000
deaths per year could be expected by 2040.",http://arxiv.org/abs/2012.06482v1
"Refining the conference experience for junior scientists in the wake of
  climate change",2020-02-18T19:41:17Z,"Ruth Johnson, Andrada Fiscutean, Serghei Mangul","With the ever-increasing carbon footprint associated with conferences,
scientists can learn to refine their conference experiences when they do need
to travel. We offer insight on how to optimize the conference experience
through attending speaker sessions, giving presentations, and networking.",http://arxiv.org/abs/2002.12268v2
"Network-based Approach and Climate Change Benefits for Forecasting the
  Amount of Indian Monsoon Rainfall",2020-04-14T16:21:02Z,"Jingfang Fan, Jun Meng, Josef Ludescher, Zhaoyuan Li, Elena Surovyatkina, Xiaosong Chen, J√ºrgen Kurths, Hans Joachim Schellnhuber","The Indian summer monsoon rainfall (ISMR) has a decisive influence on India's
agricultural output and economy. Extreme deviations from the normal seasonal
amount of rainfall can cause severe droughts or floods, affecting Indian food
production and security. Despite the development of sophisticated statistical
and dynamical climate models, a long-term and reliable prediction of the ISMR
has remained a challenging problem. Towards achieving this goal, here we
construct a series of dynamical and physical climate networks based on the
global near surface air temperature field. We uncover that some characteristics
of the directed and weighted climate networks can serve as efficient long-term
predictors for ISMR forecasting. The developed prediction method produces a
forecast skill of 0.5 with a 5-month lead-time in advance by using the previous
calendar year's data. The skill of our ISMR forecast, is comparable to the
current state-of-the-art models, however, with quite a short (i.e., within one
month) lead-time. We discuss the underlying mechanism of our predictor and
associate it with network-delayed-ENSO and ENSO-monsoon connections. Moreover,
our approach allows predicting the all India rainfall, as well as forecasting
the different Indian homogeneous regions' rainfall, which is crucial for
agriculture in India. We reveal that global warming affects the climate network
by enhancing cross-equatorial teleconnections between Southwest Atlantic,
Western part of the Indian Ocean, and North Asia-Pacific with significant
impacts on the precipitation in India. We find a hotspots area in the
mid-latitude South Atlantic, which is the basis for our predictor. Remarkably,
the significant warming trend in this area yields an improvement of the
prediction skill.",http://arxiv.org/abs/2004.06628v1
Deep Learning based Automated Forest Health Diagnosis from Aerial Images,2020-10-16T15:07:56Z,"Chia-Yen Chiang, Chloe Barnes, Plamen Angelov, Richard Jiang","Global climate change has had a drastic impact on our environment. Previous
study showed that pest disaster occured from global climate change may cause a
tremendous number of trees died and they inevitably became a factor of forest
fire. An important portent of the forest fire is the condition of forests.
Aerial image-based forest analysis can give an early detection of dead trees
and living trees. In this paper, we applied a synthetic method to enlarge
imagery dataset and present a new framework for automated dead tree detection
from aerial images using a re-trained Mask RCNN (Mask Region-based
Convolutional Neural Network) approach, with a transfer learning scheme. We
apply our framework to our aerial imagery datasets,and compare eight fine-tuned
models. The mean average precision score (mAP) for the best of these models
reaches 54%. Following the automated detection, we are able to automatically
produce and calculate number of dead tree masks to label the dead trees in an
image, as an indicator of forest health that could be linked to the causal
analysis of environmental changes and the predictive likelihood of forest fire.",http://arxiv.org/abs/2010.08437v1
"Assessing the Reliability of Wind Power Operations under a Changing
  Climate with a Non-Gaussian Bias Correction",2020-11-02T19:10:15Z,"Jiachen Zhang, Paola Crippa, Marc G. Genton, Stefano Castruccio","Facing increasing societal and economic pressure, many countries have
established strategies to develop renewable energy portfolios, whose
penetration in the market can alleviate the dependence on fossil fuels. In the
case of wind, there is a fundamental question related to the resilience, and
hence profitability of future wind farms to a changing climate, given that
current wind turbines have lifespans of up to thirty years. In this work, we
develop a new non-Gaussian method data to simulations and to estimate future
wind, predicated on a trans-Gaussian transformation and a cluster-wise
minimization of the Kullback-Leibler divergence. Future winds abundance will be
determined for Saudi Arabia, a country with a recently established plan to
develop a portfolio of up to 16 GW of wind energy. Further, we estimate the
change in profits over future decades using additional high-resolution
simulations, an improved method for vertical wind extrapolation, and power
curves from a collection of popular wind turbines. We find an overall increase
in the daily profit of $272,000 for the wind energy market for the optimal
locations for wind farming in the country.",http://arxiv.org/abs/2011.01263v2
Movement Tracks for the Automatic Detection of Fish Behavior in Videos,2020-11-28T05:51:19Z,"Declan McIntosh, Tunai Porto Marques, Alexandra Branzan Albu, Rodney Rountree, Fabio De Leo","Global warming is predicted to profoundly impact ocean ecosystems. Fish
behavior is an important indicator of changes in such marine environments.
Thus, the automatic identification of key fish behavior in videos represents a
much needed tool for marine researchers, enabling them to study climate
change-related phenomena. We offer a dataset of sablefish (Anoplopoma fimbria)
startle behaviors in underwater videos, and investigate the use of deep
learning (DL) methods for behavior detection on it. Our proposed detection
system identifies fish instances using DL-based frameworks, determines
trajectory tracks, derives novel behavior-specific features, and employs Long
Short-Term Memory (LSTM) networks to identify startle behavior in sablefish.
Its performance is studied by comparing it with a state-of-the-art DL-based
video event detector.",http://arxiv.org/abs/2011.14070v1
"Can animal manure be used to increase soil organic carbon stocks in the
  Mediterranean as a mitigation climate change strategy?",2020-07-17T11:30:56Z,"Andreas Kamilaris, Immaculada Funes Mesa, Robert Sav√©, Felicidad De Herralde, Francesc X. Prenafeta-Bold√∫","Soil organic carbon (SOC) plays an important role on improving soil
conditions and soil functions. Increasing land use changes have induced an
important decline of SOC content at global scale. Increasing SOC in
agricultural soils has been proposed as a strategy to mitigate climate change.
Animal manure has the characteristic of enriching SOC, when applied to crop
fields, while, in parallel, it could constitute a natural fertilizer for the
crops. In this paper, a simulation is performed using the area of Catalonia,
Spain as a case study for the characteristic low SOC in the Mediterranean, to
examine whether animal manure can improve substantially the SOC of agricultural
fields, when applied as organic fertilizers. Our results show that the policy
goals of the 4x1000 strategy can be achieved only partially by using manure
transported to the fields. This implies that the proposed approach needs to be
combined with other strategies.",http://arxiv.org/abs/2007.10823v1
"How can contemporary climate research help to understand epidemic
  dynamics? -- Ensemble approach and snapshot attractors",2020-08-12T09:55:40Z,Tam√°s Kov√°cs,"Standard epidemic models based on compartmental differential equations are
investigated under continuous parameter change as external forcing. We show
that seasonal modulation of the contact parameter superimposed a monotonic
decay needs a different description than that of the standard chaotic dynamics.
The concept of snapshot attractors and their natural probability distribution
has been adopted from the field of the latest climate-change-research to show
the importance of transient effect and ensemble interpretation of disease
spread. After presenting the extended bifurcation diagram of measles, the
temporal change of the phase space structure is investigated. By defining
statistical measures over the ensemble, we can interpret the internal
variability of the epidemic as the onset of complex dynamics even for those
values of contact parameter where regular behavior is expected. We argue that
anomalous outbreaks of infectious class cannot die out until transient chaos is
presented for various parameters. More important, that this fact becomes
visible by using of ensemble approach rather than single trajectory
representation. These findings are applicable generally in nonlinear dynamical
systems such as standard epidemic models regardless of parameter values.",http://arxiv.org/abs/2008.05205v1
"An empirical, Bayesian approach to modelling the impact of weather on
  crop yield: maize in the US",2020-01-08T16:53:16Z,"Raphael Shirley, Edward Pope, Myles Bartlett, Seb Oliver, Novi Quadrianto, Peter Hurley, Steven Duivenvoorden, Phil Rooney, Adam B. Barrett, Chris Kent, James Bacon","We apply an empirical, data-driven approach for describing crop yield as a
function of monthly temperature and precipitation by employing generative
probabilistic models with parameters determined through Bayesian inference. Our
approach is applied to state-scale maize yield and meteorological data for the
US Corn Belt from 1981 to 2014 as an exemplar, but would be readily
transferable to other crops, locations and spatial scales. Experimentation with
a number of models shows that maize growth rates can be characterised by a
two-dimensional Gaussian function of temperature and precipitation with monthly
contributions accumulated over the growing period. This approach accounts for
non-linear growth responses to the individual meteorological variables, and
allows for interactions between them. Our models correctly identify that
temperature and precipitation have the largest impact on yield in the six
months prior to the harvest, in agreement with the typical growing season for
US maize (April to September). Maximal growth rates occur for monthly mean
temperature 18-19$^\circ$C, corresponding to a daily maximum temperature of
24-25$^\circ$C (in broad agreement with previous work) and monthly total
precipitation 115 mm. Our approach also provides a self-consistent way of
investigating climate change impacts on current US maize varieties in the
absence of adaptation measures. Keeping precipitation and growing area fixed, a
temperature increase of $2^\circ$C, relative to 1981-2014, results in the mean
yield decreasing by 8\%, while the yield variance increases by a factor of
around 3. We thus provide a flexible, data-driven framework for exploring the
impacts of natural climate variability and climate change on globally
significant crops based on their observed behaviour. In concert with other
approaches, this can help inform the development of adaptation strategies that
will ensure food security under a changing climate.",http://arxiv.org/abs/2001.02614v1
"Bayesian Appraisal of Random Series Convergence with Application to
  Climate Change",2020-09-14T07:04:42Z,"Sucharita Roy, Sourabh Bhattacharya","Roy and Bhattacharya (2020) provided Bayesian characterization of infinite
series, and their most important application, namely, to the Dirichlet series
characterizing the (in)famous Riemann Hypothesis, revealed insights that are
not in support of the most celebrated conjecture for over 150 years.
  In contrast with deterministic series considered by Roy and Bhattacharya
(2020), in this article we take up random infinite series for our
investigation. Remarkably, our method does not require any simplifying
assumption. Albeit the Bayesian characterization theory for random series is no
different from that for the deterministic setup, construction of effective
upper bounds for partial sums, required for implementation, turns out to be a
challenging undertaking in the random setup. In this article, we construct
parametric and nonparametric upper bound forms for the partial sums of random
infinite series and demonstrate the generality of the latter in comparison to
the former. Simulation studies exhibit high accuracy and efficiency of the
nonparametric bound in all the setups that we consider.
  Finally, exploiting the property that the summands tend to zero in the case
of series convergence, we consider application of our nonparametric bound
driven Bayesian method to global climate change analysis. Specifically,
analyzing the global average temperature record over the years 1850--2016 and
Holocene global average temperature reconstruction data 12,000 years before
present, we conclude, in spite of the current global warming situation, that
global climate dynamics is subject to temporary variability only, the current
global warming being an instance, and long term global warming or cooling
either in the past or in the future, are highly unlikely.",http://arxiv.org/abs/2009.06229v1
"Multi-objective Optimal Control of Dynamic Integrated Model of Climate
  and Economy: Evolution in Action",2020-06-29T20:41:34Z,"Mostapha Kalami Heris, Shahryar Rahnamayan","One of the widely used models for studying economics of climate change is the
Dynamic Integrated model of Climate and Economy (DICE), which has been
developed by Professor William Nordhaus, one of the laureates of the 2018 Nobel
Memorial Prize in Economic Sciences. Originally a single-objective optimal
control problem has been defined on DICE dynamics, which is aimed to maximize
the social welfare. In this paper, a bi-objective optimal control problem
defined on DICE model, objectives of which are maximizing social welfare and
minimizing the temperature deviation of atmosphere. This multi-objective
optimal control problem solved using Non-Dominated Sorting Genetic Algorithm II
(NSGA-II) also it is compared to previous works on single-objective version of
the problem. The resulting Pareto front rediscovers the previous results and
generalizes to a wide range of non-dominant solutions to minimize the global
temperature deviation while optimizing the economic welfare. The previously
used single-objective approach is unable to create such a variety of
possibilities, hence, its offered solution is limited in vision and reachable
performance. Beside this, resulting Pareto-optimal set reveals the fact that
temperature deviation cannot go below a certain lower limit, unless we have
significant technology advancement or positive change in global conditions.",http://arxiv.org/abs/2007.00449v1
Using Simulated Data to Generate Images of Climate Change,2020-01-26T22:19:13Z,"Gautier Cosne, Adrien Juraver, M√©lisande Teng, Victor Schmidt, Vahe Vardanyan, Alexandra Luccioni, Yoshua Bengio","Generative adversarial networks (GANs) used in domain adaptation tasks have
the ability to generate images that are both realistic and personalized,
transforming an input image while maintaining its identifiable characteristics.
However, they often require a large quantity of training data to produce
high-quality images in a robust way, which limits their usability in cases when
access to data is limited. In our paper, we explore the potential of using
images from a simulated 3D environment to improve a domain adaptation task
carried out by the MUNIT architecture, aiming to use the resulting images to
raise awareness of the potential future impacts of climate change.",http://arxiv.org/abs/2001.09531v1
"SMArtCast: Predicting soil moisture interpolations into the future using
  Earth observation data in a deep learning framework",2020-03-16T23:06:14Z,"Conrad James Foley, Sagar Vaze, Mohamed El Amine Seddiq, Alexey Unagaev, Natalia Efremova","Soil moisture is critical component of crop health and monitoring it can
enable further actions for increasing yield or preventing catastrophic die off.
As climate change increases the likelihood of extreme weather events and
reduces the predictability of weather, and non-optimal soil moistures for crops
may become more likely. In this work, we a series of LSTM architectures to
analyze measurements of soil moisture and vegetation indiced derived from
satellite imagery. The system learns to predict the future values of these
measurements. These spatially sparse values and indices are used as input
features to an interpolation method that infer spatially dense moisture map for
a future time point. This has the potential to provide advance warning for soil
moistures that may be inhospitable to crops across an area with limited
monitoring capacity.",http://arxiv.org/abs/2003.10823v2
Farmland Parcel Delineation Using Spatio-temporal Convolutional Networks,2020-04-11T19:49:09Z,"Han Lin Aung, Burak Uzkent, Marshall Burke, David Lobell, Stefano Ermon","Farm parcel delineation provides cadastral data that is important in
developing and managing climate change policies. Specifically, farm parcel
delineation informs applications in downstream governmental policies of land
allocation, irrigation, fertilization, green-house gases (GHG's), etc. This
data can also be useful for the agricultural insurance sector for assessing
compensations following damages associated with extreme weather events - a
growing trend related to climate change. Using satellite imaging can be a
scalable and cost effective manner to perform the task of farm parcel
delineation to collect this valuable data. In this paper, we break down this
task using satellite imaging into two approaches: 1) Segmentation of parcel
boundaries, and 2) Segmentation of parcel areas. We implemented variations of
UNets, one of which takes into account temporal information, which achieved the
best results on our dataset on farmland parcels in France in 2017.",http://arxiv.org/abs/2004.05471v2
"Non-linear interlinkages and key objectives amongst the Paris Agreement
  and the Sustainable Development Goals",2020-04-16T18:26:27Z,"Felix Laumann, Julius von K√ºgelgen, Mauricio Barahona","The United Nations' ambitions to combat climate change and prosper human
development are manifested in the Paris Agreement and the Sustainable
Development Goals (SDGs), respectively. These are inherently inter-linked as
progress towards some of these objectives may accelerate or hinder progress
towards others. We investigate how these two agendas influence each other by
defining networks of 18 nodes, consisting of the 17 SDGs and climate change,
for various groupings of countries. We compute a non-linear measure of
conditional dependence, the partial distance correlation, given any subset of
the remaining 16 variables. These correlations are treated as weights on edges,
and weighted eigenvector centralities are calculated to determine the most
important nodes. We find that SDG 6, clean water and sanitation, and SDG 4,
quality education, are most central across nearly all groupings of countries.
In developing regions, SDG 17, partnerships for the goals, is strongly
connected to the progress of other objectives in the two agendas whilst,
somewhat surprisingly, SDG 8, decent work and economic growth, is not as
important in terms of eigenvector centrality.",http://arxiv.org/abs/2004.09318v1
Physics-informed GANs for Coastal Flood Visualization,2020-10-16T02:15:34Z,"Bj√∂rn L√ºtjens, Brandon Leshchinskiy, Christian Requena-Mesa, Farrukh Chishtie, Natalia D√≠az-Rodriguez, Oc√©ane Boulais, Aaron Pi√±a, Dava Newman, Alexander Lavin, Yarin Gal, Chedy Ra√Øssi","As climate change increases the intensity of natural disasters, society needs
better tools for adaptation. Floods, for example, are the most frequent natural
disaster, but during hurricanes the area is largely covered by clouds and
emergency managers must rely on nonintuitive flood visualizations for mission
planning. To assist these emergency managers, we have created a deep learning
pipeline that generates visual satellite images of current and future coastal
flooding. We advanced a state-of-the-art GAN called pix2pixHD, such that it
produces imagery that is physically-consistent with the output of an
expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery
relative to physics-based flood maps, we find that our proposed framework
outperforms baseline models in both physical-consistency and photorealism.
While this work focused on the visualization of coastal floods, we envision the
creation of a global visualization of how climate change will shape our earth.",http://arxiv.org/abs/2010.08103v2
Deep learning architectures for inference of AC-OPF solutions,2020-11-06T13:33:18Z,"Thomas Falconer, Letif Mones","We present a systematic comparison between neural network (NN) architectures
for inference of AC-OPF solutions. Using fully connected NNs as a baseline we
demonstrate the efficacy of leveraging network topology in the models by
constructing abstract representations of electrical grids in the graph domain,
for both convolutional and graph NNs. The performance of the NN architectures
is compared for regression (predicting optimal generator set-points) and
classification (predicting the active set of constraints) settings.
Computational gains for obtaining optimal solutions are also presented.",http://arxiv.org/abs/2011.03352v2
"Investigating two super-resolution methods for downscaling
  precipitation: ESRGAN and CAR",2020-12-02T14:20:17Z,"Campbell D. Watson, Chulin Wang, Timothy Lynar, Komminist Weldemariam","In an effort to provide optimal inputs to downstream modeling systems (e.g.,
a hydrodynamics model that simulates the water circulation of a lake), we
hereby strive to enhance resolution of precipitation fields from a weather
model by up to 9x. We test two super-resolution models: the enhanced
super-resolution generative adversarial networks (ESRGAN) proposed in 2017, and
the content adaptive resampler (CAR) proposed in 2020. Both models outperform
simple bicubic interpolation, with the ESRGAN exceeding expectations for
accuracy. We make several proposals for extending the work to ensure it can be
a useful tool for quantifying the impact of climate change on local ecosystems
while removing reliance on energy-intensive, high-resolution weather model
simulations.",http://arxiv.org/abs/2012.01233v1
Machine Learning for Glacier Monitoring in the Hindu Kush Himalaya,2020-12-09T12:48:06Z,"Shimaa Baraka, Benjamin Akera, Bibek Aryal, Tenzing Sherpa, Finu Shresta, Anthony Ortiz, Kris Sankaran, Juan Lavista Ferres, Mir Matin, Yoshua Bengio","Glacier mapping is key to ecological monitoring in the hkh region. Climate
change poses a risk to individuals whose livelihoods depend on the health of
glacier ecosystems. In this work, we present a machine learning based approach
to support ecological monitoring, with a focus on glaciers. Our approach is
based on semi-automated mapping from satellite images. We utilize readily
available remote sensing data to create a model to identify and outline both
clean ice and debris-covered glaciers from satellite imagery. We also release
data and develop a web tool that allows experts to visualize and correct model
predictions, with the ultimate aim of accelerating the glacier mapping process.",http://arxiv.org/abs/2012.05013v1
"FlowDB a large scale precipitation, river, and flash flood dataset",2020-12-21T07:08:41Z,"Isaac Godfried, Kriti Mahajan, Maggie Wang, Kevin Li, Pranjalya Tiwari","Flooding results in 8 billion dollars of damage annually in the US and causes
the most deaths of any weather related event. Due to climate change scientists
expect more heavy precipitation events in the future. However, no current
datasets exist that contain both hourly precipitation and river flow data. We
introduce a novel hourly river flow and precipitation dataset and a second
subset of flash flood events with damage estimates and injury counts. Using
these datasets we create two challenges (1) general stream flow forecasting and
(2) flash flood damage estimation. We have created several publicly available
benchmarks and an easy to use package. Additionally, in the future we aim to
augment our dataset with snow pack data and soil index moisture data to improve
predictions.",http://arxiv.org/abs/2012.11154v1
"Towards Reducing Energy Waste through Usage of External Communication of
  Autonomous Vehicles",2020-12-27T10:25:33Z,"Mark Colley, Marcel Walch, Enrico Rukzio","Automated vehicles can implement strategies to drive with optimized fuel
efficiency. Therefore, automated driving is seen as a major advancement in
tackling climate change. However, with automated vehicles driving in cities and
other areas rife with other road users such as human drivers, pedestrians, or
cyclists, there is the potential for ""stop-and-go"" traffic. This would greatly
diminish the possibility of automated vehicles to drive fuel-efficient. We
suggest using external communication of automated vehicles to aid in ecological
driving by providing clues to other road users to show the intent and therefore
ultimately enable smoother traffic.",http://arxiv.org/abs/2012.13906v1
"Sensitivity of the Atmospheric Water Cycle within the Habitable Zone of
  a Tidally-Locked, Earth-like Exoplanet",2020-07-08T15:01:38Z,"Marie-Pier Labont√©, Timothy M. Merlis","Synchronously orbiting, tidally-locked exoplanets with a dayside facing their
star and a permanently dark nightside orbiting dim stars are prime candidates
for habitability. Simulations of these planets often show the potential to
maintain an Earth-like climate with a complete hydrological cycle. Here, we
examine the sensitivity of the atmospheric water cycle to changes in stellar
flux and describe the main underlying mechanisms. In a slowly-rotating,
tidally-locked Earth-like atmospheric model, the response to a small (about
10%) increase in stellar irradiance from a habitable-zone control simulation is
examined. The water cycle is enhanced in response to the increased stellar
irradiance. While the evaporation increase behaves similarly to the stellar
radiation increase, the day-to-night energy transport by the mean circulation
is critical to the planet's precipitation changes. Increased efficiency of the
energy transport in a warmer climate shapes the substellar precipitation
increase. On the nightside, precipitation changes are weak as a result of the
large cancellation between the increased energy transport and the increased
longwave emission. The day-to-night energy transport efficiency is sensitive to
the variation of the atmosphere's vertical stratification. Due to weak
temperature gradients in upper troposphere and a moist adiabat maintained in
the substellar region, variations in the substellar surface temperature and
specific humidity govern the increase of the planet's stratification with
warming. This suggests a scaling of nightside's precipitation based on the
substellar surface thermodynamic changes, a sensitivity that holds over a wider
range of stellar irradiance changes.",http://arxiv.org/abs/2007.04172v1
"Regional Impacts of COVID-19 on Carbon Dioxide Detected Worldwide from
  Space",2020-11-25T13:52:12Z,"Brad Weir, David Crisp, Christopher W O'Dell, Sourish Basu, Abhishek Chatterjee, Jana Kolassa, Tomohiro Oda, Steven Pawson, Benjamin Poulter, Zhen Zhang, Philippe Ciais, Steven J Davis, Zhu Liu, Lesley E Ott","Activity reductions in early 2020 due to the Coronavirus Disease 2019
pandemic led to unprecedented decreases in carbon dioxide (CO2) emissions.
Despite their record size, the resulting atmospheric signals are smaller than
and obscured by climate variability in atmospheric transport and biospheric
fluxes, notably that related to the 2019-2020 Indian Ocean Dipole. Monitoring
CO2 anomalies and distinguishing human and climatic causes thus remains a new
frontier in Earth system science. We show, for the first time, that the impact
of short-term, regional changes in fossil fuel emissions on CO2 concentrations
was observable from space. Starting in February and continuing through May,
column CO2 over many of the World's largest emitting regions was 0.14 to 0.62
parts per million less than expected in a pandemic-free scenario, consistent
with reductions of 3 to 13 percent in annual, global emissions. Current
spaceborne technologies are therefore approaching levels of accuracy and
precision needed to support climate mitigation strategies with future missions
expected to meet those needs.",http://arxiv.org/abs/2011.12740v2
Lake Ice Detection from Sentinel-1 SAR with Deep Learning,2020-02-17T16:31:41Z,"Manu Tom, Roberto Aguilar, Pascal Imhof, Silvan Leinss, Emmanuel Baltsavias, Konrad Schindler","Lake ice, as part of the Essential Climate Variable (ECV) lakes, is an
important indicator to monitor climate change and global warming. The
spatio-temporal extent of lake ice cover, along with the timings of key
phenological events such as freeze-up and break-up, provide important cues
about the local and global climate. We present a lake ice monitoring system
based on the automatic analysis of Sentinel-1 Synthetic Aperture Radar (SAR)
data with a deep neural network. In previous studies that used optical
satellite imagery for lake ice monitoring, frequent cloud cover was a main
limiting factor, which we overcome thanks to the ability of microwave sensors
to penetrate clouds and observe the lakes regardless of the weather and
illumination conditions. We cast ice detection as a two class (frozen,
non-frozen) semantic segmentation problem and solve it using a state-of-the-art
deep convolutional network (CNN). We report results on two winters ( 2016 - 17
and 2017 - 18 ) and three alpine lakes in Switzerland. The proposed model
reaches mean Intersection-over-Union (mIoU) scores >90% on average, and >84%
even for the most difficult lake. Additionally, we perform cross-validation
tests and show that our algorithm generalises well across unseen lakes and
winters.",http://arxiv.org/abs/2002.07040v2
"Carbonate-Silicate Cycle Predictions of Earth-like Planetary Climates
  and Testing the Habitable Zone Concept",2020-12-01T20:40:42Z,"Owen R. Lehmer, David C. Catling, Joshua Krissansen-Totton","In the conventional habitable zone (HZ) concept, a CO$_{2}$-H$_2$O greenhouse
maintains surface liquid water. Through the water-mediated carbonate-silicate
weathering cycle, atmospheric CO$_{2}$ partial pressure (pCO$_{2}$) responds to
changes in surface temperature, stabilizing the climate over geologic
timescales. We show that this weathering feedback ought to produce a log-linear
relationship between pCO$_{2}$ and incident flux on Earth-like planets in the
HZ. However, this trend has scatter because geophysical and physicochemical
parameters can vary, such as land area for weathering and CO$_2$ outgassing
fluxes. Using a coupled climate and carbonate-silicate weathering model, we
quantify the likely scatter in pCO$_2$ with orbital distance throughout the HZ.
From this dispersion, we predict a two-dimensional relationship between
incident flux and pCO$_2$ in the HZ and show that it could be detected from at
least 83 ($2{\sigma}$) Earth-like exoplanet observations. If fewer Earth-like
exoplanets are observed, testing the HZ hypothesis from this relationship could
be difficult.",http://arxiv.org/abs/2012.00819v1
High-resolution global irrigation prediction with Sentinel-2 30m data,2020-12-09T17:26:43Z,"Weixin, Wu, Sonal Thakkar, Will Hawkins, Hossein Vahabi, Alberto Todeschini","An accurate and precise understanding of global irrigation usage is crucial
for a variety of climate science efforts. Irrigation is highly
energy-intensive, and as population growth continues at its current pace,
increases in crop need and water usage will have an impact on climate change.
Precise irrigation data can help with monitoring water usage and optimizing
agricultural yield, particularly in developing countries. Irrigation data, in
tandem with precipitation data, can be used to predict water budgets as well as
climate and weather modeling. With our research, we produce an irrigation
prediction model that combines unsupervised clustering of Normalized Difference
Vegetation Index (NDVI) temporal signatures with a precipitation heuristic to
label the months that irrigation peaks for each cropland cluster in a given
year. We have developed a novel irrigation model and Python package
(""Irrigation30"") to generate 30m resolution irrigation predictions of cropland
worldwide. With a small crowdsourced test set of cropland coordinates and
irrigation labels, using a fraction of the resources used by the
state-of-the-art NASA-funded GFSAD30 project with irrigation data limited to
India and Australia, our model was able to achieve consistency scores in excess
of 97\% and an accuracy of 92\% in a small geo-diverse randomly sampled test
set.",http://arxiv.org/abs/2012.07658v1
"Integrated monitoring of ice in selected Swiss lakes. Final project
  report",2020-08-02T16:18:51Z,"Manu Tom, Melanie Suetterlin, Damien Bouffard, Mathias Rothermel, Stefan Wunderle, Emmanuel Baltsavias","Various lake observables, including lake ice, are related to climate and
climate change and provide a good opportunity for long-term monitoring. Lakes
(and as part of them lake ice) is therefore considered an Essential Climate
Variable (ECV) of the Global Climate Observing System (GCOS). Following the
need for an integrated multi-temporal monitoring of lake ice in Switzerland,
MeteoSwiss in the framework of GCOS Switzerland supported this 2-year project
to explore not only the use of satellite images but also the possibilities of
Webcams and in-situ measurements. The aim of this project is to monitor some
target lakes and detect the extent of ice and especially the ice-on/off dates,
with focus on the integration of various input data and processing methods. The
target lakes are: St. Moritz, Silvaplana, Sils, Sihl, Greifen and Aegeri,
whereby only the first four were mainly frozen during the observation period
and thus processed. The observation period was mainly the winter 2016-17.
During the project, various approaches were developed, implemented, tested and
compared. Firstly, low spatial resolution (250 - 1000 m) but high temporal
resolution (1 day) satellite images from the optical sensors MODIS and VIIRS
were used. Secondly, and as a pilot project, the use of existing public Webcams
was investigated for (a) validation of results from satellite data, and (b)
independent estimation of lake ice, especially for small lakes like St. Moritz,
that could not be possibly monitored in the satellite images. Thirdly, in-situ
measurements were made in order to characterize the development of the
temperature profiles and partly pressure before freezing and under the
ice-cover until melting. This report presents the results of the project work.",http://arxiv.org/abs/2008.00512v1
"Forecasts of the trend in global-mean temperature to 2100 arising from
  the scenarios of first-difference CO2 and peak fossil fuel",2020-12-11T21:40:21Z,"L. Mark W. Leggett, David. A. Ball","Two future scenarios that are not explicitly in the range of scenarios (the
Representative Concentration Pathway scenarios) utilised by the IPCC. These two
scenarios are the emissions trend under peak fossil fuel (for example, Mohr et
al., 2015); and the climate sensitivity determinable from the relationship
between first-difference CO2 and temperature recently shown by Leggett and Ball
(2015). This paper provides forecasts of a global surface temperature
trajectory to 2100 resulting from the effect of these two scenarios. The
time-series models developed both displayed high statistical significance and
converged in their forecasts, so adding to the potential robustness of the
findings. Under the effect of the combination of the peak fossil fuel and
first-difference CO2 scenarios, we found that temperature is forecast to
continue to rise, but only gently, until around 2023 where it reaches a level
slightly higher than at present; and from then to display an also gentle steady
decrease. It is shown that this trajectory is markedly lower than that
generated by the IPCC business-as-usual level-of-CO2 (RCP8.5) model (Riahi et
al. 2011). These lower results are evidence that the climate problem may
require less future preventative action than is presently being considered
necessary. If so, it is stressed that the same evidence is support for the case
that the peak fossil fuel problem would require ameliorative action. This
globally required action is the same as it would have been for climate (as
embodied in the Paris Agreement on Climate Change of the 21st Conference of the
Parties of the UNFCCC in Paris adopted in 2015): the rapid transition to a
predominantly renewable global energy system.",http://arxiv.org/abs/2012.06649v1
"On the spatial and temporal shift in the archetypal seasonal temperature
  cycle as driven by annual and semi-annual harmonics",2020-03-15T21:14:13Z,"Joshua S. North, Erin M. Schliep, Christopher K. Wikle","Statistical methods are required to evaluate and quantify the uncertainty in
environmental processes, such as land and sea surface temperature, in a
changing climate. Typically, annual harmonics are used to characterize the
variation in the seasonal temperature cycle. However, an often overlooked
feature of the climate seasonal cycle is the semi-annual harmonic, which can
account for a significant portion of the variance of the seasonal cycle and
varies in amplitude and phase across space. Together, the spatial variation in
the annual and semi-annual harmonics can play an important role in driving
processes that are tied to seasonality (e.g., ecological and agricultural
processes). We propose a multivariate spatio-temporal model to quantify the
spatial and temporal change in minimum and maximum temperature seasonal cycles
as a function of the annual and semi-annual harmonics. Our approach captures
spatial dependence, temporal dynamics, and multivariate dependence of these
harmonics through spatially and temporally-varying coefficients. We apply the
model to minimum and maximum temperature over North American for the years 1979
to 2018. Formal model inference within the Bayesian paradigm enables the
identification of regions experiencing significant changes in minimum and
maximum temperature seasonal cycles due to the relative effects of changes in
the two harmonics.",http://arxiv.org/abs/2003.06924v1
"Deep Reinforcement Learning in Electricity Generation Investment for the
  Minimization of Long-Term Carbon Emissions and Electricity Costs",2020-11-02T15:20:49Z,"Alexander J. M. Kell, Pablo Salas, Jean-Francois Mercure, Matthew Forshaw, A. Stephen McGough","A change from a high-carbon emitting electricity power system to one based on
renewables would aid in the mitigation of climate change. Decarbonization of
the electricity grid would allow for low-carbon heating, cooling and transport.
Investments in renewable energy must be made over a long time horizon to
maximise return of investment of these long life power generators. Over these
long time horizons, there exist multiple uncertainties, for example in future
electricity demand and costs to consumers and investors.
  To mitigate for imperfect information of the future, we use the deep
deterministic policy gradient (DDPG) deep reinforcement learning approach to
optimize for a low-cost, low-carbon electricity supply using a modified version
of the FTT:Power model. In this work, we model the UK and Ireland electricity
markets. The DDPG algorithm is able to learn the optimum electricity mix
through experience and achieves this between the years of 2017 and 2050. We
find that a change from fossil fuels and nuclear power to renewables, based
upon wind, solar and wave would provide a cheap and low-carbon alternative to
fossil fuels.",http://arxiv.org/abs/2011.02342v1
"Ecology of the cold-adapted species Nebria germari (Coleoptera:
  Carabidae): the role of supraglacial stony debris as refugium during the
  current interglacial period",2020-11-08T10:28:48Z,"Barbara Valle, Roberto Ambrosini, Marco Caccianiga, Mauro Gobbi","In the current scenario of climate change, cold-adapted insects are among the
most threatened organisms in high-altitude habitats of the Alps. Upslope shifts
and changes in phenology are two of the most investigated responses to climate
change, but there is an increasing interest in evaluating the presence of
high-altitude landforms acting as refugia. Nebria germari Heer, 1837
(Coleoptera: Carabidae) is a hygrophilic and cold-adapted species that still
exhibits large populations on supraglacial debris of the Eastern Alps. This
work aims at describing the ecology and phenology of the populations living on
supraglacial debris. To this end, we analysed the populations from three
Dolomitic glaciers whose surfaces are partially covered by stony debris. We
found that supraglacial debris is characterised by more stable colder and
wetter conditions than the surrounding debris slopes and by a shorter snow-free
period. The populations found on supraglacial debris were spring breeders,
differently from those documented in the 1980s on Dolomitic high alpine
grasslands, which were reported as autumn breeders. Currently Nebria germari
seems therefore to find a suitable habitat on supraglacial debris, where
micrometeorological conditions are appropriate for its life-cycle and
competition and predation are reduced.",http://arxiv.org/abs/2011.03946v2
"Global Sensitivity and Domain-Selective Testing for Functional-Valued
  Responses: An Application to Climate Economy Models",2020-06-24T16:25:44Z,"Matteo Fontana, Massimo Tavoni, Simone Vantini","Understanding the dynamics and evolution of climate change and associated
uncertainties is key for designing robust policy actions. Computer models are
key tools in this scientific effort, which have now reached a high level of
sophistication and complexity. Model auditing is needed in order to better
understand their results, and to deal with the fact that such models are
increasingly opaque with respect to their inner workings. Current techniques
such as Global Sensitivity Analysis (GSA) are limited to dealing either with
multivariate outputs, stochastic ones, or finite-change inputs. This limits
their applicability to time-varying variables such as future pathways of
greenhouse gases. To provide additional semantics in the analysis of a model
ensemble, we provide an extension of GSA methodologies tackling the case of
stochastic functional outputs with finite change inputs. To deal with finite
change inputs and functional outputs, we propose an extension of currently
available GSA methodologies while we deal with the stochastic part by
introducing a novel, domain-selective inferential technique for sensitivity
indices. Our method is explored via a simulation study that shows its
robustness and efficacy in detecting sensitivity patterns. We apply it to real
world data, where its capabilities can provide to practitioners and
policymakers additional information about the time dynamics of sensitivity
patterns, as well as information about robustness.",http://arxiv.org/abs/2006.13850v4
"Predicting Playa Inundation Using a Long Short-Term Memory Neural
  Network",2020-10-16T19:52:32Z,"Kylen Solvik, Anne M. Bartuszevige, Meghan Bogaerts, Maxwell B. Joseph","In the Great Plains, playas are critical wetland habitats for migratory birds
and a source of recharge for the agriculturally-important High Plains aquifer.
The temporary wetlands exhibit complex hydrology, filling rapidly via local
rain storms and then drying through evaporation and groundwater infiltration.
Using a long short-term memory (LSTM) neural network to account for these
complex processes, we modeled playa inundation for 71,842 playas in the Great
Plains from 1984-2018. At the level of individual playas, the model achieved an
F1-score of 0.538 on a withheld test set, displaying the ability to predict
complex inundation patterns. When averaging over all the playas in the entire
region, the model is able to very closely track inundation trends, even during
periods of drought. Our results demonstrate potential for using LSTMs to model
complex hydrological dynamics. Our modeling approach could be used to model
playa inundation into the future under different climate scenarios to better
understand how wetland habitats and groundwater will be impacted by changing
climate.",http://arxiv.org/abs/2010.08605v1
"Street to Cloud: Improving Flood Maps With Crowdsourcing and Semantic
  Segmentation",2020-11-05T16:36:58Z,"Veda Sunkara, Matthew Purri, Bertrand Le Saux, Jennifer Adams","To address the mounting destruction caused by floods in climate-vulnerable
regions, we propose Street to Cloud, a machine learning pipeline for
incorporating crowdsourced ground truth data into the segmentation of satellite
imagery of floods. We propose this approach as a solution to the
labor-intensive task of generating high-quality, hand-labeled training data,
and demonstrate successes and failures of different plausible crowdsourcing
approaches in our model. Street to Cloud leverages community reporting and
machine learning to generate novel, near-real time insights into the extent of
floods to be used for emergency response.",http://arxiv.org/abs/2011.08010v1
Mega Regional Heat Patterns in US Urban Corridors,2020-11-25T21:24:32Z,"Babak Jalalzadeh Fard, Udit Bhatia, Auroop R. Ganguly","Current literature suggests that urban heat-islands and their consequences
are intensifying under climate change and urbanization. Here we explore the
relatively unexplored hypothesis that emerging urban corridors (UCs) spawn
megaregions of intense heat which are evident from observations. A delineation
of the eleven United States UCs relative to their underlying climatological
regions (non-UCs) suggest a surprisingly mixed trend. Medians and trends of
winter temperatures over the last 60-years are generally higher in the UCs but
no such general trends are observed in the summer. Heat wave metrics related to
public health, energy demand and relative intensity do not exhibit
significantly higher overall trends. Temperature and heat wave indices in the
UCs exhibit high correlations with each other including across seasons.
Spatiotemporal patterns in population, along with urbanization, agriculture and
elevation, exhibit high (positive or negative) rank correlations with warming
and heatwave intensification. The findings can inform climate adaptation in
megalopolises.",http://arxiv.org/abs/2011.13031v1
"Point-process based Bayesian modeling of space-time structures of forest
  fire occurrences in Mediterranean France",2020-02-19T10:41:40Z,"Thomas Opitz, Florent Bonneu, Edith Gabriel","Due to climate change and human activity, wildfires are expected to become
more frequent and extreme worldwide, causing economic and ecological disasters.
The deployment of preventive measures and operational forecasts can be aided by
stochastic modeling that helps to understand and quantify the mechanisms
governing the occurrence intensity. We here develop a point process framework
for wildfire ignition points observed in the French Mediterranean basin since
1995, and we fit a spatio-temporal log-Gaussian Cox process with monthly
temporal resolution in a Bayesian framework using the integrated nested Laplace
approximation (INLA). Human activity is the main direct cause of wildfires and
is indirectly measured through a number of appropriately defined proxies
related to land-use covariates (urbanization, road network) in our approach,
and we further integrate covariates of climatic and environmental conditions to
explain wildfire occurrences. We include spatial random effects with Mat\'ern
covariance and temporal autoregression at yearly resolution. Two major
methodological challenges are tackled: first, handling and unifying multi-scale
structures in data is achieved through computer-intensive preprocessing steps
with GIS software and kriging techniques; second, INLA-based estimation with
high-dimensional response vectors and latent models is facilitated through
intra-year subsampling, taking into account the occurrence structure of
wildfires.",http://arxiv.org/abs/2002.12318v1
"From climate change to pandemics: decision science can help scientists
  have impact",2020-07-27T00:51:31Z,"Christopher M. Baker, Patricia T. Campbell, Iadine Chades, Angela J. Dean, Susan M. Hester, Matthew H. Holden, James M. McCaw, Jodie McVernon, Robert Moss, Freya M. Shearer, Hugh P. Possingham","Scientific knowledge and advances are a cornerstone of modern society. They
improve our understanding of the world we live in and help us navigate global
challenges including emerging infectious diseases, climate change and the
biodiversity crisis. For any scientist, whether they work primarily in
fundamental knowledge generation or in the applied sciences, it is important to
understand how science fits into a decision-making framework. Decision science
is a field that aims to pinpoint evidence-based management strategies. It
provides a framework for scientists to directly impact decisions or to
understand how their work will fit into a decision process. Decision science is
more than undertaking targeted and relevant scientific research or providing
tools to assist policy makers; it is an approach to problem formulation,
bringing together mathematical modelling, stakeholder values and logistical
constraints to support decision making. In this paper we describe decision
science, its use in different contexts, and highlight current gaps in
methodology and application. The COVID-19 pandemic has thrust mathematical
models into the public spotlight, but it is one of innumerable examples in
which modelling informs decision making. Other examples include models of storm
systems (eg. cyclones, hurricanes) and climate change. Although the decision
timescale in these examples differs enormously (from hours to decades), the
underlying decision science approach is common across all problems. Bridging
communication gaps between different groups is one of the greatest challenges
for scientists. However, by better understanding and engaging with the
decision-making processes, scientists will have greater impact and make
stronger contributions to important societal problems.",http://arxiv.org/abs/2007.13261v2
Predicting the ecological outcomes of global consumption,2020-03-09T16:12:16Z,"Payal Bal, Simon Kapitza, Natasha Cadenhead, Tom Kompas, Pham Van Ha, Brendan Wintle","Mapping pathways to achieving the sustainable development goals requires
understanding and predicting how social, economic and political factors impact
biodiversity. Trends in demography, economic growth, regional alliances and
consumption behaviours can have profound effects on the environment by driving
resource use and production. While these distant socio-economic drivers impact
species and ecosystems at global scales, for example by driving greenhouse gas
emissions and climate change, the most prevalent human impacts on biodiversity
manifest through habitat loss and land use change decisions at finer scales. We
provide the first integrated ecological-economic analysis pathway capable of
supporting both national policy design challenges and global scale assessment
of biodiversity risks posed by socio-economic drivers such as population
growth, consumption and trade. To achieve this, we provide state-of-the-art
integration of economic, land use, and biodiversity modelling, and illustrate
its application using two case studies. We evaluate the national-level
implications of change in trading conditions under a multi-lateral free trade
agreement for the bird biodiversity of Vietnam. We review the implications for
land-use and biodiversity under coupled socio-economic (Shared Socioeconomic
Pathways) and climate (Resource Concentration Pathways) scenarios for
Australia. Our study provides a roadmap for setting up high dimensional
integrated analyses foe evaluating global priorities for protecting nature and
livelihoods in vulnerable areas with the greatest conflicts for economic,
social and environmental opportunities.",http://arxiv.org/abs/2003.04231v1
"Multicaloric effects in Metamagnetic Heusler Ni-Mn-In under uniaxial
  stress and magnetic field",2020-10-22T08:13:34Z,"Adri√† Gr√†cia-Condal, Tino Gottschall, Lukas Pfeuffer, Oliver Gutfleisch, Antoni Planes, Llu√≠s Ma√±osa","The world's growing hunger for artificial cold on the one hand, and the ever
more stringent climate targets on the other, pose an enormous challenge to
mankind. Novel, efficient and environmentally friendly refrigeration
technologies based on solid-state refrigerants can offer a way out of the
problems arising from climate-damaging substances used in conventional
vapor-compressors. Multicaloric materials stand out because of their large
temperature changes which can be induced by the application of different
external stimuli such as a magnetic, electric, or a mechanical field. Despite
the high potential for applications and the interesting physics of this group
of materials, only few studies focus on their investigation by direct methods.
In this paper, we report on the advanced characterization of all relevant
physical quantities that determine the multicaloric effect of a Ni-Mn-In
Heusler compound. We have used a purpose-designed calorimeter to determine the
isothermal entropy and adiabatic temperature changes resulting from the
combined action of magnetic field and uniaxial stress on this metamagnetic
shape-memory alloy. From these results, we can conclude that the multicaloric
response of this alloy by appropriate changes of uniaxial stress and magnetic
field largely outperforms the caloric response of the alloy when subjected to
only a single stimulus. We anticipate that our findings can be applied to other
multicaloric materials, thus inspiring the development of refrigeration devices
based on the multicaloric effect.",http://arxiv.org/abs/2010.11511v1
"The Earths long-term climate changes and ice ages: a derivation of
  Milankovitch cycles from first principles",2020-11-08T14:05:08Z,R. C. T. Rainey,"Long-term changes in the tilt of the Earths axis, relative to the plane of
its orbit, are of great significance to long-term climate change, because they
control the size of the arctic and antarctic circles. These Milankovitch cycles
have generally been calculated by numerical integration of Newtons equations of
motion, and there is some controversy over the results because they are
sensitive to numerical drift over the very long computer simulations involved.
In this paper the cycles are calculated from first principles, without any
reliance on computer simulation. The problem is one of planetary precession,
and is solvable by the methods used to study the precession of a spinning top.
It is shown that the main component of Milankovitch cycles has a period of
41,000 years and is due to one of the modes of precession of the Earth-Venus
system. The other mode of this system produces a component of period 29,500
years, and a third component of period 54,000 years results from the influence
of the precession of the orbits of Jupiter and Saturn.
  These results agree closely with several of the numerical simulations in the
literature, and strongly suggest that other different results in the literature
are incorrect.",http://arxiv.org/abs/2011.03985v3
"EarthNet2021: A novel large-scale dataset and challenge for forecasting
  localized climate impacts",2020-12-11T11:21:00Z,"Christian Requena-Mesa, Vitus Benson, Joachim Denzler, Jakob Runge, Markus Reichstein","Climate change is global, yet its concrete impacts can strongly vary between
different locations in the same region. Seasonal weather forecasts currently
operate at the mesoscale (> 1 km). For more targeted mitigation and adaptation,
modelling impacts to < 100 m is needed. Yet, the relationship between driving
variables and Earth's surface at such local scales remains unresolved by
current physical models. Large Earth observation datasets now enable us to
create machine learning models capable of translating coarse weather
information into high-resolution Earth surface forecasts. Here, we define
high-resolution Earth surface forecasting as video prediction of satellite
imagery conditional on mesoscale weather forecasts. Video prediction has been
tackled with deep learning models. Developing such models requires
analysis-ready datasets. We introduce EarthNet2021, a new, curated dataset
containing target spatio-temporal Sentinel 2 satellite imagery at 20 m
resolution, matched with high-resolution topography and mesoscale (1.28 km)
weather variables. With over 32000 samples it is suitable for training deep
neural networks. Comparing multiple Earth surface forecasts is not trivial.
Hence, we define the EarthNetScore, a novel ranking criterion for models
forecasting Earth surface reflectance. For model intercomparison we frame
EarthNet2021 as a challenge with four tracks based on different test sets.
These allow evaluation of model validity and robustness as well as model
applicability to extreme events and the complete annual vegetation cycle. In
addition to forecasting directly observable weather impacts through
satellite-derived vegetation indices, capable Earth surface models will enable
downstream applications such as crop yield prediction, forest health
assessments, coastline management, or biodiversity monitoring. Find data, code,
and how to participate at www.earthnet.tech .",http://arxiv.org/abs/2012.06246v1
Advancing Renewable Electricity Consumption With Reinforcement Learning,2020-03-09T20:57:58Z,Filip Tolovski,"As the share of renewable energy sources in the present electric energy mix
rises, their intermittence proves to be the biggest challenge to carbon free
electricity generation. To address this challenge, we propose an electricity
pricing agent, which sends price signals to the customers and contributes to
shifting the customer demand to periods of high renewable energy generation. We
propose an implementation of a pricing agent with a reinforcement learning
approach where the environment is represented by the customers, the electricity
generation utilities and the weather conditions.",http://arxiv.org/abs/2003.04310v1
"Satellite-based Prediction of Forage Conditions for Livestock in
  Northern Kenya",2020-04-08T16:03:50Z,"Andrew Hobbs, Stacey Svetlichnaya","This paper introduces the first dataset of satellite images labeled with
forage quality by on-the-ground experts and provides proof of concept for
applying computer vision methods to index-based drought insurance. We also
present the results of a collaborative benchmark tool used to crowdsource an
accurate machine learning model on the dataset. Our methods significantly
outperform the existing technology for an insurance program in Northern Kenya,
suggesting that a computer vision-based approach could substantially benefit
pastoralists, whose exposure to droughts is severe and worsening with climate
change.",http://arxiv.org/abs/2004.04081v2
An Overview of Microinverter Design Characteristics and MPPT Control,2020-09-13T18:09:14Z,"Sean Ritson, Ahmad Elkhateb","Micro-inverter technologies are becoming increasingly popular as a choice of
grid connection for small-scale photovoltaic systems. Efficiently harvesting
the maximum energy from a photovoltaic system reduces the Levelized cost for
solar energy, enhancing its role in combatting climate change. Various
topologies are proposed through research and have been summarised in this
paper. Furthermore, this paper investigates two popular Maximum Power Point
Tracking (MPPT) methods through simulation using Matlab Simulink.",http://arxiv.org/abs/2009.06055v1
"Environmental conditions and human activity nexus. The case of Northern
  Italy during COVID-19 lockdown",2020-10-15T13:00:10Z,"Sebastian Raimondo, Barbara Benigni, Manlio De Domenico","During COVID-19, draconian countermeasures forbidding non-essential human
activities have been adopted worldwide, providing an unprecedented setup for
testing sustainability policies. We unravel causal relationships among 16
environmental conditions and human activity variables and argue that, despite a
measurable decrease in NO2 concentration due to human activities, locking down
a region is insufficient to significantly reduce emissions. Policy strategies
more effective than lockdowns must be considered for pollution control and
climate change mitigation.",http://arxiv.org/abs/2010.07721v2
"Spreading and vanishing for a monostable reaction-diffusion equation
  with forced speed",2020-04-23T09:35:42Z,"Juliette Bouhours, Thomas Giletti","Invasion phenomena for heterogeneous reaction-diffusion equations are
contemporary and challenging questions in applied mathematics. In this paper we
are interested in the question of spreading for a reaction-diffusion equation
when the subdomain where the reaction term is positive is shifting/contracting
at a given speed $c$. This problem arises in particular in the modelling of the
impact of climate change on population dynamics. By placing ourselves in the
appropriate moving frame, this leads us to consider a
reaction-diffusion-advection equation with a heterogeneous in space reaction
term, in dimension $N\geq1$. We investigate the behaviour of the solution $u$
depending on the value of the advection constant~$c$, which typically stands
for the velocity of climate change. We find that, when the initial datum is
compactly supported, there exists precisely three ranges for $c$ leading to
drastically different situations. In the lower speed range the solution always
spreads, while in the upper range it always vanishes. More surprisingly, we
find that that both spreading and vanishing may occur in an intermediate speed
range. The threshold between those two outcomes is always sharp, both with
respect to $c$ and to the initial condition. We also briefly consider the case
of an exponentially decreasing initial condition, where we relate the
decreasing rate of the initial condition with the range of values of~$c$ such
that spreading occurs.",http://arxiv.org/abs/2004.11042v1
"Modeling Non-Stationary Temperature Maxima Based on Extremal Dependence
  Changing with Event Magnitude",2020-06-02T12:40:44Z,"Peng Zhong, Rapha√´l Huser, Thomas Opitz","The modeling of spatio-temporal trends in temperature extremes can help
better understand the structure and frequency of heatwaves in a changing
climate. Here, we study annual temperature maxima over Southern Europe using a
century-spanning dataset observed at 44 monitoring stations. Extending the
spectral representation of max-stable processes, our modeling framework relies
on a novel construction of max-infinitely divisible processes, which include
covariates to capture spatio-temporal non-stationarities. Our new model keeps a
popular max-stable process on the boundary of the parameter space, while
flexibly capturing weakening extremal dependence at increasing quantile levels
and asymptotic independence. This is achieved by linking the overall magnitude
of a spatial event to its spatial correlation range, in such a way that more
extreme events become less spatially dependent, thus more localized. Our model
reveals salient features of the spatio-temporal variability of European
temperature extremes, and it clearly outperforms natural alternative models.
Results show that the spatial extent of heatwaves is smaller for more severe
events at higher altitudes, and that recent heatwaves are moderately wider. Our
probabilistic assessment of the 2019 annual maxima confirms the severity of the
2019 heatwaves both spatially and at individual sites, especially when compared
to climatic conditions prevailing in 1950-1975.",http://arxiv.org/abs/2006.01569v2
"Past production constrains current energy demands: persistent scaling in
  global energy consumption and implications for climate change mitigation",2020-06-05T22:22:12Z,"Timothy J. Garrett, Matheus R. Grasselli, Stephen Keen","Climate change has become intertwined with the global economy. Here, we
describe the importance of inertia to continued growth in energy consumption.
Drawing from thermodynamic arguments, and using 38 years of available
statistics between 1980 to 2017, we find a persistent time-independent scaling
between the historical time integral $W$ of world inflation-adjusted economic
production $Y$, or $W\left(t\right) = \int_0^t Y\left(t'\right)dt'$, and
current rates of world primary energy consumption $\mathcal E$, such that
$\lambda = \mathcal{E}/W = 5.9\pm0.1$ Gigawatts per trillion 2010 US dollars.
This empirical result implies that population expansion is a symptom rather
than a cause of the current exponential rise in $\mathcal E$ and carbon dioxide
emissions $C$, and that it is past innovation of economic production efficiency
$Y/\mathcal{E}$ that has been the primary driver of growth, at predicted rates
that agree well with data. Options for stabilizing $C$ are then limited to
rapid decarbonization of $\mathcal E$ through sustained implementation of over
one Gigawatt of renewable or nuclear power capacity per day. Alternatively,
assuming continued reliance on fossil fuels, civilization could shift to a
steady-state economy that devotes economic production exclusively to
maintenance rather than expansion. If this were instituted immediately,
continual energy consumption would still be required, so atmospheric carbon
dioxide concentrations would not balance natural sinks until concentrations
exceeded 500 ppmv, and double pre-industrial levels if the steady-state was
attained by 2030.",http://arxiv.org/abs/2006.03718v1
"RainBench: Towards Global Precipitation Forecasting from Satellite
  Imagery",2020-12-17T15:35:24Z,"Christian Schroeder de Witt, Catherine Tong, Valentina Zantedeschi, Daniele De Martini, Freddie Kalaitzis, Matthew Chantry, Duncan Watson-Parris, Piotr Bilinski","Extreme precipitation events, such as violent rainfall and hail storms,
routinely ravage economies and livelihoods around the developing world. Climate
change further aggravates this issue. Data-driven deep learning approaches
could widen the access to accurate multi-day forecasts, to mitigate against
such events. However, there is currently no benchmark dataset dedicated to the
study of global precipitation forecasts. In this paper, we introduce
\textbf{RainBench}, a new multi-modal benchmark dataset for data-driven
precipitation forecasting. It includes simulated satellite data, a selection of
relevant meteorological data from the ERA5 reanalysis product, and IMERG
precipitation data. We also release \textbf{PyRain}, a library to process large
precipitation datasets efficiently. We present an extensive analysis of our
novel dataset and establish baseline results for two benchmark medium-range
precipitation forecasting tasks. Finally, we discuss existing data-driven
weather forecasting methodologies and suggest future research avenues.",http://arxiv.org/abs/2012.09670v1
"Atmospheric convection plays a key role in the climate of tidally-locked
  terrestrial exoplanets: insights from high-resolution simulations",2020-04-06T21:36:48Z,"Denis E. Sergeev, F. Hugo Lambert, Nathan J. Mayne, Ian A. Boutle, James Manners, Krisztian Kohary","Using a 3D general circulation model (GCM), we investigate the sensitivity of
the climate of tidally-locked Earth-like exoplanets, Trappist-1e and Proxima
Centauri b, to the choice of a convection parameterization. Compared to a
mass-flux convection parameterization, a simplified convection adjustment
parameterization leads to a $>$60% decrease of the cloud albedo, increasing the
mean day-side temperature by $\approx$10 K. The representation of convection
also affects the atmospheric conditions of the night side, via a change in
planetary-scale wave patterns. As a result, using the convection adjustment
scheme makes the night-side cold traps warmer by 17-36 K for the planets in our
simulations. The day-night thermal contrast is sensitive to the representation
of convection in 3D GCM simulations, so caution should be taken when
interpreting emission phase curves. The choice of convection treatment,
however, does not alter the simulated climate enough to result in a departure
from habitable conditions, at least for the atmospheric composition and
planetary parameters used in our study. The near-surface conditions both in the
Trappist-1e and Proxima b cases remain temperate, allowing for an active water
cycle. We further advance our analysis using high-resolution model experiments,
in which atmospheric convection is simulated explicitly. Our results suggest
that in a hypothetical global convection-permitting simulation the surface
temperature contrast would be higher than in the coarse-resolution simulations
with parameterized convection. In other words, models with parameterized
convection may overestimate the inter-hemispheric heat redistribution
efficiency.",http://arxiv.org/abs/2004.03007v1
"Machine learning methods for the detection of polar lows in satellite
  mosaics: major issues and their solutions",2020-11-09T22:43:05Z,"Mikhail Krinitskiy, Polina Verezemskaya, Svyatoslav Elizarov, Sergey Gulev","Polar mesocyclones (PMCs) and their intense subclass polar lows (PLs) are
relatively small atmospheric vortices that form mostly over the ocean in high
latitudes. PLs can strongly influence deep ocean water formation since they are
associated with strong surface winds and heat fluxes. Detection and tracking of
PLs are crucial for understanding the climatological dynamics of PLs and for
the analysis of their impacts on other components of the climatic system. At
the same time, visual tracking of PLs is a highly time-consuming procedure that
requires expert knowledge and extensive examination of source data.
  There are known procedures involving deep convolutional neural networks
(DCNNs) for the detection of large-scale atmospheric phenomena in reanalysis
data that demonstrate a high quality of detection. However, one cannot apply
these procedures to satellite data directly since, unlike reanalyses, satellite
products register all the scales of atmospheric vortices. It is also known that
DCNNs were originally designed to be scale-invariant. This leads to the problem
of filtering the scale of detected phenomena. There are other problems to be
solved, such as a low signal-to-noise ratio of satellite data and an unbalanced
number of negative (without PLs) and positive (where a PL is presented) classes
in a satellite dataset.
  In our study, we propose a deep learning approach for the detection of PLs
and PMCs in remote sensing data, which addresses class imbalance and scale
filtering problems. We also outline potential solutions for other problems,
along with promising improvements to the presented approach.",http://arxiv.org/abs/2011.04811v1
"Life Cycle Assessment of high rate algal ponds for wastewater treatment
  and resource recovery",2020-03-13T10:46:43Z,"Larissa Terumi Arashiro, Neus Montero, Ivet Ferrer, Francisco Gabriel Acien, Cintia Gomez, Marianna Garfi","The aim of this study was to assess the potential environmental impacts
associated with high rate algal ponds (HRAP) systems for wastewater treatment
and resource recovery in small communities. To this aim, a Life Cycle
Assessment (LCA) and an economic assessment were carried out evaluating two
alternatives: i) a HRAPs system for wastewater treatment where microalgal
biomass is valorised for energy recovery (biogas production); ii) a HRAPs
system for wastewater treatment where microalgal biomass is reused for
nutrients recovery (biofertiliser production). Additionally, both alternatives
were compared to a typical small-sized activated sludge system. The results
showed that HRAPs system coupled with biogas production appeared to be more
environmentally friendly than HRAPs system coupled with biofertiliser
production in the climate change, ozone layer depletion, photochemical oxidant
formation, and fossil depletion impact categories. Different climatic
conditions have strongly influenced the results obtained in the eutrophication
and metal depletion impact categories, with the HRAPs system located where warm
temperatures and high solar radiation are predominant showing lower impact. In
terms of costs, HRAPs systems seemed to be more economically feasible when
combined with biofertiliser production instead of biogas.",http://arxiv.org/abs/2003.06194v1
"The effect of high nitrogen pressures on the habitable zone and an
  appraisal of greenhouse states",2020-04-01T04:36:37Z,Ramses M. Ramirez,"The habitable zone is the main tool that mission architectures utilize to
select potentially habitable planets for follow up spectroscopic observation.
Given its importance, the precise size and location of the habitable zone
remains a hot topic, as many studies, using a hierarchy of models, have
assessed various factors including: atmospheric composition, time, and
planetary mass. However, little work has assessed how the habitable zone
changes with variations in background nitrogen pressure, which is directly
connected to the habitability and life bearing potential of planets. Here, I
use an advanced energy balance model with clouds to show that our solar system
habitable zone is about 0.9 to 1.7 AU, assuming a 5 bar nitrogen background
pressure and a maximum 100 percent cloud cover at the inner edge. This width is
about 20 percent wider than the conservative habitable zone estimate. Similar
extensions are calculated for A to M stars. I also show that cooling clouds and
hazes and high background pressures can decrease the runaway greenhouse
threshold temperature to approximately 300 K (or less) for planets orbiting any
star type. This is because the associated increase in planetary albedo enables
stable climates closer to the star, where rapid destabilization can be
triggered from a lower mean surface temperature. Enhanced longwave emission for
planets with very high stratospheric temperatures also permits stable climates
at smaller orbital distances. The model predicts a runaway greenhouse above
approximately 330 K for planets orbiting the Sun, which is consistent with
previous work. However, moist greenhouses only occur for planets orbiting
A-stars.",http://arxiv.org/abs/2004.00229v1
"""Perchance to dream?"": Assessing effect of dispersal strategies on the
  fitness of expanding populations",2020-04-17T12:40:58Z,"Nikolay Markov, Evgeny Ivanko","Unraveling patterns of animals' movements is important for understanding the
fundamental basics of biogeography, tracking range shifts resulting from
climate change, predicting and preventing biological invansions. Many
researchers have modeled animals' dispersal studying their behavior under the
assumptions of some movement strategies pre-determined or affected by some
external factor(s) but none of them have compared the efficiency of different
dispersal strategies in providing population survival and fitness. We
hypothesize that 1) successful expansion could result from some evolutionary
stable strategy (ESS) and 2) such strategy could be based particularly on
deferred gain, when animals invest in travel to reach some high-quality habitat
(""habitat of dream""). Using simulation model we compare the ecological success
of three strategies: i) ""Smart"" - choosing the locally optimal cell; ii)
""Random"" - random movement between cells without taking into account the
quality of the environment; iii) ""Dreamer"" - movements that aims to find ""a
habitat of dream"" with quality much higher than that of the initial and
neighboring cells. The population fitness was measured as survival rate,
dispersal distance, accumulated energy and quality of settled habitat. The most
general conclusion is that while survival and wealth of the population is
affected presumably by overall habitat quality, the dispersal depends mainly on
the behavioral strategy. The ""Dreamer"" strategy or the strategy of deferred
gain belongs to the Pareto frontier in the Fitness$\times$Dispersal space but
only in optimal and suboptimal habitat and in the relatively mild climate.",http://arxiv.org/abs/2004.08216v1
"NERO: A Near High-Bandwidth Memory Stencil Accelerator for Weather
  Prediction Modeling",2020-09-17T12:46:17Z,"Gagandeep Singh, Dionysios Diamantopoulos, Christoph Hagleitner, Juan Gomez-Luna, Sander Stuijk, Onur Mutlu, Henk Corporaal","Ongoing climate change calls for fast and accurate weather and climate
modeling. However, when solving large-scale weather prediction simulations,
state-of-the-art CPU and GPU implementations suffer from limited performance
and high energy consumption. These implementations are dominated by complex
irregular memory access patterns and low arithmetic intensity that pose
fundamental challenges to acceleration. To overcome these challenges, we
propose and evaluate the use of near-memory acceleration using a reconfigurable
fabric with high-bandwidth memory (HBM). We focus on compound stencils that are
fundamental kernels in weather prediction models. By using high-level synthesis
techniques, we develop NERO, an FPGA+HBM-based accelerator connected through
IBM CAPI2 (Coherent Accelerator Processor Interface) to an IBM POWER9 host
system. Our experimental results show that NERO outperforms a 16-core POWER9
system by 4.2x and 8.3x when running two different compound stencil kernels.
NERO reduces the energy consumption by 22x and 29x for the same two kernels
over the POWER9 system with an energy efficiency of 1.5 GFLOPS/Watt and 17.3
GFLOPS/Watt. We conclude that employing near-memory acceleration solutions for
weather prediction modeling is promising as a means to achieve both high
performance and high energy efficiency.",http://arxiv.org/abs/2009.08241v1
AutoMat: Accelerated Computational Electrochemical systems Discovery,2020-11-03T20:45:29Z,"Emil Annevelink, Rachel Kurchin, Eric Muckley, Lance Kavalsky, Vinay I. Hegde, Valentin Sulzer, Shang Zhu, Jiankun Pu, David Farina, Matthew Johnson, Dhairya Gandhi, Adarsh Dave, Hongyi Lin, Alan Edelman, Bharath Ramsundar, James Saal, Christopher Rackauckas, Viral Shah, Bryce Meredig, Venkatasubramanian Viswanathan","Large-scale electrification is vital to addressing the climate crisis, but
several scientific and technological challenges remain to fully electrify both
the chemical industry and transportation. In both of these areas, new
electrochemical materials will be critical, but their development currently
relies heavily on human-time-intensive experimental trial and error and
computationally expensive first-principles, meso-scale and continuum
simulations. We present an automated workflow, AutoMat, that accelerates these
computational steps by introducing both automated input generation and
management of simulations across scales from first principles to continuum
device modeling. Furthermore, we show how to seamlessly integrate
multi-fidelity predictions such as machine learning surrogates or automated
robotic experiments ""in-the-loop"". The automated framework is implemented with
design space search techniques to dramatically accelerate the overall materials
discovery pipeline by implicitly learning design features that optimize device
performance across several metrics. We discuss the benefits of AutoMat using
examples in electrocatalysis and energy storage and highlight lessons learned.",http://arxiv.org/abs/2011.04426v4
Ambient heat and human sleep,2020-11-13T23:04:42Z,"Kelton Minor, Andreas Bjerre-Nielsen, Sigga Svala Jonasdottir, Sune Lehmann, Nick Obradovich","Ambient temperatures are rising globally, with the greatest increases
recorded at night. Concurrently, the prevalence of insufficient sleep is
increasing in many populations, with substantial costs to human health and
well-being. Even though nearly a third of the human lifespan is spent asleep,
it remains unknown whether temperature and weather impact objective measures of
sleep in real-world settings, globally. Here we link billions of sleep
measurements from wearable devices comprising over 7 million nighttime sleep
records across 68 countries to local daily meteorological data from 2015 to
2017. Rising nighttime temperatures shorten within-person sleep duration
primarily through delayed onset, increasing the probability of insufficient
sleep. The effect of temperature on sleep loss is substantially larger for
residents from lower income countries and older adults, and females are
affected more than are males. Nighttime temperature increases inflict the
greatest sleep loss during summer and fall months, and we do not find evidence
of short-term acclimatization. Coupling historical behavioral measurements with
output from climate models, we project that climate change will further erode
human sleep, producing substantial geographic inequalities. Our findings have
significant implications for adaptation planning and illuminate a pathway
through which rising temperatures may globally impact public health.",http://arxiv.org/abs/2011.07161v1
Characterization of Industrial Smoke Plumes from Remote Sensing Data,2020-11-23T11:54:32Z,"Michael Mommert, Mario Sigel, Marcel Neuhausler, Linus Scheibenreif, Damian Borth","The major driver of global warming has been identified as the anthropogenic
release of greenhouse gas (GHG) emissions from industrial activities. The
quantitative monitoring of these emissions is mandatory to fully understand
their effect on the Earth's climate and to enforce emission regulations on a
large scale. In this work, we investigate the possibility to detect and
quantify industrial smoke plumes from globally and freely available multi-band
image data from ESA's Sentinel-2 satellites. Using a modified ResNet-50, we can
detect smoke plumes of different sizes with an accuracy of 94.3%. The model
correctly ignores natural clouds and focuses on those imaging channels that are
related to the spectral absorption from aerosols and water vapor, enabling the
localization of smoke. We exploit this localization ability and train a U-Net
segmentation model on a labeled sub-sample of our data, resulting in an
Intersection-over-Union (IoU) metric of 0.608 and an overall accuracy for the
detection of any smoke plume of 94.0%; on average, our model can reproduce the
area covered by smoke in an image to within 5.6%. The performance of our model
is mostly limited by occasional confusion with surface objects, the inability
to identify semi-transparent smoke, and human limitations to properly identify
smoke based on RGB-only images. Nevertheless, our results enable us to reliably
detect and qualitatively estimate the level of smoke activity in order to
monitor activity in industrial plants across the globe. Our data set and code
base are publicly available.",http://arxiv.org/abs/2011.11344v1
"The Impact of Planetary Rotation Rate on the Reflectance and Thermal
  Emission Spectrum of Terrestrial Exoplanets Around Sun-like Stars",2020-02-06T23:06:15Z,"Scott D. Guzewich, Jacob Lustig-Yaeger, Christopher Evan Davis, Ravi Kumar Kopparapu, Michael J. Way, Victoria S. Meadows","Robust atmospheric and radiative transfer modeling will be required to
properly interpret reflected light and thermal emission spectra of terrestrial
exoplanets. This will help break observational degeneracies between the
numerous atmospheric, planetary, and stellar factors that drive planetary
climate. Here we simulate the climates of Earth-like worlds around the Sun with
increasingly slow rotation periods, from Earth-like to fully Sun-synchronous,
using the ROCKE-3D general circulation model. We then provide these results as
input to the Spectral Planet Model (SPM), which employs the SMART radiative
transfer model to simulate the spectra of a planet as it would be observed from
a future space-based telescope. We find that the primary observable effects of
slowing planetary rotation rate are the altered cloud distributions, altitudes,
and opacities which subsequently drive many changes to the spectra by altering
the absorption band depths of biologically-relevant gas species (e.g., H2O, O2,
and O3). We also identify a potentially diagnostic feature of synchronously
rotating worlds in mid-infrared H2O absorption/emission lines.",http://arxiv.org/abs/2002.02549v3
Modelling the 3D Climate of Venus with OASIS,2020-02-21T19:05:35Z,"Jo√£o M. Mendon√ßa, Lars A. Buchhave","Flexible 3D models to explore the vast diversity of terrestrial planets and
interpret observational data are still in their early stages. In this work, we
present OASIS: a novel and flexible 3D virtual planet laboratory. With OASIS we
envision a platform that couples self-consistently seven individual modules
representing the main physical and chemical processes that shape planetary
environments. Additionally, OASIS is capable of producing simulated spectra
from different instruments and observational techniques. In this work we focus
on the benchmark test of coupling four of the physical modules: fluid dynamics,
radiation, turbulence and surface/soil. To test the OASIS platform, we produced
3D simulations of the Venus climate and its atmospheric circulation and study
how the modeled atmosphere changes with various cloud covers, atmospheric heat
capacity, and surface friction. 3D simulations of Venus are challenging because
they require long integration times with a computationally expensive radiative
transfer code. By comparing OASIS results with observational data, we verify
that the new model is able to successfully simulate Venus. With simulated
spectra produced directly from the 3D simulations, we explore the capabilities
of future missions, like LUVOIR, to observe Venus analogs located at a distance
of 10 pc. With OASIS, we have taken the first steps to build a sophisticated
and very flexible platform capable of studying the environment of terrestrial
planets, which will be an essential tool to characterize observed terrestrial
planets and plan future observations.",http://arxiv.org/abs/2002.09506v3
"Integration of max-stable processes and Bayesian model averaging to
  predict extreme climatic events in multi-model ensembles",2020-07-19T17:30:35Z,"Yonggwan Shin, Youngsaeng Lee, Juntae Choi, Jeong-Soo Park","Projections of changes in extreme climate are sometimes predicted by using
multi-model ensemble methods such as Bayesian model averaging (BMA) embedded
with the generalized extreme value (GEV) distribution. BMA is a popular method
for combining the forecasts of individual simulation models by weighted
averaging and characterizing the uncertainty induced by simulating the model
structure. This method is referred to as the GEV-embedded BMA. It is, however,
based on a point-wise analysis of extreme events, which means it overlooks the
spatial dependency between nearby grid cells. Instead of a point-wise model, a
spatial extreme model such as the max-stable process (MSP) is often employed to
improve precision by considering spatial dependency. We propose an approach
that integrates the MSP into BMA, which is referred to as the MSP-BMA herein.
The superiority of the proposed method over the GEV-embedded BMA is
demonstrated by using extreme rainfall intensity data on the Korean peninsula
from Coupled Model Intercomparison Project Phase 5 (CMIP5) multi-models. The
reanalysis data called APHRODITE (Asian Precipitation Highly-Resolved
Observational Data Integration Towards Evaluation, v1101) and 17 CMIP5 models
are examined for 10 grid boxes in Korea. In this example, the MSP-BMA achieves
a variance reduction over the GEV-embedded BMA. The bias inflation by MSP-BMA
over the GEV-embedded BMA is also discussed. A by-product technical advantage
of the MSP-BMA is that tedious `regridding' is not required before and after
the analysis while it should be done for the GEV-embedded BMA.",http://arxiv.org/abs/2007.09726v1
"Interpretable, Multidimensional, Multimodal Anomaly Detection with
  Negative Sampling for Detection of Device Failure",2020-07-12T21:07:34Z,John Sipple,"Complex devices are connected daily and eagerly generate vast streams of
multidimensional state measurements. These devices often operate in distinct
modes based on external conditions (day/night, occupied/vacant, etc.), and to
prevent complete or partial system outage, we would like to recognize as early
as possible when these devices begin to operate outside the normal modes.
Unfortunately, it is often impractical or impossible to predict failures using
rules or supervised machine learning, because failure modes are too complex,
devices are too new to adequately characterize in a specific environment, or
environmental change puts the device into an unpredictable condition. We
propose an unsupervised anomaly detection method that creates a negative sample
from the positive, observed sample, and trains a classifier to distinguish
between positive and negative samples. Using the Contraction Principle, we
explain why such a classifier ought to establish suitable decision boundaries
between normal and anomalous regions, and show how Integrated Gradients can
attribute the anomaly to specific variables within the anomalous state vector.
We have demonstrated that negative sampling with random forest or neural
network classifiers yield significantly higher AUC scores than Isolation
Forest, One Class SVM, and Deep SVDD, against (a) a synthetic dataset with
dimensionality ranging between 2 and 128, with 1, 2, and 3 modes, and with and
without noise dimensions; (b) four standard benchmark datasets; and (c) a
multidimensional, multimodal dataset from real climate control devices.
Finally, we describe how negative sampling with neural network classifiers have
been successfully deployed at large scale to predict failures in real time in
over 15,000 climate-control and power meter devices in 145 Google office
buildings.",http://arxiv.org/abs/2007.10088v1
"TRU-NET: A Deep Learning Approach to High Resolution Prediction of
  Rainfall",2020-08-20T17:27:59Z,"Rilwan Adewoyin, Peter Dueben, Peter Watson, Yulan He, Ritabrata Dutta","Climate models (CM) are used to evaluate the impact of climate change on the
risk of floods and strong precipitation events. However, these numerical
simulators have difficulties representing precipitation events accurately,
mainly due to limited spatial resolution when simulating multi-scale dynamics
in the atmosphere. To improve the prediction of high resolution precipitation
we apply a Deep Learning (DL) approach using an input of CM simulations of the
model fields (weather variables) that are more predictable than local
precipitation. To this end, we present TRU-NET (Temporal Recurrent U-Net), an
encoder-decoder model featuring a novel 2D cross attention mechanism between
contiguous convolutional-recurrent layers to effectively model multi-scale
spatio-temporal weather processes. We use a conditional-continuous loss
function to capture the zero-skewed %extreme event patterns of rainfall.
Experiments show that our model consistently attains lower RMSE and MAE scores
than a DL model prevalent in short term precipitation prediction and improves
upon the rainfall predictions of a state-of-the-art dynamical weather model.
Moreover, by evaluating the performance of our model under various, training
and testing, data formulation strategies, we show that there is enough data for
our deep learning approach to output robust, high-quality results across
seasons and varying regions.",http://arxiv.org/abs/2008.09090v2
"Investigating Differences in Crowdsourced News Credibility Assessment:
  Raters, Tasks, and Expert Criteria",2020-08-21T15:19:18Z,"Md Momen Bhuiyan, Amy X. Zhang, Connie Moon Sehat, Tanushree Mitra","Misinformation about critical issues such as climate change and vaccine
safety is oftentimes amplified on online social and search platforms. The
crowdsourcing of content credibility assessment by laypeople has been proposed
as one strategy to combat misinformation by attempting to replicate the
assessments of experts at scale. In this work, we investigate news credibility
assessments by crowds versus experts to understand when and how ratings between
them differ. We gather a dataset of over 4,000 credibility assessments taken
from 2 crowd groups---journalism students and Upwork workers---as well as 2
expert groups---journalists and scientists---on a varied set of 50 news
articles related to climate science, a topic with widespread disconnect between
public opinion and expert consensus. Examining the ratings, we find differences
in performance due to the makeup of the crowd, such as rater demographics and
political leaning, as well as the scope of the tasks that the crowd is assigned
to rate, such as the genre of the article and partisanship of the publication.
Finally, we find differences between expert assessments due to differing expert
criteria that journalism versus science experts use---differences that may
contribute to crowd discrepancies, but that also suggest a way to reduce the
gap by designing crowd tasks tailored to specific expert criteria. From these
findings, we outline future research directions to better design crowd
processes that are tailored to specific crowds and types of content.",http://arxiv.org/abs/2008.09533v1
Investigating Ground-level Ozone Formation: A Case Study in Taiwan,2020-12-18T05:41:42Z,"Yu-Wen Chen, Sourav Medya, Yi-Chun Chen","Tropospheric ozone (O3) is a greenhouse gas which can absorb heat and make
the weather even hotter during extreme heatwaves. Besides, it is an influential
ground-level air pollutant which can severely damage the environment. Thus
evaluating the importance of various factors related to the O3 formation
process is essential. However, O3 simulated by the available climate models
exhibits large variance in different places, indicating the insufficiency of
models in explaining the O3 formation process correctly. In this paper, we aim
to identify and understand the impact of various factors on O3 formation and
predict the O3 concentrations under different pollution-reduced and climate
change scenarios. We employ six supervised methods to estimate the observed O3
using fourteen meteorological and chemical variables. We find that the deep
neural network (DNN) and long short-term memory (LSTM) based models can predict
O3 concentrations accurately. We also demonstrate the importance of several
variables in this prediction task. The results suggest that while Nitrogen
Oxides negatively contributes to predicting O3, solar radiation makes a
significantly positive contribution. Furthermore, we apply our two best models
on O3 prediction under different global warming and pollution reduction
scenarios to improve the policy-making decisions in the O3 reduction.",http://arxiv.org/abs/2012.10058v3
"Impact of solar magnetic field amplitude and geometry on cosmic rays
  diffusion coefficients in the inner heliosphere",2020-10-05T09:29:09Z,"Barbara Perri, Allan Sacha Brun, Antoine Strugarek, Victor R√©ville","Cosmic rays (CRs) are tracers of solar events when they are associated with
solar flares, but also galactic events when they come from outside our solar
system. SEPs are correlated with the 11-year solar cycle while GCRs are
anti-correlated due to their interaction with the heliospheric magnetic field
and the solar wind. Our aim is to quantify separately the impact of the
amplitude and the geometry of the magnetic field on the propagation of CRs of
various energies in the inner heliosphere. We focus especially on the diffusion
caused by the magnetic field along and across the field lines. To do so, we use
the results of 3D MHD wind simulations running from the lower corona up to 1
AU. The wind is modeled using a polytropic approximation, and fits and power
laws are used to account for the turbulence. Using these results, we compute
the parallel and perpendicular diffusion coefficients of the Parker CR
transport equation, yielding 3D maps of the diffusion of CRs in the inner
heliosphere. By varying the amplitude of the magnetic field, we change the
amplitude of the diffusion by the same factor, and the radial gradients by
changing the spread of the current sheet. By varying the geometry of the
magnetic field, we change the latitudinal gradients of diffusion by changing
the position of the current sheets. By varying the energy, we show that the
distribution of values for SEPs is more peaked than GCRs. For realistic solar
configurations, we show that diffusion is highly non-axisymmetric due to the
configuration of the current sheets, and that the distribution varies a lot
with the distance to the Sun with a drift of the peak value. This study shows
that numerical simulations and theory can help quantify better the influence of
the various magnetic field parameters on the propagation of CRs. This study is
a first step towards generating synthetic CR rates from numerical simulations.",http://arxiv.org/abs/2010.01880v1
Monitoring the Impact of Wildfires on Tree Species with Deep Learning,2020-11-04T19:42:04Z,"Wang Zhou, Levente Klein","One of the impacts of climate change is the difficulty of tree regrowth after
wildfires over areas that traditionally were covered by certain tree species.
Here a deep learning model is customized to classify land covers from four-band
aerial imagery before and after wildfires to study the prolonged consequences
of wildfires on tree species. The tree species labels are generated from
manually delineated maps for five land cover classes: Conifer, Hardwood, Shrub,
ReforestedTree and Barren land. With an accuracy of $92\%$ on the test split,
the model is applied to three wildfires on data from 2009 to 2018. The model
accurately delineates areas damaged by wildfires, changes in tree species and
rebound of burned areas. The result shows clear evidence of wildfires impacting
the local ecosystem and the outlined approach can help monitor reforested
areas, observe changes in forest composition and track wildfire impact on tree
species.",http://arxiv.org/abs/2011.02514v2
Hetero-functional Graph Resilience of the Future American Electric Grid,2020-06-18T17:06:07Z,"Dakota J. Thompson, Wester C. H. Schoonenberg, Amro M. Farid","As climate change takes hold in the 21st century, it places an impetus to
decarbonize the American electric power system with renewable energy resources.
This paper presents a structural resilience analysis of the American electric
power system that incrementally incorporates architectural changes including
meshed distribution lines, distributed generation, and energy storage
solutions. A hetero-functional graph analysis confirms our formal graph
understandings from network science in terms of cumulative degree distributions
and traditional attack vulnerability measures. Additionally, The paper shows
that hetero-functional graphs more precisely describe the changes in
functionality associated with the addition of distributed generation and energy
storage. Finally, it demonstrates that the addition of all three types of
mitigation measures enhance the grid's structural resilience; even in the
presence of disruptive attacks. The paper concludes that there is no structural
trade-off between grid sustainability and resilience.",http://arxiv.org/abs/2006.10678v1
"Evaluation of the cumulated impacts on the marine resource of a
  socio-ecological coral system: approach by agent-based modeling",2020-08-21T14:51:14Z,Olivier Rousselle,"In the context of climate change and significant changes in human activities
around the world, coral reefs are subject to many disruptions. We develop here
a tool to help decision-making in Moorea (French Polynesia), based on
multi-agent modeling. We model the trophic interactions with a Lotka-Volterra
model, and also the interactions between fishermen, trophic groups and tourist
operators. The results are generated through global, temporal (time series),
and spatial (GIS maps) outputs. The model produced here can be transposed to
other ecological and economic situations, and other geographical areas, by
modifying the parameters and changing the input map data.",http://arxiv.org/abs/2008.09521v1
"Space-time clustering of flash floods in a changing climate (China,
  1950-2015)",2020-06-23T08:39:26Z,"Nan Wang, Luigi Lombardo, Marj Tonini, Weiming Cheng, Liang Guo, Junnan Xiong","The persistence over space and time of flash flood disasters -- flash floods
that have caused either economical or life losses, or both -- is a diagnostic
measure of areas subjected to hydrological risk. The concept of persistence can
be assessed via clustering analyses, performed here to analyse the national
inventory of flash floods disasters in China occurred in the period 1950-2015.
Specifically, we investigated the spatio-temporal pattern distribution of the
flash floods and their clustering behavior by using both global and local
methods: the first, based on the Ripley's K-function, and the second on scan
statistics. As a result, we could visualize patterns of aggregated events,
estimate the cluster duration and make assumptions about their evolution over
time, also with respect precipitations trend. Due to the large spatial (the
whole Chinese territory) and temporal scale of the dataset (66 years), we were
able to capture whether certain clusters gather in specific locations and
times, but also whether their magnitude tends to increase or decrease. Overall,
the eastern regions in China are much more subjected to flash floods compared
to the rest of the country. Detected clusters revealed that these phenomena
predominantly occur between July and October, a period coinciding with the wet
season in China. The number of detected clusters increases with time, but the
associated duration drastically decreases in the recent period. This may
indicate a change towards triggering mechanisms which are typical of
short-duration extreme rainfall events. Finally, being flash floods directly
linked to precipitation and their extreme realization, we indirectly assessed
whether the magnitude of the trigger itself has also varied through space and
time, enabling considerations in the context of climatic changes.",http://arxiv.org/abs/2006.16865v1
Saturn's Seasonal Atmosphere at Northern Summer Solstice,2020-12-16T22:12:14Z,"L. N. Fletcher, L. Sromovsky, V. Hue, J. I. Moses, S. Guerlet, R. A. West, T. Koskinen","The incredible longevity of Cassini's orbital mission at Saturn has provided
the most comprehensive exploration of a seasonal giant planet to date. This
review explores Saturn's changing global temperatures, composition, and aerosol
properties between northern spring and summer solstice (2015-2017), extending
our previous review of Cassini's remote sensing investigations (2004-14,
Fletcher et al., 2018) to the grand finale. The result is an unprecedented
record of Saturn's climate that spans almost half a Saturnian year, which can
be used to test the seasonal predictions of radiative climate models, neutral
and ion photochemistry models, and atmospheric circulation models. Hemispheric
asymmetries in tropospheric and stratospheric temperatures were observed to
reverse from northern winter to northern summer; spatial distributions of
hydrocarbons and para-hydrogen shifted in response to atmospheric dynamics
(e.g., seasonally-reversing Hadley cells, polar stratospheric vortex formation,
equatorial stratospheric oscillations, and inter-hemispheric transport); and
upper tropospheric and stratospheric aerosols exhibited changes in optical
thickness that modulated Saturn's visible colours (from blue hues to a golden
appearance in the north near solstice), reflectivity, and near-infrared
emission. Numerical simulations of radiative balance and photochemistry do a
good job in reproducing the observed seasonal change and phase lags, but
discrepancies between models and observations still persist, indicating a
crucial role for atmospheric dynamics and the need to couple chemical and
radiative schemes to the next generation of circulation models. With Cassini's
demise, an extended study of Saturn's seasons, from northern summer to autumn,
will require the capabilities of ground- and space-based observatories, as we
eagerly await the next orbital explorer at Saturn.",http://arxiv.org/abs/2012.09288v3
"Structural Forecasting for Tropical Cyclone Intensity Prediction:
  Providing Insight with Deep Learning",2020-10-07T21:01:06Z,"Trey McNeely, Niccol√≤ Dalmasso, Kimberly M. Wood, Ann B. Lee","Tropical cyclone (TC) intensity forecasts are ultimately issued by human
forecasters. The human in-the-loop pipeline requires that any forecasting
guidance must be easily digestible by TC experts if it is to be adopted at
operational centers like the National Hurricane Center. Our proposed framework
leverages deep learning to provide forecasters with something neither
end-to-end prediction models nor traditional intensity guidance does: a
powerful tool for monitoring high-dimensional time series of key physically
relevant predictors and the means to understand how the predictors relate to
one another and to short-term intensity changes.",http://arxiv.org/abs/2010.05783v3
"AI Chiller: An Open IoT Cloud Based Machine Learning Framework for the
  Energy Saving of Building HVAC System via Big Data Analytics on the Fusion of
  BMS and Environmental Data",2020-10-09T09:51:03Z,Yong Yu,"Energy saving and carbon emission reduction in buildings is one of the key
measures in combating climate change. Heating, Ventilation, and Air
Conditioning (HVAC) system account for the majority of the energy consumption
in the built environment, and among which, the chiller plant constitutes the
top portion. The optimization of chiller system power consumption had been
extensively studied in the mechanical engineering and building service domains.
Many works employ physical models from the domain knowledge. With the advance
of big data and AI, the adoption of machine learning into the optimization
problems becomes popular. Although many research works and projects turn to
this direction for energy saving, the application into the optimization problem
remains a challenging task. This work is targeted to outline a framework for
such problems on how the energy saving should be benchmarked, if holistic or
individually modeling should be used, how the optimization is to be conducted,
why data pattern augmentation at the initial deployment is a must, why the
gradually increasing changes strategy must be used. Results of analysis on
historical data and empirical experiment on live data are presented.",http://arxiv.org/abs/2011.01047v1
Finite Sample Smeariness of Fr√©chet Means and Application to Climate,2020-05-05T16:28:35Z,"Shayan Hundrieser, Benjamin Eltzner, Stephan F. Huckemann","Fr\'echet means on non-Euclidean spaces may exhibit nonstandard asymptotic
rates rendering quantile-based asymptotic inference inapplicable. We show here
that this affects, among others, all circular distributions whose support
exceeds a half circle. We exhaustively describe this phenomenon and introduce a
new concept which we call finite samples smeariness (FSS). In the presence of
FSS, it turns out that quantile-based tests for equality of Fr\'echet means
systematically feature effective levels higher than their nominal level which
perseveres asymptotically in case of Type I FSS. In contrast, suitable
bootstrap-based tests correct for FSS and asymptotically attain the correct
level. For illustration of the relevance of FSS in real data, we apply our
method to directional wind data from two European cities. It turns out that
quantile based tests, not correcting for FSS, find a multitude of significant
wind changes. This multitude condenses to a few years featuring significant
wind changes, when our bootstrap tests are applied, correcting for FSS.",http://arxiv.org/abs/2005.02321v3
"Ship-track-based assessments overestimate the cooling effect of
  anthropogenic aerosol",2020-05-28T17:20:28Z,"Franziska Glassmeier, Fabian Hoffmann, Jill S. Johnson, Takanobu Yamaguchi, Ken S. Carslaw, Graham Feingold","The effect of anthropogenic aerosol on the reflectivity of stratocumulus
cloud decks through changes in cloud amount is a major uncertainty in climate
projections. The focus of this study is the frequently occurring
non-precipitating stratocumulus. In this regime, cloud amount can decrease
through aerosol-enhanced cloud-top mixing. The climatological relevance of this
effect is debated because ship exhaust does not appear to generate significant
change in the amount of these clouds. Through a novel analysis of detailed
numerical simulations in comparison to satellite data, we show that results
from ship-track studies cannot be generalized to estimate the climatological
forcing of anthropogenic aerosol. We specifically find that the
ship-track-derived sensitivity of the radiative effect of non-precipitating
stratocumulus to aerosol overestimates their cooling effect by up to 200%. This
offsetting warming effect needs to be taken into account if we are to constrain
the aerosol-cloud radiative forcing of stratocumulus.",http://arxiv.org/abs/2005.14159v1
Renewable Power Trades and Network Congestion Externalities,2020-05-28T19:23:50Z,"Nayara Aguiar, Indraneel Chakraborty, Vijay Gupta","Integrating renewable energy production into the electricity grid is an
important policy goal to address climate change. However, such an integration
faces economic and technological challenges. As power generation by renewable
sources increases, power transmission patterns over the electric grid change.
Due to physical laws, these new transmission patterns lead to non-intuitive
grid congestion externalities. We derive the conditions under which negative
network externalities due to power trades occur. Calibration using a stylized
framework and data from Europe shows that each additional unit of power traded
between northern and western Europe reduces transmission capacity for the
southern and eastern regions by 27% per unit traded. Such externalities suggest
that new investments in the electric grid infrastructure cannot be made
piecemeal. In our example, power infrastructure investment in northern and
western Europe needs an accompanying investment in southern and eastern Europe
as well. An economic challenge is regions facing externalities do not always
have the financial ability to invest in infrastructure. Power transit fares can
help finance power infrastructure investment in regions facing network
congestion externalities. The resulting investment in the overall electricity
grid facilitates integration of renewable energy production.",http://arxiv.org/abs/2006.00916v2
"Multistability and rare spontaneous transitions in barotropic
  $Œ≤$-plane turbulence",2020-09-18T15:48:02Z,"Eric Simonnet, Joran Rolland, Freddy Bouchet","We demonstrate that turbulent zonal jets, analogous to Jovian ones, which are
quasi-stationary, are actually metastable. After extremely long times, they
randomly switch to new configurations with a different number of jets. The
genericity of this phenomenon suggests that most quasi-stationary turbulent
planetary atmospheres might have many climates and attractors for fixed values
of the external forcing parameters. A key message is that this situation will
usually not be detected by simply running the numerical models, because of the
extremely long mean transition time to change from one climate to another. In
order to study such phenomena, we need to use specific tools: rare event
algorithms and large deviation theory. With these tools, we make a full
statistical mechanics study of a classical barotropic beta-plane
quasigeostrophic model. It exhibits robust bimodality with abrupt transitions.
We show that new jets spontaneously nucleate from westward jets. The
numerically computed mean transition time is consistent with an Arrhenius law
showing an exponential decrease of the probability as the Ekman dissipation
decreases. This phenomenology is controlled by rare noise-driven paths called
{\it instantons}. Moreover, we compute the saddles of the corresponding
effective dynamics. For the dynamics of states with three alternating jets, we
uncover an unexpectedly rich dynamics governed by the symmetric group ${\cal
S}_3$ of permutations, with two distinct families of instantons, which is a
surprise for a system where everything seemed stationary in the hundreds of
previous simulations of this model. We discuss the future generalization of our
approach to more realistic models.",http://arxiv.org/abs/2009.09913v3
Twenty years of PWV measurements in the Chajnantor Area,2020-07-08T16:51:58Z,"Fernando Cort√©s, Katherine Cort√©s, Rodrigo Reeves, Ricardo Bustos, Simon Radford","Context. Interest in the use of the Chajnantor area for millimeter and
submillimeter astronomy is increasing because of its excellent atmospheric
conditions. Knowing the general site annual variability in precipitable water
vapor (PWV) can contribute to the planning of new observatories in the area.
Aims. We seek to create a 20-year atmospheric database (1997 - 2017) for the
Chajnantor area in northern Chile using a single common physical unit, PWV.We
plan to extract weather relations between the Chajnantor Plateau and the summit
of Cerro Chajnantor to evaluate potential sensitivity improvements for
telescopes fielded in the higher site. We aim to validate the use of
submillimeter tippers to be used at other sites and use the PWV database to
detect a potential signature for local climate change over 20 years. Methods.
We revised our method to convert from submillimeter tipper opacity to PWV. We
now include the ground temperature as an input parameter to the conversion
scheme and, therefore, achieve a higher conversion accuracy. Results.We found a
decrease in the measured PWV at the summit of Cerro Chajnantor with respect to
the plateau of 28%. In addition, we found a PWV difference of 1:9% with only 27
m of altitude difference between two sites in the Chajnantor Plateau: the
Atacama Pathfinder Experiment (APEX) and the Cosmic Background Imager (CBI)
near the Atacama Large Millimeter Array (ALMA) center. This difference is
possibly due to local topographic conditions that favor the discrepancy in PWV.
The scale height for the plateau was extracted from the measurements of the
plateau and the Cerro Chajnantor summit, giving a value of 1537 m. Considering
the results obtained in this work from the long-term study, we do not see
evidence of PWV trends in the 20-year period of the analysis that would suggest
climate change in such a timescale.",http://arxiv.org/abs/2007.04262v1
"The role of elevated terrain and the Gulf of Mexico in the production of
  severe local storm environments over North America",2020-07-31T02:05:01Z,"Funing Li, Daniel R. Chavas, Kevin A. Reed, Nan Rosenbloom, Daniel T. Dawson II","The prevailing conceptual model for the production of severe local storm
(SLS) environments over North America asserts that upstream elevated terrain
and the Gulf of Mexico are both essential to their formation. This work tests
this hypothesis using two prescribed-ocean climate model experiments with North
American topography removed or the Gulf of Mexico converted to land and
analyzes how SLS environments and associated synoptic-scale drivers (southerly
Great Plains low-level jets, drylines, elevated mixed layers, and extratropical
cyclones) change relative to a control historical run. Overall, SLS
environments depend strongly on upstream elevated terrain but weakly on the
Gulf of Mexico. Removing elevated terrain substantially reduces SLS
environments especially over the continental interior due to broad reductions
in both thermodynamic and kinematic parameters, leaving a more zonally-uniform
residual distribution that is maximized near the Gulf coast and decays toward
the continental interior. This response is associated with a strong reduction
in synoptic-scale drivers and a cooler and drier mean-state atmosphere.
Replacing the Gulf of Mexico with land modestly reduces SLS environments
thermodynamically over the Great Plains and increases them kinematically over
the eastern U.S, shifting the primary local maximum eastward into Illinois; it
also eliminates the secondary, smaller local maximum over southern Texas. This
response is associated with modest changes in synoptic-scale drivers and a
warmer and drier lower-tropospheric mean state. These experiments provide
insight into the role of elevated terrain and the Gulf of Mexico in modifying
the spatial distribution and seasonality of SLS environments.",http://arxiv.org/abs/2007.15803v2
"Low complexity model to study scale dependence of phytoplankton dynamics
  in the tropical Pacific",2020-09-08T10:21:50Z,"Jozef Skakala, Paolo Lazzari","We demonstrate that a simple model based on reaction-diffusion-advection
(RDA) equation forced by realistic surface velocities and nutrients is skilled
in reproducing the distributions of the surface phytoplankton chlorophyll in
the tropical Pacific. We use the low-complexity RDA model to investigate the
scale-relationships in the impact of different drivers (turbulent diffusion,
mean and eddy advection, primary productivity) on the phytoplankton chlorophyll
concentrations. We find that in the 1/4{\deg} (~25km) model, advection has a
substantial impact on the rate of primary productivity, whilst the turbulent
diffusion term has a fairly negligible impact. Turbulent diffusion has an
impact on the phytoplankton variability, with the impact being scale-propagated
and amplified by the larger scale surface currents. We investigate the impact
of a surface nutrient decline and some changes to mesoscale eddy kinetic energy
(climate change projections) on the surface phytoplankton concentrations. The
RDA model suggests that unless mesoscale eddies radically change, phytoplankton
chlorophyll scales sub-linearly with the nutrients, and it is relatively stable
with respect to the nutrient concentrations. Furthermore we explore how a white
multiplicative Gaussian noise introduced into the RDA model on its resolution
scale propagates across spatial scales through the non-linear model dynamics
under different sets of phytoplankton drivers. The unifying message of this
work is that the low complexity (e.g. RDA) models can be successfully used to
realistically model some specific aspects of marine ecosystem dynamics and by
using those models one can explore many questions that would be beyond
computational affordability of the higher-complexity ecosystem models.",http://arxiv.org/abs/2009.03629v2
Surrogate sea ice model enables efficient tuning,2020-06-01T19:42:43Z,"Kelly Kochanski, Ivana Cvijanovic, Donald Lucas","Predicting changes in sea ice cover is critical for shipping, ecosystem
monitoring, and climate modeling. Current sea ice models, however, predict more
ice than is observed in the Arctic, and less in the Antarctic. Improving the
fit of these physics-based models to observations is challenging because the
models are expensive to run, and therefore expensive to optimize. Here, we
construct a machine learning surrogate that emulates the effect of changing
model physics on forecasts of sea ice area from the Los Alamos Sea Ice Model
(CICE). We use the surrogate model to investigate the sensitivity of CICE to
changes in the parameters governing: ice's ridging and albedo; snow's albedo,
aging, and thermal conductivity; the effect of meltwater on albedo; and the
effect of ponds on albedo. We find that CICE's sensitivity to these model
parameters differs between hemispheres. We propose that future sea ice modelers
separate the snow conductivity and snow grain size distributions on a seasonal
and inter-hemispheric basis, and we recommend optimal values of these
parameters. This will make it possible to make models that fit observations of
both Arctic and Antarctic sea ice more closely. These results demonstrate that
important aspects of the behavior of a leading sea ice model can be captured by
a relatively simple support vector regression surrogate model, and that this
surrogate dramatically increases the ease of tuning the full simulation.",http://arxiv.org/abs/2006.12977v1
"Microsimulation Analysis for Network Traffic Assignment (MANTA) at
  Metropolitan-Scale for Agile Transportation Planning",2020-07-04T22:07:21Z,"Pavan Yedavalli, Krishna Kumar, Paul Waddell","Abrupt changes in the environment, such as unforeseen events due to climate
change, have triggered massive and precipitous changes in human mobility. The
ability to quickly predict traffic patterns in different scenarios has become
more urgent to support short-term operations and long-term transportation
planning. This requires modeling entire metropolitan areas to recognize the
upstream and downstream effects on the network. However, there is a well-known
trade-off between increasing the level of detail of a model and decreasing
computational performance. To achieve the level of detail required for traffic
microsimulation, current implementations often compromise by simulating small
spatial scales, and those that operate at larger scales often require access to
expensive high-performance computing systems or have computation times on the
order of days or weeks that discourage productive research and real-time
planning. This paper addresses the performance shortcomings by introducing a
new platform, MANTA (Microsimulation Analysis for Network Traffic Assignment),
for traffic microsimulation at the metropolitan-scale. MANTA employs a highly
parallelized GPU implementation that is capable of running metropolitan-scale
simulations within a few minutes. The runtime to simulate all morning trips,
using half-second timesteps, for the nine-county San Francisco Bay Area is just
over four minutes, not including routing and initialization. This computational
performance significantly improves the state of the art in large-scale traffic
microsimulation. MANTA expands the capacity to analyze detailed travel patterns
and travel choices of individuals for infrastructure planning.",http://arxiv.org/abs/2007.03614v2
"Anticipation-induced social tipping -- Can the environment be stabilised
  by social dynamics?",2020-12-03T14:55:30Z,"Paul Manuel M√ºller, Jobst Heitzig, J√ºrgen Kurths, Kathy L√ºdge, Marc Wiedermann","In the past decades human activities caused global Earth system changes,
e.g., climate change or biodiversity loss. Simultaneously, these associated
impacts have increased environmental awareness within societies across the
globe, thereby leading to dynamical feedbacks between the social and natural
Earth system. Contemporary modelling attempts of Earth system dynamics rarely
incorporate such co-evolutions and interactions are mostly studied
unidirectionally through direct or remembered past impacts. Acknowledging that
societies have the additional capability for foresight, this work proposes a
conceptual feedback model of socio-ecological co-evolution with the specific
construct of anticipation acting as a mediator between the social and natural
system. Our model reproduces results from previous sociological threshold
models with bi-stability if one assumes a static environment. Once the
environment changes in response to societal behaviour the system instead
converges towards a globally stable, but not necessarily desired, attractor.
Ultimately, we show that anticipation of future ecological states then leads to
metastability of the system where desired states can persist for a long time.
We thereby demonstrate that foresight and anticipation form an important
mechanism which, once its time horizon becomes large enough, fosters social
tipping towards behaviour that can stabilise the environment and prevents
potential socio-ecological collapse.",http://arxiv.org/abs/2012.01977v1
"Future Proofing a Building Design Using History Matching Inspired
  Level-Set Techniques",2020-01-07T12:06:23Z,"Evan Baker, Peter Challenor, Matt Eames","History Matching is a technique used to calibrate complex computer models,
that is, finding the input settings which lead to the simulated output matching
up with real world observations. Key to this technique is the construction of
emulators, which provide fast probabilistic predictions of future simulations.
In this work, we adapt the History Matching framework to tackle the problem of
level set estimation, that is, finding input settings where the output is below
(or above) some threshold. The developed methodology is heavily motivated by a
specific case study: how can one design a building that will be sufficiently
protected against overheating and sufficiently energy efficient, whilst
considering the expected increases in temperature due to climate change? We
successfully manage to address this - greatly reducing a large initial set of
candidate building designs down to a small set of acceptable potential
buildings.",http://arxiv.org/abs/2001.01992v2
"Assessment of Sea-Level Rise Impacts on Salt-Wedge Intrusion in
  Idealized and Neretva River Estuary",2020-01-14T13:31:22Z,"Nino Krvavica, Igor Ru≈æiƒá","Understanding the response of estuaries to sea-level rise is crucial in
developing a suitable mitigation and climate change adaptation strategy. This
study investigates the impacts of rising sea levels on salinity intrusion in
salt-wedge estuaries. The sea-level rise impacts are assessed in idealized
estuaries using simple expressions derived from a two-layer hydraulic theory,
and in the Neretva River Estuary in Croatia using a two-layer time-dependent
model. The assessment is based on three indicators - the salt-wedge intrusion
length, the seawater volume, and the river inflows needed to restore the
baseline intrusion. The potential SLR was found to increase all three
considered indicators. Theoretical analysis in idealized estuaries suggests
that shallower estuaries are more sensitive to SLR. Numerical results for the
Neretva River Estuary showed that SLR may increase salt-wedge intrusion length,
volume, and corrective river inflow. However, the results are highly non-linear
because of the channel geometry, especially for lower river inflows. A
theoretical assessment of channel bed slope impacts on limiting a potential
intrusion is therefore additionally discussed. This findings emphasize the need
to use several different indicators when assessing SLR impacts.",http://arxiv.org/abs/2001.04765v1
Prediction of Bayesian Intervals for Tropical Storms,2020-03-10T22:31:58Z,"Max Chiswick, Sam Ganzfried","Building on recent research for prediction of hurricane trajectories using
recurrent neural networks (RNNs), we have developed improved methods and
generalized the approach to predict Bayesian intervals in addition to simple
point estimates. Tropical storms are capable of causing severe damage, so
accurately predicting their trajectories can bring significant benefits to
cities and lives, especially as they grow more intense due to climate change
effects. By implementing the Bayesian interval using dropout in an RNN, we
improve the actionability of the predictions, for example by estimating the
areas to evacuate in the landfall region. We used an RNN to predict the
trajectory of the storms at 6-hour intervals. We used latitude, longitude,
windspeed, and pressure features from a Statistical Hurricane Intensity
Prediction Scheme (SHIPS) dataset of about 500 tropical storms in the Atlantic
Ocean. Our results show how neural network dropout values affect predictions
and intervals.",http://arxiv.org/abs/2003.05024v1
"Optimal Combination of Arctic Sea Ice Extent Measures: A Dynamic Factor
  Modeling Approach",2020-03-31T15:02:27Z,"Francis X. Diebold, Maximilian G√∂bel, Philippe Goulet Coulombe, Glenn D. Rudebusch, Boyuan Zhang","The diminishing extent of Arctic sea ice is a key indicator of climate change
as well as an accelerant for future global warming. Since 1978, Arctic sea ice
has been measured using satellite-based microwave sensing; however, different
measures of Arctic sea ice extent have been made available based on differing
algorithmic transformations of the raw satellite data. We propose and estimate
a dynamic factor model that combines four of these measures in an optimal way
that accounts for their differing volatility and cross-correlations. We then
use the Kalman smoother to extract an optimal combined measure of Arctic sea
ice extent. It turns out that almost all weight is put on the NSIDC Sea Ice
Index, confirming and enhancing confidence in the Sea Ice Index and the NASA
Team algorithm on which it is based.",http://arxiv.org/abs/2003.14276v2
"Saturation of the Infrared Absorption by Carbon Dioxide in the
  Atmosphere",2020-03-27T10:09:57Z,Dieter Schildknecht,"Based on new radiative transfer numerical evaluations, we reconsider an
argument presented by Schack in 1972 that says that saturation of the
absorption of infrared radiation by carbon dioxide in the atmosphere sets in as
soon as the relative concentration of carbon dioxide exceeds a lower limit of
approximately 300 ppm. We provide a concise brief and explicit representation
of the greenhouse effect of the earth's atmosphere. We find an equilibrium
climate sensitivity (temperature increase $\Delta T$ due to doubling of
atmospheric $CO_2$ concentration) of $\Delta T \simeq 0.5 ^0C$. We elaborate on
the consistency of these results on $\Delta T$ with results observationally
obtained by satellite-based measurements of short-time radiation-flux versus
surface-temperature changes.",http://arxiv.org/abs/2004.00708v2
"TrueBranch: Metric Learning-based Verification of Forest Conservation
  Projects",2020-04-21T02:52:27Z,"Simona Santamaria, David Dao, Bj√∂rn L√ºtjens, Ce Zhang","International stakeholders increasingly invest in offsetting carbon
emissions, for example, via issuing Payments for Ecosystem Services (PES) to
forest conservation projects. Issuing trusted payments requires a transparent
monitoring, reporting, and verification (MRV) process of the ecosystem services
(e.g., carbon stored in forests). The current MRV process, however, is either
too expensive (on-ground inspection of forest) or inaccurate (satellite).
Recent works propose low-cost and accurate MRV via automatically determining
forest carbon from drone imagery, collected by the landowners. The automation
of MRV, however, opens up the possibility that landowners report untruthful
drone imagery. To be robust against untruthful reporting, we propose
TrueBranch, a metric learning-based algorithm that verifies the truthfulness of
drone imagery from forest conservation projects. TrueBranch aims to detect
untruthfully reported drone imagery by matching it with public satellite
imagery. Preliminary results suggest that nominal distance metrics are not
sufficient to reliably detect untruthfully reported imagery. TrueBranch
leverages metric learning to create a feature embedding in which truthfully and
untruthfully collected imagery is easily distinguishable by distance
thresholding.",http://arxiv.org/abs/2004.09725v1
"Venturing the Definition of Green Energy Transition: A systematic
  literature review",2020-04-22T13:25:53Z,"Pedro V Hernandez Serrano, Amrapali Zaveri","The issue of climate change has become increasingly noteworthy in the past
years, the transition towards a renewable energy system is a priority in the
transition to a sustainable society. In this document, we explore the
definition of green energy transition, how it is reached, and what are the
driven factors to achieve it. To answer that firstly, we have conducted a
literature review discovering definitions from different disciplines, secondly,
gathering the key factors that are drivers for energy transition, finally, an
analysis of the factors is conducted within the context of European Union data.
Preliminary results have shown that household net income and governmental legal
actions related to environmental issues are potential candidates to predict
energy transition within countries. With this research, we intend to spark new
research directions in order to get a common social and scientific
understanding of green energy transition.",http://arxiv.org/abs/2004.10562v2
"Streamline-Based Simulation of Carbon Dioxide Sequestration in Saline
  Aquifers",2020-04-25T13:21:31Z,"Feyi Olalotiti-Lawal, Shusei Tanaka, Akhil Datta-Gupta","Subsurface sequestration of CO2 has received attention from the global
scientific community in response to climate change concerns due to higher
concentrations of CO2 in the atmosphere. Mathematical models have thus been
developed to aid the understanding of multiphase flow of CO2 and trapping
mechanisms during subsurface sequestration. Solutions to these models have
ranged from analytical, semi-analytical and numerical methods, each having its
merits and limitations in terms of underlying physics, computational speed and
accuracy. We present a streamline-based method for modeling CO2 transport in
saline aquifers which leverages sub-grid resolution capabilities of streamlines
in capturing small- and large-scale heterogeneity effects during CO2 injection.
Our approach is based on an iterative IMPES scheme and accounts for the
physical processes characteristic of CO2 injection in saline aquifers. These
include compressibility, gravity, capillarity, mutual solubility, precipitation
and formation dry-out effects. Our streamline simulation method provides an
extension of previous streamline-based models through rigorous treatment of
transverse fluxes arising from compressibility, gravity and capillary effects.
We present series of examples encompassing different levels of geologic and
geometrical complexity to illustrate the relevance, accuracy and computational
efficiency of the approach.",http://arxiv.org/abs/2004.12139v1
End-to-end NILM System Using High Frequency Data and Neural Networks,2020-04-29T00:58:08Z,"Franco Marchesoni-Acland, Camilo Mari√±o, El√≠as Masquil, Pablo Masaferro, Alicia Fern√°ndez","Improving energy efficiency is a necessity in the fight against climate
change. Non Intrusive Load Monitoring (NILM) systems give important information
about the household consumption that can be used by the electric utility or the
end users. In this work the implementation of an end-to-end NILM system is
presented, which comprises a custom high frequency meter and neural-network
based algorithms. The present article presents a novel way to include high
frequency information as input of neural network models by means of
multivariate time series that include carefully selected features. Furthermore,
it provides a detailed assessment of the generalization error and shows that
this class of models generalize well to new instances of seen-in-training
appliances. An evaluation database formed of measurements in two Uruguayan
homes is collected and discussion on general unsupervised approaches is
provided.",http://arxiv.org/abs/2004.13905v1
Access to mass rapid transit in OECD urban areas,2020-09-08T12:43:22Z,"Vincent Verbavatz, Marc Barthelemy","As mitigating car traffic in cities has become paramount to abate climate
change effects, fostering public transport in cities appears ever-more
appealing. A key ingredient in that purpose is easy access to mass rapid
transit (MRT) systems. So far, we have however few empirical estimates of the
coverage of MRT in urban areas, computed as the share of people living in MRT
catchment areas, say for instance within walking distance. In this work, we
clarify a universal definition of such a metrics, the ""People Near Transit
(PNT)"", and present measures of this quantity for 85 urban areas in OECD
countries, the largest dataset of such a quantity so far. By suggesting a
standardized protocol, we make our dataset sound and expandable to other
countries and cities in the world, which grounds our work into solid basis for
multiple reuses in transport, environmental or economic studies.",http://arxiv.org/abs/2009.03700v1
"What Makes People Join Conspiracy Communities?: Role of Social Factors
  in Conspiracy Engagement",2020-09-09T19:23:11Z,"Shruti Phadke, Mattia Samory, Tanushree Mitra","Widespread conspiracy theories, like those motivating anti-vaccination
attitudes or climate change denial, propel collective action and bear
society-wide consequences. Yet, empirical research has largely studied
conspiracy theory adoption as an individual pursuit, rather than as a socially
mediated process. What makes users join communities endorsing and spreading
conspiracy theories? We leverage longitudinal data from 56 conspiracy
communities on Reddit to compare individual and social factors determining
which users join the communities. Using a quasi-experimental approach, we first
identify 30K future conspiracists-(FC) and 30K matched non-conspiracists-(NC).
We then provide empirical evidence of importance of social factors across six
dimensions relative to the individual factors by analyzing 6 million Reddit
comments and posts. Specifically in social factors, we find that dyadic
interactions with members of the conspiracy communities and marginalization
outside of the conspiracy communities, are the most important social precursors
to conspiracy joining-even outperforming individual factor baselines. Our
results offer quantitative backing to understand social processes and echo
chamber effects in conspiratorial engagement, with important implications for
democratic institutions and online communities.",http://arxiv.org/abs/2009.04527v2
"Complex solid solution electrocatalyst discovery by prediction and
  high-throughput experimentation",2020-09-17T20:55:01Z,"Thomas A. A. Batchelor, Tobias L√∂ffler, Bin Xiao, Olga A. Krysiak, Valerie Strotk√∂tter, Jack K. Pedersen, Christian M. Clausen, Alan Savan, Wolfgang Schuhmann, Jan Rossmeisl, Alfred Ludwig","Efficient discovery of electrocatalysts for electrochemical energy conversion
reactions is of utmost importance to combat climate change. With the example of
the oxygen reduction reaction we show that by utilising a data-driven discovery
cycle, the multidimensionality challenge offered by compositionally complex
solid solution (high entropy alloy) electrocatalysts can be mastered.
Iteratively refined computational models predict activity trends for quinary
target compositions, around which continuous composition spread thin-film
libraries are synthesized. High-throughput characterisation datasets are then
input for refinement of the model. The refined model correctly predicts
activity maxima of the exemplary model system Ag-Ir-Pd-Pt-Ru for the oxygen
reduction reaction. The method can identify optimal complex solid solutions for
electrochemical reactions in an unprecedented manner.",http://arxiv.org/abs/2009.08529v1
"HydroDeep -- A Knowledge Guided Deep Neural Network for
  Geo-Spatiotemporal Data Analysis",2020-10-09T02:20:59Z,"Aishwarya Sarkar, Jien Zhang, Chaoqun Lu, Ali Jannesari","Due to limited evidence and complex causes of regional climate change, the
confidence in predicting fluvial floods remains low. Understanding the
fundamental mechanisms intrinsic to geo-spatiotemporal information is crucial
to improve the prediction accuracy. This paper demonstrates a hybrid neural
network architecture - HydroDeep, that couples a process-based hydro-ecological
model with a combination of Deep Convolutional Neural Network (CNN) and Long
Short-Term Memory (LSTM) Network. HydroDeep outperforms the independent CNN's
and LSTM's performance by 1.6% and 10.5% respectively in Nash-Sutcliffe
efficiency. Also, we show that HydroDeep pre-trained in one region is adept at
passing on its knowledge to distant places via unique transfer learning
approaches that minimize HydroDeep's training duration for a new region by
learning its regional geo-spatiotemporal features in a reduced number of
iterations.",http://arxiv.org/abs/2010.04328v2
"Bayesian Characterization of Uncertainties Surrounding Fluvial Flood
  Hazard Estimates",2020-10-09T20:26:31Z,"Sanjib Sharma, Ganesh Raj Ghimire, Rocky Talchabhadel, Jeeban Panthi, Benjamin Seiyon Lee, Fengyun Sun, Rupesh Baniya, Tirtha Raj Adhikari","Fluvial floods drive severe risk to riverine communities. There is a strong
evidence of increasing flood hazards in many regions around the world. The
choice of methods and assumptions used in flood hazard estimates can impact the
design of risk management strategies. In this study, we characterize the
expected flood hazards conditioned on the uncertain model structures, model
parameters and prior distributions of the parameters. We construct a Bayesian
framework for river stage return level estimation using a nonstationary
statistical model that relies exclusively on Indian Ocean Dipole Index. We show
that ignoring uncertainties can lead to biased estimation of expected flood
hazards. We find that the considered model parametric uncertainty is more
influential than model structures and model priors. Our results highlight the
importance of incorporating uncertainty in river stage estimates, and are of
practical use for informing water infrastructure designs in a changing climate.",http://arxiv.org/abs/2010.04789v3
"Estimation of Groundwater Storage Variations in Indus River Basin using
  GRACE Data",2020-10-23T05:40:04Z,"Yahya Sattar, Zubair Khalid","The depletion and variations of groundwater storage~(GWS) are of critical
importance for sustainable groundwater management. In this work, we use Gravity
Recovery and Climate Experiment (GRACE) to estimate variations in the
terrestrial water storage~(TWS) and use it in conjunction with the Global Land
Data Assimilation System~(GLDAS) data to extract GWS variations over time for
Indus river basin~(IRB). We present a data processing framework that processes
and combines these data-sets to provide an estimate of GWS changes. We also
present the design of a band-limited optimally concentrated window function for
spatial localization of the data in the region of interest. We construct the
so-called optimal window for the IRB region and use it in our processing
framework to analyze the GWS variations from 2005 to 2015. Our analysis reveals
the expected seasonal variations in GWS and signifies groundwater depletion on
average over the time period. Our proposed processing framework can be used to
analyze spatio-temporal variations in TWS and GWS for any region of interest.",http://arxiv.org/abs/2010.12175v1
"The Gray Rhino of Pandemic Preparedness: Proactive digital, data, and
  organizational infrastructure to help humanity build resilience in the face
  of pandemics",2020-11-05T11:55:42Z,Abhishek Gupta,"COVID-19 has exposed glaring holes in our existing digital, data, and
organizational practices. Researchers ensconced in epidemiological and human
health work have repeatedly pointed out how urban encroachment, climate change,
and other human-triggered activities and patterns are going to make zoonotic
pandemics more frequent and commonplace. The Gray Rhino mindset provides a
useful reframing (as opposed to viewing pandemics such as the current one as a
Black Swan event) that can help us recover faster from these (increasingly)
frequent occurrences and build resiliency in our digital, data, and
organizational infrastructure. Mitigating the social and economic impacts of
pandemics can be eased through building infrastructure that elucidate leading
indicators via passive intelligence gathering so that responses to containing
the spread of pandemics are not blanket measures; instead, they can be
fine-grained allowing for more efficient utilization of scarce resources and
minimizing disruption to our way of life.",http://arxiv.org/abs/2011.02773v1
"ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep
  Learning on Satellite Imagery",2020-11-11T00:28:40Z,"Jeremy Irvin, Hao Sheng, Neel Ramachandran, Sonja Johnson-Yu, Sharon Zhou, Kyle Story, Rose Rustowicz, Cooper Elsworth, Kemen Austin, Andrew Y. Ng","Characterizing the processes leading to deforestation is critical to the
development and implementation of targeted forest conservation and management
policies. In this work, we develop a deep learning model called ForestNet to
classify the drivers of primary forest loss in Indonesia, a country with one of
the highest deforestation rates in the world. Using satellite imagery,
ForestNet identifies the direct drivers of deforestation in forest loss patches
of any size. We curate a dataset of Landsat 8 satellite images of known forest
loss events paired with driver annotations from expert interpreters. We use the
dataset to train and validate the models and demonstrate that ForestNet
substantially outperforms other standard driver classification approaches. In
order to support future research on automated approaches to deforestation
driver classification, the dataset curated in this study is publicly available
at https://stanfordmlgroup.github.io/projects/forestnet .",http://arxiv.org/abs/2011.05479v1
"pymgrid: An Open-Source Python Microgrid Simulator for Applied
  Artificial Intelligence Research",2020-11-11T23:05:12Z,"Gonzague Henri, Tanguy Levent, Avishai Halev, Reda Alami, Philippe Cordier","Microgrids, self contained electrical grids that are capable of disconnecting
from the main grid, hold potential in both tackling climate change mitigation
via reducing CO2 emissions and adaptation by increasing infrastructure
resiliency. Due to their distributed nature, microgrids are often
idiosyncratic; as a result, control of these systems is nontrivial. While
microgrid simulators exist, many are limited in scope and in the variety of
microgrids they can simulate. We propose pymgrid, an open-source Python package
to generate and simulate a large number of microgrids, and the first
open-source tool that can generate more than 600 different microgrids. pymgrid
abstracts most of the domain expertise, allowing users to focus on control
algorithms. In particular, pymgrid is built to be a reinforcement learning (RL)
platform, and includes the ability to model microgrids as Markov decision
processes. pymgrid also introduces two pre-computed list of microgrids,
intended to allow for research reproducibility in the microgrid setting.",http://arxiv.org/abs/2011.08004v1
"Quantifying model uncertainty for the observed non-Gaussian data by the
  Hellinger distance",2020-11-23T02:01:18Z,"Yayun Zheng, Fang Yang, Jinqiao Duan, J√ºrgen Kurths","Mathematical models for complex systems under random fluctuations often
certain uncertain parameters. However, quantifying model uncertainty for a
stochastic differential equation with an $\alpha$-stable L\'evy process is
still lacking. Here, we propose an approach to infer all the uncertain
non-Gaussian parameters and other system parameters by minimizing the Hellinger
distance over the parameter space. The Hellinger distance measures the
similarity between an empirical probability density of non-Gaussian
observations and a solution (as a probability density) of the associated
nonlocal Fokker-Planck equation. Numerical experiments verify that our method
is feasible for estimating single and multiple parameters. Meanwhile, we find
an optimal estimation interval of the estimated parameters. This method is
beneficial for extracting governing dynamical system models under non-Gaussian
fluctuations, as in the study of abrupt climate changes in the
Dansgaard-Oeschger events.",http://arxiv.org/abs/2011.11170v2
Synchronization in leader-follower switching dynamics,2020-02-18T07:25:10Z,"Jinha Park, B. Kahng","The features of animal population dynamics, for instance, flocking and
migration, are often synchronized for survival under large-scale climate change
or perceived threats. These coherent phenomena have been explained using
synchronization models. However, such models do not take into account
asynchronous and adaptive updating of an individual's status at each time.
Here, we modify the Kuramoto model slightly by classifying oscillators as
leaders or followers, according to their angular velocity at each time, where
individuals interact asymmetrically according to their leader/follower status.
As the angular velocities of the oscillators are updated, the leader and
follower status may also be reassigned. Owing to this adaptive dynamics,
oscillators may cooperate by taking turns acting as a leader or follower. This
may result in intriguing patterns of synchronization transitions, including
hybrid phase transitions, and produce the leader-follower switching pattern
observed in bird migration patterns.",http://arxiv.org/abs/2002.07412v1
A knowledge-based model of civilization under climate change,2020-02-24T12:25:15Z,Boris M. Dolgonosov,"Civilization produces knowledge, which acts as the driving force of its
development. A macro-model of civilization that accounts for the effect of
knowledge production on population, energy consumption and environmental
conditions is developed. The model includes dynamic equations for world
population, amount of knowledge circulating in civilization, the share of
fossil fuels in total energy consumption, atmospheric CO2 concentration, and
global mean surface temperature. Energy dissipation in knowledge production and
direct loss of knowledge are taken into account. The model is calibrated using
historical data for each variable. About 90 scenarios were calculated. It was
shown that there are two control parameters - sensitivity of the population to
temperature rise and coefficient of knowledge loss - which determine the future
of civilization. In the two-dimensional space of these parameters, there is an
area of sustainable development and an area of loss of stability. Calculations
show that civilization is located just on the critical curve separating these
areas, that is, at the edge of stability. A small deviation can ultimately lead
either to a steady state of 10+ billion people or to the complete extinction of
civilization. There are no intermediate steady states.",http://arxiv.org/abs/2002.10196v1
"On Solving SAR Imaging Inverse Problems Using Non-Convex Regularization
  with a Cauchy-based Penalty",2020-05-01T23:56:07Z,"Oktay Karaku≈ü, Alin Achim","Synthetic aperture radar (SAR) imagery can provide useful information in a
multitude of applications, including climate change, environmental monitoring,
meteorology, high dimensional mapping, ship monitoring, or planetary
exploration. In this paper, we investigate solutions to a number of inverse
problems encountered in SAR imaging. We propose a convex proximal splitting
method for the optimization of a cost function that includes a non-convex
Cauchy-based penalty. The convergence of the overall cost function optimization
is ensured through careful selection of model parameters within a
forward-backward (FB) algorithm. The performance of the proposed penalty
function is evaluated by solving three standard SAR imaging inverse problems,
including super-resolution, image formation, and despeckling, as well as ship
wake detection for maritime applications. The proposed method is compared to
several methods employing classical penalty functions such as total variation
($TV$) and $L_1$ norms, and to the generalized minimax-concave (GMC) penalty.
We show that the proposed Cauchy-based penalty function leads to better image
reconstruction results when compared to the reference penalty functions for all
SAR imaging inverse problems in this paper.",http://arxiv.org/abs/2005.00657v2
"A new segmentation method for the homogenisation of GNSS-derived IWV
  time-series",2020-05-10T14:59:28Z,"Annarosa Quarello, Olivier Bock, Emilie Lebarbier","Homogenization is an important and crucial step to improve the usage of
observational data for climate analysis. This work is motivated by the analysis
of long series of GNSS Integrated Water Vapour (IWV) data which have not yet
been used in this context. This paper proposes a novel segmentation method that
integrates a periodic bias and a heterogeneous, monthly varying, variance. The
method consists in estimating first the variance using a robust estimator and
then estimating the segmentation and periodic bias iteratively. This strategy
allows for the use of the dynamic programming algorithm that remains the most
efficient exact algorithm to estimate the change-point positions. The
statistical performance of the method is assessed through numerical
experiments. An application to a real data set of 120 global GNSS stations is
presented. The method is implemented in the R package GNSSseg that will be
available on the CRAN.",http://arxiv.org/abs/2005.04683v1
"Non-Separable Spatio-temporal Models via Transformed Gaussian Markov
  Random Fields",2020-05-11T22:07:35Z,"Douglas R. M. Azevedo, Marcos O. Prates, Michael R. Willig","Models that capture the spatial and temporal dynamics are applicable in many
science fields. Non-separable spatio-temporal models were introduced in the
literature to capture these features. However, these models are generally
complicated in construction and interpretation. We introduce a class of
non-separable Transformed Gaussian Markov Random Fields (TGMRF) in which the
dependence structure is flexible and facilitates simple interpretations
concerning spatial, temporal and spatio-temporal parameters. Moreover, TGMRF
models have the advantage of allowing specialists to define any desired
marginal distribution in model construction without suffering from
spatio-temporal confounding. Consequently, the use of spatio-temporal models
under the TGMRF framework leads to a new class of general models, such as
spatio-temporal Gamma random fields, that can be directly used to model Poisson
intensity for space-time data. The proposed model was applied to identify
important environmental characteristics that affect variation in the abundance
of Nenia tridens, a dominant species of snail in a well-studied tropical
ecosystem, and to characterize its spatial and temporal trends, which are
particularly critical during the Anthropocene, an epoch of time characterized
by human-induced environmental change associated with climate and land use.",http://arxiv.org/abs/2005.05464v1
"Toward Large-Scale Autonomous Monitoring and Sensing of Underwater
  Pollutants",2020-05-15T20:12:55Z,"Huber Flores, Naser Hossein Motlagh, Agustin Zuniga, Mohan Liyanage, Monica Passananti, Sasu Tarkoma, Moustafa Youssef, Petteri Nurmi","Marine pollution is a growing worldwide concern, affecting health of marine
ecosystems, human health, climate change, and weather patterns. To reduce
underwater pollution, it is critical to have access to accurate information
about the extent of marine pollutants as otherwise appropriate countermeasures
and cleaning measures cannot be chosen. Currently such information is difficult
to acquire as existing monitoring solutions are highly laborious or costly,
limited to specific pollutants, and have limited spatial and temporal
resolution. In this article, we present a research vision of large-scale
autonomous marine pollution monitoring that uses coordinated groups of
autonomous underwater vehicles (AUV)s to monitor extent and characteristics of
marine pollutants. We highlight key requirements and reference technologies to
establish a research roadmap for realizing this vision. We also address the
feasibility of our vision, carrying out controlled experiments that address
classification of pollutants and collaborative underwater processing, two key
research challenges for our vision.",http://arxiv.org/abs/2005.09571v1
"Wind Speed Prediction and Visualization Using Long Short-Term Memory
  Networks (LSTM)",2020-05-22T17:51:13Z,"Md Amimul Ehsan, Amir Shahirinia, Nian Zhang, Timothy Oladunni","Climate change is one of the most concerning issues of this century. Emission
from electric power generation is a crucial factor that drives the concern to
the next level. Renewable energy sources are widespread and available globally,
however, one of the major challenges is to understand their characteristics in
a more informative way. This paper proposes the prediction of wind speed that
simplifies wind farm planning and feasibility study. Twelve artificial
intelligence algorithms were used for wind speed prediction from collected
meteorological parameters. The model performances were compared to determine
the wind speed prediction accuracy. The results show a deep learning approach,
long short-term memory (LSTM) outperforms other models with the highest
accuracy of 97.8%.",http://arxiv.org/abs/2005.12401v1
"Lessons learned in a decade of research software engineering GPU
  applications",2020-05-27T08:11:30Z,"Ben van Werkhoven, Willem Jan Palenstijn, Alessio Sclocco","After years of using Graphics Processing Units (GPUs) to accelerate
scientific applications in fields as varied as tomography, computer vision,
climate modeling, digital forensics, geospatial databases, particle physics,
radio astronomy, and localization microscopy, we noticed a number of technical,
socio-technical, and non-technical challenges that Research Software Engineers
(RSEs) may run into. While some of these challenges, such as managing different
programming languages within a project, or having to deal with different memory
spaces, are common to all software projects involving GPUs, others are more
typical of scientific software projects. Among these challenges we include
changing resolutions or scales, maintaining an application over time and making
it sustainable, and evaluating both the obtained results and the achieved
performance. %In this paper, we present the challenges and lessons learned from
research software engineering GPU applications.",http://arxiv.org/abs/2005.13227v1
"Public-Private Partnership in the Management of Natural Disasters: A
  Review",2020-06-10T14:16:07Z,Selene Perazzini,"Natural hazards can considerably impact the overall society of a country. As
some degree of public sector involvement is always necessary to deal with the
consequences of natural disasters, central governments have increasingly
invested in proactive risk management planning. In order to empower and involve
the whole society, some countries have established public-private partnerships,
mainly with the insurance industry, with satisfactorily outcomes. Although they
have proven necessary and most often effective, the public-private initiatives
have often incurred high debts or have failed to achieved the desired risk
reduction objectives. We review the role of these partnerships in the
management of natural risks, with particular attention to the insurance sector.
Among other country-specific issues, poor risk knowledge and weak governance
have widely challenged the initiatives during the recent years, while the
future is threatened by the uncertainty of climate change and unsustainable
development. In order to strengthen the country's resilience, a greater
involvement of all segments of the community, especially the weakest layers, is
needed and the management of natural risks should be included in a sustainable
development plan.",http://arxiv.org/abs/2006.05845v1
Evidence for increasing frequency of extreme coastal sea levels,2020-06-11T20:39:54Z,"Tony E. Wong, Travis Torline, Mingxuan Zhang","Projections of extreme sea levels (ESLs) are critical for managing coastal
risks, but are made complicated by deep uncertainties. One key uncertainty is
the choice of model structure used to estimate coastal hazards. Differences in
model structural choices contribute to uncertainty in estimated coastal hazard,
so it is important to characterize how model structural choice affects
estimates of ESL. Here, we present a collection of 36 ESL data sets, from tide
gauge stations along the United States East and Gulf Coasts. The data are
processed using both annual block maxima and peaks-over-thresholds approaches
for modeling distributions of extremes. We use these data sets to fit a suite
of potentially nonstationary extreme value models by covarying the ESL
statistics with multiple climate variables. We demonstrate how this data set
enables inquiry into deep uncertainty surrounding coastal hazards. For all of
the models and sites considered here, we find that accounting for changes in
the frequency of coastal extreme sea levels provides a better fit than using a
stationary extreme value model.",http://arxiv.org/abs/2006.06804v1
Towards Sealed Resistive Plate Chambers,2020-06-15T11:35:24Z,"L. Lopes P. Assis, A. Blanco, P. Fonte, M. Pimenta","The mitigation of human-induced climate change is of crucial importance for
the sustainability of humankind. For this aim the RPC community has exerted
considerable effort over the last decade to reduce the emission of greenhouse
gases from our detectors. These included searching for new eco-friendly gases,
implementing recovery and/or recirculation systems, improving gastightness and
using new materials and approaches in detector conception and operation for the
reduction of gas flow rates.
  Along this line of work, we present here an RPC architecture aimed at a
dramatic reduction of gas use in chambers meant for low-rate operation. Two
chambers were tested for more than six months with zero gas flow showing no
evidence of time-related effects, allowing to consider that permanently sealed
RPCs may be within reach, with obvious practical and environmental advantages.",http://arxiv.org/abs/2006.08291v1
"Improving Piecewise Linear Snow Density Models through Hierarchical
  Spatial and Orthogonal Functional Smoothing",2020-06-16T17:11:03Z,"Philip White, Durban Keeler, Daniel Sheanshang, Summer Rupper","Snow density estimates as a function of depth are used for understanding
climate processes, evaluating water accumulation trends in polar regions, and
estimating glacier mass balances. The common and interpretable
physically-derived differential equation models for snow density are piecewise
linear as a function of depth (on a transformed scale); thus, they can fail to
capture important data features. Moreover, the differential equation parameters
show strong spatial autocorrelation. To address these issues, we allow the
parameters of the physical model, including random change points over depth, to
vary spatially. We also develop a framework for functionally smoothing the
physically-motivated model. To preserve inference on the interpretable physical
model, we project the smoothing function into the physical model's spatially
varying null space. The proposed spatially and functionally smoothed snow
density model better fits the data while preserving inference on physical
parameters. Using this model, we find significant spatial variation in the
parameters that govern snow densification.",http://arxiv.org/abs/2006.09329v3
"A Methodology for Assessing the Environmental Effects Induced by ICT
  Services. Part I: Single Services",2020-06-18T19:55:23Z,"Vlad C. CoroamƒÉ, Pernilla Bergmark, Mattias H√∂jer, Jens Malmodin","Information and communication technologies (ICT) are increasingly seen as key
enablers for climate change mitigation measures. They can make existing
products and activities more efficient or substitute them altogether.
Consequently, different initiatives have started to estimate the environmental
effects of ICT services. Such assessments, however, lack scientific rigor and
often rely on crude assumptions and methods, leading to inaccurate or even
misleading results. The few methodological attempts that exist do not address
several crucial aspects, and are thus insufficient to foster good as-sessment
practice. Starting from such a high level standard from the European
Telecommunication Standardisation Institute (ETSI) and the International
Telecommunication Union (ITU), this article identifies the shortcomings of
existing methodologies and proposes solutions. It addresses several aspects for
the assessment of single ICT services: the goal and scope definition (analyzing
differences between ICT substitution and optimization, the time perspective of
the assessment, the challenge of a hypothetical baseline for the situation
without the ICT solution, and the differences between modelling and case
studies) as well as the often ignored influence of rebound effects and the
difficult extrapolation from case studies to larger populations.",http://arxiv.org/abs/2006.10831v1
HydroNets: Leveraging River Structure for Hydrologic Modeling,2020-07-01T16:32:07Z,"Zach Moshe, Asher Metzger, Gal Elidan, Frederik Kratzert, Sella Nevo, Ran El-Yaniv","Accurate and scalable hydrologic models are essential building blocks of
several important applications, from water resource management to timely flood
warnings. However, as the climate changes, precipitation and rainfall-runoff
pattern variations become more extreme, and accurate training data that can
account for the resulting distributional shifts become more scarce. In this
work we present a novel family of hydrologic models, called HydroNets, which
leverages river network structure. HydroNets are deep neural network models
designed to exploit both basin specific rainfall-runoff signals, and upstream
network dynamics, which can lead to improved predictions at longer horizons.
The injection of the river structure prior knowledge reduces sample complexity
and allows for scalable and more accurate hydrologic modeling even with only a
few years of data. We present an empirical study over two large basins in India
that convincingly support the proposed model and its advantages.",http://arxiv.org/abs/2007.00595v1
"Dismantling a dogma: the inflated significance of neutral genetic
  diversity in conservation genetics",2020-07-06T07:35:29Z,"Jo√£o C Teixeira, Christian D Huber","The current rate of species extinction is rapidly approaching unprecedented
highs and life on Earth presently faces a sixth mass extinction event driven by
anthropogenic activity, climate change and ecological collapse. The field of
conservation genetics aims at preserving species by using their levels of
genetic diversity, usually measured as neutral genome-wide diversity, as a
barometer for evaluating population health and extinction risk. A fundamental
assumption is that higher levels of genetic diversity lead to an increase in
fitness and long-term survival of a species. Here, we argue against the
perceived importance of neutral genetic diversity for the conservation of wild
populations and species. We demonstrate that no simple general relationship
exists between neutral genetic diversity and the risk of species extinction.
Instead, a better understanding of the properties of functional genetic
diversity, demographic history, and ecological relationships, is necessary for
developing and implementing effective conservation genetic strategies.",http://arxiv.org/abs/2007.02569v1
"Carbontracker: Tracking and Predicting the Carbon Footprint of Training
  Deep Learning Models",2020-07-06T20:24:31Z,"Lasse F. Wolff Anthony, Benjamin Kanding, Raghavendra Selvan","Deep learning (DL) can achieve impressive results across a wide variety of
tasks, but this often comes at the cost of training models for extensive
periods on specialized hardware accelerators. This energy-intensive workload
has seen immense growth in recent years. Machine learning (ML) may become a
significant contributor to climate change if this exponential trend continues.
If practitioners are aware of their energy and carbon footprint, then they may
actively take steps to reduce it whenever possible. In this work, we present
Carbontracker, a tool for tracking and predicting the energy and carbon
footprint of training DL models. We propose that energy and carbon footprint of
model development and training is reported alongside performance metrics using
tools like Carbontracker. We hope this will promote responsible computing in ML
and encourage research into energy-efficient deep neural networks.",http://arxiv.org/abs/2007.03051v1
"Study on Computational Thinking as Problem-solving Skill: Comparison
  Based on Students Mindset in Engineering and Social Science",2020-07-08T12:19:04Z,Andik Asmara,"One of the capabilities which 21st-century skill compulsory a person is
critical thinking and problem-solving skill that becomes top positions rank.
Focus on problem-solving skills can be taught to a child, especially begun in
elementary school refer to prior research focus on K-12. Computational thinking
was one problem-solving skill that popular to implemented and studied in the
current decade. This study was conducted to explore students' capability to be
able solving of the problem based on the possibility use the computational
thinking way. Participants in this study came from six international students
that study in Taiwan and from two deferent sciences disciplines, engineering,
and social science. A qualitative method was used to analyze data interviews,
took example cases from the global issue that is Climate Change. The result
founded that survive in a new environment was become evidence of their
implementation of problem-solving skills. Problem-solving mindset both students
of engineering and social science had discrepancy, those are how to use precise
structure in the algorithm.",http://arxiv.org/abs/2007.04060v1
"Simulation of Blockchain based Power Trading with Solar Power Prediction
  in Prosumer Consortium Model",2020-07-23T09:52:36Z,"Kaung Si Thu, Weerakorn Ongsakul","Prosumer consortium energy transactive models can be one of the solutions for
energy costs, increasing performance and for providing reliable electricity
utilizing distributed power generation, to a local group or community, like a
university. This research study demonstrates the simulation of blockchain based
power trading, supplemented by the solar power prediction using MLFF neural
network training in two prosumer nodes. This study can be the initial step in
the implementation of a power trading market model based on a decentralized
blockchain system, with distributed generations in a university grid system.
This system can balance the electricity demand and supply within the institute,
enable secure and rapid transactions, and the local market system can be
reinforced by forecasting solar generation. The performance of the MLFF
training can predict almost 90% accuracy of the model as short term ahead
forecasting. Because of it, the prosumer bodies can complete the decision
making before trading to their benefit.",http://arxiv.org/abs/2007.11885v2
"SAR image wave spectra to retrieve the thickness of grease-pancake sea
  ice using viscous wave models",2020-07-16T11:35:34Z,"Giacomo De Carolis, Piero Olla, Francesca De Santi","Young sea ice composed of grease and pancake ice (GPI), as well as thin
floes, considered to be the most common form of sea ice fringing Antarctica, is
now becoming the 'new normal' also in the Arctic. Investigations to determine
how an increase in GPI is affecting the climate in the far north and globally,
require specific tools to monitor the GPI's thickness distribution. Directional
wave spectra from satellite SAR imagery are used to determine the change in
wave dispersion as a wave train enters GPI fields. The ice cover thickness is
then estimated by fitting the dispersion data with two models of wave
propagation in ice cover ocean: the Keller's model and the close-packing (CP)
model.
  For both models, an empirical constitutive equation for GPI viscosity as a
function of the ice thickness is derived and discussed. Examples of GPI
thickness retrievals are shown for a Sentinel-1 C band SAR image taken in the
Beaufort Sea on 1 November 2015, and three CosmoSkyMed X band SAR images taken
in the Weddell Sea on March 2019. The estimated GPI thicknesses are consistent
with concurrent SMOS measurements and the available local samplings.",http://arxiv.org/abs/2007.15639v1
A Natural Disasters Index,2020-08-09T06:37:10Z,"Thilini V. Mahanama, Abootaleb Shirvani","Natural disasters, such as tornadoes, floods, and wildfire pose risks to life
and property, requiring the intervention of insurance corporations. One of the
most visible consequences of changing climate is an increase in the intensity
and frequency of extreme weather events. The relative strengths of these
disasters are far beyond the habitual seasonal maxima, often resulting in
subsequent increases in property losses. Thus, insurance policies should be
modified to endure increasingly volatile catastrophic weather events. We
propose a Natural Disasters Index (NDI) for the property losses caused by
natural disasters in the United States based on the ""Storm Data"" published by
the National Oceanic and Atmospheric Administration. The proposed NDI is an
attempt to construct a financial instrument for hedging the intrinsic risk. The
NDI is intended to forecast the degree of future risk that could forewarn the
insurers and corporations allowing them to transfer insurance risk to capital
market investors. This index could also be modified to other regions and
countries.",http://arxiv.org/abs/2008.03672v1
"Impact of natural disasters on consumer behavior: case of the 2017 El
  Nino phenomenon in Peru",2020-08-11T17:44:39Z,"Hugo Alatrista-Salas, Vincent Gauthier, Miguel Nunez-del-Prado, Monique Becker","El Nino is an extreme weather event featuring unusual warming of surface
waters in the eastern equatorial Pacific Ocean. This phenomenon is
characterized by heavy rains and floods that negatively affect the economic
activities of the impacted areas. Understanding how this phenomenon influences
consumption behavior at different granularity levels is essential for
recommending strategies to normalize the situation. With this aim, we performed
a multi-scale analysis of data associated with bank transactions involving
credit and debit cards. Our findings can be summarized into two main results:
Coarse-grained analysis reveals the presence of the El Ni\~no phenomenon and
the recovery time in a given territory, while fine-grained analysis
demonstrates a change in individuals' purchasing patterns and in merchant
relevance as a consequence of the climatic event. The results also indicate
that society successfully withstood the natural disaster owing to the economic
structure built over time. In this study, we present a new method that may be
useful for better characterizing future extreme events.",http://arxiv.org/abs/2008.04887v1
"An energy-based macroeconomic model validated by global historical
  series since 1820",2020-08-25T12:45:58Z,"Herve Bercegol, Henri Benisty","Global historical series spanning the last two centuries recently became
available for primary energy consumption (PEC) and Gross Domestic Product
(GDP). Based on a thorough analysis of the data, we propose a new, simple
macroeconomic model whereby physical power is fueling economic power. From 1820
to 1920, the linearity between global PEC and world GDP justifies basic
equations where, originally, PEC incorporates unskilled human labor that
consumes and converts energy from food. In a consistent model, both physical
capital and human capital are fed by PEC and represent a form of stored energy.
In the following century, from 1920 to 2016, GDP grows quicker than PEC.
Periods of quasi-linearity of the two variables are separated by distinct
jumps, which can be interpreted as radical technology shifts. The GDP to PEC
ratio accumulates game-changing innovation, at an average growth rate
proportional to PEC. These results seed alternative strategies for modeling and
for political management of the climate crisis and the energy transition.",http://arxiv.org/abs/2008.10967v3
Graph Neural Networks for Improved El Ni√±o Forecasting,2020-12-02T23:40:53Z,"Salva R√ºhling Cachay, Emma Erickson, Arthur Fender C. Bucker, Ernest Pokropek, Willa Potosnak, Salomey Osei, Bj√∂rn L√ºtjens","Deep learning-based models have recently outperformed state-of-the-art
seasonal forecasting models, such as for predicting El Ni\~no-Southern
Oscillation (ENSO). However, current deep learning models are based on
convolutional neural networks which are difficult to interpret and can fail to
model large-scale atmospheric patterns called teleconnections. Hence, we
propose the application of spatiotemporal Graph Neural Networks (GNN) to
forecast ENSO at long lead times, finer granularity and improved predictive
skill than current state-of-the-art methods. The explicit modeling of
information flow via edges may also allow for more interpretable forecasts.
Preliminary results are promising and outperform state-of-the art systems for
projections 1 and 3 months ahead.",http://arxiv.org/abs/2012.01598v3
"The Fake News Effect: Experimentally Identifying Motivated Reasoning
  Using Trust in News",2020-12-03T02:33:04Z,Michael Thaler,"Motivated reasoning posits that people distort how they process information
in the direction of beliefs they find attractive. This paper creates a novel
experimental design to identify motivated reasoning from Bayesian updating when
people have preconceived beliefs. It analyzes how subjects assess the veracity
of information sources that tell them the median of their belief distribution
is too high or too low. Bayesians infer nothing about the source veracity, but
motivated beliefs are evoked. Evidence supports politically-motivated reasoning
about immigration, income mobility, crime, racial discrimination, gender,
climate change, and gun laws. Motivated reasoning helps explain belief biases,
polarization, and overconfidence.",http://arxiv.org/abs/2012.01663v4
Towards an AI assistant for power grid operators,2020-12-03T16:12:58Z,"Antoine Marot, Alexandre Rozier, Matthieu Dussartre, Laure Crochepierre, Benjamin Donnot","Power grids are becoming more complex to operate in the digital age given the
current energy transition to cope with climate change. As a result, real-time
decision-making is getting more challenging as the human operator has to deal
with more information, more uncertainty, more applications, and more
coordination. While supervision has been primarily used to help them make
decisions over the last decades, it cannot reasonably scale up anymore. There
is a great need for rethinking the human-machine interface under more unified
and interactive frameworks. Taking advantage of the latest developments in
Human-Machine Interface and Artificial Intelligence, we expose our vision of a
new assistant framework relying on an hypervision interface and greater
bidirectional interaction. We review the known principles of decision-making
driving our assistant design alongside with its supporting assistance
functions. We finally share some guidelines to make progress towards the
development of such an assistant.",http://arxiv.org/abs/2012.02026v2
"Impact of weather factors on migration intention using machine learning
  algorithms",2020-12-04T16:59:15Z,"John Aoga, Juhee Bae, Stefanija Veljanoska, Siegfried Nijssen, Pierre Schaus","A growing attention in the empirical literature has been paid to the
incidence of climate shocks and change in migration decisions. Previous
literature leads to different results and uses a multitude of traditional
empirical approaches.
  This paper proposes a tree-based Machine Learning (ML) approach to analyze
the role of the weather shocks towards an individual's intention to migrate in
the six agriculture-dependent-economy countries such as Burkina Faso, Ivory
Coast, Mali, Mauritania, Niger, and Senegal. We perform several tree-based
algorithms (e.g., XGB, Random Forest) using the train-validation-test workflow
to build robust and noise-resistant approaches. Then we determine the important
features showing in which direction they are influencing the migration
intention. This ML-based estimation accounts for features such as weather
shocks captured by the Standardized Precipitation-Evapotranspiration Index
(SPEI) for different timescales and various socioeconomic features/covariates.
  We find that (i) weather features improve the prediction performance although
socioeconomic characteristics have more influence on migration intentions, (ii)
country-specific model is necessary, and (iii) international move is influenced
more by the longer timescales of SPEIs while general move (which includes
internal move) by that of shorter timescales.",http://arxiv.org/abs/2012.02794v1
"An Enriched Automated PV Registry: Combining Image Recognition and 3D
  Building Data",2020-12-07T13:45:08Z,"Benjamin Rausch, Kevin Mayer, Marie-Louise Arlt, Gunther Gust, Philipp Staudt, Christof Weinhardt, Dirk Neumann, Ram Rajagopal","While photovoltaic (PV) systems are installed at an unprecedented rate,
reliable information on an installation level remains scarce. As a result,
automatically created PV registries are a timely contribution to optimize grid
planning and operations. This paper demonstrates how aerial imagery and
three-dimensional building data can be combined to create an address-level PV
registry, specifying area, tilt, and orientation angles. We demonstrate the
benefits of this approach for PV capacity estimation. In addition, this work
presents, for the first time, a comparison between automated and
officially-created PV registries. Our results indicate that our enriched
automated registry proves to be useful to validate, update, and complement
official registries.",http://arxiv.org/abs/2012.03690v1
"Towards explainable message passing networks for predicting carbon
  dioxide adsorption in metal-organic frameworks",2020-12-02T12:54:26Z,"Ali Raza, Faaiq Waqar, Arni Sturluson, Cory Simon, Xiaoli Fern","Metal-organic framework (MOFs) are nanoporous materials that could be used to
capture carbon dioxide from the exhaust gas of fossil fuel power plants to
mitigate climate change. In this work, we design and train a message passing
neural network (MPNN) to predict simulated CO$_2$ adsorption in MOFs. Towards
providing insights into what substructures of the MOFs are important for the
prediction, we introduce a soft attention mechanism into the readout function
that quantifies the contributions of the node representations towards the graph
representations. We investigate different mechanisms for sparse attention to
ensure only the most relevant substructures are identified.",http://arxiv.org/abs/2012.03723v1
"Enhanced spatio-temporal electric load forecasts using less data with
  active deep learning",2020-12-08T12:55:29Z,"Arsam Aryandoust, Anthony Patt, Stefan Pfenninger","An effective way to oppose global warming and mitigate climate change is to
electrify our energy sectors and supply their electric power from renewable
wind and solar. Spatio-temporal predictions of electric load become
increasingly important for planning this transition, while deep learning
prediction models provide increasingly accurate predictions for it. The data
used for training deep learning models, however, is usually collected at random
using a passive learning approach. This naturally results in a large demand for
data and associated costs for sensors like smart meters, posing a large barrier
for electric utilities in decarbonizing their grids. Here, we test active
learning where we leverage additional computation for collecting a more
informative subset of data. We show how electric utilities can apply active
learning to better distribute smart meters and collect their data for more
accurate predictions of load with about half the data compared to when applying
passive learning.",http://arxiv.org/abs/2012.04407v2
"Nonlinear Complex PCA for spatio-temporal analysis of global soil
  moisture",2020-12-09T12:03:34Z,"Diego Bueso, Maria Piles, Gustau Camps-Valls","Soil moisture (SM) is a key state variable of the hydrological cycle, needed
to monitor the effects of a changing climate on natural resources. Soil
moisture is highly variable in space and time, presenting seasonalities,
anomalies and long-term trends, but also, and important nonlinear behaviours.
Here, we introduce a novel fast and nonlinear complex PCA method to analyze the
spatio-temporal patterns of the Earth's surface SM. We use global SM estimates
acquired during the period 2010-2017 by ESA's SMOS mission. Our approach
unveils both time and space modes, trends and periodicities unlike standard PCA
decompositions. Results show the distribution of the total SM variance among
its different components, and indicate the dominant modes of temporal
variability in surface soil moisture for different regions. The relationship of
the derived SM spatio-temporal patterns with El Ni{\~n}o Southern Oscillation
(ENSO) conditions is also explored.",http://arxiv.org/abs/2012.04996v1
Convolutional LSTM Neural Networks for Modeling Wildland Fire Dynamics,2020-12-11T23:31:43Z,"John Burge, Matthew Bonanni, Matthias Ihme, Lily Hu","As the climate changes, the severity of wildland fires is expected to worsen.
Models that accurately capture fire propagation dynamics greatly help efforts
for understanding, responding to and mitigating the damages caused by these
fires. Machine learning techniques provide a potential approach for developing
such models. The objective of this study is to evaluate the feasibility of
using a Convolutional Long Short-Term Memory (ConvLSTM) recurrent neural
network to model the dynamics of wildland fire propagation. The machine
learning model is trained on simulated wildfire data generated by a
mathematical analogue model. Three simulated datasets are analyzed, each with
increasing degrees of complexity. The simplest dataset includes a constant wind
direction as a single confounding factor, whereas the most complex dataset
includes dynamic wind, complex terrain, spatially varying moisture content and
heterogenous vegetation density distributions. We examine how effective the
ConvLSTM can learn the fire-spread dynamics over consecutive time steps. It is
shown that ConvLSTMs can capture local fire transmission events, as well as the
overall fire dynamics, such as the rate at which the fire spreads. Finally, we
demonstrate that ConvLSTMs outperform other network architectures that have
previously been used to model similar wildland fire dynamics.",http://arxiv.org/abs/2012.06679v2
"Quantification of the Impact of GHG Emissions on Unit Commitment in
  Microgrids",2020-12-13T17:55:57Z,"Ogun Yurdakul, Fikret Sivrikaya, Sahin Albayrak","The global climate change creates a dire need to mitigate greenhouse gas
(GHG) emissions from thermal generation resources (TGRs). While microgrids are
instrumental in enabling the deeper penetration of renewable resources, the
short-term planning of microgrids needs to explicitly assess the full range of
impact of GHG emissions. To this end, we propose a novel unit commitment (UC)
approach, which enables the representation of GHG emissions from TGRs, the
specification of GHG emission constraints, and the ex-ante evaluation of carbon
tax payment with all other costs and benefits. We quantify the relative merits
of the proposed approach vis-\`a-vis the classical UC approach via
representative studies. The results indicate that the proposed UC approach
yields lower costs than does the classical UC approach and achieves a greater
reduction in costs as carbon tax rate increases. Further, increasing carbon tax
rates can markedly disincentivize TGR generation under the proposed approach.",http://arxiv.org/abs/2012.07118v1
"Privacy Preserving Demand Forecasting to Encourage Consumer Acceptance
  of Smart Energy Meters",2020-12-14T12:04:34Z,"Christopher Briggs, Zhong Fan, Peter Andras","In this proposal paper we highlight the need for privacy preserving energy
demand forecasting to allay a major concern consumers have about smart meter
installations. High resolution smart meter data can expose many private aspects
of a consumer's household such as occupancy, habits and individual appliance
usage. Yet smart metering infrastructure has the potential to vastly reduce
carbon emissions from the energy sector through improved operating
efficiencies. We propose the application of a distributed machine learning
setting known as federated learning for energy demand forecasting at various
scales to make load prediction possible whilst retaining the privacy of
consumers' raw energy consumption data.",http://arxiv.org/abs/2012.07449v1
"Pandemic Informatics: Preparation, Robustness, and Resilience; Vaccine
  Distribution, Logistics, and Prioritization; and Variants of Concern",2020-12-16T22:33:29Z,"Elizabeth Bradley, Madhav Marathe, Melanie Moses, William D Gropp, Daniel Lopresti","Infectious diseases cause more than 13 million deaths a year, worldwide.
Globalization, urbanization, climate change, and ecological pressures have
significantly increased the risk of a global pandemic. The ongoing COVID-19
pandemic-the first since the H1N1 outbreak more than a decade ago and the worst
since the 1918 influenza pandemic-illustrates these matters vividly. More than
47M confirmed infections and 1M deaths have been reported worldwide as of
November 4, 2020 and the global markets have lost trillions of dollars. The
pandemic will continue to have significant disruptive impacts upon the United
States and the world for years; its secondary and tertiary impacts might be
felt for more than a decade. An effective strategy to reduce the national and
global burden of pandemics must: 1) detect timing and location of occurrence,
taking into account the many interdependent driving factors; 2) anticipate
public reaction to an outbreak, including panic behaviors that obstruct
responders and spread contagion; 3) and develop actionable policies that enable
targeted and effective responses.",http://arxiv.org/abs/2012.09300v3
"Towards Optimal District Heating Temperature Control in China with Deep
  Reinforcement Learning",2020-12-17T11:16:08Z,"Adrien Le-Coz, Tahar Nabil, Francois Courtot","Achieving efficiency gains in Chinese district heating networks, thereby
reducing their carbon footprint, requires new optimal control methods going
beyond current industry tools. Focusing on the secondary network, we propose a
data-driven deep reinforcement learning (DRL) approach to address this task. We
build a recurrent neural network, trained on simulated data, to predict the
indoor temperatures. This model is then used to train two DRL agents, with or
without expert guidance, for the optimal control of the supply water
temperature. Our tests in a multi-apartment setting show that both agents can
ensure a higher thermal comfort and at the same time a smaller energy cost,
compared to an optimized baseline strategy.",http://arxiv.org/abs/2012.09508v2
GRACE -- gravity data for understanding the deep Earth's interior,2020-12-20T16:20:20Z,"Mioara Mandea, V√©ronique Dehant, Anny Cazenave","While the main causes of the temporal gravity variations observed by the
GRACE space mission result from water mass redistributions occurring at the
surface of the Earth in response to climatic and anthropogenic forcings (e.g.,
changes in land hydrology, in ocean mass, in mass of glaciers and ice sheets),
solid Earth's mass redistributions are also recorded by these observations.
This is the case, in particular, for the Glacial Isostatic Adjustment (GIA) or
the viscous response of the mantle to the last deglaciation. However, it is
only recently showed that the gravity data also contain the signature of flows
inside the outer core and their effects on the core-mantle boundary (CMB).
Detecting deep Earth's processes in GRACE observations offers an exciting
opportunity to provide additional insight on the dynamics of the core-mantle
interface. Here, we present one aspect of the GRACEFUL (GRavimetry, mAgnetism
and CorE Flow) project, i.e. the possibility to use the gravity field data for
understanding the dynamic processes inside the fluid core and core-mantle
boundary of the Earth, beside that offered by the geomagnetic field variations.",http://arxiv.org/abs/2012.10964v1
"Modelling Human Routines: Conceptualising Social Practice Theory for
  Agent-Based Simulation",2020-12-22T10:06:47Z,"Rijk Mercuur, Virginia Dignum, Catholijn M. Jonker","Our routines play an important role in a wide range of social challenges such
as climate change, disease outbreaks and coordinating staff and patients in a
hospital. To use agent-based simulations (ABS) to understand the role of
routines in social challenges we need an agent framework that integrates
routines. This paper provides the domain-independent Social Practice Agent
(SoPrA) framework that satisfies requirements from the literature to simulate
our routines. By choosing the appropriate concepts from the literature on agent
theory, social psychology and social practice theory we ensure SoPrA correctly
depicts current evidence on routines. By creating a consistent, modular and
parsimonious framework suitable for multiple domains we enhance the usability
of SoPrA. SoPrA provides ABS researchers with a conceptual, formal and
computational framework to simulate routines and gain new insights into social
systems.",http://arxiv.org/abs/2012.11903v1
"A case study investigation of summer temperature conditions at two
  coastal sites in the UK, and analysis of future temperatures and heat wave
  structures in a warming climate scenario",2020-03-21T17:10:21Z,"Alexandra Edey, Ralph Burton, Alan Gadian","Using observations for two UK coastal sites <3 km from the sea, one on the
West coast near the nuclear new build (NNB) site Hinkley Point C (HPC) and the
other on the East coast, near the proposed NNB site Bradwell B (BRB), changes
in surface two-metre temperatures are analysed. The output from a numerical
model (WRF) experiment is used for a control period, 1990-1995, [Gadian et al.
2018] and for the period 2031-2036. The nested convective permitting model at a
resolution of O(3km) is driven by a global channel model at a resolution of
O(20km), enabling a more detailed comparison on the weather scale than is
available with current climate models. Further, using the RCP8.5 warming
scenario, the results are analysed. In the future scenario, there is an
increase in the number of days where the summer (JJA) model temperatures exceed
25{\deg}C. There is a future warming of 1.2{\deg}C (BRB) and 1.1{\deg}C (HPC)
in the mean JJA maximum daily temperatures compared with the control values and
an average annual maximum daily temperature warming of 1.2{\deg}C (BRB) and
0.5{\deg}C (HPC). For the control period, the model under-predicts both the
maximum and particularly the minimum temperatures. Results indicate there will
be a >25% increase in the number of summer days when the maximum temperature
exceeds 25{\deg}C, a 60% increase when the temperature exceeds the minimum of
13{\deg}C, (Tables 3, 4) and an increase in heat wave events per annum of
greater than 10 days, [Gadian et al. 2018]. The increases in summer
temperatures are larger than those predicted in the 2013 IPCC assessment
[Collins et al 2013], but consistent with the maximum temperatures and
increased number of hot days listed in UKCP18 [Met Office 2019(b)]. The higher
resolution model results suggest that the IPCC report underestimates the
increases in maximum temperatures at these locations.",http://arxiv.org/abs/2003.09702v1
Global to local impacts on atmospheric CO2 caused by COVID-19 lockdown,2020-10-25T03:40:50Z,"Ning Zeng, Pengfei Han, Di Liu, Zhiqiang Liu, Tomohiro Oda, Cory Martin, Zhu Liu, Bo Yao, Wanqi Sun, Pucai Wang, Qixiang Cai, Russell Dickerson, Shamil Maksyutov","The world-wide lockdown in response to the COVID-19 pandemic in year 2020 led
to economic slowdown and large reduction of fossil fuel CO2 emissions, but it
is unclear how much it would reduce atmospheric CO2 concentration, and whether
it can be observed. We estimated that a 7.9% reduction in emissions for 4
months would result in a 0.25 ppm decrease in the Northern Hemisphere CO2, an
increment that is within the capability of current CO2 analyzers, but is a few
times smaller than natural CO2 variabilities caused by weather and the
biosphere such as El Nino. We used a state-of-the-art atmospheric transport
model to simulate CO2, driven by a new daily fossil fuel emissions dataset and
hourly biospheric fluxes from a carbon cycle model forced with observed climate
variability. Our results show a 0.13 ppm decrease in atmospheric column CO2
anomaly averaged over 50S-50N for the period February-April 2020 relative to a
10-year climatology. A similar decrease was observed by the carbon satellite
GOSAT3. Using model sensitivity experiments, we further found that COVID, the
biosphere and weather contributed 54%, 23%, and 23% respectively. This
seemingly small change stands out as the largest sub-annual anomaly in the last
10 years. Measurements from global ground stations were analyzed. At city
scale, on-road CO2 enhancement measured in Beijing shows reduction of 20-30
ppm, consistent with drastically reduced traffic during the lockdown. The
ability of our current carbon monitoring systems in detecting the small and
short-lasting COVID signal on the background of fossil fuel CO2 accumulated
over the last two centuries is encouraging. The COVID-19 pandemic is an
unintended experiment whose impact suggests that to keep atmospheric CO2 at a
climate-safe level will require sustained effort of similar magnitude and
improved accuracy and expanded spatiotemporal coverage of our monitoring
systems.",http://arxiv.org/abs/2010.13025v1
"Tree species effects on topsoil carbon stock and concentration are
  mediated by tree species type, mycorrhizal association, and N-fixing ability
  at the global scale",2020-11-07T12:52:41Z,"Yan Peng, Inger Kappel Schmidt, Haifeng Zheng, Petr Hedƒõnec, Luciana Ruggiero Bachega, Kai Yue, Fuzhong Wu, Lars Vesterdal","Selection of appropriate tree species is an important forest management
decision that may affect sequestration of carbon (C) in soil. However,
information about tree species effects on soil C stocks at the global scale
remains unclear. Here, we quantitatively synthesized 850 observations from
field studies that were conducted in a common garden or monoculture plantations
to assess how tree species type (broadleaf vs. conifer), mycorrhizal
association (arbuscular mycorrhizal (AM) vs. ectomycorrhizal (ECM)), and
N-fixing ability (N-fixing vs. non-N-fixing), directly and indirectly, affect
topsoil (with a median depth of 10 cm) C concentration and stock, and how such
effects were influenced by environmental factors such as geographical location
and climate. We found that (1) tree species type, mycorrhizal association, and
N-fixing ability were all important factors affecting soil C, with lower forest
floor C stocks under broadleaved (44%), AM (39%), or N-fixing (28%) trees
respectively, but higher mineral soil C concentration (11%, 22%, and 156%) and
stock (9%, 10%, and 6%) under broadleaved, AM, and N-fixing trees respectively;
(2) tree species type, mycorrhizal association, and N-fixing ability affected
forest floor C stock and mineral soil C concentration and stock directly or
indirectly through impacting soil properties such as microbial biomass C and
nitrogen; (3) tree species effects on mineral soil C concentration and stock
were mediated by latitude, MAT, MAP, and forest stand age. These results reveal
how tree species and their specific traits influence forest floor C stock and
mineral soil C concentration and stock at a global scale. Insights into the
underlying mechanisms of tree species effects found in our study would be
useful to inform tree species selection in forest management or afforestation
aiming to sequester more atmospheric C in soil for mitigation of climate
change.",http://arxiv.org/abs/2011.03767v3
"Impact of gravity waves on the middle atmosphere of Mars: a
  non-orographic gravity wave parameterization based on Global Climate modeling
  and MCS observations",2020-02-03T13:39:25Z,"G. Gilli, F. Forget, A. Spiga, T. Navarro, E. Millour, L. Montabone, A. Kleinb√∂hl, D. M. Kass, D. J. McCleese, J. T. Schofield","The impact of gravity waves (GW) on diurnal tides and the global circulation
in the middle/upper atmosphere of Mars is investigated using a General
Circulation Model (GCM). We have implemented a stochastic parameterization of
non-orographic GW into the Laboratoire de M\'et\'eorologie Dynamique (LMD) Mars
GCM (LMD-MGCM) following an innovative approach. The source is assumed to be
located above typical convective cells ($\sim$ 250 Pa) and the effect of GW on
the circulation and predicted thermal structure above 1 Pa ($\sim$ 50 km) is
analyzed. We focus on the comparison between model simulations and observations
by the Mars Climate Sounder (MCS) on board Mars Reconnaissance Orbiter during
Martian Year 29. MCS data provide the only systematic measurements of the
Martian mesosphere up to 80 km to date. The primary effect of GW is to damp the
thermal tides by reducing the diurnal oscillation of the meridional and zonal
winds. The GW drag reaches magnitudes of the order of 1 m/s/sol above 10$^{-2}$
Pa in the northern hemisphere winter solstice and produces major changes in the
zonal wind field (from tens to hundreds of m/s), while the impact on the
temperature field is relatively moderate (10-20K). It suggests that GW induced
alteration of the meridional flow is the main responsible for the simulated
temperature variation. The results also show that with the GW scheme included,
the maximum day-night temperature difference due to the diurnal tide is around
10K, and the peak of the tide is shifted toward lower altitudes, in better
agreement with MCS observations.",http://arxiv.org/abs/2002.00723v1
"Great SCO2T! Rapid tool for carbon sequestration science, engineering,
  and economics",2020-05-27T22:30:07Z,"Richard S. Middleton, Jeffrey M. Bielicki, Bailian Chen, Andres F. Clarens, Robert P. Currier, Kevin M. Ellett, Dylan R. Harp, Brendan A. Hoover, Ryan M. Kammer, Dane N. McFarlane, Jonathan D. Ogland-Hand, Rajesh J. Pawar, Philip H. Stauffer, Hari S. Viswanathan, Sean P. Yaw","CO2 capture and storage (CCS) technology is likely to be widely deployed in
coming decades in response to major climate and economics drivers: CCS is part
of every clean energy pathway that limits global warming to 2C or less and
receives significant CO2 tax credits in the United States. These drivers are
likely to stimulate capture, transport, and storage of hundreds of millions or
billions of tonnes of CO2 annually. A key part of the CCS puzzle will be
identifying and characterizing suitable storage sites for vast amounts of CO2.
We introduce a new software tool called SCO2T (Sequestration of CO2 Tool,
pronounced ""Scott"") to rapidly characterizing saline storage reservoirs. The
tool is designed to rapidly screen hundreds of thousands of reservoirs, perform
sensitivity and uncertainty analyses, and link sequestration engineering
(injection rates, reservoir capacities, plume dimensions) to sequestration
economics (costs constructed from around 70 separate economic inputs). We
describe the novel science developments supporting SCO2T including a new
approach to estimating CO2 injection rates and CO2 plume dimensions as well as
key advances linking sequestration engineering with economics. Next, we perform
a sensitivity and uncertainty analysis of geology combinations (including
formation depth, thickness, permeability, porosity, and temperature) to
understand the impact on carbon sequestration. Through the sensitivity analysis
we show that increasing depth and permeability both can lead to increased CO2
injection rates, increased storage potential, and reduced costs, while
increasing porosity reduces costs without impacting the injection rate (CO2 is
injected at a constant pressure in all cases) by increasing the reservoir
capacity.",http://arxiv.org/abs/2005.13688v1
Effect of vegetation on the temperatures of Trappist-1 planets,2020-01-24T11:19:54Z,"Antonio Vecchio, Leonardo Primavera, Fabio Lepreti, Tommaso Alberti, Vincenzo Carbone","TRAPPIST-1 is an ultra-cool dwarf hosting a system consisting of seven
planets. While orbital properties, radii and masses of the planets are nowadays
well constrained, one of the open fascinating issues is the possibility that an
environment hospitable to life could develop on some of these planets. Here we
use a simple formulation of an energy balance model that includes the
vegetation coverage to investigate the possibility of life affecting the
climate of the planets in the TRAPPIST-1 system. Results confirm that planet
T-e has the best chance for a habitable world and indicate that vegetation
coverage significantly affects the resulting temperatures and habitability
properties. The influence of vegetation has been evaluated in different
scenarios characterized by different vegetation types, land-sea distributions
and levels of greenhouse effect. While changes in vegetation type produce small
changes, about $0.1\%$, in the habitable surface fraction, different land-sea
distributions, by also affecting the vegetation growth, produce different
temperature distributions. Finally at latitudes where vegetation grows, the
lowering of local albedo still represents a relevant contribution in settling
the planetary temperature profiles even when levels of greenhouse effect higher
than the Earth-like case are considered.",http://arxiv.org/abs/2001.08946v1
"Variations in stability revealed by temporal asymmetries in contraction
  of phase space flow",2020-02-28T21:10:36Z,"Zachary C Williams, Dylan E McNamara","Empirical diagnosis of stability has received considerable attention, mostly
focused on variance metrics for early warning signals of abrupt system change.
Despite this, the theoretical foundation and application has been limited to
relatively simple system changes such as bifurcating fixed points where
variability is extrinsic to the steady state. There is currently no foundation
and associated metric for empirically exploring stability in wide ranging
systems that contain variability in both internal steady state dynamics and in
response to external perturbations. Utilizing connections between stability,
dissipation, and phase space flow, we show that stability correlates with
temporal asymmetry in a measure of phase space flow contraction. Our method is
general as it reveals stability variation independent of assumptions about the
nature of system variability or attractor shape. After showing efficacy in a
variety of model systems, we apply our technique for measuring stability to
monthly returns of the S&P 500 index in the time periods surrounding the global
stock market crash of October 1987. Market stability is shown to be higher in
the several years preceding and subsequent to the 1987 market crash. We
anticipate our technique will have wide applicability in climate, ecological,
financial, and social systems where stability is a pressing concern.",http://arxiv.org/abs/2003.07174v2
The rise of science in low-carbon energy technologies,2020-04-21T12:47:04Z,"Kerstin H√∂tte, Anton Pichler, Fran√ßois Lafond","Successfully combating climate change will require substantial technological
improvements in Low-Carbon Energy Technologies (LCETs), but designing efficient
allocation of R\&D budgets requires a better understanding of how LCETs rely on
scientific knowledge. Using data covering almost all US patents and scientific
articles that are cited by them over the past two centuries, we describe the
evolution of knowledge bases of ten key LCETs and show how technological
interdependencies have changed over time. The composition of low-carbon energy
innovations shifted over time, from Hydro and Wind energy in the 19th and early
20th century, to Nuclear fission after World War II, and more recently to Solar
PV and back to Wind. In recent years, Solar PV, Nuclear fusion and Biofuels
(including energy from waste) have 35-65\% of their citations directed toward
scientific papers, while this ratio is less than 10\% for Wind, Solar thermal,
Hydro, Geothermal, and Nuclear fission. Over time, the share of patents citing
science and the share of citations that are to scientific papers has been
increasing for all technology types. The analysis of the scientific knowledge
base of each LCET reveals three fairly separate clusters, with nuclear energy
technologies, Biofuels and Waste, and all the other LCETs. Our detailed
description of knowledge requirements for each LCET helps to design of targeted
innovation policies.",http://arxiv.org/abs/2004.09959v2
"Resilient In-Season Crop Type Classification in Multispectral Satellite
  Observations using Growth Stage Normalization",2020-09-21T21:55:32Z,"Hannah Kerner, Ritvik Sahajpal, Sergii Skakun, Inbal Becker-Reshef, Brian Barker, Mehdi Hosseini, Estefania Puricelli, Patrick Gray","Crop type classification using satellite observations is an important tool
for providing insights about planted area and enabling estimates of crop
condition and yield, especially within the growing season when uncertainties
around these quantities are highest. As the climate changes and extreme weather
events become more frequent, these methods must be resilient to changes in
domain shifts that may occur, for example, due to shifts in planting timelines.
In this work, we present an approach for within-season crop type classification
using moderate spatial resolution (30 m) satellite data that addresses domain
shift related to planting timelines by normalizing inputs by crop growth stage.
We use a neural network leveraging both convolutional and recurrent layers to
predict if a pixel contains corn, soybeans, or another crop or land cover type.
We evaluated this method for the 2019 growing season in the midwestern US,
during which planting was delayed by as much as 1-2 months due to extreme
weather that caused record flooding. We show that our approach using growth
stage-normalized time series outperforms fixed-date time series, and achieves
overall classification accuracy of 85.4% prior to harvest (September-November)
and 82.8% by mid-season (July-September).",http://arxiv.org/abs/2009.10189v1
Sparse Regression for Extreme Values,2020-07-08T21:24:49Z,"Andersen Chang, Minjie Wang, Genevera Allen","We study the problem of selecting features associated with extreme values in
high dimensional linear regression. Normally, in linear modeling problems, the
presence of abnormal extreme values or outliers is considered an anomaly which
should either be removed from the data or remedied using robust regression
methods. In many situations, however, the extreme values in regression modeling
are not outliers but rather the signals of interest; consider traces from
spiking neurons, volatility in finance, or extreme events in climate science,
for example. In this paper, we propose a new method for sparse high-dimensional
linear regression for extreme values which is motivated by the Subbotin, or
generalized normal distribution, which we call the extreme value linear
regression model. For our method, we utilize an $\ell_p$ norm loss where $p$ is
an even integer greater than two; we demonstrate that this loss increases the
weight on extreme values. We prove consistency and variable selection
consistency for the extreme value linear regression with a Lasso penalty, which
we term the Extreme Lasso, and we also analyze the theoretical impact of
extreme value observations on the model parameter estimates using the concept
of influence functions. Through simulation studies and a real-world data
example, we show that the Extreme Lasso outperforms other methods currently
used in the literature for selecting features of interest associated with
extreme values in high-dimensional regression.",http://arxiv.org/abs/2007.04441v2
Symmetric and antisymmetric components of polar-amplified warming,2020-12-16T19:47:20Z,"Spencer A. Hill, Natalie J. Burls, Alexey Fedorov, Timothy M. Merlis","CO$_2$-forced surface warming in general circulation models (GCMs) is
initially polar-amplified in the Arctic but not Antarctic -- a largely
hemispherically antisymmetric signal. Nevertheless, we show in CESM1 and eleven
LongRunMIP GCMs that the hemispherically symmetric component of
global-mean-normalized, zonal-mean warming ($T^*_\mathrm{sym}$) under
4\(\times\)CO$_2$ changes weakly or becomes moderately more polar-amplified
from the first decade to near-equilibrium. Conversely, the antisymmetric
warming component ($T^*_\mathrm{asym}$) weakens with time in all models,
moderately in some including FAMOUS but effectively vanishing in others
including CESM1. We explore mechanisms underlying the robust $T^*_\mathrm{sym}$
behavior with a diffusive moist energy balance model (MEBM), which given
radiative feedback parameter ($\lambda$) and ocean heat uptake ($\mathcal{O}$)
fields diagnosed from CESM1 adequately reproduces the CESM1 $T^*_\mathrm{sym}$
and $T^*_\mathrm{asym}$ fields. In further MEBM simulations perturbing
$\lambda$ and $\mathcal{O}$, $T^*_\mathrm{sym}$ is sensitive to their symmetric
components only, and more to that of $\lambda$. A three-box, two-timescale
model fitted to FAMOUS and CESM1 reveals a curiously short Antarctic
fast-response timescale in FAMOUS. In additional CESM1 simulations spanning a
broader range of forcings, $T^*_\mathrm{sym}$ changes modestly across
2-16\(\times\)CO$_2$, and $T^*_\mathrm{sym}$ in a Pliocene-like simulation is
more polar-amplified but likewise approximately time-invariant. Determining the
real-world relevance of these behaviors -- which imply that a surprising amount
of information about near-equilibrium polar amplification emerges within
decades -- merits further study.",http://arxiv.org/abs/2012.09228v3
"Estimating heterogeneous wildfire effects using synthetic controls and
  satellite remote sensing",2020-12-09T16:22:16Z,"Feliu Serra-Burriel, Pedro Delicado, Andrew T. Prata, Fernando M. Cucchietti","Wildfires have become one of the biggest natural hazards for environments
worldwide. The effects of wildfires are heterogeneous, meaning that the
magnitude of their effects depends on many factors such as geographical region,
climate and land cover/vegetation type. Yet, which areas are more affected by
these events remains unclear. Here we present a novel application of the
Generalised Synthetic Control (GSC) method that enables quantification and
prediction of vegetation changes due to wildfires through a time-series
analysis of in situ and satellite remote sensing data. We apply this method to
medium to large wildfires ($>$ 1000 acres) in California throughout a time-span
of two decades (1996--2016). The method's ability for estimating counterfactual
vegetation characteristics for burned regions is explored in order to quantify
abrupt system changes. We find that the GSC method is better at predicting
vegetation changes than the more traditional approach of using nearby regions
to assess wildfire impacts. We evaluate the GSC method by comparing its
predictions of spectral vegetation indices to observations during pre-wildfire
periods and find improvements in correlation coefficient from $R^2 = 0.66$ to
$R^2 = 0.93$ in Normalised Difference Vegetation Index (NDVI), from $R^2 =
0.48$ to $R^2 = 0.81$ for Normalised Burn Ratio (NBR), and from $R^2 = 0.49$ to
$R^2 = 0.85$ for Normalised Difference Moisture Index (NDMI). Results show
greater changes in NDVI, NBR, and NDMI post-fire on regions classified as
having a lower Burning Index. The GSC method also reveals that wildfire effects
on vegetation can last for more than a decade post-wildfire, and in some cases
never return to their previous vegetation cycles within our study period.
Lastly, we discuss the usefulness of using GSC in remote sensing analyses.",http://arxiv.org/abs/2012.05140v3
Diurnal Self-Aggregation,2020-01-14T12:31:22Z,"Jan O. Haerter, Bettina Meyer, Silas Boye Nissen","Convective self-aggregation is a modelling paradigm for thunderstorm
organisation over a constant-temperature tropical sea surface. This setup can
give rise to cloud clusters over timescales of weeks. In reality, sea surface
temperatures do oscillate diurnally, affecting the atmospheric state. Over
land, surface temperatures vary more strongly, and rain rate is significantly
influenced. Here, we carry out a substantial suite of cloud-resolving numerical
experiments, and find that even weak surface temperature oscillations enable
qualitatively different dynamics to emerge: the spatial distribution of
rainfall is only homogeneous during the first day. Already on the second day,
the rain field is firmly structured. In later days, the clustering becomes
stronger and alternates from day-to-day. We show that these features are robust
to changes in resolution, domain size, and surface temperature, but can be
removed by a reduction of the amplitude of oscillation, suggesting a transition
to a clustered state. Maximal clustering occurs at a scale of
$\mathbf{l_{max}\approx 180\;km}$, a scale we relate to the emergence of
mesoscale convective systems. At $\mathbf{l_{max}}$ rainfall is strongly
enhanced and far exceeds the rainfall expected at random. We explain the
transition to clustering using simple conceptual modelling. Our results may
help clarify how continental extremes build up and how cloud clustering over
the tropical ocean could emerge much faster than through conventional
self-aggregation alone.",http://arxiv.org/abs/2001.04740v1
"Hierarchical Integrated Spatial Process Modeling of Monotone West
  Antarctic Snow Density Curves",2020-01-15T19:21:59Z,"Philip A. White, Durban G. Keeler, Summer Rupper","Snow density estimates below the surface, used with airplane-acquired
ice-penetrating radar measurements, give a site-specific history of snow water
accumulation. Because it is infeasible to drill snow cores across all of
Antarctica to measure snow density and because it is critical to understand how
climatic changes are affecting the world's largest freshwater reservoir, we
develop methods that enable snow density estimation with uncertainty in regions
where snow cores have not been drilled.
  In inland West Antarctica, snow density increases monotonically as a function
of depth, except for possible micro-scale variability or measurement error, and
it cannot exceed the density of ice. We present a novel class of integrated
spatial process models that allow interpolation of monotone snow density
curves. For computational feasibility, we construct the space-depth process
through kernel convolutions of log-Gaussian spatial processes. We discuss model
comparison, model fitting, and prediction. Using this model, we extend
estimates of snow density beyond the depth of the original core and estimate
snow density curves where snow cores have not been drilled. Along flight lines
with ice-penetrating radar, we use interpolated snow density curves to estimate
recent water accumulation and find predominantly decreasing water accumulation
over recent decades.",http://arxiv.org/abs/2001.05520v4
"Intra-annual Principal Modes and Evolution Mechanism of the El Nino
  Southern Oscillation",2020-03-28T18:46:57Z,"Yongwen Zhang, Jingfang Fan, Xiaoteng Li, Wenqi Liu, Xiaosong Chen","The El Nino-Southern Oscillation (ENSO) is one of the most important
phenomena in climate. By studying the fluctuations of surface air temperature
within one year between 1979-01-01 and 2016-12-31 of the region (30S-30N,
0E-360E) with eigen-decomposition, we find that the temperature fluctuations
are dominated by the two principal modes whose temporal evolutions respond
significantly to the ENSO variability. According to introduce a
micro-correlation, we find that the coupling between the first principal mode
and the temperature fluctuations in the El Nino region could result in
different ENSO phases. Without this coupling, the El Nino region is in a normal
phase. With the strong coupling between the El Nino region and the Northern
Hemisphere, an El Nino event will appear with a high probability. Then this
coupling changes to be strong between the El Nino region and the Southern
Hemisphere accounting for the fast decay of El Nino after boreal winter, even
leading to a La Nina event. Moreover, the coupling between the El Nino region
and the second principal mode is found to be strong in normal or La Nina phases
in response to the normal or strong Walker Circulations. We conjecture that the
temporal evolutions of these couplings for the first and second principal modes
are controlled by the meridional and zonal ocean-atmospheric circulations
respectively.",http://arxiv.org/abs/2003.12867v1
"Historical Evolution of Global Inequality in Carbon Emissions and
  Footprints versus Redistributive Scenarios",2020-03-30T00:51:40Z,"Gregor Semieniuk, Victor M. Yakovenko","Ambitious scenarios of carbon emission redistribution for mitigating climate
change in line with the Paris Agreement and reaching the sustainable
development goal of eradicating poverty have been proposed recently. They imply
a strong reduction in carbon footprint inequality by 2030 that effectively
halves the Gini coefficient to about 0.25. This paper examines feasibility of
these scenarios by analyzing the historical evolution of both weighted
international inequality in CO2 emissions attributed territorially and global
inequality in carbon footprints attributed to end consumers. For the latter, a
new dataset is constructed that is more comprehensive than existing ones. In
both cases, we find a decreasing trend in global inequality, partially
attributed to the move of China from the lower to the middle part of the
distribution, with footprints more unequal than territorial emissions. These
results show that realization of the redistributive scenarios would require an
unprecedented reduction in global inequality far below historical levels.
Moreover, the territorial emissions data, available for more recent years up to
2017, show a saturation of the decreasing Gini coefficient at a level of 0.5.
This observation confirms an earlier prediction based on maximal entropy
reasoning that the Lorenz curve converges to the exponential distribution. This
saturation further undermines feasibility of the redistributive scenarios,
which are also hindered by structural tendencies that reinforce carbon
footprint inequality under global capitalism. One way out of this conundrum is
a fast decarbonization of the global energy supply in order to decrease global
carbon emissions without relying crucially on carbon inequality reduction.",http://arxiv.org/abs/2004.00111v1
"A Bayesian approach to recover the theoretical temperature-dependent
  hatch date distribution from biased samples: the case of the common
  dolphinfish (Coryphaena hippurus)",2020-03-30T16:04:45Z,"Vicen√ß Molt√≥, Andres Ospina-Alvarez, Mark Gatt, Miquel Palmer, Ignacio A. Catal√°n","Reproductive phenology, growth and mortality rates are key ecological
parameters that determine population dynamics and are therefore of vital
importance to stock assessment models for fisheries management. In many fish
species, the spawning phenology is sensitive to environmental factors that
modulate or trigger the spawning event, which differ between regions and
seasons. In addition, climate change may also alter patterns of reproductive
phenology at the community level. Usually, hatch-date distributions are
determined back-calculating the age estimated on calcified structures from the
capture date. However, these estimated distributions could be biased due to
mortality processes or time spaced samplings derived from fishery. Here, we
present a Bayesian approach that functions as a predictive model for the
hatching date of individuals from a fishery-dependent sampling with temporal
biases. We show that the shape and shift of the observed distribution is
corrected. This model can be applied in fisheries with multiple cohorts, for
species with a wide geographical distribution and living under contrasting
environmental regimes and individuals with different life histories such as
thermo-dependent growth, length-dependent mortality rates, etc.",http://arxiv.org/abs/2004.01000v2
Deep Learning Based Multi-Label Text Classification of UNGA Resolutions,2020-04-01T18:54:38Z,"Francesco Sovrano, Monica Palmirani, Fabio Vitali","The main goal of this research is to produce a useful software for United
Nations (UN), that could help to speed up the process of qualifying the UN
documents following the Sustainable Development Goals (SDGs) in order to
monitor the progresses at the world level to fight poverty, discrimination,
climate changes. In fact human labeling of UN documents would be a daunting
task given the size of the impacted corpus. Thus, automatic labeling must be
adopted at least as a first step of a multi-phase process to reduce the overall
effort of cataloguing and classifying. Deep Learning (DL) is nowadays one of
the most powerful tools for state-of-the-art (SOTA) AI for this task, but very
often it comes with the cost of an expensive and error-prone preparation of a
training-set. In the case of multi-label text classification of domain-specific
text it seems that we cannot effectively adopt DL without a big-enough
domain-specific training-set. In this paper, we show that this is not always
true. In fact we propose a novel method that is able, through statistics like
TF-IDF, to exploit pre-trained SOTA DL models (such as the Universal Sentence
Encoder) without any need for traditional transfer learning or any other
expensive training procedure. We show the effectiveness of our method in a
legal context, by classifying UN Resolutions according to their most related
SDGs.",http://arxiv.org/abs/2004.03455v1
"Natural Disaster Classification using Aerial Photography Explainable for
  Typhoon Damaged Feature",2020-04-21T16:21:52Z,"Takato Yasuno, Masazumi Amakata, Masahiro Okano","Recent years, typhoon damages has become social problem owing to climate
change. In 9 September 2019, Typhoon Faxai passed on the Chiba in Japan, whose
damages included with electric provision stop because of strong wind recorded
on the maximum 45 meter per second. A large amount of tree fell down, and the
neighbor electric poles also fell down at the same time. These disaster
features have caused that it took 18 days for recovery longer than past ones.
Immediate responses are important for faster recovery. As long as we can,
aerial survey for global screening of devastated region would be required for
decision support to respond where to recover ahead. This paper proposes a
practical method to visualize the damaged areas focused on the typhoon disaster
features using aerial photography. This method can classify eight classes which
contains land covers without damages and areas with disaster. Using target
feature class probabilities, we can visualize disaster feature map to scale a
color range. Furthermore, we can realize explainable map on each unit grid
images to compute the convolutional activation map using Grad-CAM. We
demonstrate case studies applied to aerial photographs recorded at the Chiba
region after typhoon.",http://arxiv.org/abs/2004.10130v5
A graph-based formulation for modeling macro-energy systems,2020-04-21T17:47:23Z,Leonard G√∂ke,"Averting the impending harms of climate change requires to replace fossil
fuels with renewables as a primary source of energy. Non-electric renewable
potential being limited, this implies extending the use of electricity
generated from wind and solar beyond the power sector, either by direct
electrification or synthetic fuels. Modeling the transformation towards such an
energy system is challenging, because it imposes to consider fluctuations of
wind and solar and the manifold ways the demand side could adjust to these
fluctuations.
  This paper introduces a graph-based method to formulate energy system models
to address these challenges. By organizing sets in rooted trees, two features
to facilitate modeling high shares of renewables and sector integration are
enabled. First, the method allows the level of temporal and spatial detail to
be varied by energy carrier. This enables modeling with a high level of detail
and a large scope, while keeping models computationally tractable. Second, the
degree to which energy carriers are substitutable when converted, stored,
transported, or consumed can be modeled to achieve a detailed but flexible
representation of sector integration. An application of the formulation
demonstrates that the variation of temporal detail achieves an average
reduction in computation time of 70\%.",http://arxiv.org/abs/2004.10184v4
"Drivers of carbon fluxes in Alpine tundra: a comparison of three
  empirical model approaches",2020-04-29T15:16:26Z,"Marta Magnani, Ilaria Baneschi, Mariasilvia Giamberini, Pietro Mosca, Brunella Raco, Antonello Provenzale","In high mountains, the effects of climate change are manifesting most
rapidly. This is especially critical for the high-altitude carbon cycle, for
which new feedbacks could be triggered. However, mountain carbon dynamics is
only partially known. In particular, models of the processes driving carbon
fluxes in high-altitude grasslands and Alpine tundra need to be improved. Here,
we propose a comparison of three empirical approaches using systematic
statistical analysis, to identify the environmental variables controlling
$CO_2$ fluxes. The methods were applied to a complete dataset of simultaneous
in situ measurements of the net $CO_2$ exchange, ecosystem respiration and
basic environmental variables in three sampling sites in the same catchment.
Large year-to-year variations in the gross primary production (GPP) and
ecosystem respiration (ER) dependences on solar irradiance and temperature were
observed,. We thus implemented a multi regression model in which additional
variables were introduced as perturbations of the standard exponential and
rectangular hyperbolic functions for ER and GPP, respectively. A comparison of
this model with other common modelling strategies, showed the benefits of this
approach, resulting in large explained variances (83% to 94%). The optimum
ensemble of variables explaining the inter- and intra-annual flux variability
included solar irradiance, soil moisture and day of the year for GPP, and air
temperature, soil moisture, air pressure and day of the year for the ER, in
agreement with other studies. The modelling approach discussed here provides a
basis for selecting drivers of carbon fluxes and understanding their role in
high-altitude Alpine ecosystems, also allowing for future short-range
assessments of local trends.",http://arxiv.org/abs/2004.14262v1
"Integration of hydrothermal liquefaction and carbon capture and storage
  for the production of advanced liquid biofuels with negative CO2 emissions",2020-09-08T09:25:29Z,"E. M. Lozano, T. H. Pedersen, L. A. Rosendahl","The technical and economic feasibility to deliver sustainable liquid biocrude
through hydrothermal liquefaction (HTL) while enabling negative carbon dioxide
emissions is evaluated in this paper, looking into the potential of the process
in the context of negative emission technologies (NETs) for climate change
mitigation. In the HTL process, a gas phase consisting mainly of carbon dioxide
is obtained as a side product driving a potential for the implementation of
carbon capture and storage in the process (BECCS) that has not been explored
yet in the existing literature and is undertaken in this study. To this end,
the process is divided in a standard HTL base and a carbon capture add-on,
having forestry residues as feedstock. The Selexol technology is adapted in a
novel scheme to simultaneously separate the CO2 from the HTL gas and recover
the excess hydrogen for biocrude upgrading. The cost evaluation indicates that
the additional cost of the carbon capture can be compensated by revenues from
the excess process heat and the European carbon allowance market. The impact in
the MFSP of the HTL base case ranges from -7% to 3%, with -15% in the most
favorable scenario, with a GHG emissions reduction potential of 102-113%
compared to the fossil baseline. These results show that the implementation of
CCS in the HTL process is a promising alternative from technical, economic and
environmental perspective in future scenarios in which advanced liquid biofuels
and NETs are expected to play a role in the decarbonization of the energy
system.",http://arxiv.org/abs/2009.03600v1
"Deep Switching Auto-Regressive Factorization:Application to Time Series
  Forecasting",2020-09-10T20:15:59Z,"Amirreza Farnoosh, Bahar Azari, Sarah Ostadabbas","We introduce deep switching auto-regressive factorization (DSARF), a deep
generative model for spatio-temporal data with the capability to unravel
recurring patterns in the data and perform robust short- and long-term
predictions. Similar to other factor analysis methods, DSARF approximates high
dimensional data by a product between time dependent weights and spatially
dependent factors. These weights and factors are in turn represented in terms
of lower dimensional latent variables that are inferred using stochastic
variational inference. DSARF is different from the state-of-the-art techniques
in that it parameterizes the weights in terms of a deep switching vector
auto-regressive likelihood governed with a Markovian prior, which is able to
capture the non-linear inter-dependencies among weights to characterize
multimodal temporal dynamics. This results in a flexible hierarchical deep
generative factor analysis model that can be extended to (i) provide a
collection of potentially interpretable states abstracted from the process
dynamics, and (ii) perform short- and long-term vector time series prediction
in a complex multi-relational setting. Our extensive experiments, which include
simulated data and real data from a wide range of applications such as climate
change, weather forecasting, traffic, infectious disease spread and nonlinear
physical systems attest the superior performance of DSARF in terms of long- and
short-term prediction error, when compared with the state-of-the-art methods.",http://arxiv.org/abs/2009.05135v1
"Time-periodic measures, random periodic orbits, and the linear response
  for dissipative non-autonomous stochastic differential equations",2020-09-15T15:02:23Z,"Michal Branicki, Kenneth Uda","We consider a class of dissipative stochastic differential equations (SDE's)
with time-periodic coefficients in finite dimension, and the response of
time-asymptotic probability measures induced by such SDE's to sufficiently
regular, small perturbations of the underlying dynamics. Understanding such a
response provides a systematic way to study changes of statistical observables
in response to perturbations, and it is often very useful for sensitivity
analysis, uncertainty quantification, and for improving probabilistic
predictions of nonlinear dynamical systems, especially in high dimensions.
Here, we are concerned with the linear response to small perturbations in the
case when the time-asymptotic probability measures are time-periodic. First, we
establish sufficient conditions for the existence of stable random
time-periodic orbits generated by the underlying SDE. Ergodicity of
time-periodic probability measures supported on these random periodic orbits is
subsequently discussed. Then, we derive the so-called fluctuation-dissipation
relations which allow to describe the linear response of statistical
observables to small perturbations away from the time-periodic ergodic regime
in a manner which only exploits the unperturbed dynamics. The results are
formulated in an abstract setting but they apply to problems ranging from
aspects of climate modelling, to molecular dynamics, to the study of
approximation capacity of neural~networks and robustness of their estimates.",http://arxiv.org/abs/2009.07147v3
"Coordinated PV re-phasing: a novel method to maximize renewable energy
  integration in LV networks by mitigating network unbalances",2020-09-17T13:02:22Z,"W. G. Chaminda Bandara, G. M. R. I. Godaliyadda, M. P. B. Ekanayake, J. B. Ekanayake","As combating climate change has become a top priority and as many countries
are taking steps to make their power generation sustainable, there is a marked
increase in the use of renewable energy sources (RESs) for electricity
generation. Among these RESs, solar photovoltaics (PV) is one of the most
popular sources of energy connected to LV distribution networks. With the
greater integration of solar PV into LV distribution networks, utility
providers impose caps to solar penetration in order to operate their network
safely and within acceptable norms. One parameter that restricts solar PV
penetration is unbalances created by loads and single-phase rooftop schemes
connected to LV distribution grids. In this paper, a novel method is proposed
to mitigate voltage unbalance in LV distribution grids by optimally re-phasing
grid-connected rooftop PV systems. A modified version of the discrete bacterial
foraging optimization algorithm (DBFOA) is introduced as the optimization
technique to minimize the overall voltage unbalance of the network as the
objective function, subjected to various network and operating parameters. The
impact of utilizing the proposed PV re-phasing technique as opposed to a fixed
phase configuration are compared based on overall voltage unbalance, which was
observed hourly throughout the day. The case studies show that the proposed
approach can significantly mitigate the overall voltage unbalance during the
daytime and can facilitate to increase the usable PV capacity of the considered
network by 77%.",http://arxiv.org/abs/2009.08260v1
A social license for nuclear technologies,2020-09-16T19:50:27Z,Seth A. Hoedl,"Nuclear energy technologies have the potential to help mitigate climate
change. However, these technologies face many challenges, including high costs,
societal concern and opposition, and health, safety, environmental and
proliferation risks. Many companies and academic research groups are pursuing
advanced designs, both fission and fusion-based, to address both costs and
these risks. This Chapter complements these efforts by analyzing how nuclear
technologies can address societal concerns through the acquisition of a social
license, a nebulous concept that represents ""society's consent"" and that has
been used to facilitate and improve a wide range of publicly and privately
funded projects and activities subject to a range of regulatory oversight,
including large industrial facilities, controversial genetic engineering
research, and environmental management. Suggestions for public engagement and
consent-based siting, two aspects of a social license, have been made before.
This chapter modernizes these suggestions by briefly reviewing the social
license and engagement literature. The Chapter discusses, in the context of how
to acquire a social license, the role of government regulation, the role of
project proponents and government actors, and the role of four key principles,
including engendering trust, transparency, meaningful public engagement, and
protection of health, safety and the environment. Further, the Chapter uses the
social license concept to explain why some nuclear waste repositories have
succeeded while others languish and provides concrete recommendations for the
deployment of new nuclear waste repositories and advanced power plants, both
fission and fusion-based.",http://arxiv.org/abs/2009.09844v1
"Network analysis of ballast-mediated species transfer reveals important
  introduction and dispersal patterns in the Arctic",2020-09-27T02:15:29Z,"Mandana Saebi, Jian Xu, Salvatore R. Curasi, Erin K. Grey, Nitesh V. Chawla, David M. Lodge","Rapid climate change has wide-ranging implications for the Arctic region,
including sea ice loss, increased geopolitical attention, and expanding
economic activity, including a dramatic increase in shipping activity. As a
result, the risk of harmful non-native marine species being introduced into
this critical region will increase unless policy and management steps are
implemented in response. Using big data about shipping, ecoregions, and
environmental conditions, we leverage network analysis and data mining
techniques to assess, visualize, and project ballast water-mediated species
introductions into the Arctic and dispersal of non-native species within the
Arctic. We first identify high-risk connections between the Arctic and
non-Arctic ports that could be sources of non-native species over 15 years
(1997-2012) and observe the emergence of shipping hubs in the Arctic where the
cumulative risk of non-native species introduction is increasing. We then
consider how environmental conditions can constrain this Arctic introduction
network for species with different physiological limits, thus providing a
species-level tool for decision-makers. Next, we focus on within-Arctic
ballast-mediated species dispersal where we use higher-order network analysis
to identify critical shipping routes that may facilitate species dispersal
within the Arctic. The risk assessment and projection framework we propose
could inform risk-based assessment and management of ship-borne invasive
species in the Arctic.",http://arxiv.org/abs/2009.12728v1
"Rain-Code Fusion : Code-to-code ConvLSTM Forecasting Spatiotemporal
  Precipitation",2020-09-30T11:33:45Z,"Takato Yasuno, Akira Ishii, Masazumi Amakata","Recently, flood damage has become a social problem owing to unexperienced
weather conditions arising from climate change. An immediate response to heavy
rain is important for the mitigation of economic losses and also for rapid
recovery. Spatiotemporal precipitation forecasts may enhance the accuracy of
dam inflow prediction, more than 6 hours forward for flood damage mitigation.
However, the ordinary ConvLSTM has the limitation of predictable range more
than 3-timesteps in real-world precipitation forecasting owing to the
irreducible bias between target prediction and ground-truth value. This paper
proposes a rain-code approach for spatiotemporal precipitation code-to-code
forecasting. We propose a novel rainy feature that represents a temporal rainy
process using multi-frame fusion for the timestep reduction. We perform
rain-code studies with various term ranges based on the standard ConvLSTM. We
applied to a dam region within the Japanese rainy term hourly precipitation
data, under 2006 to 2019 approximately 127 thousands hours, every year from May
to October. We apply the radar analysis hourly data on the central broader
region with an area of 136 x 148 km2 . Finally we have provided sensitivity
studies between the rain-code size and hourly accuracy within the several
forecasting range.",http://arxiv.org/abs/2009.14573v6
Can Federated Learning Save The Planet?,2020-10-13T16:45:01Z,"Xinchi Qiu, Titouan Parcollet, Daniel J. Beutel, Taner Topal, Akhil Mathur, Nicholas D. Lane","Despite impressive results, deep learning-based technologies also raise
severe privacy and environmental concerns induced by the training procedure
often conducted in data centers. In response, alternatives to centralized
training such as Federated Learning (FL) have emerged. Perhaps unexpectedly,
FL, in particular, is starting to be deployed at a global scale by companies
that must adhere to new legal demands and policies originating from governments
and the civil society for privacy protection. However, the potential
environmental impact related to FL remains unclear and unexplored. This paper
offers the first-ever systematic study of the carbon footprint of FL. First, we
propose a rigorous model to quantify the carbon footprint, hence facilitating
the investigation of the relationship between FL design and carbon emissions.
Then, we compare the carbon footprint of FL to traditional centralized
learning. Our findings show FL, despite being slower to converge, can be a
greener technology than data center GPUs. Finally, we highlight and connect the
reported results to the future challenges and trends in FL to reduce its
environmental impact, including algorithms efficiency, hardware capabilities,
and stronger industry transparency.",http://arxiv.org/abs/2010.06537v6
"Sea-level rise and continuous adaptation: residual damage rises faster
  than the protection costs",2020-10-14T23:48:52Z,"Diego Rybski, Boris F. Prahl, Markus Boettle, J√ºrgen P. Kropp","Damage cost curves -- relating the typical damage of a natural hazard to its
physical magnitude -- represent an indispensable ingredient necessary for
climate change impact assessments. Combining such curves with the occurrence
probability of the considered natural hazard, expected damage and related risk
can be estimated. Here we study recently published city scale damage cost
curves for coastal flooding and demonstrate which insights can be gained from
the functions only. Therefore, we include protection cost curves -- relating
the typical investment costs necessary to protect a city against a natural
hazard of certain magnitude -- which are analogous to and consistent with the
above mentioned damage cost curves. Specifically, we motivate log-logistic
functions, which exhibit a power-law increase at the lower end, and fit them to
the cost curves. As expected, cities with large maximum potential loss
(typically large cities) are also more costly to protect. Moreover, we study
the idealized case of continuous adaptation, i.e.\ increasing protection levels
in the same pace as sea-level rise, and compare the associated costs with
residual damage from extreme events exceeding the protection. Based on the
fitted exponents we find that in almost all cities the residual damage rises
faster than the protection costs. Raising coastal protection can lead to lull
oneself in a deceptive safety.",http://arxiv.org/abs/2010.07439v1
"Implementation of a cathode directed streamer model in Air under
  different voltage stresses",2020-10-15T07:43:03Z,"F Boakye-Mensah, N Bonifaci, R Hanna, I Niyonzima","To find a viable alternative to SF6 with growing climate change regulations,
proper evaluation of alternatives such as compressed air ought to be done. For
medium voltage applications, the withstand voltage is used as the dimensioning
criteria and this is dependent on the initiation and propagation of streamers
which are precursors to electrical breakdown. For design optimization, a
thorough understanding of the initiation and propagation mechanisms of such
electrical discharges under different stresses, pressure etc. ought to be
studied experimentally and numerically also via a predictive model. Most of the
numerical studies have so far been done via homemade codes as streamer models
are not readily available in commercial software because of the complexity and
non-linearity of such computations. Recently, with the increased robustness of
the plasma module of the commercial finite element software, COMSOL(tm)
Multiphysics, streamer discharge models can be developed with reasonable
accuracy. In this paper, an implementation and validation approach is presented
for streamer evolution in air for different voltage stresses. Results of
simulations for short gaps ($\le$ 5 mm) under Standard Temperature and Pressure
(STP) conditions have been presented, analyzed and compared with some classical
papers to evaluate the suitability of such a model for further studies of
non-thermal electrical discharges. Index Terms-medium voltage, streamer
discharges, eco-friendly gas, numerical models.",http://arxiv.org/abs/2010.07570v1
"An Introduction to Electrocatalyst Design using Machine Learning for
  Renewable Energy Storage",2020-10-14T19:34:17Z,"C. Lawrence Zitnick, Lowik Chanussot, Abhishek Das, Siddharth Goyal, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Thibaut Lavril, Aini Palizhati, Morgane Riviere, Muhammed Shuaibi, Anuroop Sriram, Kevin Tran, Brandon Wood, Junwoong Yoon, Devi Parikh, Zachary Ulissi","Scalable and cost-effective solutions to renewable energy storage are
essential to addressing the world's rising energy needs while reducing climate
change. As we increase our reliance on renewable energy sources such as wind
and solar, which produce intermittent power, storage is needed to transfer
power from times of peak generation to peak demand. This may require the
storage of power for hours, days, or months. One solution that offers the
potential of scaling to nation-sized grids is the conversion of renewable
energy to other fuels, such as hydrogen or methane. To be widely adopted, this
process requires cost-effective solutions to running electrochemical reactions.
An open challenge is finding low-cost electrocatalysts to drive these reactions
at high rates. Through the use of quantum mechanical simulations (density
functional theory), new catalyst structures can be tested and evaluated.
Unfortunately, the high computational cost of these simulations limits the
number of structures that may be tested. The use of machine learning may
provide a method to efficiently approximate these calculations, leading to new
approaches in finding effective electrocatalysts. In this paper, we provide an
introduction to the challenges in finding suitable electrocatalysts, how
machine learning may be applied to the problem, and the use of the Open
Catalyst Project OC20 dataset for model training.",http://arxiv.org/abs/2010.09435v1
Digital image processing to detect subtle motion in stony coral,2020-10-31T16:19:58Z,"Shuaifeng Li, Liza M. Roger, Lokander Kumar, Nastassja Lewinski, Judith Klein, Alex Gagnon, Hollie M. Putnam, Jinkyu Yang","Coral reef ecosystems support significant biological activities and harbor
huge diversity, but they are facing a severe crisis driven by anthropogenic
activities and climate change. An important behavioral trait of the coral
holobiont is coral motion, which may play an essential role in feeding,
competition, reproduction, and thus survival and fitness. Therefore,
characterizing coral behavior through motion analysis will aid our
understanding of basic biological and physical coral functions. However, tissue
motion in the stony scleractinian corals that contribute most to coral reef
construction are subtle and may be imperceptible to both the human eye and
commonly used imaging techniques. Here we propose and apply a systematic
approach to quantify and visualize subtle coral motion across a series of light
and dark cycles in the scleractinian coral Montipora capricornis. We use
digital image correlation and optical flow techniques to quantify and
characterize minute coral motions under different light conditions. In
addition, as a visualization tool, motion magnification algorithm magnifies
coral motions in different frequencies, which explicitly displays the
distinctive dynamic modes of coral movement. We quantified and compared the
displacement, strain, optical flow, and mode shape of coral motion under
different light conditions. Our approach provides an unprecedented insight into
micro-scale coral movement and behavior through macro-scale digital imaging,
thus offering a useful empirical toolset for the coral research community.",http://arxiv.org/abs/2011.00304v1
"Influencing dynamics on social networks without knowledge of network
  microstructure",2020-11-11T13:49:48Z,"Matthew Garrod, Nick S. Jones","Social network based information campaigns can be used for promoting
beneficial health behaviours and mitigating polarisation (e.g. regarding
climate change or vaccines). Network-based intervention strategies typically
rely on full knowledge of network structure. It is largely not possible or
desirable to obtain population-level social network data due to availability
and privacy issues. It is easier to obtain information about individuals'
attributes (e.g. age, income), which are jointly informative of an individual's
opinions and their social network position. We investigate strategies for
influencing the system state in a statistical mechanics based model of opinion
formation. Using synthetic and data based examples we illustrate the advantages
of implementing coarse-grained influence strategies on Ising models with
modular structure in the presence of external fields. Our work provides a
scalable methodology for influencing Ising systems on large graphs and the
first exploration of the Ising influence problem in the presence of ambient
(social) fields. By exploiting the observation that strong ambient fields can
simplify control of networked dynamics, our findings open the possibility of
efficiently computing and implementing public information campaigns using
insights from social network theory without costly or invasive levels of data
collection.",http://arxiv.org/abs/2011.05774v2
Hurricane Forecasting: A Novel Multimodal Machine Learning Framework,2020-11-11T23:55:33Z,"L√©onard Boussioux, Cynthia Zeng, Th√©o Gu√©nais, Dimitris Bertsimas","This paper describes a novel machine learning (ML) framework for tropical
cyclone intensity and track forecasting, combining multiple ML techniques and
utilizing diverse data sources. Our multimodal framework, called Hurricast,
efficiently combines spatial-temporal data with statistical data by extracting
features with deep-learning encoder-decoder architectures and predicting with
gradient-boosted trees. We evaluate our models in the North Atlantic and
Eastern Pacific basins on 2016-2019 for 24-hour lead time track and intensity
forecasts and show they achieve comparable mean absolute error and skill to
current operational forecast models while computing in seconds. Furthermore,
the inclusion of Hurricast into an operational forecast consensus model could
improve over the National Hurricane Center's official forecast, thus
highlighting the complementary properties with existing approaches. In summary,
our work demonstrates that utilizing machine learning techniques to combine
different data sources can lead to new opportunities in tropical cyclone
forecasting.",http://arxiv.org/abs/2011.06125v4
"OGNet: Towards a Global Oil and Gas Infrastructure Database using Deep
  Learning on Remotely Sensed Imagery",2020-11-14T06:20:21Z,"Hao Sheng, Jeremy Irvin, Sasankh Munukutla, Shawn Zhang, Christopher Cross, Kyle Story, Rose Rustowicz, Cooper Elsworth, Zutao Yang, Mark Omara, Ritesh Gautam, Robert B. Jackson, Andrew Y. Ng","At least a quarter of the warming that the Earth is experiencing today is due
to anthropogenic methane emissions. There are multiple satellites in orbit and
planned for launch in the next few years which can detect and quantify these
emissions; however, to attribute methane emissions to their sources on the
ground, a comprehensive database of the locations and characteristics of
emission sources worldwide is essential. In this work, we develop deep learning
algorithms that leverage freely available high-resolution aerial imagery to
automatically detect oil and gas infrastructure, one of the largest
contributors to global methane emissions. We use the best algorithm, which we
call OGNet, together with expert review to identify the locations of oil
refineries and petroleum terminals in the U.S. We show that OGNet detects many
facilities which are not present in four standard public datasets of oil and
gas infrastructure. All detected facilities are associated with characteristics
known to contribute to methane emissions, including the infrastructure type and
the number of storage tanks. The data curated and produced in this study is
freely available at http://stanfordmlgroup.github.io/projects/ognet .",http://arxiv.org/abs/2011.07227v1
"Towards Optimal Coordination between Regional Groups: HVDC Supplementary
  Power Control",2020-11-25T14:43:35Z,"Andrea Tosatto, Georgios Misyris, Adri√† Junyent-Ferr√©, Fei Teng, Spyros Chatzivasileiadis","With Europe dedicated to limiting climate change and greenhouse gas
emissions, large shares of Renewable Energy Sources (RES) are being integrated
in the national grids, phasing out conventional generation. The new challenges
arising from the energy transition will require a better coordination between
neighboring system operators to maintain system security. To this end, this
paper studies the benefit of exchanging primary frequency reserves between
asynchronous areas using the Supplementary Power Control (SPC) functionality of
High-Voltage Direct-Current (HVDC) lines. First, we focus on the derivation of
frequency metrics for asynchronous AC systems coupled by HVDC interconnectors.
We compare two different control schemes for HVDC converters, which allow for
unilateral or bilateral exchanges of reserves between neighboring systems.
Second, we formulate frequency constraints and include them in a unit
commitment problem to ensure the N-1 security criterion. A data-driven approach
is proposed to better represent the frequency nadir constraint by means of
cutting hyperplanes. Our results suggest that the exchange of primary reserves
through HVDC can reduce up to 10% the cost of reserve procurement while
maintaining the system N-1 secure.",http://arxiv.org/abs/2011.14007v2
"Gaussian Process Regression for Probabilistic Short-term Solar Output
  Forecast",2020-02-23T15:54:37Z,"Fatemeh Najibi, Dimitra Apostolopoulou, Eduardo Alonso","With increasing concerns of climate change, renewable resources such as
photovoltaic (PV) have gained popularity as a means of energy generation. The
smooth integration of such resources in power system operations is enabled by
accurate forecasting mechanisms that address their inherent intermittency and
variability. This paper proposes a probabilistic framework to predict
short-term PV output taking into account the uncertainty of weather. To this
end, we make use of datasets that comprise of power output and meteorological
data such as irradiance, temperature, zenith, and azimuth. First, we categorise
the data into four groups based on solar output and time by using k-means
clustering. Next, a correlation study is performed to choose the weather
features which affect solar output to a greater extent. Finally, we determine a
function that relates the aforementioned selected features with solar output by
using Gaussian Process Regression and Matern 5/2 as a kernel function. We
validate our method with five solar generation plants in different locations
and compare the results with existing methodologies. More specifically, in
order to test the proposed model, two different methods are used: (i) 5-fold
cross-validation; and (ii) holding out 30 random days as test data. To confirm
the model accuracy, we apply our framework 30 independent times on each of the
four clusters. The average error follows a normal distribution, and with 95%
confidence level, it takes values between -1.6% to 1.4%.",http://arxiv.org/abs/2002.10878v1
"Evaluating the leeway coefficient for different ocean drifters using
  operational models",2020-05-19T15:37:24Z,"Graig Sutherland, Nancy Soontiens, Fraser Davidson, Gregory C. Smith, Natacha Bernier, Hauke Blanken, Douglas Schillinger, Guillaume Marcotte, Johannes R√∂hrs, Knut-Frode Dagestad, Kai H. Christensen, Oyvind Breivik","The water following characteristics of six different drifter types are
investigated using two different operational marine environmental prediction
systems: one produced by Environment and Climate Change Canada (ECCC) and the
other produced by the Norwegian Meteorological Institute (METNO). These marine
prediction systems include ocean circulation models, atmospheric models, and
surface wave models. Two leeway models are tested for use in drift object
prediction: an implicit leeway model where the Stokes drift is implicit in the
leeway coefficient, and an explicit leeway model where the Stokes drift is
provided by the wave model. Both leeway coefficients are allowed to vary in
direction and time in order to perfectly reproduce the observed drifter
trajectory. This creates a time series of the leeway coefficients which exactly
reproduce the observed drifter trajectories. Mean values for the leeway
coefficients are consistent with previous studies which utilized direct
observations of the leeway. For all drifters and models, the largest source of
variance in the leeway coefficient occurs at the inertial frequency and the
evidence suggests it is related to uncertainties in the ocean inertial
currents.",http://arxiv.org/abs/2005.09527v1
"1150 year long ice core record of the Ross Sea Polynya, Antarctica",2020-05-29T16:32:53Z,"Thomas Manuel Beers, Paul Andrew Mayewski, Andrei Kurbatov, Daniel Dixon, Nancy Bertler, Sean Birkel, Kirk A. Maasch, Michael Handley, Jeff Auger, Thomas Blunier, Peter Neff, Andrea Tuohy, Marius Folden Simonsen","Knowledge of the past behavior of Antarctic polynyas such as the Ross and
Weddell Seas contributes to the understanding of biological productivity, sea
ice production, katabatic and Southern Hemisphere Westerly (SHW) wind strength,
Antarctic bottom water (ABW) formation, and marine CO2 sequestration. Previous
studies link barium (Ba) marine sedimentation to polynya primary productivity
(Bonn et al., 1998; McManus et al., 2002; Pirrung et al., 2008), polynya area
to katabatic wind strength and proximal cyclones (Bromwich et al., 1998;
Drucker et al., 2011), and highlight the influence of Ross Ice Shelf calving
event effects on the Ross Sea Polynya (RSP) (Rhodes et al., 2009). Here we use
the RICE ice core, located just 120 km from the Ross Ice Shelf front to capture
1150 years of RSP behavior. We link atmospheric Ba fluctuations to Ba marine
sedimentation in the summer Ross Sea Polynya, creating the first deep ice core
based RSP proxy. RSP area is currently the smallest ever observed over our
1150-year record, and varied throughout the Little Ice Age with fluctuations in
Amundsen Sea Low (ASL) strength related with Ross Sea cyclones. Past RSP
reconstructions allow us to predict future responses of RSP area to future
climate change, which is of special interest considering the recent
disappearance of the Weddell Sea Polynya in response to anthropogenic forcing
(de Lavergne et al., 2014).",http://arxiv.org/abs/2006.01093v1
"Optimizing carbon tax for decentralized electricity markets using an
  agent-based model",2020-05-28T06:54:43Z,"Alexander J. M. Kell, A. Stephen McGough, Matthew Forshaw","Averting the effects of anthropogenic climate change requires a transition
from fossil fuels to low-carbon technology. A way to achieve this is to
decarbonize the electricity grid. However, further efforts must be made in
other fields such as transport and heating for full decarbonization. This would
reduce carbon emissions due to electricity generation, and also help to
decarbonize other sources such as automotive and heating by enabling a
low-carbon alternative. Carbon taxes have been shown to be an efficient way to
aid in this transition. In this paper, we demonstrate how to to find optimal
carbon tax policies through a genetic algorithm approach, using the electricity
market agent-based model ElecSim. To achieve this, we use the NSGA-II genetic
algorithm to minimize average electricity price and relative carbon intensity
of the electricity mix. We demonstrate that it is possible to find a range of
carbon taxes to suit differing objectives. Our results show that we are able to
minimize electricity cost to below \textsterling10/MWh as well as carbon
intensity to zero in every case. In terms of the optimal carbon tax strategy,
we found that an increasing strategy between 2020 and 2035 was preferable. Each
of the Pareto-front optimal tax strategies are at least above
\textsterling81/tCO2 for every year. The mean carbon tax strategy was
\textsterling240/tCO2.",http://arxiv.org/abs/2006.01601v1
"Sustainability of ICT hardware procurement in Switzerland -- A
  status-quo analysis of the public procurement sector",2020-06-09T16:04:36Z,"Tobias Welz, Matthias Stuermer","Sustainable procurement requires organizations to align their purchasing
behavior with regard to broader goals linked to resource efficiency, climate
change, social responsibility and other sustainability criteria. The level of
sustainability in Information and Communication Technology (ICT) hardware
procurement is analyzed for two reasons: First, ICT hardware belongs to the six
key product groups in sustainable procurement. Second, ICT in general is
expected to be an important enabler for low-carbon economies, providing
solutions to reduce Green-House Gas (GHG) emissions. While the advantages of
sustainable procurement are obvious, certain barriers hinder the adoption in
day-to-day procurement. This case study on ICT hardware discusses the three
important barriers ""lack of clear definitions per product group"", ""missing
market intelligence about sustainable products"" and ""inflexible procedures and
attitudes as barriers for innovative approaches"" based on an in-depth analysis
of sustainable procurement of ICT hardware by the public sector in Switzerland.
To this end, tender data published on the national procurement platform
simap.ch is screened for sustainability criteria using the Common Procurement
Vocabulary (CPV) nomenclature to identify relevant ICT projects. The results
reveal to which extent such criteria as well as their determinants are
currently included in public tenders. Using two different approaches, only a
small number of tenders were found containing sustainability criteria of a wide
range from basic to comprehensive. The overall performance of Swiss public
procurement is benchmarked by comparing the identified sustainability criteria
with available criteria from international key actors in sustainable
procurement. Thus, this analysis provides novel insights on how public agencies
today take sustainability into account when procuring ICT hardware.",http://arxiv.org/abs/2006.05372v1
"The 2019/20 Australian wildfires generated a persistent smoke-charged
  vortex rising up to 35 km altitude",2020-06-12T16:07:47Z,"Sergey Khaykin, Bernard Legras, Silvia Bucci, Pasquale Sellitto, Lars Isaksen, Florent Tence, Slimane Bekki, Adam Bourassa, Landon Rieger, Daniel Zawada, Julien Jumelet, Sophie Godin-Beekman","The Australian bushfires around the turn of the year 2020 generated an
unprecedented perturbation of stratospheric composition, dynamical circulation
and radiative balance. Here we show from satellite observations that the
resulting planetary-scale blocking of solar radiation by the smoke is larger
than any previously documented wildfires and of the same order as the radiative
forcing produced by moderate volcanic eruptions. A striking effect of the solar
heating of an intense smoke patch was the generation of a self-maintained
anticyclonic vortex measuring 1000 km in diameter and featuring its own ozone
hole. The highly stable vortex persisted in the stratosphere for over 13 weeks,
travelled 66,000 km and lifted a confined bubble of smoke and moisture to 35 km
altitude. Its evolution was tracked by several satellite-based sensors and was
successfully resolved by the European Centre for Medium-Range Weather Forecasts
operational system, primarily based on satellite data. Because wildfires are
expected to increase in frequency and strength in a changing climate, we
suggest that extraordinary events of this type may contribute significantly to
the global stratospheric composition in the coming decades.",http://arxiv.org/abs/2006.07284v2
"Investigating the biological potential of galactic cosmic ray-induced
  radiation-driven chemical disequilibrium in the Martian subsurface
  environment",2020-06-15T15:38:41Z,Dimitra Atri,"There is growing evidence suggesting the presence of aqueous environment on
ancient Mars, raising the question of the possibility of life in such an
environment. Subsequently, with the erosion of the Martian atmosphere resulting
in drastic changes in its climate, surface water disappeared, shrinking
habitable spaces on the planet, with only a limited amount of water remaining
near the surface in form of brines and water-ice deposits. Life, if it ever
existed, would have had to adapt to harsh modern conditions, which includes low
temperatures and surface pressure, and high radiation dose. Presently, there is
no evidence of any biological activity on the planet's surface, however, the
subsurface environment, which is yet to be explored, is less harsh, has traces
of water in form of water-ice and brines, and undergoes radiation-driven redox
chemistry. I hypothesize that Galactic Cosmic Ray (GCR)-induced
radiation-driven chemical disequilibrium can be used for metabolic energy by
extant life, and host organisms using mechanisms seen in similar chemical and
radiation environments on Earth. I propose a GCR-induced radiolytic zone, and
discuss the prospects of finding such life with Rosalind Franklin rover of the
ExoMars mission.",http://arxiv.org/abs/2006.08483v1
"A Methodology for Assessing the Environmental Effects Induced by ICT
  Services. Part II: Multiple Services and Companies",2020-06-18T20:12:26Z,"Pernilla Bergmark, Vlad C. CoroamƒÉ, Mattias H√∂jer, Craig Donovan","Information and communication technologies (ICT) can make existing products
and activities more efficient or substitute them altogether and could thus
become crucial for the mitigation of climate change. In this context,
individual ICT companies, industry organizations and international initiatives
have started to estimate the environmental effects of ICT services. Often such
assessments rely on crude assumptions and methods, yielding inaccurate or even
misleading results. The few existing methodological attempts are too general to
provide guidance to practitioners. The starting points of this paper are i) a
high level standard from the European Telecommunication Standardisation
Institute (ETSI) and the International Telecommunication Union (ITU), and ii)
its suggested enhancements for single service assessment outlined in ""A
Methodology for Assessing the Environmental Effects Induced by ICT Services
Part I: Single services"" (Part I in short). Building on the assessment of
single services, the current article identifies and addresses shortcomings of
existing methodologies and industry practices with regard to multiple services
assessment. For a collection of services, it addresses the goal and scope
definition, the so far ignored aggregation of effects among several services,
and the allocation between several companies contributing to one or more
services. The article finally brings these considerations together with those
of Part I into a workflow for performing such assessments in practice.",http://arxiv.org/abs/2006.10838v1
Graph Learning for Inverse Landscape Genetics,2020-06-22T15:26:30Z,"Prathamesh Dharangutte, Christopher Musco","The problem of inferring unknown graph edges from numerical data at a graph's
nodes appears in many forms across machine learning. We study a version of this
problem that arises in the field of \emph{landscape genetics}, where genetic
similarity between organisms living in a heterogeneous landscape is explained
by a weighted graph that encodes the ease of dispersal through that landscape.
Our main contribution is an efficient algorithm for \emph{inverse landscape
genetics}, which is the task of inferring this graph from measurements of
genetic similarity at different locations (graph nodes). Inverse landscape
genetics is important in discovering impediments to species dispersal that
threaten biodiversity and long-term species survival. In particular, it is
widely used to study the effects of climate change and human development.
Drawing on influential work that models organism dispersal using graph
\emph{effective resistances} (McRae 2006), we reduce the inverse landscape
genetics problem to that of inferring graph edges from noisy measurements of
these resistances, which can be obtained from genetic similarity data. Building
on the NeurIPS 2018 work of Hoskins et al. 2018 on learning edges in social
networks, we develop an efficient first-order optimization method for solving
this problem. Despite its non-convex nature, experiments on synthetic and real
genetic data establish that our method provides fast and reliable convergence,
significantly outperforming existing heuristics used in the field. By providing
researchers with a powerful, general purpose algorithmic tool, we hope our work
will have a positive impact on accelerating work on landscape genetics.",http://arxiv.org/abs/2006.12334v3
Impact of Tides on the Potential for Exoplanets to Host Exomoons,2020-07-03T04:04:11Z,"Armen Tokadjian, Anthony L. Piro","Exomoons may play an important role in determining the habitability of worlds
outside of our solar system. They can stabilize conditions, alter the climate
by breaking tidal locking with the parent star, drive tidal heating, and
perhaps even host life themselves. However, the ability of an exoplanet to
sustain an exomoon depends on complex tidal interactions. Motivated by this, we
make use of simplified tidal lag models to follow the evolution of the
separations and orbital and rotational periods in planet, star, and moon
systems. We apply these models to known exoplanet systems to assess the
potential for these exoplanets to host exomoons. We find that there are at
least 36 systems in which an exoplanet in the habitable zone may host an
exomoon for longer than one gigayear. This includes Kepler-1625b, an exoplanet
with an exomoon candidate, which we determine would be able to retain a
Neptune-sized moon for longer than a Hubble time. These results may help
provide potential targets for future observation. In many cases, there remains
considerable uncertainty in the composition of specific exoplanets. We show the
detection (or not) of an exomoon would provide an important constraint on the
planet structure due to differences in their tidal response.",http://arxiv.org/abs/2007.01487v2
"Degrees of individual and groupwise backward and forward responsibility
  in extensive-form games with ambiguity, and their application to social
  choice problems",2020-07-09T13:19:13Z,"Jobst Heitzig, Sarah Hiller","Many real-world situations of ethical relevance, in particular those of
large-scale social choice such as mitigating climate change, involve not only
many agents whose decisions interact in complicated ways, but also various
forms of uncertainty, including quantifiable risk and unquantifiable ambiguity.
In such problems, an assessment of individual and groupwise moral
responsibility for ethically undesired outcomes or their responsibility to
avoid such is challenging and prone to the risk of under- or overdetermination
of responsibility. In contrast to existing approaches based on strict causation
or certain deontic logics that focus on a binary classification of
`responsible' vs `not responsible', we here present several different
quantitative responsibility metrics that assess responsibility degrees in units
of probability. For this, we use a framework based on an adapted version of
extensive-form game trees and an axiomatic approach that specifies a number of
potentially desirable properties of such metrics, and then test the developed
candidate metrics by their application to a number of paradigmatic social
choice situations. We find that while most properties one might desire of such
responsibility metrics can be fulfilled by some variant, an optimal metric that
clearly outperforms others has yet to be found.",http://arxiv.org/abs/2007.07352v1
"A Comprehensive Review of Deep Learning Applications in Hydrology and
  Water Resources",2020-06-17T16:57:17Z,"Muhammed Sit, Bekir Z. Demiray, Zhongrun Xiang, Gregory J. Ewing, Yusuf Sermet, Ibrahim Demir","The global volume of digital data is expected to reach 175 zettabytes by
2025. The volume, variety, and velocity of water-related data are increasing
due to large-scale sensor networks and increased attention to topics such as
disaster response, water resources management, and climate change. Combined
with the growing availability of computational resources and popularity of deep
learning, these data are transformed into actionable and practical knowledge,
revolutionizing the water industry. In this article, a systematic review of
literature is conducted to identify existing research which incorporates deep
learning methods in the water sector, with regard to monitoring, management,
governance and communication of water resources. The study provides a
comprehensive review of state-of-the-art deep learning approaches used in the
water industry for generation, prediction, enhancement, and classification
tasks, and serves as a guide for how to utilize available deep learning methods
for future water resources challenges. Key issues and challenges in the
application of these techniques in the water domain are discussed, including
the ethics of these technologies for decision-making in water resources
management and governance. Finally, we provide recommendations and future
directions for the application of deep learning models in hydrology and water
resources.",http://arxiv.org/abs/2007.12269v1
How the growth of ice depends on the fluid dynamics underneath,2020-07-28T14:16:08Z,"Ziqi Wang, Enrico Calzavarini, Chao Sun, Federico Toschi","Convective flows coupled with solidification or melting in water bodies play
a major role in shaping geophysical landscapes. Particularly in relation to the
global climate warming scenario, it is essential to be able to accurately
quantify how water-body environments dynamically interplay with ice formation
or melting process. Previous studies have revealed the complex nature of the
icing process, but have often ignored one of the most remarkable particularity
of water, its density anomaly, and the induced stratification layers
interacting and coupling in a complex way in presence of turbulence and phase
change. By combining experiments, numerical simulations, and theoretical
modeling, we investigate solidification of freshwater, properly considering
phase transition, water density anomaly, and real physical properties of ice
and water phases, which we show to be essential for correctly predicting the
different qualitative and quantitative behaviors. We identify, with increasing
thermal driving, four distinct flow-dynamics regimes, where different levels of
coupling among ice front, stably and unstably stratified water layers occur.
Despite the complex interaction between the ice front and fluid motions,
remarkably, the average ice thickness and growth rate can be well captured with
the theoretical model. It is revealed that the thermal driving has major
effects on the temporal evolution of the global icing process, which can vary
from a few days to a few hours in the current parameter regime. Our model can
be applied to general situations where the icing dynamics occurs under
different thermal and geometrical conditions (e.g. cooling conditions or water
layer depth).",http://arxiv.org/abs/2007.14252v2
"How Does ENSO Impact the Solar Radiation Forecast in South America? The
  Self-affinity Analysis Approach",2020-07-23T21:31:54Z,"Thiago B. Murari, Aloisio S. Nascimento Filho, Marcelo A. Moret, Sergio Pitombo, Alex A. B. Santos","The major challenge we face today in the energy sector is to meet the growing
demand for electricity with less impact on the environment. South America is an
important player in the renewable energy resource. Brazil accelerated the
growth of photovoltaic installed capacity in 2018. From April of 2017 to April
of 2018, the capacity increased $1351.5\%$. It is expected to reach the value
of $2.4\ GW$ until the end of the year. The new Chilean regulation request that
20\% of the total electricity production in 2025 must come from renewable
energy sources. The aim of this paper is to establish time series behavior
changes between El Ni\~no Southern Oscillation and the solar radiation resource
in South America. The results can be used to validate forecasts of energy
production for new solar plants. The method used to verify the behavior of the
time series was the Detrended Fluctuation Analysis. Solar radiation data were
collected in twenty-five cities distributed inside the Brazilian solar belt,
plus six cities in Chile, covering the continent from east to west, in a region
with high potential of solar photovoltaic generation. The results shows the
impact of El Ni\~no Southern Oscillation on the climatic behavior of the
evaluated data. It is a factor that may lead to the wrong forecast of the long
term potential solar power generation for the region.",http://arxiv.org/abs/2007.15637v1
"Could the Migration of Jupiter have Accelerated the Atmospheric
  Evolution of Venus?",2020-08-11T18:00:21Z,"Stephen R. Kane, Pam Vervoort, Jonathan Horner, Francisco J. Pozuelos","In the study of planetary habitability and terrestrial atmospheric evolution,
the divergence of surface conditions for Venus and Earth remains an area of
active research. Among the intrinsic and external influences on the Venusian
climate history are orbital changes due to giant planet migration that have
both variable incident flux and tidal heating consequences. Here, we present
the results of a study that explores the effect of Jupiter's location on the
orbital parameters of Venus and subsequent potential water loss scenarios. Our
dynamical simulations show that various scenarios of Jovian migration could
have resulted in orbital eccentricities for Venus as high as 0.31. We quantify
the implications of the increased eccentricity, including tidal energy, surface
energy flux, and the variable insolation flux expected from the faint young
Sun. The tidal circularization timescale calculations demonstrate that a
relatively high tidal dissipation factor is required to reduce the eccentricity
of Venus to the present value, which implies a high initial water inventory. We
further estimate the consequences of high orbital eccentricity on water loss,
and estimate that the water loss rate may have increased by at least $\sim$5\%
compared with the circular orbit case as a result of orbital forcing. We argue
that these eccentricity variations for the young Venus may have accelerated the
atmospheric evolution of Venus towards the inevitable collapse of the
atmosphere into a runaway greenhouse state. The presence of giant planets in
exoplanetary systems may likewise increase the expected rate of Venus analogs
in those systems.",http://arxiv.org/abs/2008.04927v2
"Modelling uncertainty in coupled electricity and gas systems -- is it
  worth the effort?",2020-08-17T11:08:09Z,"Iegor Riepin, Thomas M√∂bius, Felix M√ºsgens","The interdependence of electricity and natural gas markets is becoming a
major topic in energy research. Integrated energy models are used to assist
decision-making for businesses and policymakers addressing challenges of energy
transition and climate change. The analysis of complex energy systems requires
large-scale models, which are based on extensive databases, intertemporal
dynamics and a multitude of decision variables. Integrating such energy system
models results in increased system complexity. This complexity poses a
challenge for energy modellers to address multiple uncertainties that affect
both markets. Stochastic optimisation approaches enable an adequate
consideration of uncertainties in investment and operation planning; however,
stochastic modelling of integrated large-scale energy systems further scales
the level of complexity. In this paper, we combine integrated and stochastic
optimisation problems and parametrise our model for European electricity and
gas markets. We analyse and compare the impact of uncertain input parameters,
such as gas and electricity demand, renewable energy capacities and fuel and
CO2 prices, on the quality of the solution obtained in the integrated
optimisation problem. Our results quantify the value of encoding uncertainty as
a part of a model. While the methodological contribution should be of interest
for energy modellers, our findings are relevant for industry experts and
stakeholders with an empirical interest in the European energy system.",http://arxiv.org/abs/2008.07221v2
"A modelling study of hydrodynamical and biogeochemical processes within
  the California Upwelling System",2020-08-28T17:15:18Z,"Karsten Alexander Lettmann, Florian Hahner, Vanessa Schakau, Tim W√ºllner, Cora Kohlmeier","The ROMS modeling system was applied to the California Upwelling System
(CalUS) to understand the key hydrodynamic conditions and dynamics of the
nitrogen-based ecosystem using the NPZD model proposed by Powell et al. (2006).
A new type of sponge layer has been successfully implemented in the ROMS
modelling system in order to stabilize the hydrodynamic part of the modeling
system when using so-called reduced boundary conditions. The hydrodynamic
performance of the model was examined using a tidal analysis based on tidal
measurement data, a comparison of the modeled sea surface temperature (SST)
with buoy and satellite data, and vertical sections of the currents along the
coast and the water temperature. This validation process shows that the
hydrodynamic module used in this study can reproduce the basic hydrodynamic and
circulation characteristics within the CalUS. The results of the ecosystem
model show the characteristic features of upwelling regions as well as the
well-known spotty horizontal structures of the zooplankton community. The model
thus provides a solid basis for the hydrodynamic and ecological characteristics
of the CalUS and enables the ecological model to be expanded into a complex
ecological model for investigating the effects of climate change on the
ecological balance in the area investigated.",http://arxiv.org/abs/2008.12749v1
Measuring and Managing Carbon Risk in Investment Portfolios,2020-08-30T15:41:02Z,"Th√©o Roncalli, Th√©o Le Guenedal, Fr√©d√©ric Lepetit, Thierry Roncalli, Takaya Sekine","This article studies the impact of carbon risk on stock pricing. To address
this, we consider the seminal approach of G\""orgen \textsl{et al.} (2019), who
proposed estimating the carbon financial risk of equities by their carbon beta.
To achieve this, the primary task is to develop a brown-minus-green (or BMG)
risk factor, similar to Fama and French (1992). Secondly, we must estimate the
carbon beta using a multi-factor model. While G\""orgen \textsl{et al.} (2019)
considered that the carbon beta is constant, we propose a time-varying
estimation model to assess the dynamics of the carbon risk. Moreover, we test
several specifications of the BMG factor to understand which climate
change-related dimensions are priced in by the stock market. In the second part
of the article, we focus on the carbon risk management of investment
portfolios. First, we analyze how carbon risk impacts the construction of a
minimum variance portfolio. As the goal of this portfolio is to reduce
unrewarded financial risks of an investment, incorporating the carbon risk into
this approach fulfils this objective. Second, we propose a new framework for
building enhanced index portfolios with a lower exposure to carbon risk than
capitalization-weighted stock indices. Finally, we explore how carbon
sensitivities can improve the robustness of factor investing portfolios.",http://arxiv.org/abs/2008.13198v1
Artificial Intelligence & Cooperation,2020-12-10T23:54:31Z,"Elisa Bertino, Finale Doshi-Velez, Maria Gini, Daniel Lopresti, David Parkes","The rise of Artificial Intelligence (AI) will bring with it an
ever-increasing willingness to cede decision-making to machines. But rather
than just giving machines the power to make decisions that affect us, we need
ways to work cooperatively with AI systems. There is a vital need for research
in ""AI and Cooperation"" that seeks to understand the ways in which systems of
AIs and systems of AIs with people can engender cooperative behavior. Trust in
AI is also key: trust that is intrinsic and trust that can only be earned over
time. Here we use the term ""AI"" in its broadest sense, as employed by the
recent 20-Year Community Roadmap for AI Research (Gil and Selman, 2019),
including but certainly not limited to, recent advances in deep learning.
  With success, cooperation between humans and AIs can build society just as
human-human cooperation has. Whether coming from an intrinsic willingness to be
helpful, or driven through self-interest, human societies have grown strong and
the human species has found success through cooperation. We cooperate ""in the
small"" -- as family units, with neighbors, with co-workers, with strangers --
and ""in the large"" as a global community that seeks cooperative outcomes around
questions of commerce, climate change, and disarmament. Cooperation has evolved
in nature also, in cells and among animals. While many cases involving
cooperation between humans and AIs will be asymmetric, with the human
ultimately in control, AI systems are growing so complex that, even today, it
is impossible for the human to fully comprehend their reasoning,
recommendations, and actions when functioning simply as passive observers.",http://arxiv.org/abs/2012.06034v1
"Improving living biomass C-stock loss estimates by combining optical
  satellite, airborne laser scanning, and NFI data",2020-12-14T20:16:42Z,"Johannes Breidenbach, Janis Ivanovs, Annika Kangas, Thomas Nord-Larsen, Mats Nilson, Rasmus Astrup","Policy measures and management decisions aiming at enhancing the role of
forests in mitigating climate-change require reliable estimates of C-stock
dynamics in greenhouse gas inventories (GHGIs). Aim of this study was to
assemble design-based estimators to provide estimates relevant for GHGIs using
national forest inventory (NFI) data. We improve basic expansion (BE) estimates
of living-biomass C-stock loss using field-data only, by leveraging with
remotely-sensed auxiliary data in model-assisted (MA) estimates. Our case
studies from Norway, Sweden, Denmark, and Latvia covered an area of >70 Mha.
Landsat-based Forest Cover Loss (FCL) and one-time wall-to-wall airborne laser
scanning (ALS) data served as auxiliary data. ALS provided information on the
C-stock before a potential disturbance indicated by FCL. The use of FCL in MA
estimators resulted in considerable efficiency gains which in most cases were
further increased by using ALS in addition. A doubling of efficiency was
possible for national estimates and even larger efficiencies were observed at
the sub-national level. Average annual estimates were considerably more precise
than pooled estimates using NFI data from all years at once. The combination of
remotely-sensed with NFI field data yields reliable estimates which is not
necessarily the case when using remotely-sensed data without reference
observations.",http://arxiv.org/abs/2012.07921v2
Co-optimizing the Smart Grid and Electric Public Transit Bus System,2020-12-15T04:54:00Z,"Mertcan Yetkin, Brandon R. Augustino, Alberto J. Lamadrid, Lawrence V. Snyder","As climate change provides impetus for investing in smart cities, with
electrified public transit systems, we consider electric public transportation
buses in an urban area, which play a role in the power system operations in
addition to their typical function of serving public transit demand. Our model
considers a social planner, such that the transit authority and the operator of
the electricity system co-optimize the power system to minimize the total
operational cost of the grid, while satisfying additional transportation
constraints on buses. We provide deterministic and stochastic formulations to
co-optimize the system. Each stochastic formulation provides a different set of
recourse actions to manage the variable renewable energy uncertainty: ramping
up/down the conventional generators, or charging/discharging of the transit
fleet. We demonstrate the capabilities of the model and the benefit obtained
via a coordinated strategy. We compare the efficacies of these recourse actions
to provide additional managerial insights. We analyze the effect of different
pricing strategies on the co-optimization. We also conduct congestion analysis
in the power network, comparing our cooperative approach to a non-cooperative
strategy when we assume electrified fleet sizes grow with greater battery
capacities. Given the recent momentum towards building smarter cities and
electrifying transit systems, our results provide policy directions towards a
sustainable future. We test our models using modified MATPOWER case files and
verify our results with different sized power networks. This study is motivated
by a project with a large transit authority in California.",http://arxiv.org/abs/2012.08087v3
"LSTM-based Space Occupancy Prediction towards Efficient Building Energy
  Management",2020-12-15T06:32:07Z,Juye Kim,"Energy consumed in buildings takes significant portions of the total global
energy usage. A large amount of building energy is used for heating, cooling,
ventilation, and air-conditioning (HVAC). However, compared to its importance,
building energy management systems nowadays are limited in controlling HVAC
based on simple rule-based control (RBC) technologies. The ability to design
systems that can efficiently manage HVAC can reduce energy usage and greenhouse
gas emissions, and, all in all, it can help us to mitigate climate change. This
paper proposes predictive time-series models of occupancy patterns using LSTM.
Prediction signal for future room occupancy status on the next time span (e.g.,
next 30 minutes) can be directly used to operate HVAC. For example, based on
the prediction and considering the time for cooling or heating, HVAC can be
turned on before the room is being used (e.g., turn on 10 minutes earlier).
Also, based on the next room empty prediction timing, HVAC can be turned off
earlier, and it can help us increase the efficiency of HVAC while not
decreasing comfort. We demonstrate our approach's capabilities using real-world
energy data collected from multiple rooms of a university building. We show
that LSTM's room occupancy prediction based HVAC control could save energy
usage by 50% compared to conventional RBC based control.",http://arxiv.org/abs/2012.08114v2
A mathematical model of national-level food system sustainability,2020-12-14T16:06:35Z,"Conor Goold, Simone Pfuderer, William H. M. James, Nik Lomax, Fiona Smith, Lisa M. Collins","The global food system faces various endogeneous and exogeneous, biotic and
abiotic risk factors, including a rising human population, higher population
densities, price volatility and climate change. Quantitative models play an
important role in understanding food systems' expected responses to shocks and
stresses. Here, we present a stylised mathematical model of a national-level
food system that incorporates domestic supply of a food commodity,
international trade, consumer demand, and food commodity price. We derive a
critical compound parameter signalling when domestic supply will become
unsustainable and the food system entirely dependent on imports, which results
in higher commodity prices, lower consumer demand and lower inventory levels.
Using Bayesian estimation, we apply the dynamic food systems model to infer the
sustainability of the UK pork industry. We find that the UK pork industry is
currently sustainable but because the industry is dependent on imports to meet
demand, a decrease in self-sufficiency below 50% (current levels are 60-65%)
would lead it close to the critical boundary signalling its collapse. Our model
provides a theoretical foundation for future work to determine more complex
causal drivers of food system vulnerability.",http://arxiv.org/abs/2012.08355v1
"Vulnerability analysis in Complex Networks under a Flood Risk Reduction
  point of view",2020-12-21T12:34:52Z,"Leonardo B. L. Santos, Giovanni G. Soares, Tanishq Garg, Aurelienne A. S. Jorge, Luciana R. Londe, Regina T. Reani, Roberta B. Bacelar, Carlos E. S. Oliveira, Vander L. S. Freitas, Igor M. Sokolov","The measurement and mapping of transportation network vulnerability to
natural hazards constitute subjects of global interest, especially due to
climate change, and for a sustainable development agenda. During a flood, some
elements of a transportation network can be affected, causing loss of life of
people and damage to vehicles, streets/roads, and other logistics services,
sometimes with severe economic impacts. The Network Science approach may offer
a valuable perspective considering one type of vulnerability related to network
type critical infrastructures: the topological vulnerability. The topological
vulnerability index associated with an element is defined as the reduction in
the network's average efficiency due to the removal of the set of edges related
to that element. We present a topological vulnerability index analysis for the
highways in the state of Santa Catarina, Brazil, and produce a map considering
that index and the areas susceptible to urban floods and landslides. The risk
knowledge, combining hazard and vulnerability, is the first pillar of an Early
Warning System, and represent an important tool for stakeholders from the
transportation sector in a disaster risk reduction agenda.",http://arxiv.org/abs/2012.12029v3
Time-resolved rotational velocities in the upper atmosphere of WASP-33 b,2020-10-05T16:06:53Z,"P. Wilson Cauley, Ji Wang, Evgenya L. Shkolnik, Ilya Ilyin, Klaus G. Strassmeier, Seth Redfield, Adam Jensen","While steady empirical progress has been made in understanding the structure
and composition of hot planet atmospheres, direct measurements of velocity
signatures, including winds, rotation, and jets, have lagged behind.
Quantifying atmospheric dynamics of hot planets is critical to a complete
understanding of their atmospheres and such measurements may even illuminate
other planetary properties, such as magnetic field strengths. In this
manuscript we present the first detection of the Balmer lines H$\alpha$ and
H$\beta$ in the atmosphere of the ultra-hot Jupiter WASP-33 b. Using
atmospheric models which include the effects of atmospheric dynamics, we show
that the shape of the average Balmer line transmission spectrum is consistent
with rotational velocities in the planet's thermosphere of $v_\text{rot} =
10.1^{+0.8}_{-1.0}$ km s$^{-1}$. We also measure a low-significance
day-to-night side velocity shift of $-4.6^{+3.4}_{-3.4}$ km s$^{-1}$ in the
transmission spectrum which is naturally explained by a global wind across the
planet's terminator. In a separate analysis the time-resolved velocity
centroids of individual transmission spectra show unambiguous evidence of
rotation, with a best-fit velocity of $10.0^{+2.4}_{-2.0}$ km s$^{-1}$,
consistent with the value of $v_\text{rot}$ derived from the shape of the
average Balmer line transmission spectrum. Our observations and analysis
confirm the power of high signal-to-noise, time-resolved transmission spectra
to measure the velocity structures in exoplanet atmospheres. The large
rotational and wind velocities we measure highlight the need for more detailed
3D global climate simulations of the rarefied upper-atmospheres of ultra-hot
gas giants.",http://arxiv.org/abs/2010.02118v2
"The role of negative emissions in meeting China's 2060 carbon neutrality
  goal",2020-10-13T22:36:39Z,"Jay Fuhrman, Andres F. Clarens, Haewon McJeon, Pralit Patel, Scott C. Doney, William M. Shobe, Shreekar Pradhan","China's pledge to reach carbon neutrality before 2060 is an ambitious goal
and could provide the world with much-needed leadership on how to limit warming
to +1.5C warming above pre-industrial levels by the end of the century. But the
pathways that would achieve net zero by 2060 are still unclear, including the
role of negative emissions technologies. We use the Global Change Analysis
Model to simulate how negative emissions technologies, in general, and direct
air capture (DAC) in particular, could contribute to China's meeting this
target. Our results show that negative emissions could play a large role,
offsetting on the order of 3 GtCO2 per year from difficult-to-mitigate sectors
such as freight transportation and heavy industry. This includes up to a 1.6
GtCO2 per year contribution from DAC, constituting up to 60% of total projected
negative emissions in China. But DAC, like bioenergy with carbon capture and
storage and afforestation, has not yet been demonstrated at anywhere
approaching the scales required to meaningfully contribute to climate
mitigation. Deploying NETs at these scales will have widespread impacts on
financial systems and natural resources such as water, land, and energy in
China.",http://arxiv.org/abs/2010.06723v2
"A global method to identify trees outside of closed-canopy forests with
  medium-resolution satellite imagery",2020-05-13T15:58:01Z,"John Brandt, Fred Stolle","Scattered trees outside of dense, closed-canopy forests are very important
for carbon sequestration, supporting livelihoods, maintaining ecosystem
integrity, and climate change adaptation and mitigation. In contrast to trees
inside of closed-canopy forests, not much is known about the spatial extent and
distribution of scattered trees at a global scale. Due to the cost of
high-resolution satellite imagery, global monitoring systems rely on
medium-resolution satellites to monitor land use. Here we present a globally
consistent method to identify trees with canopy diameters greater than three
meters with medium-resolution optical and radar imagery. Biweekly cloud-free,
pan-sharpened 10 meter Sentinel-2 optical imagery and Sentinel-1 radar imagery
are used to train a fully convolutional network, consisting of a convolutional
gated recurrent unit layer and a feature pyramid attention layer. Tested across
more than 215,000 Sentinel-1 and Sentinel-2 pixels distributed from -60 to +60
latitude, the proposed model exceeds 75% user's and producer's accuracy
identifying trees in hectares with a low to medium density (less than 40%) of
tree cover, and 95% user's and producer's accuracy in hectares with dense
(greater than 40%) tree cover. The proposed method increases the accuracy of
monitoring tree presence in areas with sparse and scattered tree cover (less
than 40%) by as much as 20%, and reduces commission and omission error in
mountainous and very cloudy regions by nearly half. When applied across large,
heterogeneous landscapes, the results demonstrate potential to map trees in
high detail and accuracy over diverse landscapes across the globe. This
information is important for understanding current land cover and can be used
to detect changes in land cover such as agroforestry, buffer zones around
biological hotspots, and expansion or encroachment of forests.",http://arxiv.org/abs/2005.08702v2
"Re-examining rates of lithium-ion battery technology improvement and
  cost decline",2020-07-28T00:15:47Z,"Micah S. Ziegler, Jessika E. Trancik","Lithium-ion technologies are increasingly employed to electrify
transportation and provide stationary energy storage for electrical grids, and
as such their development has garnered much attention. However, their
deployment is still relatively limited, and their broader adoption will depend
on their potential for cost reduction and performance improvement.
Understanding this potential can inform critical climate change mitigation
strategies, including public policies and technology development efforts.
However, many existing models of past cost decline, which often serve as
starting points for forecasting models, rely on limited data series and
measures of technological progress. Here we systematically collect, harmonize,
and combine various data series of price, market size, research and
development, and performance of lithium-ion technologies. We then develop
representative series for these measures and employ performance curve models to
estimate improvement rates. We also develop a method to incorporate additional
performance characteristics into these models, including energy density and
specific energy performance metrics. When energy density is incorporated into
the definition of service provided by a lithium-ion cell, estimated
technological improvement rates increase considerably, suggesting that
previously reported improvement rates might underestimate the rate of
lithium-ion technologies' change. Moreover, our estimates suggest the degree to
which lithium-ion technologies' price decline might have been limited by
performance requirements other than cost per energy capacity. These rates also
suggest that battery technologies developed for stationary applications, where
restrictions on volume and mass are relaxed, might achieve faster cost
declines, though engineering-based mechanistic cost modeling is required to
further characterize this potential.",http://arxiv.org/abs/2007.13920v2
"The impact of analytical outage modeling on expansion planning problems
  in the area of power systems",2020-01-23T21:25:43Z,"S. Tsianikas, N. Yousefi, J. Zhou, D. W. Coit","Expansion planning problems refer to the monetary and unit investment needed
for energy production or storage. An inherent element in these problems is the
element of stochasticity in various aspects, such as the generation output of
the units, climate change or frequency and duration of grid outages. Especially
for the latter one, outage modeling is crucial to be carefully considered when
designing systems with distributed generation at their core, such as
microgrids. In most studies so far, a single statistical distribution is used,
such as a Poisson Process. However, by taking a closer look at the real outage
data provided by the state of NY, it is observed that the outages do not seem
to come from the same distribution. In some years, there is a huge spike in the
average duration per outage and this is because of catastrophic events.
Therefore, in this study we propose and test an alternative modeling for outage
events. This alternative scheme will be based on the premise that outages can
be broadly classified into two categories: regular and severe. Under this
taxonomy, it can still be assumed that each type of events follows a Poisson
Process but outages, in general, follow a Poisson Process which is truly a
superposition of these two types. A reinforcement learning approach is used to
solve the expansion planning problem and real location-specific data are used.
The results verify our initial hypothesis and show that the optimization
results are significantly affected by the outage modeling. To sum up, modeling
accurately the grid outage events and measuring directly the reliability
performance of an energy system during catastrophic failures could provide
invaluable tools and insights that could therefore be used for the best
possible preparation for this type of outages.",http://arxiv.org/abs/2001.08815v1
"Spatial Analysis of Seasonal Precipitation over Iran: Co-Variation with
  Climate Indices",2020-01-15T16:31:30Z,"Majid Dehghani, Somayeh Salehi, Amir Mosavi, Narjes Nabipour, Shahaboddin Shamshirband, Pedram Ghamisi","Temporary changes in precipitation may lead to sustained and severe drought
or massive floods in different parts of the world. Knowing variation in
precipitation can effectively help the water resources decision-makers in water
resources management. Large-scale circulation drivers have a considerable
impact on precipitation in different parts of the world. In this research, the
impact of El Ni\~no-Southern Oscillation (ENSO), Pacific Decadal Oscillation
(PDO), and North Atlantic Oscillation (NAO) on seasonal precipitation over Iran
was investigated. For this purpose, 103 synoptic stations with at least 30
years of data were utilized. The Spearman correlation coefficient between the
indices in the previous 12 months with seasonal precipitation was calculated,
and the meaningful correlations were extracted. Then the month in which each of
these indices has the highest correlation with seasonal precipitation was
determined. Finally, the overall amount of increase or decrease in seasonal
precipitation due to each of these indices was calculated. Results indicate the
Southern Oscillation Index (SOI), NAO, and PDO have the most impact on seasonal
precipitation, respectively. Also, these indices have the highest impact on the
precipitation in winter, autumn, spring, and summer, respectively. SOI has a
diverse impact on winter precipitation compared to the PDO and NAO, while in
the other seasons, each index has its special impact on seasonal precipitation.
Generally, all indices in different phases may decrease the seasonal
precipitation up to 100%. However, the seasonal precipitation may increase more
than 100% in different seasons due to the impact of these indices. The results
of this study can be used effectively in water resources management and
especially in dam operation.",http://arxiv.org/abs/2001.09757v1
"A review of machine learning applications in wildfire science and
  management",2020-03-02T03:59:38Z,"Piyush Jain, Sean C P Coogan, Sriram Ganapathi Subramanian, Mark Crowley, Steve Taylor, Mike D Flannigan","Artificial intelligence has been applied in wildfire science and management
since the 1990s, with early applications including neural networks and expert
systems. Since then the field has rapidly progressed congruently with the wide
adoption of machine learning (ML) in the environmental sciences. Here, we
present a scoping review of ML in wildfire science and management. Our
objective is to improve awareness of ML among wildfire scientists and managers,
as well as illustrate the challenging range of problems in wildfire science
available to data scientists. We first present an overview of popular ML
approaches used in wildfire science to date, and then review their use in
wildfire science within six problem domains: 1) fuels characterization, fire
detection, and mapping; 2) fire weather and climate change; 3) fire occurrence,
susceptibility, and risk; 4) fire behavior prediction; 5) fire effects; and 6)
fire management. We also discuss the advantages and limitations of various ML
approaches and identify opportunities for future advances in wildfire science
and management within a data science context. We identified 298 relevant
publications, where the most frequently used ML methods included random
forests, MaxEnt, artificial neural networks, decision trees, support vector
machines, and genetic algorithms. There exists opportunities to apply more
current ML methods (e.g., deep learning and agent based learning) in wildfire
science. However, despite the ability of ML models to learn on their own,
expertise in wildfire science is necessary to ensure realistic modelling of
fire processes across multiple scales, while the complexity of some ML methods
requires sophisticated knowledge for their application. Finally, we stress that
the wildfire research and management community plays an active role in
providing relevant, high quality data for use by practitioners of ML methods.",http://arxiv.org/abs/2003.00646v2
"Assessing the Significance of Directed and Multivariate Measures of
  Linear Dependence Between Time Series",2020-03-09T02:06:01Z,"Oliver M. Cliff, Leonardo Novelli, Ben D. Fulcher, James M. Shine, Joseph T. Lizier","Inferring linear dependence between time series is central to our
understanding of natural and artificial systems. Unfortunately, the hypothesis
tests that are used to determine statistically significant directed or
multivariate relationships from time-series data often yield spurious
associations (Type I errors) or omit causal relationships (Type II errors).
This is due to the autocorrelation present in the analysed time series -- a
property that is ubiquitous across diverse applications, from brain dynamics to
climate change. Here we show that, for limited data, this issue cannot be
mediated by fitting a time-series model alone (e.g., in Granger causality or
prewhitening approaches), and instead that the degrees of freedom in
statistical tests should be altered to account for the effective sample size
induced by cross-correlations in the observations. This insight enabled us to
derive modified hypothesis tests for any multivariate correlation-based
measures of linear dependence between covariance-stationary time series,
including Granger causality and mutual information with Gaussian marginals. We
use both numerical simulations (generated by autoregressive models and digital
filtering) as well as recorded fMRI-neuroimaging data to show that our tests
are unbiased for a variety of stationary time series. Our experiments
demonstrate that the commonly used $F$- and $\chi^2$-tests can induce
significant false-positive rates of up to $100\%$ for both measures, with and
without prewhitening of the signals. These findings suggest that many
dependencies reported in the scientific literature may have been, and may
continue to be, spuriously reported or missed if modified hypothesis tests are
not used when analysing time series.",http://arxiv.org/abs/2003.03887v3
"Epidemics, the Ising-model and percolation theory: a comprehensive
  review focussed on Covid-19",2020-03-26T12:19:50Z,"Isys F. Mello, Lucas Squillante, Gabriel O. Gomes, Antonio C. Seridonio, M. de Souza","We revisit well-established concepts of epidemiology, the Ising-model, and
percolation theory. Also, we employ a spin $S$ = 1/2 Ising-like model and a
(logistic) Fermi-Dirac-like function to describe the spread of Covid-19. Our
analysis reinforces well-established literature results, namely: \emph{i}) that
the epidemic curves can be described by a Gaussian-type function; \emph{ii})
that the temporal evolution of the accumulative number of infections and
fatalities follow a logistic function, which has some resemblance with a
distorted Fermi-Dirac-like function; \emph{iii}) the key role played by the
quarantine to block the spread of Covid-19 in terms of an \emph{interacting}
parameter, which emulates the contact between infected and non-infected people.
Furthermore, in the frame of elementary percolation theory, we show that:
\emph{i}) the percolation probability can be associated with the probability of
a person being infected with Covid-19; \emph{ii}) the concepts of blocked and
non-blocked connections can be associated, respectively, with a person
respecting or not the social distancing, impacting thus in the probability of
an infected person to infect other people. Increasing the number of infected
people leads to an increase in the number of net connections, giving rise thus
to a higher probability of new infections (percolation). We demonstrate the
importance of social distancing in preventing the spread of Covid-19 in a
pedagogical way. Given the impossibility of making a precise forecast of the
disease spread, we highlight the importance of taking into account additional
factors, such as climate changes and urbanization, in the mathematical
description of epidemics. Yet, we make a connection between the standard
mathematical models employed in epidemics and well-established concepts in
condensed matter Physics, such as the Fermi gas and the Landau Fermi-liquid
picture.",http://arxiv.org/abs/2003.11860v2
Image-based phenotyping of diverse Rice (Oryza Sativa L.) Genotypes,2020-04-06T09:04:14Z,"Mukesh Kumar Vishal, Dipesh Tamboli, Abhijeet Patil, Rohit Saluja, Biplab Banerjee, Amit Sethi, Dhandapani Raju, Sudhir Kumar, R N Sahoo, Viswanathan Chinnusamy, J Adinarayana","Development of either drought-resistant or drought-tolerant varieties in rice
(Oryza sativa L.), especially for high yield in the context of climate change,
is a crucial task across the world. The need for high yielding rice varieties
is a prime concern for developing nations like India, China, and other
Asian-African countries where rice is a primary staple food. The present
investigation is carried out for discriminating drought tolerant, and
susceptible genotypes. A total of 150 genotypes were grown under controlled
conditions to evaluate at High Throughput Plant Phenomics facility, Nanaji
Deshmukh Plant Phenomics Centre, Indian Council of Agricultural Research-Indian
Agricultural Research Institute, New Delhi. A subset of 10 genotypes is taken
out of 150 for the current investigation. To discriminate against the
genotypes, we considered features such as the number of leaves per plant, the
convex hull and convex hull area of a plant-convex hull formed by joining the
tips of the leaves, the number of leaves per unit convex hull of a plant,
canopy spread - vertical spread, and horizontal spread of a plant. We trained
You Only Look Once (YOLO) deep learning algorithm for leaves tips detection and
to estimate the number of leaves in a rice plant. With this proposed framework,
we screened the genotypes based on selected traits. These genotypes were
further grouped among different groupings of drought-tolerant and drought
susceptible genotypes using the Ward method of clustering.",http://arxiv.org/abs/2004.02498v1
White Paper on 6G Drivers and the UN SDGs,2020-04-30T11:23:02Z,"Marja Matinmikko-Blue, Sirpa Aalto, Muhammad Imran Asghar, Hendrik Berndt, Yan Chen, Sudhir Dixit, Risto Jurva, Pasi Karppinen, Markku Kekkonen, Marianne Kinnula, Panagiotis Kostakos, Johanna Lindberg, Edward Mutafungwa, Kirsi Ojutkangas, Elina Rossi, Seppo Yrjola, Anssi Oorni, Petri Ahokangas, Muhammad-Zeeshan Asghar, Fan Chen, Netta Iivari, Marcos Katz, Atte Kinnula, Josef Noll, Harri Oinas-Kukkonen, Ian Oppermann, Ella Peltonen, Hanna Saarela, Harri Saarnisaari, Anna Suorsa, Gustav Wikstrom, Volker Ziegler","The commercial launch of 6G communications systems and United Nations
Sustainable Development Goals, UN SDGs, are both targeted for 2030. 6G
communications is expected to boost global growth and productivity, create new
business models and transform many aspects of society. The UN SDGs are a way of
framing opportunities and challenges of a desirable future world and cover
topics as broad as ending poverty, gender equality, climate change and smart
cities. The relationship between these potentially mutually reinforcing forces
is currently under-defined. Building on the vision for 6G, a review of
megatrends, on-going activities on the relation of mobile communications to the
UN SDGs and existing indicators, a novel linkage between 6G and the UN SDGs is
proposed via indicators. The white paper has also launched the work of deriving
new 6G related indicators to guide the research of 6G systems. The novel
linkage is built on the envisaged three-fold role of 6G as a provider of
services to help steer and support communities and countries towards reaching
the UN SDGs, as an enabler of measuring tool for data collection to help
reporting of indicators with hyperlocal granularity, and as a reinforcer of new
ecosystems based on 6G technology enablers and 6G network of networks to be
developed in line with the UN SDGs that incorporates future mobile
communication technologies available in 2030. Related challenges are also
identified. An action plan is presented along with prioritized focus areas
within the mobile communication sector technology and industry evolution to
best support the achievement of the UN SDGs.",http://arxiv.org/abs/2004.14695v1
"Quantifying uncertainty in spatio-temporal changes of upper-ocean heat
  content estimates: an internationally coordinated comparison",2020-09-07T20:24:40Z,"Abhishek Savita, Catia M. Domingues, Tim Boyer, Viktor Gouretski, Masayoshi Ishii, Gregory C. Johnson, John M. Lyman, Josh K. Willis, Simon J. Marsland, William Hobbs, John A. Church, Didier P. Monselesan, Peter Dobrohotoff, Rebecca Cowley, Susan E. Wijffels","The Earth system is accumulating energy due to human-induced activities. More
than 90 percent of this energy has been stored in the ocean as heat since 1970,
with about 64 percent of that in the upper 700 m. Differences in upper ocean
heat content anomaly (OHCA) estimates, however, exist. Here, we evaluate spread
in upper OHCA estimates arising from choices in instrumental bias corrections
and mapping methods, in addition to the effect of using a common ocean mask.
The same dataset was mapped by six research groups for 1970 to 2008, with six
instrumental bias corrections applied to expendable bathythermograph (XBT)
data. We find that use of a common ocean mask may impact estimation of global
OHCA by +- 13 percent. Uncertainty due to mapping method dominates over XBT
bias correction at a global scale and is largest in the Indian Ocean and in the
eddy-rich and frontal regions of all basins. Uncertainty due to XBT bias
correction is largest in the Pacific Ocean within 30N to 30S. In both mapping
and XBT cases, spread is higher since the 1990s. Important differences in
spatial trends among mapping methods are found in the well-observed Northwest
Atlantic and the poorly-observed Southern Ocean. Although our results cannot
identify the best mapping or bias correction schemes, they identify where and
when greater uncertainties exist, and so where further refinements may yield
the largest improvements. Our results highlight the need for a future
international coordination to evaluate performance of existing mapping methods.",http://arxiv.org/abs/2009.03403v1
Workflow Provenance in the Lifecycle of Scientific Machine Learning,2020-09-30T13:09:48Z,"Renan Souza, Leonardo G. Azevedo, V√≠tor Louren√ßo, Elton Soares, Raphael Thiago, Rafael Brand√£o, Daniel Civitarese, Emilio Vital Brazil, Marcio Moreno, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, Marco A. S. Netto","Machine Learning (ML) has already fundamentally changed several businesses.
More recently, it has also been profoundly impacting the computational science
and engineering domains, like geoscience, climate science, and health science.
In these domains, users need to perform comprehensive data analyses combining
scientific data and ML models to provide for critical requirements, such as
reproducibility, model explainability, and experiment data understanding.
However, scientific ML is multidisciplinary, heterogeneous, and affected by the
physical constraints of the domain, making such analyses even more challenging.
In this work, we leverage workflow provenance techniques to build a holistic
view to support the lifecycle of scientific ML. We contribute with (i)
characterization of the lifecycle and taxonomy for data analyses; (ii) design
principles to build this view, with a W3C PROV compliant data representation
and a reference system architecture; and (iii) lessons learned after an
evaluation in an Oil & Gas case using an HPC cluster with 393 nodes and 946
GPUs. The experiments show that the principles enable queries that integrate
domain semantics with ML models while keeping low overhead (<1%), high
scalability, and an order of magnitude of query acceleration under certain
workloads against without our representation.",http://arxiv.org/abs/2010.00330v3
"A game-theoretic analysis of networked system control for common-pool
  resource management using multi-agent reinforcement learning",2020-10-15T14:12:26Z,"Arnu Pretorius, Scott Cameron, Elan van Biljon, Tom Makkink, Shahil Mawjee, Jeremy du Plessis, Jonathan Shock, Alexandre Laterre, Karim Beguir","Multi-agent reinforcement learning has recently shown great promise as an
approach to networked system control. Arguably, one of the most difficult and
important tasks for which large scale networked system control is applicable is
common-pool resource management. Crucial common-pool resources include arable
land, fresh water, wetlands, wildlife, fish stock, forests and the atmosphere,
of which proper management is related to some of society's greatest challenges
such as food security, inequality and climate change. Here we take inspiration
from a recent research program investigating the game-theoretic incentives of
humans in social dilemma situations such as the well-known tragedy of the
commons. However, instead of focusing on biologically evolved human-like
agents, our concern is rather to better understand the learning and operating
behaviour of engineered networked systems comprising general-purpose
reinforcement learning agents, subject only to nonbiological constraints such
as memory, computation and communication bandwidth. Harnessing tools from
empirical game-theoretic analysis, we analyse the differences in resulting
solution concepts that stem from employing different information structures in
the design of networked multi-agent systems. These information structures
pertain to the type of information shared between agents as well as the
employed communication protocol and network topology. Our analysis contributes
new insights into the consequences associated with certain design choices and
provides an additional dimension of comparison between systems beyond
efficiency, robustness, scalability and mean control performance.",http://arxiv.org/abs/2010.07777v1
"Investigation of superspreading COVID-19 outbreaks events in meat and
  poultry processing plants in Germany: A cross-sectional study",2020-11-23T00:41:03Z,"Roman Pokora, Susan Kutschbach, Matthias Weigl, Detlef Braun, Annegret Epple, Eva Lorenz, Stefan Grund, Juergen Hecht, Helmut Hollich, Peter Rietschel, Frank Schneider, Roland Sohmen, Katherine Taylor, Isabel Dienstbuehl","Since May 2020, several COVID-19 outbreaks have occurred in the German meat
industry despite various protective measures, and temperature and ventilation
conditions were considered as possible high-risk factors. This cross-sectional
study examined meat and poultry plants to examine possible risk factors.
Companies completed a self-administered questionnaire on the work environment
and protective measures taken to prevent SARS-CoV-2 infection. Multivariable
logistic regression analysis adjusted for the possibility to distance at least
1.5 meters, break rules, and employment status was performed to identify risk
factors associated with COVID-19 cases. Twenty-two meat and poultry plants with
19,072 employees participated. The prevalence of COVID-19 in the seven plants
with more than 10 cases was 12.1% and was highest in the deboning and meat
cutting area with 16.1%. A subsample analysis where information on maximal
ventilation rate per employee was available revealed an effect for ventilation
rate (adjusted odds ratio (AOR) 0.996, 95% CI 0.993-0.999). When including
temperature as an interaction term in the working area, the effect of the
ventilation rate did not change. Increasing room temperatures resulted in a
lower chance of obtaining a positive COVID-19 test result (AOR 0.90 95% CI
0.82-0.99), and a 0.1% greater chance of a positive COVID-19 test for the
interaction term (AOR 1.001, 95% CI 1.000-1.003). Our results further indicate
that climate conditions and low outdoor air flow are factors that can promote
the spread of SARS-CoV-2 aerosols. A possible requirement for pandemic
mitigation strategies in industrial workplace settings is to increase the
ventilation rate.",http://arxiv.org/abs/2011.11153v1
"SLIC-UAV: A Method for monitoring recovery in tropical restoration
  projects through identification of signature species using UAVs",2020-06-11T17:22:56Z,"Jonathan Williams, Carola-Bibiane Sch√∂nlieb, Tom Swinfield, Bambang Irawan, Eva Achmad, Muhammad Zudhi, Habibi, Elva Gemita, David A. Coomes","Logged forests cover four million square kilometres of the tropics and
restoring these forests is essential if we are to avoid the worst impacts of
climate change, yet monitoring recovery is challenging. Tracking the abundance
of visually identifiable, early-successional species enables successional
status and thereby restoration progress to be evaluated. Here we present a new
pipeline, SLIC-UAV, for processing Unmanned Aerial Vehicle (UAV) imagery to map
early-successional species in tropical forests. The pipeline is novel because
it comprises: (a) a time-efficient approach for labelling crowns from UAV
imagery; (b) machine learning of species based on spectral and textural
features within individual tree crowns, and (c) automatic segmentation of
orthomosaiced UAV imagery into 'superpixels', using Simple Linear Iterative
Clustering (SLIC). Creating superpixels reduces the dataset's dimensionality
and focuses prediction onto clusters of pixels, greatly improving accuracy. To
demonstrate SLIC-UAV, support vector machines and random forests were used to
predict the species of hand-labelled crowns in a restoration concession in
Indonesia. Random forests were most accurate at discriminating species for
whole crowns, with accuracy ranging from 79.3% when mapping five common
species, to 90.5% when mapping the three most visually-distinctive species. In
contrast, support vector machines proved better for labelling automatically
segmented superpixels, with accuracy ranging from 74.3% to 91.7% for the same
species. Models were extended to map species across 100 hectares of forest. The
study demonstrates the power of SLIC-UAV for mapping characteristic
early-successional tree species as an indicator of successional stage within
tropical forest restoration areas. Continued effort is needed to develop
easy-to-implement and low-cost technology to improve the affordability of
project management.",http://arxiv.org/abs/2006.06624v1
Green Algorithms: Quantifying the carbon footprint of computation,2020-07-15T11:05:33Z,"Lo√Øc Lannelongue, Jason Grealey, Michael Inouye","Climate change is profoundly affecting nearly all aspects of life on earth,
including human societies, economies and health. Various human activities are
responsible for significant greenhouse gas emissions, including data centres
and other sources of large-scale computation. Although many important
scientific milestones have been achieved thanks to the development of
high-performance computing, the resultant environmental impact has been
underappreciated. In this paper, we present a methodological framework to
estimate the carbon footprint of any computational task in a standardised and
reliable way, based on the processing time, type of computing cores, memory
available and the efficiency and location of the computing facility. Metrics to
interpret and contextualise greenhouse gas emissions are defined, including the
equivalent distance travelled by car or plane as well as the number of
tree-months necessary for carbon sequestration. We develop a freely available
online tool, Green Algorithms (www.green-algorithms.org), which enables a user
to estimate and report the carbon footprint of their computation. The Green
Algorithms tool easily integrates with computational processes as it requires
minimal information and does not interfere with existing code, while also
accounting for a broad range of CPUs, GPUs, cloud computing, local servers and
desktop computers. Finally, by applying Green Algorithms, we quantify the
greenhouse gas emissions of algorithms used for particle physics simulations,
weather forecasts and natural language processing. Taken together, this study
develops a simple generalisable framework and freely available tool to quantify
the carbon footprint of nearly any computation. Combined with a series of
recommendations to minimise unnecessary CO2 emissions, we hope to raise
awareness and facilitate greener computation.",http://arxiv.org/abs/2007.07610v5
"MOVES IV. Modelling the influence of stellar XUV-flux, cosmic rays, and
  stellar energetic particles on the atmospheric composition of the hot Jupiter
  HD 189733b",2020-12-22T16:12:15Z,"Patrick Barth, Christiane Helling, Eva E. St√ºeken, Vincent Bourrier, Nathan Mayne, Paul B. Rimmer, Moira Jardine, Aline A. Vidotto, Peter J. Wheatley, Rim Fares","Hot Jupiters provide valuable natural laboratories for studying potential
contributions of high-energy radiation to prebiotic synthesis in the
atmospheres of exoplanets. In this fourth paper of the MOVES (Multiwavelength
Observations of an eVaporating Exoplanet and its Star) programme, we study the
effect of different types of high-energy radiation on the production of organic
and prebiotic molecules in the atmosphere of the hot Jupiter HD 189733b. Our
model combines X-ray and UV observations from the MOVES programme and 3D
climate simulations from the 3D Met Office Unified Model to simulate the
atmospheric composition and kinetic chemistry with the STAND2019 network. Also,
the effects of galactic cosmic rays and stellar energetic particles are
included. We find that the differences in the radiation field between the
irradiated dayside and the shadowed nightside lead to stronger changes in the
chemical abundances than the variability of the host star's XUV emission. We
identify ammonium (NH4+) and oxonium (H3O+) as fingerprint ions for the
ionization of the atmosphere by both galactic cosmic rays and stellar
particles. All considered types of high-energy radiation have an enhancing
effect on the abundance of key organic molecules such as hydrogen cyanide
(HCN), formaldehyde (CH2O), and ethylene (C2H4). The latter two are
intermediates in the production pathway of the amino acid glycine (C2H5NO2) and
abundant enough to be potentially detectable by JWST.",http://arxiv.org/abs/2012.12132v3
"Localized magnetic field structures and their boundaries in the near-Sun
  solar wind from Parker Solar Probe measurements",2020-03-09T16:03:17Z,"V. Krasnoselskikh, A. Larosa, O. Agapitov, T. Dudok de Wit, M. Moncuquet, F. S. Mozer, M. Stevens, S. D. Bale, J. Bonnell, C. Froment, K. Goetz, K. Goodrich, P. Harvey, J. Kasper, R. MacDowall, D. Malaspina, M. Pulupa, N. Raouafi, C. Revillet, M. Velli, J. Wygant","One of the discoveries made by Parker Solar Probe during first encounters
with the Sun is the ubiquitous presence of relatively small-scale structures
standing out as sudden deflections of the magnetic field. They were called
switchbacks as some of them show up the full reversal of the radial component
of the magnetic field and then return to regular conditions. Analyzing the
magnetic field and plasma perturbations associated with switchbacks we identify
three types of structures with slightly different characteristics: 1. Alfvenic
structures, where the variations of the magnetic field components take place
while the magnitude of the field remains constant; 2. Compressional, the field
magnitude varies together with changes of the components; 3. Structures
manifesting full reversal of the magnetic field (extremal class of Alfvenic
structures). Processing of structures boundaries and plasma bulk velocity
perturbations lead to the conclusion that they represent localized magnetic
field tubes with enhanced parallel plasma velocity and ion beta moving together
with respect to surrounding plasma. The magnetic field deflections before and
after the switchbacks reveal the existence of total axial current. The electric
currents are concentrated on the relatively narrow boundary layers on the
surface of the tubes and determine the magnetic field perturbations inside the
tube. These currents are closed on the structure surface, and typically have
comparable azimuthal and the axial components. The surface of the structure may
also accommodate an electromagnetic wave, that assists to particles in carrying
currents. We suggest that the two types of structures we analyzed here may
represent the local manifestations of the tube deformations corresponding to a
saturated stage of the Firehose instability development.",http://arxiv.org/abs/2003.05409v1
On Absolute and Relative Change,2020-11-30T14:05:41Z,"Silvan Brauen, Philipp Erpf, Micha Wasem","Based on an axiomatic approach we propose two related novel one-parameter
families of indicators of change which put in a relation classical indicators
of change such as absolute change, relative change and the log-ratio.",http://arxiv.org/abs/2011.14807v1
Detecting Hierarchical Changes in Latent Variable Models,2020-11-18T18:46:10Z,"Shintaro Fukushima, Kenji Yamanishi","This paper addresses the issue of detecting hierarchical changes in latent
variable models (HCDL) from data streams. There are three different levels of
changes for latent variable models: 1) the first level is the change in data
distribution for fixed latent variables, 2) the second one is that in the
distribution over latent variables, and 3) the third one is that in the number
of latent variables. It is important to detect these changes because we can
analyze the causes of changes by identifying which level a change comes from
(change interpretability). This paper proposes an information-theoretic
framework for detecting changes of the three levels in a hierarchical way. The
key idea to realize it is to employ the MDL (minimum description length) change
statistics for measuring the degree of change, in combination with DNML
(decomposed normalized maximum likelihood) code-length calculation. We give a
theoretical basis for making reliable alarms for changes. Focusing on
stochastic block models, we employ synthetic and benchmark datasets to
empirically demonstrate the effectiveness of our framework in terms of change
interpretability as well as change detection.",http://arxiv.org/abs/2011.09465v3
Change Point Detection in Software Performance Testing,2020-03-01T21:01:25Z,"David Daly, William Brown, Henrik Ingo, Jim O'Leary, David Bradford","We describe our process for automatic detection of performance changes for a
software product in the presence of noise. A large collection of tests run
periodically as changes to our software product are committed to our source
repository, and we would like to identify the commits responsible for
performance regressions. Previously, we relied on manual inspection of time
series graphs to identify significant changes. That was later replaced with a
threshold-based detection system, but neither system was sufficient for finding
changes in performance in a timely manner. This work describes our recent
implementation of a change point detection system built upon the E-Divisive
means algorithm. The algorithm produces a list of change points representing
significant changes from a given history of performance results. A human
reviews the list of change points for actionable changes, which are then
triaged for further inspection. Using change point detection has had a dramatic
impact on our ability to detect performance changes. Quantitatively, it has
dramatically dropped our false positive rate for performance changes, while
qualitatively it has made the entire performance evaluation process easier,
more productive (ex. catching smaller regressions), and more timely.",http://arxiv.org/abs/2003.00584v1
Semantic Change Pattern Analysis,2020-03-07T02:22:19Z,"Wensheng Cheng, Yan Zhang, Xu Lei, Wen Yang, Guisong Xia","Change detection is an important problem in vision field, especially for
aerial images. However, most works focus on traditional change detection, i.e.,
where changes happen, without considering the change type information, i.e.,
what changes happen. Although a few works have tried to apply semantic
information to traditional change detection, they either only give the label of
emerging objects without taking the change type into consideration, or set some
kinds of change subjectively without specifying semantic information. To make
use of semantic information and analyze change types comprehensively, we
propose a new task called semantic change pattern analysis for aerial images.
Given a pair of co-registered aerial images, the task requires a result
including both where and what changes happen. We then describe the metric
adopted for the task, which is clean and interpretable. We further provide the
first well-annotated aerial image dataset for this task. Extensive baseline
experiments are conducted as reference for following works. The aim of this
work is to explore high-level information based on change detection and
facilitate the development of this field with the publicly available dataset.",http://arxiv.org/abs/2003.03492v1
"Two-Phase Object-Based Deep Learning for Multi-temporal SAR Image Change
  Detection",2020-01-17T11:51:35Z,"Xinzheng Zhang, Guo Liu, Ce Zhang, Peter M Atkinson, Xiaoheng Tan, Xin Jian, Xichuan Zhou, Yongming Li","Change detection is one of the fundamental applications of synthetic aperture
radar (SAR) images. However, speckle noise presented in SAR images has a much
negative effect on change detection. In this research, a novel two-phase
object-based deep learning approach is proposed for multi-temporal SAR image
change detection. Compared with traditional methods, the proposed approach
brings two main innovations. One is to classify all pixels into three
categories rather than two categories: unchanged pixels, changed pixels caused
by strong speckle (false changes), and changed pixels formed by real terrain
variation (real changes). The other is to group neighboring pixels into
segmented into superpixel objects (from pixels) such as to exploit local
spatial context. Two phases are designed in the methodology: 1) Generate
objects based on the simple linear iterative clustering algorithm, and
discriminate these objects into changed and unchanged classes using fuzzy
c-means (FCM) clustering and a deep PCANet. The prediction of this Phase is the
set of changed and unchanged superpixels. 2) Deep learning on the pixel sets
over the changed superpixels only, obtained in the first phase, to discriminate
real changes from false changes. SLIC is employed again to achieve new
superpixels in the second phase. Low rank and sparse decomposition are applied
to these new superpixels to suppress speckle noise significantly. A further
clustering step is applied to these new superpixels via FCM. A new PCANet is
then trained to classify two kinds of changed superpixels to achieve the final
change maps. Numerical experiments demonstrate that, compared with benchmark
methods, the proposed approach can distinguish real changes from false changes
effectively with significantly reduced false alarm rates, and achieve up to
99.71% change detection accuracy using multi-temporal SAR imagery.",http://arxiv.org/abs/2001.06252v1
"A Weakly Supervised Convolutional Network for Change Segmentation and
  Classification",2020-11-06T20:20:45Z,"Philipp Andermatt, Radu Timofte","Fully supervised change detection methods require difficult to procure
pixel-level labels, while weakly supervised approaches can be trained with
image-level labels. However, most of these approaches require a combination of
changed and unchanged image pairs for training. Thus, these methods can not
directly be used for datasets where only changed image pairs are available. We
present W-CDNet, a novel weakly supervised change detection network that can be
trained with image-level semantic labels. Additionally, W-CDNet can be trained
with two different types of datasets, either containing changed image pairs
only or a mixture of changed and unchanged image pairs. Since we use
image-level semantic labels for training, we simultaneously create a change
mask and label the changed object for single-label images. W-CDNet employs a
W-shaped siamese U-net to extract feature maps from an image pair which then
get compared in order to create a raw change mask. The core part of our model,
the Change Segmentation and Classification (CSC) module, learns an accurate
change mask at a hidden layer by using a custom Remapping Block and then
segmenting the current input image with the change mask. The segmented image is
used to predict the image-level semantic label. The correct label can only be
predicted if the change mask actually marks relevant change. This forces the
model to learn an accurate change mask. We demonstrate the segmentation and
classification performance of our approach and achieve top results on AICD and
HRSCD, two public aerial imaging change detection datasets as well as on a Food
Waste change detection dataset. Our code is available at
https://github.com/PhiAbs/W-CDNet .",http://arxiv.org/abs/2011.03577v1
The generalized Price equation: forces that change population statistics,2020-03-19T18:43:59Z,"Steven A. Frank, William Godsoe","The Price equation partitions the change in the expected value of a
population measure. The first component describes the partial change caused by
altered frequencies. The second component describes the partial change caused
by altered measurements. In biology, frequency changes often associate with the
direct effect of natural selection. Measure changes reflect processes during
transmission that alter trait values. More broadly, the two components describe
the direct forces that change population composition and the altered frame of
reference that changes measured values. The classic Price equation is limited
to population statistics that can expressed as the expected value of a measure.
Many statistics cannot be expressed as expected values, such as the harmonic
mean and the family of rescaled diversity measures. We generalize the Price
equation to any population statistic that can be expressed as a function of
frequencies and measurements. We obtain the generalized partition between the
direct forces that cause frequency change and the altered frame of reference
that changes measurements.",http://arxiv.org/abs/2003.08976v1
Evolutionary Conflict Checking,2020-09-22T06:25:55Z,"Tao Ji, Liqian Chen, Xiaoguang Mao, Xin Yi, Jiahong Jiang","During the software evolution, existing features may be adversely affected by
new changes, which is well known as regression errors. Maintaining a
high-quality test suite is helpful to prevent regression errors, whereas it
heavily depends on developers. Continuously augmenting the existing test suite
based on the new changes can assist developers in investigating the impact of
these new changes. And by comparing the executions of the generated test case
on two versions, existing techniques are able to detect some common errors.
However, the requirements and oracles on the new changes with existing program
behaviors are missing. In addition, the new changes may introduce new bugs when
they are not sufficiently examined with other unchanged code, which finally
fails to meet developers' real intentions on changes. In this paper, we propose
the notion of evolutionary conflict checking to validate new changes. By
extracting developers' intention reflected by new changes and transforming the
linear evolutionary process into one three-way merge, we detect conflicts
between existing behaviors and new changes. Our experimental results indicate
that evolutionary conflict checking is able to be applied for guaranteeing
software quality after changes.",http://arxiv.org/abs/2009.10340v1
"Process Knowledge Driven Change Point Detection for Automated
  Calibration of Discrete Event Simulation Models Using Machine Learning",2020-05-11T19:07:26Z,"Suleyman Yildirim, Alper Ekrem Murat, Murat Yildirim, Suzan Arslanturk","Initial development and subsequent calibration of discrete event simulation
models for complex systems require accurate identification of dynamically
changing process characteristics. Existing data driven change point methods
(DD-CPD) assume changes are extraneous to the system, thus cannot utilize
available process knowledge. This work proposes a unified framework for
process-driven multi-variate change point detection (PD-CPD) by combining
change point detection models with machine learning and process-driven
simulation modeling. The PD-CPD, after initializing with DD-CPD's change
point(s), uses simulation models to generate system level outputs as
time-series data streams which are then used to train neural network models to
predict system characteristics and change points. The accuracy of the
predictive models measures the likelihood that the actual process data conforms
to the simulated change points in system characteristics. PD-CPD iteratively
optimizes change points by repeating simulation and predictive model building
steps until the set of change point(s) with the maximum likelihood is
identified. Using an emergency department case study, we show that PD-CPD
significantly improves change point detection accuracy over DD-CPD estimates
and is able to detect actual change points.",http://arxiv.org/abs/2005.05385v2
Contact network changes in ordered and disordered disk packings,2020-08-05T01:15:05Z,"P. J. Tuckman, K. VanderWerf, Y. Yuan, S. Zhang, J. Zhang, M. D. Shattuck, C. S. O'Hern","We investigate the mechanical response of packings of purely repulsive,
frictionless disks to quasistatic deformations. The deformations include simple
shear strain at constant packing fraction and at constant pressure,
""polydispersity"" strain (in which we change the particle size distribution) at
constant packing fraction and at constant pressure, and isotropic compression.
For each deformation, we show that there are two classes of changes in the
interparticle contact networks: jump changes and point changes. Jump changes
occur when a contact network becomes mechanically unstable, particles
""rearrange"", and the potential energy (when the strain is applied at constant
packing fraction) or enthalpy (when the strain is applied at constant pressure)
and all derivatives are discontinuous. During point changes, a single contact
is either added to or removed from the contact network. For repulsive linear
spring interactions, second- and higher-order derivatives of the potential
energy/enthalpy are discontinuous at a point change, while for Hertzian
interactions, third- and higher-order derivatives of the potential
energy/enthalpy are discontinuous. We illustrate the importance of point
changes by studying the transition from a hexagonal crystal to a disordered
crystal induced by applying polydispersity strain. During this transition, the
system only undergoes point changes, with no jump changes. We emphasize that
one must understand point changes, as well as jump changes, to predict the
mechanical properties of jammed packings.",http://arxiv.org/abs/2008.01895v1
"On the ergodic properties of time changes of partially hyperbolic
  homogeneous flows",2020-10-04T15:25:29Z,Changguang Dong,"For any accessible partially hyperbolic homogeneous flow, we show that all
smooth time changes are K and hence mixing of all orders. We also establish
stable ergodicity for time-one map of these time changes.",http://arxiv.org/abs/2010.01602v2
Detecting multiple change points: a PULSE criterion,2020-03-03T01:18:44Z,"Wenbiao Zhao, Xuehu Zhu, Lixing Zhu","The research described herewith investigates detecting change points of means
and of variances in a sequence of observations. The number of change points can
be divergent at certain rate as the sample size goes to infinity. We define a
MOSUM-based objective function for this purpose. Unlike all existing
MOSUM-based methods, the novel objective function exhibits an useful ``PULSE""
pattern near change points in the sense: at the population level, the value at
any change point plus 2 times of the segment length of the moving average
attains a local minimum tending to zero following by a local maximum going to
infinity. This feature provides an efficient way to simultaneously identify all
change points at the sample level. In theory, the number of change points can
be consistently estimated and the locations can also be consistently estimated
in a certain sense. Further, because of its visualization nature, in practice,
the locations can be relatively more easily identified by plots than existing
methods in the literature. The method can also handle the case in which the
signals of some change points are very weak in the sense that those changes go
to zero. Further, the computational cost is very inexpensive. The numerical
studies we conduct validate its good performance.",http://arxiv.org/abs/2003.01280v1
A Data-driven Change-point Estimator,2020-10-23T14:43:23Z,Stefanie Schwaar,"The q-weighted CUSUM and their corresponding estimator are well known
statistics for change-point detection and estimation. They have the difficulty
that the performance is highly dependent on the location of the change. An
adaptive estimator with data-driven weights is presented to overcome this
problem, and it is shown that the corresponding adaptive change-point tests are
valid.",http://arxiv.org/abs/2010.12449v1
A Lane-Changing Prediction Method Based on Temporal Convolution Network,2020-11-01T07:33:10Z,"Yue Zhang, Yajie Zou, Jinjun Tang, Jian Liang","Lane-changing is an important driving behavior and unreasonable lane changes
can result in potentially dangerous traffic collisions. Advanced Driver
Assistance System (ADAS) can assist drivers to change lanes safely and
efficiently. To capture the stochastic time series of lane-changing behavior,
this study proposes a temporal convolutional network (TCN) to predict the
long-term lane-changing trajectory and behavior. In addition, the convolutional
neural network (CNN) and recurrent neural network (RNN) methods are considered
as the benchmark models to demonstrate the learning ability of the TCN. The
lane-changing dataset was collected by the driving simulator. The prediction
performance of TCN is demonstrated from three aspects: different input
variables, different input dimensions and different driving scenarios.
Prediction results show that the TCN can accurately predict the long-term
lane-changing trajectory and driving behavior with shorter computational time
compared with two benchmark models. The TCN can provide accurate lane-changing
prediction, which is one key information for the development of accurate ADAS.",http://arxiv.org/abs/2011.01224v1
Limits on Testing Structural Changes in Ising Models,2020-11-07T03:33:56Z,"Aditya Gangrade, Bobak Nazer, Venkatesh Saligrama","We present novel information-theoretic limits on detecting sparse changes in
Ising models, a problem that arises in many applications where network changes
can occur due to some external stimuli. We show that the sample complexity for
detecting sparse changes, in a minimax sense, is no better than learning the
entire model even in settings with local sparsity. This is a surprising fact in
light of prior work rooted in sparse recovery methods, which suggest that
sample complexity in this context scales only with the number of network
changes. To shed light on when change detection is easier than structured
learning, we consider testing of edge deletion in forest-structured graphs, and
high-temperature ferromagnets as case studies. We show for these that testing
of small changes is similarly hard, but testing of \emph{large} changes is
well-separated from structure learning. These results imply that testing of
graphical models may not be amenable to concepts such as restricted strong
convexity leveraged for sparsity pattern recovery, and algorithm development
instead should be directed towards detection of large changes.",http://arxiv.org/abs/2011.03678v1
"Robust multiple change-point detection for multivariate variability
  using data depth",2020-11-18T22:07:20Z,"Kelly Ramsay, Shoja'eddin Chenouri","In this paper, we introduce two robust, nonparametric methods for multiple
change-point detection in the variability of a multivariate sequence of
observations. We demonstrate that changes in ranks generated from data depth
functions can be used to detect changes in the variability of a sequence of
multivariate observations. In order to detect more than one change, the first
algorithm uses methods similar to that of wild-binary segmentation. The second
algorithm estimates change-points by maximizing a penalized version of the
classical Kruskal Wallis ANOVA test statistic. We show that this objective
function can be maximized via the well-known PELT algorithm. Under mild,
nonparametric assumptions both of these algorithms are shown to be consistent
for the correct number of change-points and the correct location(s) of the
change-point(s). We demonstrate the efficacy of these methods with a simulation
study, where we compare our new methods to several competing methods. We show
our methods outperform existing methods in this problem setting, and our
methods can estimate changes accurately when the data are heavy tailed or
skewed.",http://arxiv.org/abs/2011.09558v3
Optimal Change-Point Detection and Localization,2020-10-22T06:26:01Z,"Nicolas Verzelen, Magalie Fromont, Matthieu Lerasle, Patricia Reynaud-Bouret","Given a times series ${\bf Y}$ in $\mathbb{R}^n$, with a piece-wise contant
mean and independent components, the twin problems of change-point detection
and change-point localization respectively amount to detecting the existence of
times where the mean varies and estimating the positions of those
change-points. In this work, we tightly characterize optimal rates for both
problems and uncover the phase transition phenomenon from a global testing
problem to a local estimation problem. Introducing a suitable definition of the
energy of a change-point, we first establish in the single change-point setting
that the optimal detection threshold is $\sqrt{2\log\log(n)}$. When the energy
is just above the detection threshold, then the problem of localizing the
change-point becomes purely parametric: it only depends on the difference in
means and not on the position of the change-point anymore. Interestingly, for
most change-point positions, it is possible to detect and localize them at a
much smaller energy level. In the multiple change-point setting, we establish
the energy detection threshold and show similarly that the optimal localization
error of a specific change-point becomes purely parametric. Along the way,
tight optimal rates for Hausdorff and $l_1$ estimation losses of the vector of
all change-points positions are also established. Two procedures achieving
these optimal rates are introduced. The first one is a least-squares estimator
with a new multiscale penalty that favours well spread change-points. The
second one is a two-step multiscale post-processing procedure whose
computational complexity can be as low as $O(n\log(n))$. Notably, these two
procedures accommodate with the presence of possibly many low-energy and
therefore undetectable change-points and are still able to detect and localize
high-energy change-points even with the presence of those nuisance parameters.",http://arxiv.org/abs/2010.11470v2
"Item Quality Control in Educational Testing: Change Point Model,
  Compound Risk, and Sequential Detection",2020-08-23T20:46:48Z,"Yunxiao Chen, Yi-Hsuan Lee, Xiaoou Li","In standardized educational testing, test items are reused in multiple test
administrations. To ensure the validity of test scores, the psychometric
properties of items should remain unchanged over time. In this paper, we
consider the sequential monitoring of test items, in particular, the detection
of abrupt changes to their psychometric properties, where a change can be
caused by, for example, leakage of the item or change of the corresponding
curriculum. We propose a statistical framework for the detection of abrupt
changes in individual items. This framework consists of (1) a multi-stream
Bayesian change point model describing sequential changes in items, (2) a
compound risk function quantifying the risk in sequential decisions, and (3)
sequential decision rules that control the compound risk. Throughout the
sequential decision process, the proposed decision rule balances the trade-off
between two sources of errors, the false detection of pre-change items and the
non-detection of post-change items. An item-specific monitoring statistic is
proposed based on an item response theory model that eliminates the confounding
from the examinee population which changes over time. Sequential decision rules
and their theoretical properties are developed under two settings: the oracle
setting where the Bayesian change point model is completely known and a more
realistic setting where some parameters of the model are unknown. Simulation
studies are conducted under settings that mimic real operational tests.",http://arxiv.org/abs/2008.10104v2
Detection and Description of Change in Visual Streams,2020-03-27T20:49:38Z,"Davis Gilton, Ruotian Luo, Rebecca Willett, Greg Shakhnarovich","This paper presents a framework for the analysis of changes in visual
streams: ordered sequences of images, possibly separated by significant time
gaps. We propose a new approach to incorporating unlabeled data into training
to generate natural language descriptions of change. We also develop a
framework for estimating the time of change in visual stream. We use learned
representations for change evidence and consistency of perceived change, and
combine these in a regularized graph cut based change detector. Experimental
evaluation on visual stream datasets, which we release as part of our
contribution, shows that representation learning driven by natural language
descriptions significantly improves change detection accuracy, compared to
methods that do not rely on language.",http://arxiv.org/abs/2003.12633v2
"ChangeBeadsThreader: An Interactive Environment for Tailoring
  Automatically Untangled Changes",2020-03-31T10:47:10Z,"Satoshi Yamashita, Shinpei Hayashi, Motoshi Saeki","To improve the usability of a revision history, change untangling, which
reconstructs the history to ensure that changes in each commit belong to one
intentional task, is important. Although there are several untangling
approaches based on the clustering of fine-grained editing operations of source
code, they often produce unsuitable result for a developer, and manual
tailoring of the result is necessary. In this paper, we propose
ChangeBeadsThreader (CBT), an interactive environment for splitting and merging
change clusters to support the manual tailoring of untangled changes. CBT
provides two features: 1) a two-dimensional space where fine-grained change
history is visualized to help users find the clusters to be merged and 2) an
augmented diff view that enables users to confirm the consistency of the
changes in a specific cluster for finding those to be split. These features
allow users to easily tailor automatically untangled changes.",http://arxiv.org/abs/2003.14086v1
"Correlation-aware Unsupervised Change-point Detection via Graph Neural
  Networks",2020-04-24T18:28:57Z,"Ruohong Zhang, Yu Hao, Donghan Yu, Wei-Cheng Chang, Guokun Lai, Yiming Yang","Change-point detection (CPD) aims to detect abrupt changes over time series
data. Intuitively, effective CPD over multivariate time series should require
explicit modeling of the dependencies across input variables. However, existing
CPD methods either ignore the dependency structures entirely or rely on the
(unrealistic) assumption that the correlation structures are static over time.
In this paper, we propose a Correlation-aware Dynamics Model for CPD, which
explicitly models the correlation structure and dynamics of variables by
incorporating graph neural networks into an encoder-decoder framework.
Extensive experiments on synthetic and real-world datasets demonstrate the
advantageous performance of the proposed model on CPD tasks over strong
baselines, as well as its ability to classify the change-points as correlation
changes or independent changes. Keywords: Multivariate Time Series,
Change-point Detection, Graph Neural Networks",http://arxiv.org/abs/2004.11934v2
"Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for
  Change Captioning",2020-09-30T00:13:49Z,"Xiangxi Shi, Xu Yang, Jiuxiang Gu, Shafiq Joty, Jianfei Cai","Change Captioning is a task that aims to describe the difference between
images with natural language. Most existing methods treat this problem as a
difference judgment without the existence of distractors, such as viewpoint
changes. However, in practice, viewpoint changes happen often and can overwhelm
the semantic difference to be described. In this paper, we propose a novel
visual encoder to explicitly distinguish viewpoint changes from semantic
changes in the change captioning task. Moreover, we further simulate the
attention preference of humans and propose a novel reinforcement learning
process to fine-tune the attention directly with language evaluation rewards.
Extensive experimental results show that our method outperforms the
state-of-the-art approaches by a large margin in both Spot-the-Diff and
CLEVR-Change datasets.",http://arxiv.org/abs/2009.14352v1
Optimal multiple change-point detection for high-dimensional data,2020-11-16T09:45:07Z,"Emmanuel Pilliat, Alexandra Carpentier, Nicolas Verzelen","This manuscript makes two contributions to the field of change-point
detection. In a generalchange-point setting, we provide a generic algorithm for
aggregating local homogeneity testsinto an estimator of change-points in a time
series. Interestingly, we establish that the errorrates of the collection of
tests directly translate into detection properties of the
change-pointestimator. This generic scheme is then applied to various problems
including covariance change-point detection, nonparametric change-point
detection and sparse multivariate mean change-point detection. For the latter,
we derive minimax optimal rates that are adaptive to theunknown sparsity and to
the distance between change-points when the noise is Gaussian. Forsub-Gaussian
noise, we introduce a variant that is optimal in almost all sparsity regimes.",http://arxiv.org/abs/2011.07818v2
"Quickest Detection over Sensor Networks with Unknown Post-Change
  Distribution",2020-12-23T23:55:18Z,"Deniz Sargun, C. Emre Koksal","We propose a quickest change detection problem over sensor networks where
both the subset of sensors undergoing a change and the local post-change
distributions are unknown. Each sensor in the network observes a local discrete
time random process over a finite alphabet. Initially, the observations are
independent and identically distributed (i.i.d.) with known pre-change
distributions independent from other sensors. At a fixed but unknown change
point, a fixed but unknown subset of the sensors undergo a change and start
observing samples from an unknown distribution. We assume the change can be
quantified using concave (or convex) local statistics over the space of
distributions. We propose an asymptotically optimal and computationally
tractable stopping time for Lorden's criterion. Under this scenario, our
proposed method uses a concave global cumulative sum (CUSUM) statistic at the
fusion center and suppresses the most likely false alarms using information
projection. Finally, we show some numerical results of the simulation of our
algorithm for the problem described.",http://arxiv.org/abs/2012.13027v2
"DASNet: Dual attentive fully convolutional siamese networks for change
  detection of high resolution satellite images",2020-03-07T16:57:10Z,"Jie Chen, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu, Yu Liu, Haifeng Li","Change detection is a basic task of remote sensing image processing. The
research objective is to identity the change information of interest and filter
out the irrelevant change information as interference factors. Recently, the
rise of deep learning has provided new tools for change detection, which have
yielded impressive results. However, the available methods focus mainly on the
difference information between multitemporal remote sensing images and lack
robustness to pseudo-change information. To overcome the lack of resistance of
current methods to pseudo-changes, in this paper, we propose a new method,
namely, dual attentive fully convolutional Siamese networks (DASNet) for change
detection in high-resolution images. Through the dual-attention mechanism,
long-range dependencies are captured to obtain more discriminant feature
representations to enhance the recognition performance of the model. Moreover,
the imbalanced sample is a serious problem in change detection, i.e. unchanged
samples are much more than changed samples, which is one of the main reasons
resulting in pseudo-changes. We put forward the weighted double margin
contrastive loss to address this problem by punishing the attention to
unchanged feature pairs and increase attention to changed feature pairs. The
experimental results of our method on the change detection dataset (CDD) and
the building change detection dataset (BCDD) demonstrate that compared with
other baseline methods, the proposed method realizes maximum improvements of
2.1\% and 3.6\%, respectively, in the F1 score. Our Pytorch implementation is
available at https://github.com/lehaifeng/DASNet.",http://arxiv.org/abs/2003.03608v2
Simulating Lexical Semantic Change from Sense-Annotated Data,2020-01-09T20:37:49Z,"Dominik Schlechtweg, Sabine Schulte im Walde","We present a novel procedure to simulate lexical semantic change from
synchronic sense-annotated data, and demonstrate its usefulness for assessing
lexical semantic change detection models. The induced dataset represents a
stronger correspondence to empirically observed lexical semantic change than
previous synthetic datasets, because it exploits the intimate relationship
between synchronic polysemy and diachronic change. We publish the data and
provide the first large-scale evaluation gold standard for LSC detection
models.",http://arxiv.org/abs/2001.03216v1
"Network topology change-point detection from graph signals with prior
  spectral signatures",2020-10-21T23:21:37Z,"Chiraag Kaushik, T. Mitchell Roddenberry, Santiago Segarra","We consider the problem of sequential graph topology change-point detection
from graph signals. We assume that signals on the nodes of the graph are
regularized by the underlying graph structure via a graph filtering model,
which we then leverage to distill the graph topology change-point detection
problem to a subspace detection problem. We demonstrate how prior information
on the spectral signature of the post-change graph can be incorporated to
implicitly denoise the observed sequential data, thus leading to a natural
CUSUM-based algorithm for change-point detection. Numerical experiments
illustrate the performance of our proposed approach, particularly underscoring
the benefits of (potentially noisy) prior information.",http://arxiv.org/abs/2010.11345v1
"Novel Change of Measure Inequalities with Applications to PAC-Bayesian
  Bounds and Monte Carlo Estimation",2020-02-25T05:36:22Z,"Yuki Ohnishi, Jean Honorio","We introduce several novel change of measure inequalities for two families of
divergences: $f$-divergences and $\alpha$-divergences. We show how the
variational representation for $f$-divergences leads to novel change of measure
inequalities. We also present a multiplicative change of measure inequality for
$\alpha$-divergences and a generalized version of Hammersley-Chapman-Robbins
inequality. Finally, we present several applications of our change of measure
inequalities, including PAC-Bayesian bounds for various classes of losses and
non-asymptotic intervals for Monte Carlo estimates.",http://arxiv.org/abs/2002.10678v2
"Unsupervised Learning Methods for Visual Place Recognition in Discretely
  and Continuously Changing Environments",2020-01-24T12:23:15Z,"Stefan Schubert, Peer Neubert, Peter Protzel","Visual place recognition in changing environments is the problem of finding
matchings between two sets of observations, a query set and a reference set,
despite severe appearance changes. Recently, image comparison using CNN-based
descriptors showed very promising results. However, existing experiments from
the literature typically assume a single distinctive condition within each set
(e.g., reference: day, query: night). We demonstrate that as soon as the
conditions change within one set (e.g., reference: day, query: traversal
daytime-dusk-night-dawn), different places under the same condition can
suddenly look more similar than same places under different conditions and
state-of-the-art approaches like CNN-based descriptors fail. This paper
discusses this practically very important problem of in-sequence condition
changes and defines a hierarchy of problem setups from (1) no in-sequence
changes, (2) discrete in-sequence changes, to (3) continuous in-sequence
changes. We will experimentally evaluate the effect of these changes on two
state-of-the-art CNN-descriptors. Our experiments emphasize the importance of
statistical standardization of descriptors and shows its limitations in case of
continuous changes. To address this practically most relevant setup, we
investigate and experimentally evaluate the application of unsupervised
learning methods using two available PCA-based approaches and propose a novel
clustering-based extension of the statistical normalization.",http://arxiv.org/abs/2001.08960v1
"Real-Time Video Content Popularity Detection Based on Mean Change Point
  Analysis",2020-03-26T17:18:41Z,"Sotiris Skaperas, Lefteris Mamatas, Arsenia Chorti","Video content is responsible for more than 70% of the global IP traffic.
Consequently, it is important for content delivery infrastructures to rapidly
detect and respond to changes in content popularity dynamics. In this paper, we
propose the employment of on-line change point (CP) analysis to implement
real-time, autonomous and low-complexity video content popularity detection.
Our proposal, denoted as real-time change point detector (RCPD), estimates the
existence, the number and the direction of changes on the average number of
video visits by combining: (i) off-line and on-line CP detection algorithms;
(ii) an improved time-series segmentation heuristic for the reliable detection
of multiple CPs; and (iii) two algorithms for the identification of the
direction of changes. The proposed detector is validated against synthetic
data, as well as a large database of real YouTube video visits. It is
demonstrated that the RCPD can accurately identify changes in the average
content popularity and the direction of change. In particular, the success rate
of the RCPD over synthetic data is shown to exceed 94% for medium and large
changes in content popularity. Additionally,the dynamic time warping distance,
between the actual and the estimated changes, has been found to range
between20sampleson average, over synthetic data, to52samples, in real data.The
rapid responsiveness of the RCPD is instrumental in the deployment of
real-time, lightweight load balancing solutions, as shown in a real example.",http://arxiv.org/abs/2003.12044v1
"Consistent detection and optimal localization of all detectable change
  points in piecewise stationary arbitrarily sparse network-sequences",2020-09-04T11:17:10Z,"Sharmodeep Bhattacharyya, Shirshendu Chatterjee, Soumendu Sundar Mukherjee","We consider the offline change point detection and localization problem in
the context of piecewise stationary networks, where the observable is a finite
sequence of networks. We develop algorithms involving some suitably modified
CUSUM statistics based on adaptively trimmed adjacency matrices of the observed
networks for both detection and localization of single or multiple change
points present in the input data. We provide rigorous theoretical analysis and
finite sample estimates evaluating the performance of the proposed methods when
the input (finite sequence of networks) is generated from an inhomogeneous
random graph model, where the change points are characterized by the change in
the mean adjacency matrix. We show that the proposed algorithms can detect
(resp. localize) all change points, where the change in the expected adjacency
matrix is above the minimax detectability (resp. localizability) threshold,
consistently without any a priori assumption about (a) a lower bound for the
sparsity of the underlying networks, (b) an upper bound for the number of
change points, and (c) a lower bound for the separation between successive
change points, provided either the minimum separation between successive pairs
of change points or the average degree of the underlying networks goes to
infinity arbitrarily slowly. We also prove that the above condition is
necessary to have consistency.",http://arxiv.org/abs/2009.02112v1
"gfpop: an R Package for Univariate Graph-Constrained Change-Point
  Detection",2020-02-10T10:49:51Z,"Vincent Runge, Toby Dylan Hocking, Gaetano Romano, Fatemeh Afghah, Paul Fearnhead, Guillem Rigaill","In a world with data that change rapidly and abruptly, it is important to
detect those changes accurately. In this paper we describe an R package
implementing a generalized version of an algorithm recently proposed by Hocking
et al. [2020] for penalized maximum likelihood inference of constrained
multiple change-point models. This algorithm can be used to pinpoint the
precise locations of abrupt changes in large data sequences. There are many
application domains for such models, such as medicine, neuroscience or
genomics. Often, practitioners have prior knowledge about the changes they are
looking for. For example in genomic data, biologists sometimes expect peaks: up
changes followed by down changes. Taking advantage of such prior information
can substantially improve the accuracy with which we can detect and estimate
changes. Hocking et al. [2020] described a graph framework to encode many
examples of such prior information and a generic algorithm to infer the optimal
model parameters, but implemented the algorithm for just a single scenario. We
present the gfpop package that implements the algorithm in a generic manner in
R/C++. gfpop works for a user-defined graph that can encode prior assumptions
about the types of change that are possible and implements several loss
functions (Gauss, Poisson, binomial, biweight and Huber). We then illustrate
the use of gfpop on isotonic simulations and several applications in biology.
For a number of graphs the algorithm runs in a matter of seconds or minutes for
10^5 data points.",http://arxiv.org/abs/2002.03646v2
"Deep Image Translation with an Affinity-Based Change Prior for
  Unsupervised Multimodal Change Detection",2020-01-13T14:23:24Z,"Luigi Tommaso Luppino, Michael Kampffmeyer, Filippo Maria Bianchi, Gabriele Moser, Sebastiano Bruno Serpico, Robert Jenssen, Stian Normann Anfinsen","Image translation with convolutional neural networks has recently been used
as an approach to multimodal change detection. Existing approaches train the
networks by exploiting supervised information of the change areas, which,
however, is not always available. A main challenge in the unsupervised problem
setting is to avoid that change pixels affect the learning of the translation
function. We propose two new network architectures trained with loss functions
weighted by priors that reduce the impact of change pixels on the learning
objective. The change prior is derived in an unsupervised fashion from
relational pixel information captured by domain-specific affinity matrices.
Specifically, we use the vertex degrees associated with an absolute affinity
difference matrix and demonstrate their utility in combination with cycle
consistency and adversarial training. The proposed neural networks are compared
with state-of-the-art algorithms. Experiments conducted on three real datasets
show the effectiveness of our methodology.",http://arxiv.org/abs/2001.04271v2
"Observation of state-to-state hyperfine-changing collision in a
  Bose-Fermi mixture of $^6$Li and $^{41}$K atoms",2020-01-31T03:55:58Z,"Xiao-Qiong Wang, Yu-Xuan Wang, Xiang-Pei Liu, Ran Qi, Xing-Can Yao, Yu-Ao Chen, Jian-Wei Pan","Hyperfine-changing collisions are of fundamental interest for the studying of
ultracold heteronuclear mixtures. Here, we report the state-to-state study of
the hyperfine-changing-collision dynamics in a Bose-Fermi mixture of $^6$Li and
$^{41}$K atoms. The collision products are directly observed and the
spin-changing dynamics is measured. Based on a two-body collision model, the
experimental results are simultaneously fitted from which the spin-changing
rate coefficient of $ 1.9(2)\times 10^{-12}~\rm{cm^3\cdot s^{-1}}$ is gained,
being consistent with the multi-channel quantum defect theory calculation. We
further show that the contact parameter of $^6$Li-$^{41}$K mixture can be
extracted from the measured spin-changing dynamics. The obtained results are
consistent with the first order perturbation theory in the weakly-interacting
limit. Our system offers great promise for studying spin-changing interactions
in heteronuclear mixtures.",http://arxiv.org/abs/2001.11652v1
"Employing internal multiples in time-lapse seismic monitoring, using the
  Marchenko method",2020-03-24T09:55:00Z,"Kees Wapenaar, John van IJsseldijk","Time-lapse seismic monitoring aims at resolving changes in a producing
reservoir from changes in the reflection response. When the changes in the
reservoir are very small, the changes in the seismic response can become too
small to be reliably detected. In theory, multiple reflections can be used to
improve the detectability of traveltime changes: a wave that propagates several
times down and up through a reservoir layer will undergo a larger time shift
due to reservoir changes than a primary reflection. Since we are interested in
monitoring very local changes (usually in a thin reservoir layer), it would be
advantageous if we could identify the reservoir-related internal multiples in
the complex reflection response of the entire subsurface. We introduce a
Marchenko-based method to isolate these multiples from the complete reflection
response and illustrate the potential of this method with numerical examples.",http://arxiv.org/abs/2003.10738v1
Robustly detecting changes in warm Jupiters' transit impact parameters,2020-03-30T20:25:55Z,Rebekah I. Dawson,"Torques from a mutually inclined perturber can change a transiting planet's
impact parameter, resulting in variations in the transit shape and duration.
Detection of and upper limits on changes in impact parameter yield valuable
constraints on a planetary system's three dimensional architecture. Constraints
for warm Jupiters are particularly interesting because they allow us to test
origins theories that invoke a mutually inclined perturber. Because of warm
Jupiters' high signal-to-noise transits, changes in impact parameter are
feasible to detect. However, here we show that allowing the impact parameter to
vary uniformly and independently from transit to transit leads to incorrect
inferences about the change, propagating to incorrect inferences about the
perturber. We demonstrate that an appropriate prior on the change in impact
parameter mitigates this problem. We apply our approach to eight systems from
the literature and find evidence for changes in impact parameter for warm
Jupiter Kepler-46b. We conclude with our recommendations for light curve
fitting, including when to fit impact parameters vs. transit durations.",http://arxiv.org/abs/2003.13796v1
Dynamic complexity of Reachability: How many changes can we handle?,2020-04-27T12:27:37Z,"Samir Datta, Pankaj Kumar, Anish Mukherjee, Anuj Tawari, Nils Vortmeier, Thomas Zeume","In 2015, it was shown that reachability for arbitrary directed graphs can be
updated by first-order formulas after inserting or deleting single edges.
Later, in 2018, this was extended for changes of size $\frac{\log n}{\log \log
n}$, where $n$ is the size of the graph. Changes of polylogarithmic size can be
handled when also majority quantifiers may be used.
  In this paper we extend these results by showing that, for changes of
polylogarithmic size, first-order update formulas suffice for maintaining (1)
undirected reachability, and (2) directed reachability under insertions. For
classes of directed graphs for which efficient parallel algorithms can compute
non-zero circulation weights, reachability can be maintained with update
formulas that may use ""modulo 2"" quantifiers under changes of polylogarithmic
size. Examples for these classes include the class of planar graphs and graphs
with bounded treewidth. The latter is shown here.
  As the logics we consider cannot maintain reachability under changes of
larger sizes, our results are optimal with respect to the size of the changes.",http://arxiv.org/abs/2004.12739v1
"Detection of Change Points in Piecewise Polynomial Signals Using Trend
  Filtering",2020-09-18T00:47:00Z,"Reza V. Mehrizi, Shojaeddin Chenouri","While many approaches have been proposed for discovering abrupt changes in
piecewise constant signals, few methods are available to capture these changes
in piecewise polynomial signals. In this paper, we propose a change point
detection method, PRUTF, based on trend filtering. By providing a comprehensive
dual solution path for trend filtering, PRUTF allows us to discover change
points of the underlying signal for either a given value of the regularization
parameter or a specific number of steps of the algorithm. We demonstrate that
the dual solution path constitutes a Gaussian bridge process that enables us to
derive an exact and efficient stopping rule for terminating the search
algorithm. We also prove that the estimates produced by this algorithm are
asymptotically consistent in pattern recovery. This result holds even in the
case of staircases (consecutive change points of the same sign) in the signal.
Finally, we investigate the performance of our proposed method for various
signals and then compare its performance against some state-of-the-art methods
in the context of change point detection. We apply our method to three
real-world datasets including the UK House Price Index (HPI), the GISS surface
Temperature Analysis (GISTEMP) and the Coronavirus disease (COVID-19) pandemic.",http://arxiv.org/abs/2009.08573v2
"Online Structural Change-point Detection of High-dimensional Streaming
  Data via Dynamic Sparse Subspace Learning",2020-09-24T14:16:18Z,"Ruiyu Xu, Jianguo Wu, Xiaowei Yue, Yongxiang Li","High-dimensional streaming data are becoming increasingly ubiquitous in many
fields. They often lie in multiple low-dimensional subspaces, and the manifold
structures may change abruptly on the time scale due to pattern shift or
occurrence of anomalies. However, the problem of detecting the structural
changes in a real-time manner has not been well studied. To fill this gap, we
propose a dynamic sparse subspace learning approach for online structural
change-point detection of high-dimensional streaming data. A novel multiple
structural change-point model is proposed and the asymptotic properties of the
estimators are investigated. A tuning method based on Bayesian information
criterion and change-point detection accuracy is proposed for penalty
coefficients selection. An efficient Pruned Exact Linear Time based algorithm
is proposed for online optimization and change-point detection. The
effectiveness of the proposed method is demonstrated through several simulation
studies and a real case study on gesture data for motion tracking.",http://arxiv.org/abs/2009.11713v3
"Unsupervised Self-training Algorithm Based on Deep Learning for Optical
  Aerial Images Change Detection",2020-10-15T01:51:46Z,"Yuan Zhou, Xiangrui Li","Optical aerial images change detection is an important task in earth
observation and has been extensively investigated in the past few decades.
Generally, the supervised change detection methods with superior performance
require a large amount of labeled training data which is obtained by manual
annotation with high cost. In this paper, we present a novel unsupervised
self-training algorithm (USTA) for optical aerial images change detection. The
traditional method such as change vector analysis is used to generate the
pseudo labels. We use these pseudo labels to train a well designed
convolutional neural network. The network is used as a teacher to classify the
original multitemporal images to generate another set of pseudo labels. Then
two set of pseudo labels are used to jointly train a student network with the
same structure as the teacher. The final change detection result can be
obtained by the trained student network. Besides, we design an image filter to
control the usage of change information in the pseudo labels in the training
process of the network. The whole process of the algorithm is an unsupervised
process without manually marked labels. Experimental results on the real
datasets demonstrate competitive performance of our proposed method.",http://arxiv.org/abs/2010.07469v2
Solar Cycle Related Changes in the Helium Ionization Zones of the Sun,2020-10-21T18:01:08Z,"Courtney B. Watson, Sarbani Basu","Helioseismic data for solar cycles 23 and 24 have shown unequivocally that
solar dynamics changes with solar activity. Changes in solar structure have
been more difficult to detect. Basu & Mandel (2004) had claimed that the then
available data revealed changes in the HeII ionization zone of the Sun. The
amount of change, however, indicated the need for larger than expected changes
in the magnetic fields. Now that helioseismic data spanning two solar cycles
are available, we have redone the analysis using improved fitting techniques.
We find that there is indeed a change in the region around the HeII ionization
zone that is correlated with activity. Since the data sets now cover two solar
cycles, the time variation is easily discernible.",http://arxiv.org/abs/2010.11215v1
"Random matrix theory analysis of a temperature-related transformation in
  statistics of Fano-Feshbach resonances in Thulium atoms",2020-11-03T19:52:23Z,"E. T. Davletov, V. V. Tsyganok, V. A. Khlebnikov, D. A. Pershin, A. V. Akimov","Recently, transformation from random to chaotic behavior in the statistics of
Fano-Feshbach resonances was observed in thulium atoms with rising ensemble
temperature. We performed random matrix theory simulations of such spectra and
analyzed the resulting statistics. Our simulations show that, when evaluated in
terms of the Brody parameter, resonance statistics do not change or change
insignificantly with rising temperature if temperature is the only changing
parameter. In the experiments evaluated, temperature was changed simultaneously
with optical dipole trap depth. Thus, simulations included the Stark shift
based on the known polarizability of the free atoms and assuming their
polarizability remains the same in the bound state. Somewhat surprisingly, we
found that, while including the Stark shift does lead to minor statistical
changes, it does not change the resonance statistics and, therefore, is not
responsible for the experimentally observed statistic transformation. This
observation suggests that either our assumption regarding the polarizability of
Feshbach molecules is poor or that an additional mechanism changes the
statistics and leads to more chaotic statistical behavior.",http://arxiv.org/abs/2011.01978v1
CAT: Change-focused Android GUI Testing,2020-11-23T22:19:03Z,"Chao Peng, Ajitha Rajan","Android Apps are frequently updated, every couple of weeks, to keep up with
changing user, hardware and business demands. Correctness of App updates is
checked through extensive testing. Recent research has proposed tools for
automated GUI event generation in Android Apps. These techniques, however, are
not efficient at checking App updates as the generated GUI events do not
prioritise updates, and instead explore other App behaviours.
  We address this need in this paper with CAT (Change-focused Android GUI
Testing). For App updates, at the source code or GUI level, CAT performs change
impact analysis to identify GUI elements affected by the update. CAT then
generates length-3 GUI event sequences to interact with these GUI elements. Our
empirical evaluations using 21 publicly available open source Android Apps
demonstrated that CAT is able to automatically identify GUI elements affected
by App updates, generate and execute length-3 GUI event sequences focusing on
change-affected GUI elements. Comparison with two popular GUI event generation
tools, DroidBot and DroidMate, revealed that CAT was more effective at
interacting with the change-affected GUI elements. Finally, CAT was able to
detect previously unknown change-related bugs in two Apps.",http://arxiv.org/abs/2011.11766v1
"Detecting Abrupt Changes in the Presence of Local Fluctuations and
  Autocorrelated Noise",2020-05-04T10:51:19Z,"Gaetano Romano, Guillem Rigaill, Vincent Runge, Paul Fearnhead","Whilst there are a plethora of algorithms for detecting changes in mean in
univariate time-series, almost all struggle in real applications where there is
autocorrelated noise or where the mean fluctuates locally between the abrupt
changes that one wishes to detect. In these cases, default implementations,
which are often based on assumptions of a constant mean between changes and
independent noise, can lead to substantial over-estimation of the number of
changes. We propose a principled approach to detect such abrupt changes that
models local fluctuations as a random walk process and autocorrelated noise via
an AR(1) process. We then estimate the number and location of changepoints by
minimising a penalised cost based on this model. We develop a novel and
efficient dynamic programming algorithm, DeCAFS, that can solve this
minimisation problem; despite the additional challenge of dependence across
segments, due to the autocorrelated noise, which makes existing algorithms
inapplicable. Theory and empirical results show that our approach has greater
power at detecting abrupt changes than existing approaches. We apply our method
to measuring gene expression levels in bacteria.",http://arxiv.org/abs/2005.01379v1
"Online Change Point Detection in Molecular Dynamics With Optical Random
  Features",2020-06-15T19:07:57Z,"Am√©lie Chatelain, Elena Tommasone, Laurent Daudet, Iacopo Poli","Proteins are made of atoms constantly fluctuating, but can occasionally
undergo large-scale changes. Such transitions are of biological interest,
linking the structure of a protein to its function with a cell. Atomic-level
simulations, such as Molecular Dynamics (MD), are used to study these events.
However, molecular dynamics simulations produce time series with multiple
observables, while changes often only affect a few of them. Therefore,
detecting conformational changes has proven to be challenging for most
change-point detection algorithms. In this work, we focus on the identification
of such events given many noisy observables. In particular, we show that the
No-prior-Knowledge Exponential Weighted Moving Average (NEWMA) algorithm can be
used along optical hardware to successfully identify these changes in
real-time. Our method does not need to distinguish between the background of a
protein and the protein itself. For larger simulations, it is faster than using
traditional silicon hardware and has a lower memory footprint. This technique
may enhance the sampling of the conformational space of molecules. It may also
be used to detect change-points in other sequential data with a large number of
features.",http://arxiv.org/abs/2006.08697v2
"Development of an Algorithm for Identifying Changes in System Dynamics
  from Time Series",2020-06-28T02:41:31Z,"Ferdaus Kawsar, Mohammad Adibuzzaman","The development of an algorithm with related mathematical concepts and
supporting hypothesis for detecting changes in system dynamics from time series
along with empirical analysis and theoretical justification is presented. For
the method, changes in the second largest eigenvalue of Markov Chain (SLEM) or
mixing rate, is observed as an indicator of the changes in system dynamics. The
Markov chain is created from empirical transition probabilities of a time
series. The method is developed for the application of detecting hemorrhage
from arterial blood pressure in anesthetized swine. The rationale of the change
in the SLEM is investigated empirically with an artificial blood pressure model
and, by studying correlations with other measures such as smoothness of time
series, and density of the transition probability matrix of the Markov chain.
The mathematical analysis shows that the change in the SLEM is a consequence of
the change in the transition probabilities between different states and
reflects information about the system dynamics.",http://arxiv.org/abs/2006.15488v1
"Mixture Complexity and Its Application to Gradual Clustering Change
  Detection",2020-07-15T03:49:38Z,"Shunki Kyoya, Kenji Yamanishi","In model-based clustering using finite mixture models, it is a significant
challenge to determine the number of clusters (cluster size). It used to be
equal to the number of mixture components (mixture size); however, this may not
be valid in the presence of overlaps or weight biases. In this study, we
propose to continuously measure the cluster size in a mixture model by a new
concept called mixture complexity (MC). It is formally defined from the
viewpoint of information theory and can be seen as a natural extension of the
cluster size considering overlap and weight bias. Subsequently, we apply MC to
the issue of gradual clustering change detection. Conventionally, clustering
changes has been considered to be abrupt, induced by the changes in the mixture
size or cluster size. Meanwhile, we consider the clustering changes to be
gradual in terms of MC; it has the benefits of finding the changes earlier and
discerning the significant and insignificant changes. We further demonstrate
that the MC can be decomposed according to the hierarchical structures of the
mixture models; it helps us to analyze the detail of substructures.",http://arxiv.org/abs/2007.07467v1
Epidemic changepoint detection in the presence of nuisance changes,2020-08-19T03:32:51Z,"Julius Juodakis, Stephen Marsland","Many time series problems feature epidemic changes - segments where a
parameter deviates from a background baseline. The number and location of such
changes can be estimated in a principled way by existing detection methods,
providing that the background level is stable and known. However, practical
data often contains nuisance changes in background level, which interfere with
standard estimation techniques. Furthermore, such changes often differ from the
target segments only in duration, and appear as false alarms in the detection
results. To solve these issues, we propose a two-level detector that models and
separates nuisance and signal changes. As part of this method, we developed a
new, efficient approach to simultaneously estimate unknown, but fixed,
background level and detect epidemic changes. The analytic and computational
properties of the proposed methods are established, including consistency and
convergence. We demonstrate via simulations that our two-level detector
provides accurate estimation of changepoints under a nuisance process, while
other state-of-the-art detectors fail. Using real-world genomic and demographic
datasets, we demonstrate that our method can identify and localise target
events while separating out seasonal variations and experimental artefacts.",http://arxiv.org/abs/2008.08240v1
AC2 -- Towards Understanding Architectural Changes in Rapid Releases,2020-12-21T14:01:52Z,"A Eashaan Rao, Dheeraj Vagavolu, Sridhar Chimalakonda","Open source projects are adopting faster release cycles that reflect various
changes. Therefore, comprehending the effects of these changes on software's
architecture over the releases becomes necessary. However, it is challenging to
keep architecture in-check and add new changes simultaneously for every
release. To this end, we propose a visualization tool called AC2, which allows
its users to examine the alterations in the architecture at both higher and
lower levels of abstraction for the python projects. AC2 uses call graphs and
collaboration graphs to show the interaction between different architectural
components. The tool provides four different views to see the architectural
changes. The user can examine two releases at a time to comprehend the
architectural changes between the releases. AC2 can support the maintainers and
developers to observe changes in the project and its influence on the
architecture, which allow them to see its increasing complexity over the
releases at the component level. AC2 can be downloaded at
https://github.com/dheerajrox/AC2 and its demo can be seen at the website
https://dheerajrox.github.io/AC2doc or on youtube
https://www.youtube.com/watch?v=GNrJfZ0RCVI",http://arxiv.org/abs/2012.11348v2
An Evaluation of Change Point Detection Algorithms,2020-03-13T12:23:41Z,"Gerrit J. J. van den Burg, Christopher K. I. Williams","Change point detection is an important part of time series analysis, as the
presence of a change point indicates an abrupt and significant change in the
data generating process. While many algorithms for change point detection have
been proposed, comparatively little attention has been paid to evaluating their
performance on real-world time series. Algorithms are typically evaluated on
simulated data and a small number of commonly-used series with unreliable
ground truth. Clearly this does not provide sufficient insight into the
comparative performance of these algorithms. Therefore, instead of developing
yet another change point detection method, we consider it vastly more important
to properly evaluate existing algorithms on real-world data. To achieve this,
we present a data set specifically designed for the evaluation of change point
detection algorithms that consists of 37 time series from various application
domains. Each series was annotated by five human annotators to provide ground
truth on the presence and location of change points. We analyze the consistency
of the human annotators, and describe evaluation metrics that can be used to
measure algorithm performance in the presence of multiple ground truth
annotations. Next, we present a benchmark study where 14 algorithms are
evaluated on each of the time series in the data set. Our aim is that this data
set will serve as a proving ground in the development of novel change point
detection algorithms.",http://arxiv.org/abs/2003.06222v3
"From W-Net to CDGAN: Bi-temporal Change Detection via Deep Learning
  Techniques",2020-03-14T09:24:08Z,"Bin Hou, Qingjie Liu, Heng Wang, Yunhong Wang","Traditional change detection methods usually follow the image differencing,
change feature extraction and classification framework, and their performance
is limited by such simple image domain differencing and also the hand-crafted
features. Recently, the success of deep convolutional neural networks (CNNs)
has widely spread across the whole field of computer vision for their powerful
representation abilities. In this paper, we therefore address the remote
sensing image change detection problem with deep learning techniques. We
firstly propose an end-to-end dual-branch architecture, termed as the W-Net,
with each branch taking as input one of the two bi-temporal images as in the
traditional change detection models. In this way, CNN features with more
powerful representative abilities can be obtained to boost the final detection
performance. Also, W-Net performs differencing in the feature domain rather
than in the traditional image domain, which greatly alleviates loss of useful
information for determining the changes. Furthermore, by reformulating change
detection as an image translation problem, we apply the recently popular
Generative Adversarial Network (GAN) in which our W-Net serves as the
Generator, leading to a new GAN architecture for change detection which we call
CDGAN. To train our networks and also facilitate future research, we construct
a large scale dataset by collecting images from Google Earth and provide
carefully manually annotated ground truths. Experiments show that our proposed
methods can provide fine-grained change detection results superior to the
existing state-of-the-art baselines.",http://arxiv.org/abs/2003.06583v1
"A Cooperative Control Framework for CAV Lane Change in a Mixed Traffic
  Environment",2020-10-12T03:57:21Z,"Runjia Du, Sikai Chen, Yujie Li, Jiqian Dong, Paul Young Joun Ha, Samuel Labi","In preparing for connected and autonomous vehicles (CAVs), a worrisome aspect
is the transition era which will be characterized by mixed traffic (where CAVs
and human-driven vehicles (HDVs) share the roadway). Consistent with
expectations that CAVs will improve road safety, on-road CAVs may adopt rather
conservative control policies, and this will likely cause HDVs to unduly
exploit CAV conservativeness by driving in ways that imperil safety. A context
of this situation is lane-changing by the CAV. Without cooperation from other
vehicles in the traffic stream, it can be extremely unsafe for the CAV to
change lanes under dense, high-speed traffic conditions. The cooperation of
neighboring vehicles is indispensable. To address this issue, this paper
develops a control framework where connected HDVs and CAV can cooperate to
facilitate safe and efficient lane changing by the CAV. Throughout the
lane-change process, the safety of not only the CAV but also of all neighboring
vehicles, is ensured through a collision avoidance mechanism in the control
framework. The overall traffic flow efficiency is analyzed in terms of the
ambient level of CHDV-CAV cooperation. The analysis outcomes are including the
CAVs lane-change feasibility, the overall duration of the lane change. Lane
change is a major source of traffic disturbance at multi-lane highways that
impair their traffic flow efficiency. In providing a control framework for lane
change in mixed traffic, this study shows how CHDV-CAV cooperation could help
enhancing system efficiency.",http://arxiv.org/abs/2010.05439v1
"A Probabilistic Approach in Historical Linguistics Word Order Change in
  Infinitival Clauses: from Latin to Old French",2020-11-16T20:30:31Z,Olga Scrivner,"This research offers a new interdisciplinary approach to the field of
Linguistics by using Computational Linguistics, NLP, Bayesian Statistics and
Sociolinguistics methods. This thesis investigates word order change in
infinitival clauses from Object-Verb (OV) to Verb-Object (VO) in the history of
Latin and Old French. By applying a variationist approach, I examine a
synchronic word order variation in each stage of language change, from which I
infer the character, periodization and constraints of diachronic variation. I
also show that in discourse-configurational languages, such as Latin and Early
Old French, it is possible to identify pragmatically neutral contexts by using
information structure annotation. I further argue that by mapping pragmatic
categories into a syntactic structure, we can detect how word order change
unfolds. For this investigation, the data are extracted from annotated corpora
spanning several centuries of Latin and Old French and from additional
resources created by using computational linguistic methods. The data are then
further codified for various pragmatic, semantic, syntactic and sociolinguistic
factors. This study also evaluates previous factors proposed to account for
word order alternation and change. I show how information structure and
syntactic constraints change over time and propose a method that allows
researchers to differentiate a stable word order alternation from alternation
indicating a change. Finally, I present a three-stage probabilistic model of
word order change, which also conforms to traditional language change patterns.",http://arxiv.org/abs/2011.08262v1
Berry Flux Diagonalization: Application to Electric Polarization,2020-02-07T19:51:58Z,"John Bonini, David Vanderbilt, Karin M. Rabe","The switching polarization of a ferroelectric is a characterization of the
current that flows due to changes in polarization when the system is switched
between two states. Computation of this change in polarization in crystal
systems has been enabled by the modern theory of polarization, where it is
expressed in terms of a change in Berry phase as the material switches. It is
straightforward to compute this change of phase, but only modulo $2\pi$,
requiring a branch choice from among a lattice of values separated by $2\pi$.
The measured switching polarization depends on the actual path along which the
material switches, which in general involves nucleation and growth of domains
and is therefore quite complex. In this work, we present a physically motivated
approach for predicting the experimentally measured switching polarization that
involves separating the change in phase between two states into as many
gauge-invariant smaller phase changes as possible. As long as the magnitudes of
these smaller phase changes remain smaller than $\pi$, their sum forms a phase
change which corresponds to the change one would find along any path involving
minimal evolution of the atomic and electronic structure. We show that for
typical ferroelectrics, including those that would have otherwise required a
densely sampled path, this technique allows the switching polarization to be
computed without any need for intermediate sampling between oppositely
polarized states.",http://arxiv.org/abs/2002.02995v2
"Detection of data drift and outliers affecting machine learning model
  performance over time",2020-12-16T20:50:12Z,"Samuel Ackerman, Eitan Farchi, Orna Raz, Marcel Zalmanovici, Parijat Dube","A trained ML model is deployed on another `test' dataset where target feature
values (labels) are unknown. Drift is distribution change between the training
and deployment data, which is concerning if model performance changes. For a
cat/dog image classifier, for instance, drift during deployment could be rabbit
images (new class) or cat/dog images with changed characteristics (change in
distribution). We wish to detect these changes but can't measure accuracy
without deployment data labels. We instead detect drift indirectly by
nonparametrically testing the distribution of model prediction confidence for
changes. This generalizes our method and sidesteps domain-specific feature
representation.
  We address important statistical issues, particularly Type-1 error control in
sequential testing, using Change Point Models (CPMs; see Adams and Ross 2012).
We also use nonparametric outlier methods to show the user suspicious
observations for model diagnosis, since the before/after change confidence
distributions overlap significantly. In experiments to demonstrate robustness,
we train on a subset of MNIST digit classes, then insert drift (e.g., unseen
digit class) in deployment data in various settings (gradual/sudden changes in
the drift proportion). A novel loss function is introduced to compare the
performance (detection delay, Type-1 and 2 errors) of a drift detector under
different levels of drift class contamination.",http://arxiv.org/abs/2012.09258v3
Data segmentation algorithms: Univariate mean change and beyond,2020-12-23T17:21:33Z,"Haeran Cho, Claudia Kirch","Data segmentation a.k.a. multiple change point analysis has received
considerable attention due to its importance in time series analysis and signal
processing, with applications in a variety of fields including natural and
social sciences, medicine, engineering and finance.
  In the first part of this survey, we review the existing literature on the
canonical data segmentation problem which aims at detecting and localising
multiple change points in the mean of univariate time series. We provide an
overview of popular methodologies on their computational complexity and
theoretical properties. In particular, our theoretical discussion focuses on
the separation rate relating to which change points are detectable by a given
procedure, and the localisation rate quantifying the precision of corresponding
change point estimators, and we distinguish between whether a homogeneous or
multiscale viewpoint has been adopted in their derivation. We further highlight
that the latter viewpoint provides the most general setting for investigating
the optimality of data segmentation algorithms.
  Arguably, the canonical segmentation problem has been the most popular
framework to propose new data segmentation algorithms and study their
efficiency in the last decades. In the second part of this survey, we motivate
the importance of attaining an in-depth understanding of strengths and
weaknesses of methodologies for the change point problem in a simpler,
univariate setting, as a stepping stone for the development of methodologies
for more complex problems. We illustrate this with a range of examples
showcasing the connections between complex distributional changes and those in
the mean. We also discuss extensions towards high-dimensional change point
problems where we demonstrate that the challenges arising from high
dimensionality are orthogonal to those in dealing with multiple change points.",http://arxiv.org/abs/2012.12814v2
Unknottability of spatial graphs by region crossing changes,2020-09-25T10:45:40Z,"Yukari Funakoshi, Kenta Noguchi, Ayaka Shimizu","A region crossing change is a local transformation on spatial graph diagrams
switching the over/under relations at all the crossings on the boundary of a
region. In this paper, we show that a spatial graph of a planar graph is
unknottable by region crossing changes if and only if the spatial graph is
non-Eulerian or is Eulerian and proper.",http://arxiv.org/abs/2009.12119v2
"What happens to the chemical equilibrium in case of a reference frame
  change",2020-09-28T18:50:53Z,"Friedrich Herrmann, Michael Pohlig","It is shown that upon a reference frame change, the chemical potential
transforms in the same way as temperature and electric potential. Consequently,
during a reference frame change, a chemical equilibrium remains a chemical
equilibrium.",http://arxiv.org/abs/2010.02335v2
Rationality of quotients by finite Heisenberg groups,2020-10-11T09:01:25Z,"Stanislav Grishin, Ilya Karzhemanov, with an Appendix by Ming-chang Kang","We prove rationality of the quotient $\mathbb{C}^n / H_n$ for the finite
Heisenberg group $H_n$, any $n \ge 1$, acting on $\mathbb{C}^n$ via its
irreducible representation.",http://arxiv.org/abs/2010.05196v2
A Global Hartman-Grobman Theorem,2020-02-14T15:58:23Z,Xiaochang Wang,"We showed that for any bounded neighborhood of a hyperbolic equilibrium point
$x_0$, there is a transformation which is locally homeomorphism, such that the
system is changed into a linear system in this neighborhood.
  If the eigenvalues of $Df(x_0)$ are all located in the left-half complex
plane, then there is a homeomorphism on the whole region of attraction such
that the nonlinear system on the region of attraction is changed into a linear
system under such a coordinate change.",http://arxiv.org/abs/2002.06094v1
Chang's lemma via Pinsker's inequality,2020-05-21T15:39:23Z,"Lianna Hambardzumyan, Yaqiao Li","Extending the idea in [Impagliazzo, R., Moore, C. and Russell, A., An
entropic proof of Chang's inequality. SIAM Journal on Discrete Mathematics,
28(1), pp.173-176.] we give a short information theoretic proof for Chang's
lemma that is based on Pinsker's inequality.",http://arxiv.org/abs/2005.10830v1
Green Measures for Time Changed Markov Processes,2020-08-07T22:20:41Z,"Jos√© L. da Silva, Yuri Kondratiev","In this paper we study Green measures for certain classes of random time
change Markov processes where the random time change are inverse subordinators.
We show the existence of the Green measure for these processes under the
condition of the existence of the Green measure of the original Markov
processes and they coincide. Applications to fractional dynamics in given.",http://arxiv.org/abs/2008.03390v1
"Frequent or Systematic Changes? discussion on ""Detecting possibly
  frequent change-points: Wild Binary Segmentation 2 and steepest-drop model
  selection.""",2020-08-11T06:41:46Z,Myung Hwan Seo,"We discuss Fryzlewicz's (2020) that proposes WBS2.SDLL approach to detect
possibly frequent changes in mean of a series. Our focus is on the potential
issues related to the model misspecification. We present some numerical
examples such as the self-exciting threshold autoregression and the unit root
process, that can be confused as a frequent change-points model.",http://arxiv.org/abs/2008.04544v1
"Dimensional reduction in evolving spin-glass model: correlation of
  phenotypic responses to environmental and mutational changes",2020-01-11T06:14:07Z,"Ayaka Sakata, Kunihiko Kaneko","The evolution of high-dimensional phenotypes is investigated using a
statistical physics model consists of interacting spins, in which genotypes,
phenotypes, and environments are represented by spin configurations,
interaction matrices, and external fields, respectively. We found that
phenotypic changes upon diverse environmental change and genetic variation are
highly correlated across all spins, consistent with recent experimental
observations of biological systems. The dimension reduction in phenotypic
changes is shown to be a result of the evolution of the robustness to thermal
noise, achieved at the replica symmetric phase.",http://arxiv.org/abs/2001.03714v2
"Generalization of Change-Point Detection in Time Series Data Based on
  Direct Density Ratio Estimation",2020-01-17T15:45:38Z,"Mikhail Hushchyn, Andrey Ustyuzhanin","The goal of the change-point detection is to discover changes of time series
distribution. One of the state of the art approaches of the change-point
detection are based on direct density ratio estimation. In this work we show
how existing algorithms can be generalized using various binary classification
and regression models. In particular, we show that the Gradient Boosting over
Decision Trees and Neural Networks can be used for this purpose. The algorithms
are tested on several synthetic and real-world datasets. The results show that
the proposed methods outperform classical RuLSIF algorithm. Discussion of cases
where the proposed algorithms have advantages over existing methods are also
provided.",http://arxiv.org/abs/2001.06386v1
Misspecified and Asymptotically Minimax Robust Quickest Change Diagnosis,2020-04-21T04:37:01Z,Timothy L. Molloy,"The problem of quickly diagnosing an unknown change in a stochastic process
is studied. We establish novel bounds on the performance of misspecified
diagnosis algorithms designed for changes that differ from those of the
process, and pose and solve a new robust quickest change diagnosis problem in
the asymptotic regime of few false alarms and false isolations. Simulations
suggest that our asymptotically robust solution offers a computationally
efficient alternative to generalised likelihood ratio algorithms.",http://arxiv.org/abs/2004.09748v1
Change Point Detection by Cross-Entropy Maximization,2020-09-02T21:45:13Z,"Aur√©lien Serre, Didier Ch√©telat, Andrea Lodi","Many offline unsupervised change point detection algorithms rely on
minimizing a penalized sum of segment-wise costs. We extend this framework by
proposing to minimize a sum of discrepancies between segments. In particular,
we propose to select the change points so as to maximize the cross-entropy
between successive segments, balanced by a penalty for introducing new change
points. We propose a dynamic programming algorithm to solve this problem and
analyze its complexity. Experiments on two challenging datasets demonstrate the
advantages of our method compared to three state-of-the-art approaches.",http://arxiv.org/abs/2009.01358v1
A review on minimax rates in change point detection and localisation,2020-11-03T17:19:45Z,Yi Yu,"This paper reviews recent developments in fundamental limits and optimal
algorithms for change point analysis. We focus on minimax optimal rates in
change point detection and localisation, in both parametric and nonparametric
models. We start with the univariate mean change point analysis problem and
review the state-of-the-art results in the literature. We then move on to more
complex data types and investigate general principles behind the optimal
procedures that lead to minimax rate-optimal results.",http://arxiv.org/abs/2011.01857v1
"Loop quantum gravity, signature change, and the no-boundary proposal",2020-11-05T14:49:35Z,"Martin Bojowald, Suddhasattwa Brahma","Covariant models of loop quantum gravity generically imply dynamical
signature change at high density. This article presents detailed derivations
that show the fruitful interplay of this new kind of signature change with
wave-function proposals of quantum cosmology, such as the no-boundary and
tunneling proposals. In particular, instabilities of inhomogeneous
perturbations found in a Lorentzian path-integral treatment are naturally
cured. Importantly, dynamical signature change does not require Planckian
densities when off-shell instantons are relevant.",http://arxiv.org/abs/2011.02884v2
"OP-IMS @ DIACR-Ita: Back to the Roots: SGNS+OP+CD still rocks Semantic
  Change Detection",2020-11-06T10:02:12Z,"Jens Kaiser, Dominik Schlechtweg, Sabine Schulte im Walde","We present the results of our participation in the DIACR-Ita shared task on
lexical semantic change detection for Italian. We exploit one of the earliest
and most influential semantic change detection models based on Skip-Gram with
Negative Sampling, Orthogonal Procrustes alignment and Cosine Distance and
obtain the winning submission of the shared task with near to perfect accuracy
.94. Our results once more indicate that, within the present task setup in
lexical semantic change detection, the traditional type-based approaches yield
excellent performance.",http://arxiv.org/abs/2011.03258v1
Polynomial mixing for time-changes of unipotent flows,2020-11-23T05:24:48Z,Davide Ravotti,"Let $G$ be a connected semisimple Lie group with finite centre, and let $M=
\Gamma \backslash G$ be a compact homogeneous manifold. Under a spectral gap
assumption, we show that smooth time-changes of any unipotent flow on $M$ have
polynomial decay of correlations. Our result applies also in the case where $M$
is a finite volume, non-compact quotient under some additional assumptions on
the generator of the time-change. This generalizes a result by Forni and
Ulcigrai (JMD, 2012) for smooth time-changes of horocycle flows on compact
surfaces.",http://arxiv.org/abs/2011.11213v1
Correlation-induced orbital angular momentum changes,2020-05-04T17:21:58Z,"Yongtao Zhang, Olga Korotkova, Yangjian Cai, Greg Gbur","We demonstrate that the orbital angular momentum flux density of a paraxial
light beam can change on propagation in free space. These changes are entirely
due to the spatial coherence state of the source, and the effect is analogous
to correlation-induced changes in the intensity, spectrum and polarization of a
beam. The use of the source coherence state to control the width, shape, and
transverse shifts of the OAM flux density is demonstrated with numerical
examples.",http://arxiv.org/abs/2005.01667v1
"Asymptotic behaviour and functional limit theorems for a time changed
  Wiener process",2020-05-08T15:54:43Z,"Yuri Kondratiev, Yuliya Mishura, Ren√© L. Schilling","We study the asymptotic behaviour of a properly normalized time changed
Wiener processes. The time change reflects the fact that we consider the
Laplace operator (which generates a Wiener process) multiplied by a possibly
degenerate state-space dependent intensity $\lambda(x)$. Applying a functional
limit theorem for the superposition of stochastic processes, we prove
functional limit theorems for the normalized time changed Wiener process. The
normalization depends on the asymptotic behaviour of the intensity function
$\lambda$. One of the possible limits is a skew Brownian motion.",http://arxiv.org/abs/2005.04122v1
"Scalable robustness of interconnected systems subject to structural
  changes",2020-05-12T19:01:11Z,"Steffi Knorn, Bart Besselink","This paper studies the robustness of large-scale interconnected systems with
respect to external disturbances, focussing on their scalability properties.
Specifically, a notion of scalability is introduced that asks for these
robustness properties to remain unchanged under a structural change of the
system, such as the addition/removal of a subsystem or a change in the
interconnection structure. Both necessary and sufficient conditions, in terms
of the interconnection structure and edge weights, are given under which
elementary structural changes are scalable. The results are illustrated through
a simple example.",http://arxiv.org/abs/2005.06009v1
"Change-point tests for the tail parameter of Long Memory Stochastic
  Volatility time series",2020-06-04T06:49:20Z,"Annika Betken, Davide Giraudo, Rafa≈Ç Kulik","We consider a change-point test based on the Hill estimator to test for
structural changes in the tail index of Long Memory Stochastic Volatility time
series. In order to determine the asymptotic distribution of the corresponding
test statistic, we prove a uniform reduction principle for the tail empirical
process in a two-parameter Skorohod space. It is shown that such a process
displays a dichotomous behavior according to an interplay between the Hurst
parameter, i.e., a parameter characterizing the dependence in the data, and the
tail index. Our theoretical results are accompanied by simulation studies and
the analysis of financial time series with regard to structural changes in the
tail index.",http://arxiv.org/abs/2006.02667v1
Phase Change Material Photonics,2020-06-25T14:21:46Z,"Robert E. Simpson, Tun Cao","In the last decade phase change materials (PCM) research has switched from
practical application in optical data storage toward electrical phase change
random access memory technologies (PCRAM). As these devices are commercialised,
we expect the research direction to switch once again toward
electrical-photonic devices. The objective of this review is to introduce the
concepts in PCM-tuned photonics. We will start by highlighting the key works in
the field, before concentrating on PCM-tuned Metal-Dielectric-Metal (MDM)
structures. We will discuss how to design tuneable-MDM photonics devices, their
advantages, and their limitations. Finally we will discuss new materials for
phase change photonics.",http://arxiv.org/abs/2006.14441v1
"Anomaly of the dielectric function of water under confinement and its
  role in Van der Waals interactions",2020-07-15T03:24:21Z,R. Esquivel-Sirvent,"We present a theoretical calculation of the changes in the Hamaker constant
due to the anomalous reduction of the static dielectric function of water.
Under confinement, the dielectric function of water decreases from a bulk value
of 79 down to 2. If the confining walls are made of a dielectric material, the
Hamaker constant reduces almost by 90\%. However, if the confinement is
realized with metallic plates, there is little change in the Hamaker constant.
Additionally, we show that confinement can be used to decreases the Debye
screening length without changing the salt concentration. This, in turn, is
used to change the Hamaker constant in the presence of electrolytes.",http://arxiv.org/abs/2007.07460v1
SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection,2020-07-22T14:37:42Z,"Dominik Schlechtweg, Barbara McGillivray, Simon Hengchen, Haim Dubossarsky, Nina Tahmasebi","Lexical Semantic Change detection, i.e., the task of identifying words that
change meaning over time, is a very active research area, with applications in
NLP, lexicography, and linguistics. Evaluation is currently the most pressing
problem in Lexical Semantic Change detection, as no gold standards are
available to the community, which hinders progress. We present the results of
the first shared task that addresses this gap by providing researchers with an
evaluation framework and manually annotated, high-quality datasets for English,
German, Latin, and Swedish. 33 teams submitted 186 systems, which were
evaluated on two subtasks.",http://arxiv.org/abs/2007.11464v2
Speculative Interference Attacks: Breaking Invisible Speculation Schemes,2020-07-23T06:36:38Z,"Mohammad Behnia, Prateek Sahu, Riccardo Paccagnella, Jiyong Yu, Zirui Zhao, Xiang Zou, Thomas Unterluggauer, Josep Torrellas, Carlos Rozas, Adam Morrison, Frank Mckeen, Fangfei Liu, Ron Gabor, Christopher W. Fletcher, Abhishek Basak, Alaa Alameldeen","Recent security vulnerabilities that target speculative execution (e.g.,
Spectre) present a significant challenge for processor design. The highly
publicized vulnerability uses speculative execution to learn victim secrets by
changing cache state. As a result, recent computer architecture research has
focused on invisible speculation mechanisms that attempt to block changes in
cache state due to speculative execution. Prior work has shown significant
success in preventing Spectre and other vulnerabilities at modest performance
costs. In this paper, we introduce speculative interference attacks, which show
that prior invisible speculation mechanisms do not fully block these
speculation-based attacks. We make two key observations. First, misspeculated
younger instructions can change the timing of older, bound-to-retire
instructions, including memory operations. Second, changing the timing of a
memory operation can change the order of that memory operation relative to
other memory operations, resulting in persistent changes to the cache state.
Using these observations, we demonstrate (among other attack variants) that
secret information accessed by mis-speculated instructions can change the order
of bound-to-retire loads. Load timing changes can therefore leave
secret-dependent changes in the cache, even in the presence of invisible
speculation mechanisms. We show that this problem is not easy to fix:
Speculative interference converts timing changes to persistent cache-state
changes, and timing is typically ignored by many cache-based defenses. We
develop a framework to understand the attack and demonstrate concrete
proof-of-concept attacks against invisible speculation mechanisms. We provide
security definitions sufficient to block speculative interference attacks;
describe a simple defense mechanism with a high performance cost; and discuss
how future research can improve its performance.",http://arxiv.org/abs/2007.11818v4
On Base Change of Local Stability in Positive Characteristics,2020-01-13T06:56:11Z,"Zhi Hu, Runhong Zong","We prove that a pointed one dimensional family of varieties $\mathcal{X}\to
{b\in B}$ in positive characteristics is locally stable iff the log pair
$(\mathcal{X'}, \mathcal{X}'_{b'})$ arising from its base change to the
perfectoid base $b'\in B_{perf}$ is log canonical.",http://arxiv.org/abs/2001.04083v1
"Tracking the state transitions in changing-look active galactic nuclei
  through their polarized-light echoes",2020-03-19T09:36:49Z,"F. Marin, D. Hutsem√©kers","Context: Variations in the mass accretion rate appear to be responsible for
the rapid transitions in spectral type that are observed in increasingly more
active galactic nuclei (AGNs). These objects are now labeled ""changing-look""
AGNs and are key objects for understanding the physics of accretion onto
supermassive black holes. Aims: We aim to complement the analysis and
interpretation of changing-look AGNs by modeling the polarization variations
that can be observed, in particular, polarized-light echoes. Methods: We built
a complex and representative model of an AGN and its host galaxy and ran
radiative transfer simulations to obtain realistic time-dependent polarization
signatures of changing-look objects. Based on actual data, we allowed the
system to become several times fainter or brighter within a few years, assuming
a rapid change in accretion rate. Results: We obtain time-dependent
polarization signatures of distant high-luminosity (quasars) and nearby
low-luminosity (Seyferts) changing-look AGNs for a representative set of
inclinations. We predict the evolution of the continuum polarization for future
polarimetric campaigns with the goal to better understand the physics at work
in these objects. We also investigate highly inclined AGNs that experience
strong accretion rate variations without appearing to change state. We apply
our modeling to Mrk 1018, the best-documented case of a changing-look AGN, and
predict a variation in its polarization after the recent dimming of its
continuum.",http://arxiv.org/abs/2003.08641v1
"Change Detection in Heterogeneous Optical and SAR Remote Sensing Images
  via Deep Homogeneous Feature Fusion",2020-04-08T06:27:37Z,"Xiao Jiang, Gang Li, Yu Liu, Xiao-Ping Zhang, You He","Change detection in heterogeneous remote sensing images is crucial for
disaster damage assessment. Recent methods use homogenous transformation, which
transforms the heterogeneous optical and SAR remote sensing images into the
same feature space, to achieve change detection. Such transformations mainly
operate on the low-level feature space and may corrupt the semantic content,
deteriorating the performance of change detection. To solve this problem, this
paper presents a new homogeneous transformation model termed deep homogeneous
feature fusion (DHFF) based on image style transfer (IST). Unlike the existing
methods, the DHFF method segregates the semantic content and the style features
in the heterogeneous images to perform homogeneous transformation. The
separation of the semantic content and the style in homogeneous transformation
prevents the corruption of image semantic content, especially in the regions of
change. In this way, the detection performance is improved with accurate
homogeneous transformation. Furthermore, we present a new iterative IST (IIST)
strategy, where the cost function in each IST iteration measures and thus
maximizes the feature homogeneity in additional new feature subspaces for
change detection. After that, change detection is accomplished accurately on
the original and the transformed images that are in the same feature space.
Real remote sensing images acquired by SAR and optical satellites are utilized
to evaluate the performance of the proposed method. The experiments demonstrate
that the proposed DHFF method achieves significant improvement for change
detection in heterogeneous optical and SAR remote sensing images, in terms of
both accuracy rate and Kappa index.",http://arxiv.org/abs/2004.03830v1
Rank-based change-point analysis for long-range dependent time series,2020-04-14T15:00:24Z,"Annika Betken, Martin Wendler","We consider change-point tests based on rank statistics to test for
structural changes in long-range dependent observations. Under the hypothesis
of stationary time series and under the assumption of a change with decreasing
change-point height, the asymptotic distributions of corresponding test
statistics are derived. For this, a uniform reduction principle for the
sequential empirical process in a two-parameter Skorohod space equipped with a
weighted supremum norm is proved. Moreover, we compare the efficiency of rank
tests resulting from the consideration of different score functions. Under
Gaussianity, the asymptotic relative efficiency of rank-based tests with
respect to the CuSum test is 1, irrespective of the score function. Regarding
the practical implementation of rank-based change-point tests, we suggest to
combine self-normalized rank statistics with subsampling. The theoretical
results are accompanied by simulation studies that, in particular, allow for a
comparison of rank tests resulting from different score functions. With respect
to the finite sample performance of rank-based change-point tests, the Van der
Waerden rank test proves to be favorable in a broad range of situations.
Finally, we analyze data sets from economy, hydrology, and network traffic
monitoring in view of structural changes and compare our results to previous
analysis of the data.",http://arxiv.org/abs/2004.06574v2
"Unsupervised Change Detection in Satellite Images with Generative
  Adversarial Network",2020-09-08T10:26:04Z,"Caijun Ren, Xiangyu Wang, Jian Gao, Huanhuan Chen","Detecting changed regions in paired satellite images plays a key role in many
remote sensing applications. The evolution of recent techniques could provide
satellite images with very high spatial resolution (VHR) but made it
challenging to apply image coregistration, and many change detection methods
are dependent on its accuracy.Two images of the same scene taken at different
time or from different angle would introduce unregistered objects and the
existence of both unregistered areas and actual changed areas would lower the
performance of many change detection algorithms in unsupervised condition.To
alleviate the effect of unregistered objects in the paired images, we propose a
novel change detection framework utilizing a special neural network
architecture -- Generative Adversarial Network (GAN) to generate many better
coregistered images. In this paper, we show that GAN model can be trained upon
a pair of images through using the proposed expanding strategy to create a
training set and optimizing designed objective functions. The optimized GAN
model would produce better coregistered images where changes can be easily
spotted and then the change map can be presented through a comparison strategy
using these generated images explicitly.Compared to other deep learning-based
methods, our method is less sensitive to the problem of unregistered images and
makes most of the deep learning structure.Experimental results on synthetic
images and real data with many different scenes could demonstrate the
effectiveness of the proposed approach.",http://arxiv.org/abs/2009.03630v2
"Publication Patterns' Changes due to the COVID-19 Pandemic: A
  longitudinal and short-term scientometric analysis",2020-10-06T10:09:41Z,"Shir Aviv-Reuven, Ariel Rosenfeld","In recent months the COVID-19 (also known as SARS-CoV-2 and Coronavirus)
pandemic has spread throughout the world. In parallel, extensive scholarly
research regarding various aspects of the pandemic has been published. In this
work, we analyse the changes in biomedical publishing patterns due to the
pandemic. We study the changes in the volume of publications in both peer
reviewed journals and preprint servers, average time to acceptance of papers
submitted to biomedical journals, international (co-)authorship of these papers
(expressed by diversity and volume), and the possible association between
journal metrics and said changes. We study these possible changes using two
approaches: a short-term analysis through which changes during the first six
months of the outbreak are examined for both COVID-19 related papers and
non-COVID-19 related papers; and a longitudinal approach through which changes
are examined in comparison to the previous four years. Our results show that
the pandemic has so far had a tremendous effect on all examined accounts of
scholarly publications: A sharp increase in publication volume has been
witnessed and it can be almost entirely attributed to the pandemic; a
significantly faster mean time to acceptance for COVID-19 papers is apparent,
and it has (partially) come at the expense of non-COVID-19 papers; and a
significant reduction in international collaboration for COVID-19 papers has
also been identified. As the pandemic continues to spread, these changes may
cause a slow down in research in non-COVID-19 biomedical fields and bring about
a lower rate of international collaboration.",http://arxiv.org/abs/2010.02594v2
Octagonal continued fraction and diagonal changes,2020-10-09T16:39:24Z,Mauro Artigiani,"In this short note we show that the octagon Farey map introduced by Smillie
and Ulcigrai is an acceleration of the diagonal changes algorithm introduced by
Delecroix and Ulcigrai.",http://arxiv.org/abs/2010.04670v1
"Hierarchical Paired Channel Fusion Network for Street Scene Change
  Detection",2020-10-19T23:51:28Z,"Yinjie Lei, Duo Peng, Pingping Zhang, Qiuhong Ke, Haifeng Li","Street Scene Change Detection (SSCD) aims to locate the changed regions
between a given street-view image pair captured at different times, which is an
important yet challenging task in the computer vision community. The intuitive
way to solve the SSCD task is to fuse the extracted image feature pairs, and
then directly measure the dissimilarity parts for producing a change map.
Therefore, the key for the SSCD task is to design an effective feature fusion
method that can improve the accuracy of the corresponding change maps. To this
end, we present a novel Hierarchical Paired Channel Fusion Network (HPCFNet),
which utilizes the adaptive fusion of paired feature channels. Specifically,
the features of a given image pair are jointly extracted by a Siamese
Convolutional Neural Network (SCNN) and hierarchically combined by exploring
the fusion of channel pairs at multiple feature levels. In addition, based on
the observation that the distribution of scene changes is diverse, we further
propose a Multi-Part Feature Learning (MPFL) strategy to detect diverse
changes. Based on the MPFL strategy, our framework achieves a novel approach to
adapt to the scale and location diversities of the scene change regions.
Extensive experiments on three public datasets (i.e., PCD, VL-CMU-CD and
CDnet2014) demonstrate that the proposed framework achieves superior
performance which outperforms other state-of-the-art methods with a
considerable margin.",http://arxiv.org/abs/2010.09925v1
"Characterizing Service Provider Response to the COVID-19 Pandemic in the
  United States",2020-11-01T04:22:23Z,"Shinan Liu, Paul Schmitt, Francesco Bronzino, Nick Feamster","The COVID-19 pandemic has resulted in dramatic changes to the daily habits of
billions of people. Users increasingly have to rely on home broadband Internet
access for work, education, and other activities. These changes have resulted
in corresponding changes to Internet traffic patterns. This paper aims to
characterize the effects of these changes with respect to Internet service
providers in the United States. We study three questions: (1)How did traffic
demands change in the United States as a result of the COVID-19 pandemic?;
(2)What effects have these changes had on Internet performance?; (3)How did
service providers respond to these changes? We study these questions using data
from a diverse collection of sources. Our analysis of interconnection data for
two large ISPs in the United States shows a 30-60% increase in peak traffic
rates in the first quarter of 2020. In particular, we observe traffic
downstream peak volumes for a major ISP increase of 13-20% while upstream peaks
increased by more than 30%. Further, we observe significant variation in
performance across ISPs in conjunction with the traffic volume shifts, with
evident latency increases after stay-at-home orders were issued, followed by a
stabilization of traffic after April. Finally, we observe that in response to
changes in usage, ISPs have aggressively augmented capacity at interconnects,
at more than twice the rate of normal capacity augmentation. Similarly, video
conferencing applications have increased their network footprint, more than
doubling their advertised IP address space.",http://arxiv.org/abs/2011.00419v1
Tracking change-points in multivariate extremes,2020-11-10T12:17:38Z,"Miguel de Carvalho, Manuele Leonelli, Alex Rossi","In this paper we devise a statistical method for tracking and modeling
change-points on the dependence structure of multivariate extremes. The methods
are motivated by and illustrated on a case study on crypto-assets.",http://arxiv.org/abs/2011.05067v1
"Seeded Binary Segmentation: A general methodology for fast and optimal
  change point detection",2020-02-16T18:08:10Z,"Solt Kov√°cs, Housen Li, Peter B√ºhlmann, Axel Munk","In recent years, there has been an increasing demand on efficient algorithms
for large scale change point detection problems. To this end, we propose seeded
binary segmentation, an approach relying on a deterministic construction of
background intervals, called seeded intervals, in which single change points
are searched. The final selection of change points based on the candidates from
seeded intervals can be done in various ways, adapted to the problem at hand.
Thus, seeded binary segmentation is easy to adapt to a wide range of change
point detection problems, let that be univariate, multivariate or even
high-dimensional.
  We consider the univariate Gaussian change in mean setup in detail. For this
specific case we show that seeded binary segmentation leads to a near-linear
time approach (i.e. linear up to a logarithmic factor) independent of the
underlying number of change points. Furthermore, using appropriate selection
methods, the methodology is shown to be asymptotically minimax optimal. While
computationally more efficient, the finite sample estimation performance
remains competitive compared to state of the art procedures. Moreover, we
illustrate the methodology for high-dimensional settings with an inverse
covariance change point detection problem where our proposal leads to massive
computational gains while still exhibiting good statistical performance.",http://arxiv.org/abs/2002.06633v1
"The semi-global isometric embedding of surfaces with curvature changing
  signs stably",2020-06-08T12:26:57Z,Wentao Cao,"A semi-global isometric embedding of abstract surfaces with Gaussian
curvature changing signs of any finite order is obtained through solving the
Darboux equation.",http://arxiv.org/abs/2006.04507v1
"Changes, States, and Events: The Thread from Staticity to Dynamism in
  the Conceptual Modeling of Systems",2020-08-11T22:08:27Z,Sabah Al-Fedaghi,"This paper examines the concept of change in conceptual modeling. Change is
inherent in the nature of things and has increasingly become a focus of much
interest and investigation. Change can be modeled as a transition between two
states of a finite state machine (FSM). This change represents an exploratory
starting point in this paper. Accordingly, a sample FSM that models a car s
transmission system is re-expressed in terms of a new modeling methodology
called thinging machine (TM) modeling. Recasting the car-transmission model
involves developing (1) an S model that captures the static aspects, (2) a D
model that identifies states, and (3) a B model that specifies the behavior.
The analysis progresses as follows. - S represents an atemporal diagrammatic
description that embeds underlying compositions (static changes) from which the
roots of system behavior can be traced. - S is broken down into multiple
subsystems that correspond to static states (ordered constitutive components).
- Introducing time into static states converts these states into events, and
the behavior (B) model is constructed based on the chronology of these events.
The analysis shows that FSM states are static (atemporal) changes that
introduce temporal events as carriers of behavior. This result enhances the
semantics of the concepts of change, states, and events in modeling and shows
how to specify a system s behavior through its static description.",http://arxiv.org/abs/2008.05017v1
"Change Point Detection in Time Series Data using Autoencoders with a
  Time-Invariant Representation",2020-08-21T15:03:21Z,"Tim De Ryck, Maarten De Vos, Alexander Bertrand","Change point detection (CPD) aims to locate abrupt property changes in time
series data. Recent CPD methods demonstrated the potential of using deep
learning techniques, but often lack the ability to identify more subtle changes
in the autocorrelation statistics of the signal and suffer from a high false
alarm rate. To address these issues, we employ an autoencoder-based methodology
with a novel loss function, through which the used autoencoders learn a
partially time-invariant representation that is tailored for CPD. The result is
a flexible method that allows the user to indicate whether change points should
be sought in the time domain, frequency domain or both. Detectable change
points include abrupt changes in the slope, mean, variance, autocorrelation
function and frequency spectrum. We demonstrate that our proposed method is
consistently highly competitive or superior to baseline methods on diverse
simulated and real-life benchmark data sets. Finally, we mitigate the issue of
false detection alarms through the use of a postprocessing procedure that
combines a matched filter and a newly proposed change point score. We show that
this combination drastically improves the performance of our method as well as
all baseline methods.",http://arxiv.org/abs/2008.09524v2
"Ensemble Binary Segmentation for irregularly spaced data with
  change-points",2020-03-07T19:56:32Z,Karolos K. Korkas,"We propose a new technique for consistent estimation of the number and
locations of the change-points in the structure of an irregularly spaced time
series. The core of the segmentation procedure is the Ensemble Binary
Segmentation method (EBS), a technique in which a large number of multiple
change-point detection tasks using the Binary Segmentation (BS) method are
applied on sub-samples of the data of differing lengths, and then the results
are combined to create an overall answer. We do not restrict the total number
of change-points a time series can have, therefore, our proposed method works
well when the spacings between change-points are short. Our main change-point
detection statistic is the time-varying Autoregressive Conditional Duration
model on which we apply a transformation process in order to decorrelate it. To
examine the performance of EBS we provide a simulation study for various types
of scenarios. A proof of consistency is also provided. Our methodology is
implemented in the R package eNchange, available to download from CRAN.",http://arxiv.org/abs/2003.03649v3
"Asymptotic delay times of sequential tests based on U-statistics for
  early and late change points",2020-03-19T14:57:14Z,"Claudia Kirch, Christina Stoehr","Sequential change point tests aim at giving an alarm as soon as possible
after a structural break occurs while controlling the asymptotic false alarm
error. For such tests it is of particular importance to understand how quickly
a break is detected. While this is often assessed by simulations only, in this
paper, we derive the asymptotic distribution of the delay time for sequential
change point procedures based on U-statistics. This includes the
difference-of-means (DOM) sequential test, that has been discussed previously,
but also a new robust Wilcoxon sequential change point test. Similar to
asymptotic relative efficiency in an a-posteriori setting, the results allow us
to compare the detection delay of the two procedures. It is shown that the
Wilcoxon sequential procedure has a smaller detection delay for heavier tailed
distributions which is also confirmed by simulations. While the previous
literature only derives results for early change points, we obtain the
asymptotic distribution of the delay time for both early as well as late change
points. Finally, we evaluate how well the asymptotic distribution approximates
the actual stopping times for finite samples via a simulation study.",http://arxiv.org/abs/2003.08847v1
"A novel change point approach for the detection of gas emission sources
  using remotely contained concentration data",2020-04-06T14:17:53Z,"Idris Eckley, Claudia Kirch, Silke Weber","Motivated by an example from remote sensing of gas emission sources, we
derive two novel change point procedures for multivariate time series where, in
contrast to classical change point literature, the changes are not required to
be aligned in the different components of the time series. Instead the change
points are described by a functional relationship where the precise shape
depends on unknown parameters of interest such as the source of the gas
emission in the above example. Two different types of tests and the
corresponding estimators for the unknown parameters describing the change
locations are proposed. We derive the null asymptotics for both tests under
weak assumptions on the error time series and show asymptotic consistency under
alternatives. Furthermore, we prove consistency for the corresponding
estimators of the parameters of interest. The small sample behavior of the
methodology is assessed by means of a simulation study and the above remote
sensing example analyzed in detail.",http://arxiv.org/abs/2004.02692v1
